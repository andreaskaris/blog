
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-7.0.6">
    
    
      
        <title>CPU manager with custom MachineConfigPool - Andreas Karis Blog</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.2c0c5eaf.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.7fa14f5b.min.css">
        
          
          
          <meta name="theme-color" content="#546d78">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
    
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="blue-grey" data-md-color-accent="red">
      
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#cpu-manager-with-custom-machineconfigpool-in-ocp-4x" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Andreas Karis Blog" class="md-header__button md-logo" aria-label="Andreas Karis Blog" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Andreas Karis Blog
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              CPU manager with custom MachineConfigPool
            
          </span>
        </div>
      </div>
    </div>
    <div class="md-header__options">
      
    </div>
    
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Andreas Karis Blog" class="md-nav__button md-logo" aria-label="Andreas Karis Blog" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    Andreas Karis Blog
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      <label class="md-nav__link" for="__nav_2">
        Ceph
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Ceph" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Ceph
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../ceph/ceph-manual-test/" class="md-nav__link">
        Ceph manual test with qemu
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      <label class="md-nav__link" for="__nav_3">
        Containers
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Containers" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Containers
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../linux/cgroups/" class="md-nav__link">
        cgroups
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../linux/containers/" class="md-nav__link">
        Containers in Linux
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../list_docker_registry_containers/" class="md-nav__link">
        List container images in registry
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../linux/namespaces/" class="md-nav__link">
        namespaces
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      <label class="md-nav__link" for="__nav_4">
        Linux
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Linux" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Linux
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../linux/cgroups/" class="md-nav__link">
        cgroups
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../linux/containers/" class="md-nav__link">
        Containers in Linux
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../linux/java-idrac-issues/" class="md-nav__link">
        Java Idrac Issues
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../linux/ipxe-boot-environment/" class="md-nav__link">
        iPXE boot environment on Fedora
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../linux/hugepages/" class="md-nav__link">
        Hugepages
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../linux/meson/" class="md-nav__link">
        meson
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../linux/namespaces/" class="md-nav__link">
        namespaces
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../linux/old_java_version_with_xorgs_in_container/" class="md-nav__link">
        Old Java inside a container with xorgs
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../linux/libvirt-uefi-without-secureboot/" class="md-nav__link">
        RHEL Booting a virtual machine with UEFI but without secure boot
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../linux/setting-journalctl-limits/" class="md-nav__link">
        Setting journalctl limits
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      <label class="md-nav__link" for="__nav_5">
        Networking
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Networking" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Networking
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../networking/arp_and_the_neighbor_table/" class="md-nav__link">
        ARP and the Neighbor table
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../networking/bpf-and-tcpdump/" class="md-nav__link">
        BPF and tcpdump
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../networking/bonding_in_linux/" class="md-nav__link">
        Bonding in Linux
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../networking/juniper_x520/" class="md-nav__link">
        Configure Juniper switch
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../networking/haproxy-and-h2c/" class="md-nav__link">
        haproxy and HTTP/2
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../networking/geneve_tunneling/" class="md-nav__link">
        Geneve tunneling
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../networking/netlink/" class="md-nav__link">
        Netlink
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../networking/ovn-kind/" class="md-nav__link">
        OVN kind
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../networking/ovn_standalone_on_fedora31/" class="md-nav__link">
        OVN standalone on Fedora 31
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../networking/ovs_recirculation/" class="md-nav__link">
        OVS packet recirculation
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../networking/ovs-vxlan-tunnels-and-dscp/" class="md-nav__link">
        OVS VXLAN tunnels and DSCP
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../networking/ovs_with_gdb/" class="md-nav__link">
        OVS with GDB
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../networking/sctp/" class="md-nav__link">
        SCTP
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../networking/wireguard/" class="md-nav__link">
        Wireguard
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6" type="checkbox" id="__nav_6" checked>
      
      <label class="md-nav__link" for="__nav_6">
        OpenShift
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="OpenShift" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          OpenShift
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../alertmanager/" class="md-nav__link">
        AlertManager
      </a>
    </li>
  

          
            
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          CPU manager with custom MachineConfigPool
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        CPU manager with custom MachineConfigPool
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#how-to-apply-the-cpu-manager-to-only-a-subset-of-worker-nodes" class="md-nav__link">
    How to apply the CPU manager to only a subset of worker nodes
  </a>
  
    <nav class="md-nav" aria-label="How to apply the CPU manager to only a subset of worker nodes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#exploring-resource-limits" class="md-nav__link">
    Exploring resource limits
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#applying-custom-limits" class="md-nav__link">
    Applying custom limits
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reserving-0-cpus-for-the-host" class="md-nav__link">
    Reserving 0 CPUs for the host
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    References
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../crio-conmon-runc/" class="md-nav__link">
        Crio vs conmon vs runc
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../etcd_perf/" class="md-nav__link">
        Etcd Performance tests
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../get-vs-list-api-calls/" class="md-nav__link">
        Get vs List API Calls
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../openshift_httpbin_tshark_sidecar/" class="md-nav__link">
        Httpbin tshark sidecar container
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../kubernetes_cluster/" class="md-nav__link">
        Hints for installing kubernetes on Fedora
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../HPA/" class="md-nav__link">
        Horizontal Pod Autoscaler
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../how_rhcos_updates_work/" class="md-nav__link">
        How RHCOS updates work
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../ocp4-infra-nodes-with-machineset-without-worker-label/" class="md-nav__link">
        Infra nodes with MachineSets without worker label
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../ingresscontroller_router_sharding_ocp_on_osp/" class="md-nav__link">
        Ingress Controller Sharding OCP on OSP
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../ingress-controller-sharding-on-separate-vip/" class="md-nav__link">
        Ingress Controller Sharding on separate VIP
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../openshift_mirror_registry/" class="md-nav__link">
        Installing a cluster with a mirror registry
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../istio-1.6-on-ocp.4.x/" class="md-nav__link">
        Istio 1.6 on OpenShift 4.x
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../kata/" class="md-nav__link">
        kata containers and the kata operatora
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../kind-with-private-registry/" class="md-nav__link">
        Kind with private registry
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../mounting-container-image/" class="md-nav__link">
        Mount a container image
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../openstack/install_openshift_on_openstack/" class="md-nav__link">
        OpenShift on OpenStack
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../private-registry/" class="md-nav__link">
        Private container registry
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../proxy-ocp-4.5/" class="md-nav__link">
        Proxy OCP 4.5
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../scc/" class="md-nav__link">
        Security Context Constraints (SCC)
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../fix-selinux-labels-coreos/" class="md-nav__link">
        SElinux labels fix
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../networking/sctp/" class="md-nav__link">
        SCTP
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../openshift_troubleshooting_etcd_state/" class="md-nav__link">
        Troubleshooting etcd state
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../useful-ocp-commands/" class="md-nav__link">
        Useful commands for OpenShift
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../troubleshooting_openshift_on_openstack_worker_creation/" class="md-nav__link">
        Troubleshooting OpenShift on OpenStack worker creation
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7" type="checkbox" id="__nav_7" >
      
      <label class="md-nav__link" for="__nav_7">
        OpenStack
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="OpenStack" data-md-level="1">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          OpenStack
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../openstack/reattach_to_running_deployment/" class="md-nav__link">
        Reattach to running deployment
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../openstack/using_clouds_yaml/" class="md-nav__link">
        Using clouds.yaml
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8" type="checkbox" id="__nav_8" >
      
      <label class="md-nav__link" for="__nav_8">
        OperatorSDK
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="OperatorSDK" data-md-level="1">
        <label class="md-nav__title" for="__nav_8">
          <span class="md-nav__icon md-icon"></span>
          OperatorSDK
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../operator-sdk/operator-sdk-reconciliation/" class="md-nav__link">
        Controller Reconciliation
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="cpu-manager-with-custom-machineconfigpool-in-ocp-4x">CPU manager with custom MachineConfigPool in OCP 4.x</h1>
<h2 id="how-to-apply-the-cpu-manager-to-only-a-subset-of-worker-nodes">How to apply the CPU manager to only a subset of worker nodes</h2>
<p>Create a custom MachineConfigPool named <code>worker-cpu-manager</code>.</p>
<p><code>worker-cpu-manager.yaml</code>:</p>
<pre><code>apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: worker-cpu-manager
  labels:
    custom-kubelet: cpumanager-enabled
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,worker-cpu-manager]}
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/worker-cpu-manager: &quot;&quot;
  paused: false
</code></pre>
<p>Apply the pool:</p>
<pre><code>oc apply -f worker-cpu-manager.yaml
</code></pre>
<p>Create the <code>cpumanager-kubelet.yaml</code>:</p>
<pre><code>apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: cpumanager-enabled
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: cpumanager-enabled
  kubeletConfig:
     cpuManagerPolicy: static
     cpuManagerReconcilePeriod: 5s
</code></pre>
<pre><code>oc apply -f cpumanager-kubelet.yaml
</code></pre>
<p>Change worker node to <code>worker-cpu-manager</code> role:</p>
<pre><code>oc label node &lt;node&gt; node-role.kubernetes.io/worker-cpu-manager=
</code></pre>
<p>Verify:</p>
<pre><code># oc get nodes
NAME                             STATUS   ROLES                       AGE     VERSION
openshift-master-0.example.com   Ready    master                      4h3m    v1.17.1+1aa1c48
openshift-master-1.example.com   Ready    master                      4h2m    v1.17.1+1aa1c48
openshift-master-2.example.com   Ready    master                      3h56m   v1.17.1+1aa1c48
openshift-worker-0.example.com   Ready    worker                      18m     v1.17.1+1aa1c48
openshift-worker-1.example.com   Ready    worker,worker-cpu-manager   18m     v1.17.1+1aa1c48
</code></pre>
<pre><code>oc get machineconfig
(...)
rendered-worker-cpu-manager-bc48d7bf24df726f468b357482032845         8af4f709c4ba9c0afff3408ecc99c8fce61dd314   2.2.0             87s
rendered-worker-cpu-manager-ca0a09ddea41402490e1c39a138cd44e         8af4f709c4ba9c0afff3408ecc99c8fce61dd314   2.2.0             18m
</code></pre>
<pre><code>[root@openshift-jumpserver-0 cpuman]# diff &lt;(oc get machineconfig rendered-worker-cpu-manager-bc48d7bf24df726f468b357482032845 -o yaml)  &lt;(oc get machineconfig rendered-worker-cpu-manager-ca0a09ddea41402490e1c39a138cd44e -o yaml)
6c6
&lt;   creationTimestamp: &quot;2020-06-26T16:40:44Z&quot;
---
&gt;   creationTimestamp: &quot;2020-06-26T16:24:01Z&quot;
8c8
&lt;   name: rendered-worker-cpu-manager-bc48d7bf24df726f468b357482032845
---
&gt;   name: rendered-worker-cpu-manager-ca0a09ddea41402490e1c39a138cd44e
16,18c16,18
&lt;   resourceVersion: &quot;451269&quot;
&lt;   selfLink: /apis/machineconfiguration.openshift.io/v1/machineconfigs/rendered-worker-cpu-manager-bc48d7bf24df726f468b357482032845
&lt;   uid: bf950ef8-7270-4699-a113-dfe6c6c1d4fb
---
&gt;   resourceVersion: &quot;445466&quot;
&gt;   selfLink: /apis/machineconfiguration.openshift.io/v1/machineconfigs/rendered-worker-cpu-manager-ca0a09ddea41402490e1c39a138cd44e
&gt;   uid: 74acdcd1-d195-48a5-8689-cecca137605c
163,168d162
&lt;           verification: {}
&lt;         filesystem: root
&lt;         mode: 420
&lt;         path: /etc/kubernetes/kubelet.conf
&lt;       - contents:
&lt;           source: data:text/plain,%7B%odcy%2(...)
</code></pre>
<p>Compare unmodified node to node with CPU manager:</p>
<pre><code>[root@openshift-jumpserver-0 cpuman]# oc debug node/openshift-worker-0.example.com
Starting pod/openshift-worker-0examplecom-debug ...
To use host binaries, run `chroot /host`
Pod IP: 192.168.123.215
If you don't see a command prompt, try pressing enter.
sh-4.2# cat /host/etc/kubernetes/kubelet.conf | grep cpuManager
sh-4.2# exit
exit

Removing debug pod ...
</code></pre>
<pre><code>[root@openshift-jumpserver-0 cpuman]# oc debug node/openshift-worker-1.example.com
Starting pod/openshift-worker-1examplecom-debug ...
To use host binaries, run `chroot /host`
Pod IP: 192.168.123.204
If you don't see a command prompt, try pressing enter.
sh-4.2# cat /host/etc/kubernetes/kubelet.conf | grep cpuManager
{&quot;kind&quot;:&quot;KubeletConfiguration&quot;,&quot;apiVersion&quot;:&quot;kubelet.config.k8s.io/v1beta1&quot;,&quot;staticPodPath&quot;:&quot;/etc/kubernetes/manifests&quot;,&quot;syncFrequency&quot;:&quot;0s&quot;,&quot;fileCheckFrequency&quot;:&quot;0s&quot;,&quot;httpCheckFrequency&quot;:&quot;0s&quot;,&quot;rotateCertificates&quot;:true,&quot;serverTLSBootstrap&quot;:true,&quot;authentication&quot;:{&quot;x509&quot;:{&quot;clientCAFile&quot;:&quot;/etc/kubernetes/kubelet-ca.crt&quot;},&quot;webhook&quot;:{&quot;cacheTTL&quot;:&quot;0s&quot;},&quot;anonymous&quot;:{&quot;enabled&quot;:false}},&quot;authorization&quot;:{&quot;webhook&quot;:{&quot;cacheAuthorizedTTL&quot;:&quot;0s&quot;,&quot;cacheUnauthorizedTTL&quot;:&quot;0s&quot;}},&quot;clusterDomain&quot;:&quot;cluster.local&quot;,&quot;clusterDNS&quot;:[&quot;172.30.0.10&quot;],&quot;streamingConnectionIdleTimeout&quot;:&quot;0s&quot;,&quot;nodeStatusUpdateFrequency&quot;:&quot;0s&quot;,&quot;nodeStatusReportFrequency&quot;:&quot;0s&quot;,&quot;imageMinimumGCAge&quot;:&quot;0s&quot;,&quot;volumeStatsAggPeriod&quot;:&quot;0s&quot;,&quot;systemCgroups&quot;:&quot;/system.slice&quot;,&quot;cgroupRoot&quot;:&quot;/&quot;,&quot;cgroupDriver&quot;:&quot;systemd&quot;,&quot;cpuManagerPolicy&quot;:&quot;static&quot;,&quot;cpuManagerReconcilePeriod&quot;:&quot;5s&quot;,&quot;runtimeRequestTimeout&quot;:&quot;0s&quot;,&quot;maxPods&quot;:250,&quot;kubeAPIQPS&quot;:50,&quot;kubeAPIBurst&quot;:100,&quot;serializeImagePulls&quot;:false,&quot;evictionPressureTransitionPeriod&quot;:&quot;0s&quot;,&quot;featureGates&quot;:{&quot;LegacyNodeRoleBehavior&quot;:false,&quot;NodeDisruptionExclusion&quot;:true,&quot;RotateKubeletServerCertificate&quot;:true,&quot;SCTPSupport&quot;:true,&quot;ServiceNodeExclusion&quot;:true,&quot;SupportPodPidsLimit&quot;:true},&quot;containerLogMaxSize&quot;:&quot;50Mi&quot;,&quot;systemReserved&quot;:{&quot;cpu&quot;:&quot;500m&quot;,&quot;ephemeral-storage&quot;:&quot;1Gi&quot;,&quot;memory&quot;:&quot;1Gi&quot;}}
sh-4.2# exit
exit

Removing debug pod ...
</code></pre>
<p>Spawn 2 pods on the same worker, one with and one without CPU manager enabled:</p>
<pre><code>[root@openshift-jumpserver-0 cpuman]# cat cpumanager-pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: cpumanager
spec:
  containers:
  - name: cpumanager
    image: gcr.io/google_containers/pause-amd64:3.0
    resources:
      requests:
        cpu: 1
        memory: &quot;1G&quot;
      limits:
        cpu: 1
        memory: &quot;1G&quot;
  nodeSelector:
    cpumanager: &quot;true&quot;
[root@openshift-jumpserver-0 cpuman]# cat non-cpumanager-pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: noncpumanager
spec:
  containers:
  - name: cpumanager
    image: gcr.io/google_containers/pause-amd64:3.0
[root@openshift-jumpserver-0 cpuman]# oc get pods -o wide
NAME            READY   STATUS    RESTARTS   AGE     IP           NODE                             NOMINATED NODE   READINESS GATES
cpumanager      1/1     Running   0          11m     172.24.2.4   openshift-worker-1.example.com   &lt;none&gt;           &lt;none&gt;
noncpumanager   1/1     Running   0          8m10s   172.24.2.5   openshift-worker-1.example.com   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Verify CPU pinning for the pods' CPUs:</p>
<pre><code>[root@openshift-jumpserver-0 cpuman]# oc debug node/openshift-worker-1.example.com
Starting pod/openshift-worker-1examplecom-debug ...
To use host binaries, run `chroot /host`
systemctlPod IP: 192.168.123.204
If you don't see a command prompt, try pressing enter.
sh-4.2# chroot /host
sh-4.4# ps aux | grep pause
root       39072  0.0  0.0   1028     4 ?        Ss   16:55   0:00 /pause
root       47750  0.0  0.0   1028     4 ?        Ss   16:58   0:00 /pause
root       64183  0.0  0.0   9180   960 ?        S+   17:03   0:00 grep pause
sh-4.4# cat /proc/39072/status | grep -i cpu
Cpus_allowed:   00,00100000
Cpus_allowed_list:  20
sh-4.4# cat /proc/47750/status | grep -i cpu
Cpus_allowed:   ff,ffefffff
Cpus_allowed_list:  0-19,21-39
</code></pre>
<p>Alternatively:</p>
<pre><code>sh-4.4# systemctl status 39072
● crio-ff43bfb551a274eb9e9040510753db6271ef27c13c203fe69e87aad5f7d49f17.scope - libcontainer container ff43bfb551a274eb9e9040510753db6271ef27c13c203fe69e87aad5f7d49f17
   Loaded: loaded (/run/systemd/transient/crio-ff43bfb551a274eb9e9040510753db6271ef27c13c203fe69e87aad5f7d49f17.scope; transient)
Transient: yes
   Active: active (running) since Fri 2020-06-26 16:55:34 UTC; 15min ago
    Tasks: 1 (limit: 1024)
   Memory: 1.4M (limit: 953.6M)
      CPU: 33ms
   CGroup: /kubepods.slice/kubepods-podc405f7cf_b2d1_49c4_bd4c_09f01a0b8e2d.slice/crio-ff43bfb551a274eb9e9040510753db6271ef27c13c203fe69e87aad5f7d49f17.scope
           └─39072 /pause

Jun 26 16:55:34 openshift-worker-1.example.com systemd[1]: Started libcontainer container ff43bfb551a274eb9e9040510753db6271ef27c13c203fe69e87aad5f7d49f17.
sh-4.4# systemctl status 47750
● crio-473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260.scope - libcontainer container 473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260
   Loaded: loaded (/run/systemd/transient/crio-473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260.scope; transient)
Transient: yes
   Active: active (running) since Fri 2020-06-26 16:58:32 UTC; 13min ago
    Tasks: 1 (limit: 1024)
   Memory: 1.0M
      CPU: 35ms
   CGroup: /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod3b4059d0_8030_461a_b192_b5820f6c1119.slice/crio-473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260.scope
           └─47750 /pause

Jun 26 16:58:32 openshift-worker-1.example.com systemd[1]: Started libcontainer container 473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260.
sh-4.4# cat /sys/fs/cgroup/   
blkio/            cpu,cpuacct/      cpuset/           freezer/          memory/           net_cls,net_prio/ perf_event/       rdma/             
cpu/              cpuacct/          devices/          hugetlb/          net_cls/          net_prio/         pids/             systemd/          
sh-4.4# cat /sys/fs/cgroup/cpuset//kubepods.slice/kubepods-podc405f7cf_b2d1_49c4_bd4c_09f01a0b8e2d.slice/crio-ff43bfb551a274eb9e9040510753db6271ef27c13c203fe69e87aad5f7d49f17.scope/cpuset.cpus
20
sh-4.4# cat /sys/fs/cgroup/cpuset//kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod3b4059d0_8030_461a_b192_b5820f6c1119.slice/crio-473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260.scope/cpuset.pus
0-19,21-39
</code></pre>
<h3 id="exploring-resource-limits">Exploring resource limits</h3>
<p>Let's spawn more pods than the hypervisor has CPUs:</p>
<pre><code>[root@openshift-jumpserver-0 cpuman]# cat cpumanager-pod-generated-name.yaml 
apiVersion: v1
kind: Pod
metadata:
  generateName: cpumanager-
spec:
  containers:
  - name: cpumanager
    image: gcr.io/google_containers/pause-amd64:3.0
    resources:
      requests:
        cpu: 1
        memory: &quot;1G&quot;
      limits:
        cpu: 1
        memory: &quot;1G&quot;
  nodeSelector:
    cpumanager: &quot;true&quot;
</code></pre>
<pre><code>for i in {0..45}; do oc create -f cpumanager-pod-generated-name.yaml ; done
</code></pre>
<pre><code>[root@openshift-jumpserver-0 cpuman]# oc get pods | grep Pending
cpumanager-8rtpt   0/1     Pending   0          4m23s
cpumanager-bw848   0/1     Pending   0          4m24s
cpumanager-cm2st   0/1     Pending   0          4m24s
cpumanager-cwj82   0/1     Pending   0          4m23s
cpumanager-r5krw   0/1     Pending   0          4m24s
[root@openshift-jumpserver-0 cpuman]# oc describe node openshift-worker-1.example.com
Name:               openshift-worker-1.example.com
Roles:              worker-cpu-manager
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    cpumanager=true
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=openshift-worker-1.example.com
                    kubernetes.io/os=linux
                    node-role.kubernetes.io/worker-cpu-manager=
                    node.openshift.io/os_id=rhcos
Annotations:        k8s.ovn.org/l3-gateway-config:
                      {&quot;default&quot;:{&quot;mode&quot;:&quot;local&quot;,&quot;interface-id&quot;:&quot;br-local_openshift-worker-1.example.com&quot;,&quot;mac-address&quot;:&quot;8e:c7:71:4d:0f:49&quot;,&quot;ip-addresses&quot;:[&quot;169...
                    k8s.ovn.org/node-chassis-id: 5a8880a6-5b50-4be5-9d84-f195bbc306a2
                    k8s.ovn.org/node-join-subnets: {&quot;default&quot;:&quot;100.64.3.0/29&quot;}
                    k8s.ovn.org/node-mgmt-port-mac-address: 9a:b7:cd:ab:bf:b9
                    k8s.ovn.org/node-subnets: {&quot;default&quot;:&quot;172.24.2.0/23&quot;}
                    machineconfiguration.openshift.io/currentConfig: rendered-worker-cpu-manager-bc48d7bf24df726f468b357482032845
                    machineconfiguration.openshift.io/desiredConfig: rendered-worker-cpu-manager-bc48d7bf24df726f468b357482032845
                    machineconfiguration.openshift.io/reason: 
                    machineconfiguration.openshift.io/state: Done
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 25 Jun 2020 12:03:55 -0400
Taints:             &lt;none&gt;
Unschedulable:      false
Lease:
  HolderIdentity:  openshift-worker-1.example.com
  AcquireTime:     &lt;unset&gt;
  RenewTime:       Fri, 26 Jun 2020 14:14:05 -0400
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 26 Jun 2020 14:10:56 -0400   Fri, 26 Jun 2020 12:43:22 -0400   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 26 Jun 2020 14:10:56 -0400   Fri, 26 Jun 2020 12:43:22 -0400   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 26 Jun 2020 14:10:56 -0400   Fri, 26 Jun 2020 12:43:22 -0400   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 26 Jun 2020 14:10:56 -0400   Fri, 26 Jun 2020 12:43:33 -0400   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.123.204
  Hostname:    openshift-worker-1.example.com
Capacity:
  cpu:                40
  ephemeral-storage:  584946668Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             131924236Ki
  pods:               250
Allocatable:
  cpu:                39500m
  ephemeral-storage:  538013106513
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             130773260Ki
  pods:               250
System Info:
  Machine ID:                             21668e85e1264ac78ea115b2fe79408e
  System UUID:                            4c4c4544-004b-5a10-8050-cac04f484832
  Boot ID:                                f20ee17e-226f-4f34-b9ce-965043215c2d
  Kernel Version:                         4.18.0-147.8.1.el8_1.x86_64
  OS Image:                               Red Hat Enterprise Linux CoreOS 44.81.202005250830-0 (Ootpa)
  Operating System:                       linux
  Architecture:                           amd64
  Container Runtime Version:              cri-o://1.17.4-12.dev.rhaos4.4.git2be4d9c.el8
  Kubelet Version:                        v1.17.1
  Kube-Proxy Version:                     v1.17.1
Non-terminated Pods:                      (49 in total)
  Namespace                               Name                                    CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE
  ---------                               ----                                    ------------  ----------  ---------------  -------------  ---
  default                                 cpumanager                              1 (2%)        1 (2%)      1G (0%)          1G (0%)        78m
  default                                 cpumanager-2548b                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        7m41s
  default                                 cpumanager-44nxg                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m29s
  default                                 cpumanager-5zx5v                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m30s
  default                                 cpumanager-72spc                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m28s
  default                                 cpumanager-9754w                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m29s
  default                                 cpumanager-cff7t                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        8m6s
  default                                 cpumanager-djhz9                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m27s
  default                                 cpumanager-dl7lf                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        7m42s
  default                                 cpumanager-dnz7k                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m28s
  default                                 cpumanager-drhf8                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m27s
  default                                 cpumanager-ff7h5                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m30s
  default                                 cpumanager-fh77s                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        7m40s
  default                                 cpumanager-fsptg                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m30s
  default                                 cpumanager-h74fq                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        7m38s
  default                                 cpumanager-hb72w                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m28s
  default                                 cpumanager-hbhz2                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m29s
  default                                 cpumanager-j7smp                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m28s
  default                                 cpumanager-jlwwn                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m30s
  default                                 cpumanager-jmdfk                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m30s
  default                                 cpumanager-kvfkp                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        7m45s
  default                                 cpumanager-kx4ft                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m28s
  default                                 cpumanager-kz6zg                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        7m43s
  default                                 cpumanager-m4qrn                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m31s
  default                                 cpumanager-mzqqs                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m30s
  default                                 cpumanager-n2tt5                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m28s
  default                                 cpumanager-n79js                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m31s
  default                                 cpumanager-qwjkf                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m27s
  default                                 cpumanager-sn658                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m29s
  default                                 cpumanager-tc7q6                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m29s
  default                                 cpumanager-vss6b                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m29s
  default                                 cpumanager-w846n                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m31s
  default                                 cpumanager-wchfq                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m30s
  default                                 cpumanager-whwxr                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m28s
  default                                 cpumanager-wkhw8                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m31s
  default                                 cpumanager-wtr5z                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m31s
  default                                 cpumanager-z7zgl                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m28s
  default                                 cpumanager-zckx8                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m30s
  default                                 cpumanager-zsnwv                        1 (2%)        1 (2%)      1G (0%)          1G (0%)        4m27s
  default                                 noncpumanager                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         75m
  openshift-cluster-node-tuning-operator  tuned-tkhrp                             10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         26h
  openshift-dns                           dns-default-q8hww                       110m (0%)     0 (0%)      70Mi (0%)        512Mi (0%)     26h
  openshift-image-registry                node-ca-sfjgq                           10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         26h
  openshift-machine-config-operator       machine-config-daemon-8nw2s             40m (0%)      0 (0%)      100Mi (0%)       0 (0%)         26h
  openshift-marketplace                   certified-operators-74d989c4dd-w6l7f    10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         50m
  openshift-monitoring                    node-exporter-nlgr5                     9m (0%)       0 (0%)      210Mi (0%)       0 (0%)         26h
  openshift-multus                        multus-w2sgc                            10m (0%)      0 (0%)      150Mi (0%)       0 (0%)         26h
  openshift-ovn-kubernetes                ovnkube-node-pg57x                      200m (0%)     0 (0%)      600Mi (0%)       0 (0%)         26h
  openshift-ovn-kubernetes                ovs-node-ktvtm                          100m (0%)     0 (0%)      300Mi (0%)       0 (0%)         26h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests           Limits
  --------           --------           ------
  cpu                39499m (99%)       39 (98%)
  memory             40667235840 (30%)  39536870912 (29%)
  ephemeral-storage  0 (0%)             0 (0%)
Events:
  Type     Reason                   Age                  From                                     Message
  ----     ------                   ----                 ----                                     -------
  Normal   NodeNotSchedulable       110m                 kubelet, openshift-worker-1.example.com  Node openshift-worker-1.example.com status is now: NodeNotSchedulable
  Normal   Starting                 106m                 kubelet, openshift-worker-1.example.com  Starting kubelet.
  Normal   NodeHasSufficientMemory  106m (x2 over 106m)  kubelet, openshift-worker-1.example.com  Node openshift-worker-1.example.com status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    106m (x2 over 106m)  kubelet, openshift-worker-1.example.com  Node openshift-worker-1.example.com status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     106m (x2 over 106m)  kubelet, openshift-worker-1.example.com  Node openshift-worker-1.example.com status is now: NodeHasSufficientPID
  Warning  Rebooted                 106m                 kubelet, openshift-worker-1.example.com  Node openshift-worker-1.example.com has been rebooted, boot id: 7df19e71-4ebc-4a66-94e8-1e475d11e095
  Normal   NodeNotReady             106m                 kubelet, openshift-worker-1.example.com  Node openshift-worker-1.example.com status is now: NodeNotReady
  Normal   NodeNotSchedulable       106m                 kubelet, openshift-worker-1.example.com  Node openshift-worker-1.example.com status is now: NodeNotSchedulable
  Normal   NodeAllocatableEnforced  106m                 kubelet, openshift-worker-1.example.com  Updated Node Allocatable limit across pods
  Normal   NodeReady                106m                 kubelet, openshift-worker-1.example.com  Node openshift-worker-1.example.com status is now: NodeReady
  Normal   NodeSchedulable          100m                 kubelet, openshift-worker-1.example.com  Node openshift-worker-1.example.com status is now: NodeSchedulable
  Normal   Starting                 90m                  kubelet, openshift-worker-1.example.com  Starting kubelet.
  Normal   NodeHasSufficientMemory  90m (x2 over 90m)    kubelet, openshift-worker-1.example.com  Node openshift-worker-1.example.com status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    90m (x2 over 90m)    kubelet, openshift-worker-1.example.com  Node openshift-worker-1.example.com status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     90m (x2 over 90m)    kubelet, openshift-worker-1.example.com  Node openshift-worker-1.example.com status is now: NodeHasSufficientPID
  Warning  Rebooted                 90m                  kubelet, openshift-worker-1.example.com  Node openshift-worker-1.example.com has been rebooted, boot id: f20ee17e-226f-4f34-b9ce-965043215c2d
  Normal   NodeNotReady             90m                  kubelet, openshift-worker-1.example.com  Node openshift-worker-1.example.com status is now: NodeNotReady
  Normal   NodeNotSchedulable       90m                  kubelet, openshift-worker-1.example.com  Node openshift-worker-1.example.com status is now: NodeNotSchedulable
  Normal   NodeAllocatableEnforced  90m                  kubelet, openshift-worker-1.example.com  Updated Node Allocatable limit across pods
  Normal   NodeReady                90m                  kubelet, openshift-worker-1.example.com  Node openshift-worker-1.example.com status is now: NodeReady
  Normal   NodeSchedulable          85m                  kubelet, openshift-worker-1.example.com  Node openshift-worker-1.example.com status is now: NodeSchedulable
</code></pre>
<p>Verify processes and cgroup limits on the worker node:</p>
<pre><code>[root@openshift-jumpserver-0 cpuman]# oc debug node/openshift-worker-1.example.com
Starting pod/openshift-worker-1examplecom-debug ...
To use host binaries, run `chroot /host`
Pod IP: 192.168.123.204
If you don't see a command prompt, try pressing enter.
sh-4.2# chroot /host
sh-4.4# ps aux | grep pause
root       39072  0.0  0.0   1028     4 ?        Ss   16:55   0:00 /pause
root       47750  0.0  0.0   1028     4 ?        Ss   16:58   0:00 /pause
root      274649  0.0  0.0   1028     4 ?        Ss   18:06   0:00 /pause
root      276349  0.0  0.0   1028     4 ?        Ss   18:06   0:00 /pause
root      276526  0.0  0.0   1028     4 ?        Ss   18:06   0:00 /pause
root      276685  0.0  0.0   1028     4 ?        Ss   18:06   0:00 /pause
root      276824  0.0  0.0   1028     4 ?        Ss   18:06   0:00 /pause
root      276988  0.0  0.0   1028     4 ?        Ss   18:06   0:00 /pause
root      277279  0.0  0.0   1028     4 ?        Ss   18:06   0:00 /pause
root      291552  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      291674  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      291783  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      291900  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      292072  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      292145  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      292191  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      292638  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      292639  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      292693  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      292941  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      292949  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      292971  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      292986  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      293300  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      293422  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      293528  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      293615  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      293678  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      293823  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      293909  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      293951  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      293965  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      293983  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      294087  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      294164  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      294184  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      294239  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      294330  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      294352  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      294426  0.0  0.0   1028     4 ?        Ss   18:09   0:00 /pause
root      301832  0.0  0.0   9180   964 ?        R+   18:11   0:00 grep pause
sh-4.4# systemctl status 47750
● crio-473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260.scope - libcontainer container 473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260
   Loaded: loaded (/run/systemd/transient/crio-473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260.scope; transient)
Transient: yes
   Active: active (running) since Fri 2020-06-26 16:58:32 UTC; 1h 12min ago
    Tasks: 1 (limit: 1024)
   Memory: 1.0M
      CPU: 35ms
   CGroup: /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod3b4059d0_8030_461a_b192_b5820f6c1119.slice/crio-473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260.scope
           └─47750 /pause

Jun 26 16:58:32 openshift-worker-1.example.com systemd[1]: Started libcontainer container 473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260.
sh-4.4# cat /sys/fs/cgroup/cpuset/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod3b4059d0_8030_461a_b192_b5820f6c1119.slice/crio-473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260.scope/cpuset.cpus
0
sh-4.4# exit
exit
sh-4.2# exit
exit

Removing debug pod ...
</code></pre>
<p>Default host limits:</p>
<pre><code>sh-4.4# cat /etc/kubernetes/kubelet.conf | jq '.systemReserved'
{
  &quot;cpu&quot;: &quot;500m&quot;,
  &quot;ephemeral-storage&quot;: &quot;1Gi&quot;,
  &quot;memory&quot;: &quot;1Gi&quot;
}
</code></pre>
<h3 id="applying-custom-limits">Applying custom limits</h3>
<p>Let's now reserve 10 CPUs for the host.</p>
<p><code>cpumanager-kubeletconfig.yaml</code>:</p>
<pre><code>apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: cpumanager-enabled
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: cpumanager-enabled
  kubeletConfig:
     cpuManagerPolicy: static
     cpuManagerReconcilePeriod: 5s
     systemReserved:
       cpu: &quot;10&quot;
       memory: &quot;1Gi&quot;
       ephemeral-storage: &quot;10Gi&quot;
</code></pre>
<pre><code>[root@openshift-jumpserver-0 cpuman]# oc apply -f cpumanager-kubeletconfig.yaml 
kubeletconfig.machineconfiguration.openshift.io/cpumanager-enabled configured
[root@openshift-jumpserver-0 cpuman]# oc get -o yaml -f cpumanager-kubeletconfig.yaml 
apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {&quot;apiVersion&quot;:&quot;machineconfiguration.openshift.io/v1&quot;,&quot;kind&quot;:&quot;KubeletConfig&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;name&quot;:&quot;cpumanager-enabled&quot;},&quot;spec&quot;:{&quot;kubeletConfig&quot;:{&quot;cpuManagerPolicy&quot;:&quot;static&quot;,&quot;cpuManagerReconcilePeriod&quot;:&quot;5s&quot;,&quot;systemReserved&quot;:{&quot;cpu&quot;:&quot;10&quot;,&quot;ephemeral-storage&quot;:&quot;10Gi&quot;,&quot;memory&quot;:&quot;1Gi&quot;}},&quot;machineConfigPoolSelector&quot;:{&quot;matchLabels&quot;:{&quot;custom-kubelet&quot;:&quot;cpumanager-enabled&quot;}}}}
  creationTimestamp: &quot;2020-06-26T18:29:07Z&quot;
  finalizers:
  - b5ac419d-573d-428c-adc1-f7b5bedb27e3
  - 21fa7ce4-49fa-470d-b4ed-f0972dbdd040
  generation: 2
  name: cpumanager-enabled
  resourceVersion: &quot;484854&quot;
  selfLink: /apis/machineconfiguration.openshift.io/v1/kubeletconfigs/cpumanager-enabled
  uid: 75d54d68-7b96-4b9e-8c9a-db3820b2629b
spec:
  kubeletConfig:
    cpuManagerPolicy: static
    cpuManagerReconcilePeriod: 5s
    systemReserved:
      cpu: &quot;10&quot;
      ephemeral-storage: 10Gi
      memory: 1Gi
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: cpumanager-enabled
status:
  conditions:
  - lastTransitionTime: &quot;2020-06-26T18:29:07Z&quot;
    message: Success
    status: &quot;True&quot;
    type: Success
  - lastTransitionTime: &quot;2020-06-26T18:33:14Z&quot;
    message: Success
    status: &quot;True&quot;
    type: Success
[root@openshift-jumpserver-0 cpuman]# oc get nodes
NAME                             STATUS                     ROLES                AGE   VERSION
openshift-master-0.example.com   Ready                      master               27h   v1.17.1
openshift-master-1.example.com   Ready                      master               27h   v1.17.1
openshift-master-2.example.com   Ready                      master               27h   v1.17.1
openshift-worker-0.example.com   Ready                      worker               26h   v1.17.1
openshift-worker-1.example.com   Ready,SchedulingDisabled   worker-cpu-manager   26h   v1.17.1
[root@openshift-jumpserver-0 cpuman]# oc get machineconfig | grep worker-cpu-manager
99-worker-cpu-manager-ab32b145-ada3-4f96-adf9-1fb8388ba183-kubelet   8af4f709c4ba9c0afff3408ecc99c8fce61dd314   2.2.0             4m42s
rendered-worker-cpu-manager-171f3675f101028b058ffe27d6344bb2         8af4f709c4ba9c0afff3408ecc99c8fce61dd314   2.2.0             30s
rendered-worker-cpu-manager-bc48d7bf24df726f468b357482032845         8af4f709c4ba9c0afff3408ecc99c8fce61dd314   2.2.0             113m
rendered-worker-cpu-manager-ca0a09ddea41402490e1c39a138cd44e         8af4f709c4ba9c0afff3408ecc99c8fce61dd314   2.2.0             129m
</code></pre>
<pre><code>[root@openshift-jumpserver-0 cpuman]# for i in {0..35}; do oc create -f cpumanager-pod-generated-name.yaml ; done
pod/cpumanager-msf8c created
pod/cpumanager-2kwws created
pod/cpumanager-7mx5k created
pod/cpumanager-bq7sf created
pod/cpumanager-86kkl created
pod/cpumanager-z7h5f created
pod/cpumanager-c5pd4 created
pod/cpumanager-sr6jt created
pod/cpumanager-dp7ck created
pod/cpumanager-n4ffv created
pod/cpumanager-5q77h created
pod/cpumanager-zxbj2 created
pod/cpumanager-7f8fw created
pod/cpumanager-j5kdw created
pod/cpumanager-fhfmw created
pod/cpumanager-hz5r8 created
pod/cpumanager-44xst created
pod/cpumanager-r6h98 created
pod/cpumanager-mjsfq created
pod/cpumanager-dh897 created
pod/cpumanager-s445q created
pod/cpumanager-brwcz created
pod/cpumanager-mnbrg created
pod/cpumanager-56vmv created
pod/cpumanager-kt5fg created
pod/cpumanager-fscpd created
pod/cpumanager-hsjzn created
pod/cpumanager-cvsth created
pod/cpumanager-j7wlv created
pod/cpumanager-z87tp created
pod/cpumanager-xggjm created
pod/cpumanager-jg646 created
pod/cpumanager-ml7kg created
pod/cpumanager-qwhjm created
pod/cpumanager-m6jw8 created
pod/cpumanager-f8qsh created
[root@openshift-jumpserver-0 cpuman]# oc get pods | grep Runn
cpumanager-2kwws   1/1     Running   0          15s
cpumanager-44xst   1/1     Running   0          13s
cpumanager-56vmv   1/1     Running   0          12s
cpumanager-5q77h   1/1     Running   0          14s
cpumanager-7f8fw   1/1     Running   0          14s
cpumanager-7mx5k   1/1     Running   0          15s
cpumanager-86kkl   1/1     Running   0          15s
cpumanager-8rtpt   1/1     Running   0          32m
cpumanager-bq7sf   1/1     Running   0          15s
cpumanager-brwcz   1/1     Running   0          13s
cpumanager-bw848   1/1     Running   0          32m
cpumanager-c5pd4   1/1     Running   0          14s
cpumanager-cm2st   1/1     Running   0          32m
cpumanager-cwj82   1/1     Running   0          32m
cpumanager-dh897   1/1     Running   0          13s
cpumanager-dp7ck   1/1     Running   0          14s
cpumanager-fhfmw   1/1     Running   0          13s
cpumanager-hz5r8   1/1     Running   0          13s
cpumanager-j5kdw   1/1     Running   0          14s
cpumanager-mjsfq   1/1     Running   0          13s
cpumanager-mnbrg   1/1     Running   0          12s
cpumanager-msf8c   1/1     Running   0          15s
cpumanager-n4ffv   1/1     Running   0          14s
cpumanager-r5krw   1/1     Running   0          32m
cpumanager-r6h98   1/1     Running   0          13s
cpumanager-s445q   1/1     Running   0          13s
cpumanager-sr6jt   1/1     Running   0          14s
cpumanager-z7h5f   1/1     Running   0          15s
cpumanager-zxbj2   1/1     Running   0          14s
[root@openshift-jumpserver-0 cpuman]# oc get pods | grep Runn | wc -l
29
</code></pre>
<p>Verify host limits:</p>
<pre><code>sh-4.4# cat /etc/kubernetes/kubelet.conf | jq '.systemReserved'
{
  &quot;cpu&quot;: &quot;10&quot;,
  &quot;ephemeral-storage&quot;: &quot;10Gi&quot;,
  &quot;memory&quot;: &quot;1Gi&quot;
}
</code></pre>
<h2 id="reserving-0-cpus-for-the-host">Reserving 0 CPUs for the host</h2>
<p>This will not work - <code>cpumanager-kubeletconfig.yaml</code>:</p>
<pre><code>apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: cpumanager-enabled
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: cpumanager-enabled
  kubeletConfig:
     cpuManagerPolicy: static
     cpuManagerReconcilePeriod: 5s
     systemReserved:
       cpu: &quot;0&quot;
       memory: &quot;1Gi&quot;
       ephemeral-storage: &quot;10Gi&quot;
</code></pre>
<p>Upon worker restart, the kubelet will not be able to start, reporting:</p>
<pre><code>[root@openshift-worker-1 ~]# Jun 26 19:05:10 openshift-worker-1.example.com systemd[1]: Stopped Kubernetes Kubelet.
Jun 26 19:05:10 openshift-worker-1.example.com systemd[1]: kubelet.service: Consumed 115ms CPU time
Jun 26 19:05:10 openshift-worker-1.example.com systemd[1]: Starting Kubernetes Kubelet...
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: Flag --minimum-container-ttl-duration has been deprecated, Use --eviction-hard or --eviction-soft instead. Will be removed in a future version.
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421694    5492 flags.go:33] FLAG: --add-dir-header=&quot;false&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421726    5492 flags.go:33] FLAG: --address=&quot;0.0.0.0&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421741    5492 flags.go:33] FLAG: --allowed-unsafe-sysctls=&quot;[]&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421748    5492 flags.go:33] FLAG: --alsologtostderr=&quot;false&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421752    5492 flags.go:33] FLAG: --anonymous-auth=&quot;true&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421756    5492 flags.go:33] FLAG: --application-metrics-count-limit=&quot;100&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421759    5492 flags.go:33] FLAG: --authentication-token-webhook=&quot;false&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421763    5492 flags.go:33] FLAG: --authentication-token-webhook-cache-ttl=&quot;2m0s&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421767    5492 flags.go:33] FLAG: --authorization-mode=&quot;AlwaysAllow&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421771    5492 flags.go:33] FLAG: --authorization-webhook-cache-authorized-ttl=&quot;5m0s&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421775    5492 flags.go:33] FLAG: --authorization-webhook-cache-unauthorized-ttl=&quot;30s&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421778    5492 flags.go:33] FLAG: --azure-container-registry-config=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421781    5492 flags.go:33] FLAG: --boot-id-file=&quot;/proc/sys/kernel/random/boot_id&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421787    5492 flags.go:33] FLAG: --bootstrap-checkpoint-path=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421790    5492 flags.go:33] FLAG: --bootstrap-kubeconfig=&quot;/etc/kubernetes/kubeconfig&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421793    5492 flags.go:33] FLAG: --cert-dir=&quot;/var/lib/kubelet/pki&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421797    5492 flags.go:33] FLAG: --cgroup-driver=&quot;cgroupfs&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421800    5492 flags.go:33] FLAG: --cgroup-root=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421803    5492 flags.go:33] FLAG: --cgroups-per-qos=&quot;true&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421806    5492 flags.go:33] FLAG: --chaos-chance=&quot;0&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421811    5492 flags.go:33] FLAG: --client-ca-file=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421814    5492 flags.go:33] FLAG: --cloud-config=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421817    5492 flags.go:33] FLAG: --cloud-provider=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421820    5492 flags.go:33] FLAG: --cluster-dns=&quot;[]&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421824    5492 flags.go:33] FLAG: --cluster-domain=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421828    5492 flags.go:33] FLAG: --cni-bin-dir=&quot;/opt/cni/bin&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421831    5492 flags.go:33] FLAG: --cni-cache-dir=&quot;/var/lib/cni/cache&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421834    5492 flags.go:33] FLAG: --cni-conf-dir=&quot;/etc/cni/net.d&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421838    5492 flags.go:33] FLAG: --config=&quot;/etc/kubernetes/kubelet.conf&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421842    5492 flags.go:33] FLAG: --container-hints=&quot;/etc/cadvisor/container_hints.json&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421845    5492 flags.go:33] FLAG: --container-log-max-files=&quot;5&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421849    5492 flags.go:33] FLAG: --container-log-max-size=&quot;10Mi&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421852    5492 flags.go:33] FLAG: --container-runtime=&quot;remote&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421865    5492 flags.go:33] FLAG: --container-runtime-endpoint=&quot;/var/run/crio/crio.sock&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421868    5492 flags.go:33] FLAG: --containerd=&quot;/run/containerd/containerd.sock&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421872    5492 flags.go:33] FLAG: --contention-profiling=&quot;false&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421876    5492 flags.go:33] FLAG: --cpu-cfs-quota=&quot;true&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421879    5492 flags.go:33] FLAG: --cpu-cfs-quota-period=&quot;100ms&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421883    5492 flags.go:33] FLAG: --cpu-manager-policy=&quot;none&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421886    5492 flags.go:33] FLAG: --cpu-manager-reconcile-period=&quot;10s&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421889    5492 flags.go:33] FLAG: --docker=&quot;unix:///var/run/docker.sock&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421893    5492 flags.go:33] FLAG: --docker-endpoint=&quot;unix:///var/run/docker.sock&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421896    5492 flags.go:33] FLAG: --docker-env-metadata-whitelist=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421899    5492 flags.go:33] FLAG: --docker-only=&quot;false&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421902    5492 flags.go:33] FLAG: --docker-root=&quot;/var/lib/docker&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421905    5492 flags.go:33] FLAG: --docker-tls=&quot;false&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421908    5492 flags.go:33] FLAG: --docker-tls-ca=&quot;ca.pem&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421911    5492 flags.go:33] FLAG: --docker-tls-cert=&quot;cert.pem&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421914    5492 flags.go:33] FLAG: --docker-tls-key=&quot;key.pem&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421917    5492 flags.go:33] FLAG: --dynamic-config-dir=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421921    5492 flags.go:33] FLAG: --enable-cadvisor-json-endpoints=&quot;true&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421924    5492 flags.go:33] FLAG: --enable-controller-attach-detach=&quot;true&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421929    5492 flags.go:33] FLAG: --enable-debugging-handlers=&quot;true&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421932    5492 flags.go:33] FLAG: --enable-load-reader=&quot;false&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421951    5492 flags.go:33] FLAG: --enable-server=&quot;true&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421954    5492 flags.go:33] FLAG: --enforce-node-allocatable=&quot;[pods]&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421958    5492 flags.go:33] FLAG: --event-burst=&quot;10&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421961    5492 flags.go:33] FLAG: --event-qps=&quot;5&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421963    5492 flags.go:33] FLAG: --event-storage-age-limit=&quot;default=0&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421966    5492 flags.go:33] FLAG: --event-storage-event-limit=&quot;default=0&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421969    5492 flags.go:33] FLAG: --eviction-hard=&quot;imagefs.available&lt;15%,memory.available&lt;100Mi,nodefs.available&lt;10%,nodefs.inodesFree&lt;5%&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421979    5492 flags.go:33] FLAG: --eviction-max-pod-grace-period=&quot;0&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421982    5492 flags.go:33] FLAG: --eviction-minimum-reclaim=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421986    5492 flags.go:33] FLAG: --eviction-pressure-transition-period=&quot;5m0s&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421990    5492 flags.go:33] FLAG: --eviction-soft=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422000    5492 flags.go:33] FLAG: --eviction-soft-grace-period=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422003    5492 flags.go:33] FLAG: --exit-on-lock-contention=&quot;false&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422006    5492 flags.go:33] FLAG: --experimental-allocatable-ignore-eviction=&quot;false&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422009    5492 flags.go:33] FLAG: --experimental-bootstrap-kubeconfig=&quot;/etc/kubernetes/kubeconfig&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422012    5492 flags.go:33] FLAG: --experimental-check-node-capabilities-before-mount=&quot;false&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422015    5492 flags.go:33] FLAG: --experimental-dockershim=&quot;false&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422018    5492 flags.go:33] FLAG: --experimental-dockershim-root-directory=&quot;/var/lib/dockershim&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422021    5492 flags.go:33] FLAG: --experimental-kernel-memcg-notification=&quot;false&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422024    5492 flags.go:33] FLAG: --experimental-mounter-path=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422027    5492 flags.go:33] FLAG: --fail-swap-on=&quot;true&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422030    5492 flags.go:33] FLAG: --feature-gates=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422034    5492 flags.go:33] FLAG: --file-check-frequency=&quot;20s&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422037    5492 flags.go:33] FLAG: --global-housekeeping-interval=&quot;1m0s&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422040    5492 flags.go:33] FLAG: --hairpin-mode=&quot;promiscuous-bridge&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422043    5492 flags.go:33] FLAG: --healthz-bind-address=&quot;127.0.0.1&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422046    5492 flags.go:33] FLAG: --healthz-port=&quot;10248&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422049    5492 flags.go:33] FLAG: --help=&quot;false&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422052    5492 flags.go:33] FLAG: --hostname-override=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422055    5492 flags.go:33] FLAG: --housekeeping-interval=&quot;10s&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422058    5492 flags.go:33] FLAG: --http-check-frequency=&quot;20s&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422060    5492 flags.go:33] FLAG: --image-gc-high-threshold=&quot;85&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422063    5492 flags.go:33] FLAG: --image-gc-low-threshold=&quot;80&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422066    5492 flags.go:33] FLAG: --image-pull-progress-deadline=&quot;1m0s&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422069    5492 flags.go:33] FLAG: --image-service-endpoint=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422072    5492 flags.go:33] FLAG: --iptables-drop-bit=&quot;15&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422075    5492 flags.go:33] FLAG: --iptables-masquerade-bit=&quot;14&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422078    5492 flags.go:33] FLAG: --keep-terminated-pod-volumes=&quot;false&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422081    5492 flags.go:33] FLAG: --kube-api-burst=&quot;10&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422084    5492 flags.go:33] FLAG: --kube-api-content-type=&quot;application/vnd.kubernetes.protobuf&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422087    5492 flags.go:33] FLAG: --kube-api-qps=&quot;5&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422090    5492 flags.go:33] FLAG: --kube-reserved=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422093    5492 flags.go:33] FLAG: --kube-reserved-cgroup=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422103    5492 flags.go:33] FLAG: --kubeconfig=&quot;/var/lib/kubelet/kubeconfig&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422106    5492 flags.go:33] FLAG: --kubelet-cgroups=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422109    5492 flags.go:33] FLAG: --lock-file=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422112    5492 flags.go:33] FLAG: --log-backtrace-at=&quot;:0&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422115    5492 flags.go:33] FLAG: --log-cadvisor-usage=&quot;false&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422118    5492 flags.go:33] FLAG: --log-dir=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422121    5492 flags.go:33] FLAG: --log-file=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422124    5492 flags.go:33] FLAG: --log-file-max-size=&quot;1800&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422127    5492 flags.go:33] FLAG: --log-flush-frequency=&quot;5s&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422130    5492 flags.go:33] FLAG: --logtostderr=&quot;true&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422133    5492 flags.go:33] FLAG: --machine-id-file=&quot;/etc/machine-id,/var/lib/dbus/machine-id&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422136    5492 flags.go:33] FLAG: --make-iptables-util-chains=&quot;true&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422139    5492 flags.go:33] FLAG: --manifest-url=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422142    5492 flags.go:33] FLAG: --manifest-url-header=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422146    5492 flags.go:33] FLAG: --master-service-namespace=&quot;default&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422149    5492 flags.go:33] FLAG: --max-open-files=&quot;1000000&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422153    5492 flags.go:33] FLAG: --max-pods=&quot;110&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422156    5492 flags.go:33] FLAG: --maximum-dead-containers=&quot;-1&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422159    5492 flags.go:33] FLAG: --maximum-dead-containers-per-container=&quot;1&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422162    5492 flags.go:33] FLAG: --minimum-container-ttl-duration=&quot;6m0s&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422165    5492 flags.go:33] FLAG: --minimum-image-ttl-duration=&quot;2m0s&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422168    5492 flags.go:33] FLAG: --network-plugin=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422170    5492 flags.go:33] FLAG: --network-plugin-mtu=&quot;0&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422173    5492 flags.go:33] FLAG: --node-ip=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422176    5492 flags.go:33] FLAG: --node-labels=&quot;node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422182    5492 flags.go:33] FLAG: --node-status-max-images=&quot;50&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422185    5492 flags.go:33] FLAG: --node-status-update-frequency=&quot;10s&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422188    5492 flags.go:33] FLAG: --non-masquerade-cidr=&quot;10.0.0.0/8&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422191    5492 flags.go:33] FLAG: --oom-score-adj=&quot;-999&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422194    5492 flags.go:33] FLAG: --pod-cidr=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422196    5492 flags.go:33] FLAG: --pod-infra-container-image=&quot;k8s.gcr.io/pause:3.1&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422200    5492 flags.go:33] FLAG: --pod-manifest-path=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422211    5492 flags.go:33] FLAG: --pod-max-pids=&quot;-1&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422214    5492 flags.go:33] FLAG: --pods-per-core=&quot;0&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422217    5492 flags.go:33] FLAG: --port=&quot;10250&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422219    5492 flags.go:33] FLAG: --protect-kernel-defaults=&quot;false&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422222    5492 flags.go:33] FLAG: --provider-id=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422225    5492 flags.go:33] FLAG: --qos-reserved=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422228    5492 flags.go:33] FLAG: --read-only-port=&quot;10255&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422231    5492 flags.go:33] FLAG: --really-crash-for-testing=&quot;false&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422234    5492 flags.go:33] FLAG: --redirect-container-streaming=&quot;false&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422237    5492 flags.go:33] FLAG: --register-node=&quot;true&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422240    5492 flags.go:33] FLAG: --register-schedulable=&quot;true&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422243    5492 flags.go:33] FLAG: --register-with-taints=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422246    5492 flags.go:33] FLAG: --registry-burst=&quot;10&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422249    5492 flags.go:33] FLAG: --registry-qps=&quot;5&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422252    5492 flags.go:33] FLAG: --reserved-cpus=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422255    5492 flags.go:33] FLAG: --resolv-conf=&quot;/etc/resolv.conf&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422258    5492 flags.go:33] FLAG: --root-dir=&quot;/var/lib/kubelet&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422261    5492 flags.go:33] FLAG: --rotate-certificates=&quot;false&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422264    5492 flags.go:33] FLAG: --rotate-server-certificates=&quot;false&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422266    5492 flags.go:33] FLAG: --runonce=&quot;false&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422269    5492 flags.go:33] FLAG: --runtime-cgroups=&quot;/system.slice/crio.service&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422272    5492 flags.go:33] FLAG: --runtime-request-timeout=&quot;2m0s&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422275    5492 flags.go:33] FLAG: --seccomp-profile-root=&quot;/var/lib/kubelet/seccomp&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422278    5492 flags.go:33] FLAG: --serialize-image-pulls=&quot;true&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422281    5492 flags.go:33] FLAG: --skip-headers=&quot;false&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422284    5492 flags.go:33] FLAG: --skip-log-headers=&quot;false&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422287    5492 flags.go:33] FLAG: --stderrthreshold=&quot;2&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422290    5492 flags.go:33] FLAG: --storage-driver-buffer-duration=&quot;1m0s&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422293    5492 flags.go:33] FLAG: --storage-driver-db=&quot;cadvisor&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422296    5492 flags.go:33] FLAG: --storage-driver-host=&quot;localhost:8086&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422299    5492 flags.go:33] FLAG: --storage-driver-password=&quot;root&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422301    5492 flags.go:33] FLAG: --storage-driver-secure=&quot;false&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422312    5492 flags.go:33] FLAG: --storage-driver-table=&quot;stats&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422315    5492 flags.go:33] FLAG: --storage-driver-user=&quot;root&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422318    5492 flags.go:33] FLAG: --streaming-connection-idle-timeout=&quot;4h0m0s&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422321    5492 flags.go:33] FLAG: --sync-frequency=&quot;1m0s&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422324    5492 flags.go:33] FLAG: --system-cgroups=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422326    5492 flags.go:33] FLAG: --system-reserved=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422329    5492 flags.go:33] FLAG: --system-reserved-cgroup=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422333    5492 flags.go:33] FLAG: --tls-cert-file=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422336    5492 flags.go:33] FLAG: --tls-cipher-suites=&quot;[]&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422340    5492 flags.go:33] FLAG: --tls-min-version=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422343    5492 flags.go:33] FLAG: --tls-private-key-file=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422345    5492 flags.go:33] FLAG: --topology-manager-policy=&quot;none&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422349    5492 flags.go:33] FLAG: --v=&quot;3&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422352    5492 flags.go:33] FLAG: --version=&quot;false&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422361    5492 flags.go:33] FLAG: --vmodule=&quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422365    5492 flags.go:33] FLAG: --volume-plugin-dir=&quot;/etc/kubernetes/kubelet-plugins/volume/exec&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422369    5492 flags.go:33] FLAG: --volume-stats-agg-period=&quot;1m0s&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422394    5492 feature_gate.go:244] feature gates: &amp;{map[]}
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: Flag --minimum-container-ttl-duration has been deprecated, Use --eviction-hard or --eviction-soft instead. Will be removed in a future version.
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.423859    5492 feature_gate.go:244] feature gates: &amp;{map[LegacyNodeRoleBehavior:false NodeDisruptionExclusion:true RotateKubeletServerCertificate:true SCTPSupport:true ServiceNodeExclusion:true SupportPodPidsLimit:true]}
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.423940    5492 feature_gate.go:244] feature gates: &amp;{map[LegacyNodeRoleBehavior:false NodeDisruptionExclusion:true RotateKubeletServerCertificate:true SCTPSupport:true ServiceNodeExclusion:true SupportPodPidsLimit:true]}
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.433150    5492 mount_linux.go:168] Detected OS with systemd
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.433271    5492 server.go:424] Version: v1.17.1
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.433323    5492 feature_gate.go:244] feature gates: &amp;{map[LegacyNodeRoleBehavior:false NodeDisruptionExclusion:true RotateKubeletServerCertificate:true SCTPSupport:true ServiceNodeExclusion:true SupportPodPidsLimit:true]}
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.433385    5492 feature_gate.go:244] feature gates: &amp;{map[LegacyNodeRoleBehavior:false NodeDisruptionExclusion:true RotateKubeletServerCertificate:true SCTPSupport:true ServiceNodeExclusion:true SupportPodPidsLimit:true]}
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.433491    5492 plugins.go:100] No cloud provider specified.
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.433510    5492 server.go:540] No cloud provider specified: &quot;&quot; from the config file: &quot;&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.433519    5492 server.go:830] Client rotation is on, will bootstrap in background
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.445886    5492 bootstrap.go:84] Current kubeconfig file contents are still valid, no bootstrap necessary
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.445957    5492 certificate_store.go:129] Loading cert/key pair from &quot;/var/lib/kubelet/pki/kubelet-client-current.pem&quot;.
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.446355    5492 server.go:857] Starting client certificate rotation.
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.446367    5492 certificate_manager.go:285] Certificate rotation is enabled.
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.446467    5492 certificate_manager.go:556] Certificate expiration is 2020-07-26 09:59:31 +0000 UTC, rotation deadline is 2020-07-20 19:25:44.002114354 +0000 UTC
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.446497    5492 certificate_manager.go:291] Waiting 576h20m33.555623075s for next certificate rotation
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.447060    5492 dynamic_cafile_content.go:129] Loaded a new CA Bundle and Verifier for &quot;client-ca-bundle::/etc/kubernetes/kubelet-ca.crt&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.447171    5492 dynamic_cafile_content.go:167] Starting client-ca-bundle::/etc/kubernetes/kubelet-ca.crt
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.447357    5492 manager.go:146] cAdvisor running in container: &quot;/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service&quot;
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.448446    5492 fs.go:125] Filesystem UUIDs: map[00000000-0000-4000-a000-000000000002:/dev/sda4 1ccc3ac6-b59d-46bb-9850-229cd8b4e007:/dev/dm-0 676b3306-2850-478c-a817-3f723d49377d:/dev/sda1 D802-CD71:/dev/sda2]
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.448470    5492 fs.go:126] Filesystem partitions: map[/dev/mapper/coreos-luks-root-nocrypt:{mountpoint:/var major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:ext4 blockSize:0} /dev/shm:{mountpoint:/dev/shm major:0 minor:22 fsType:tmpfs blockSize:0} /run:{mountpoint:/run major:0 minor:24 fsType:tmpfs blockSize:0} /run/user/1000:{mountpoint:/run/user/1000 major:0 minor:44 fsType:tmpfs blockSize:0} /sys/fs/cgroup:{mountpoint:/sys/fs/cgroup major:0 minor:25 fsType:tmpfs blockSize:0}]
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.453331    5492 manager.go:193] Machine: {NumCores:40 CpuFrequency:3100000 MemoryCapacity:135090417664 HugePages:[{PageSize:1048576 NumPages:0} {PageSize:2048 NumPages:0}] MachineID:21668e85e1264ac78ea115b2fe79408e SystemUUID:4c4c4544-004b-5a10-8050-cac04f484832 BootID:16f2699c-8a76-444a-8f56-e32057152db4 Filesystems:[{Device:/run/user/1000 DeviceMajor:0 DeviceMinor:44 Capacity:13509038080 Type:vfs Inodes:16490529 HasInodes:true} {Device:/dev/shm DeviceMajor:0 DeviceMinor:22 Capacity:67545206784 Type:vfs Inodes:16490529 HasInodes:true} {Device:/run DeviceMajor:0 DeviceMinor:24 Capacity:67545206784 Type:vfs Inodes:16490529 HasInodes:true} {Device:/sys/fs/cgroup DeviceMajor:0 DeviceMinor:25 Capacity:67545206784 Type:vfs Inodes:16490529 HasInodes:true} {Device:/dev/mapper/coreos-luks-root-nocrypt DeviceMajor:253 DeviceMinor:0 Capacity:598985388032 Type:vfs Inodes:292478400 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:381549568 Type:vfs Inodes:98304 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:598995877376 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:599550590976 Scheduler:mq-deadline}] NetworkDevices:[{Name:eno1 MacAddress:18:66:da:9f:b1:0a Speed:1000 Mtu:1500} {Name:eno2 MacAddress:18:66:da:9f:b1:0b Speed:-1 Mtu:1500} {Name:eno3 MacAddress:18:66:da:9f:b1:0c Speed:-1 Mtu:1500} {Name:eno4 MacAddress:18:66:da:9f:b1:0d Speed:1000 Mtu:1500} {Name:enp4s0f0 MacAddress:a0:36:9f:e5:e9:fc Speed:10000 Mtu:1500} {Name:enp4s0f1 MacAddress:a0:36:9f:e5:e9:fe Speed:10000 Mtu:1500} {Name:enp5s0f0 MacAddress:a0:36:9f:e5:e2:a8 Speed:10000 Mtu:1500} {Name:enp5s0f1 MacAddress:a0:36:9f:e5:e2:aa Speed:10000 Mtu:1500}] Topology:[{Id:0 Memory:67476070400 HugePages:[{PageSize:1048576 NumPages:0} {PageSize:2048 NumPages:0}] Cores:[{Id:0 Threads:[0 20] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:1 Threads:[2 22] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:2 Threads:[4 24] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:3 Threads:[6 26] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:4 Threads:[8 28] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:8 Threads:[10 30] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:9 Threads:[12 32] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:10 Threads:[14 34] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:11 Threads:[16 36] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:12 Threads:[18 38] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:26214400 Type:Unified Level:3}]} {Id:1 Memory:67614347264 HugePages:[{PageSize:1048576 NumPages:0} {PageSize:2048 NumPages:0}] Cores:[{Id:0 Threads:[1 21] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:1 Threads:[3 23] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:2 Threads:[5 25] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:3 Threads:[7 27] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:4 Threads:[9 29] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:8 Threads:[11 31] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:9 Threads:[13 33] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:10 Threads:[15 35] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:11 Threads:[17 37] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:12 Threads:[19 39] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:26214400 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.453643    5492 manager.go:199] Version: {KernelVersion:4.18.0-147.8.1.el8_1.x86_64 ContainerOsVersion:Red Hat Enterprise Linux CoreOS 44.81.202005250830-0 (Ootpa) DockerVersion:Unknown DockerAPIVersion:Unknown CadvisorVersion: CadvisorRevision:}
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.454069    5492 container_manager_linux.go:265] container manager verified user specified cgroup-root exists: []
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.454102    5492 container_manager_linux.go:270] Creating Container Manager object based on Node Config: {RuntimeCgroupsName:/system.slice/crio.service SystemCgroupsName:/system.slice KubeletCgroupsName: ContainerRuntime:remote CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:systemd KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[cpu:{i:{value:0 scale:0} d:{Dec:&lt;nil&gt;} s:0 Format:DecimalSI} ephemeral-storage:{i:{value:10737418240 scale:0} d:{Dec:&lt;nil&gt;} s:10Gi Format:BinarySI} memory:{i:{value:1073741824 scale:0} d:{Dec:&lt;nil&gt;} s:1Gi Format:BinarySI}] HardEvictionThresholds:[{Signal:memory.available Operator:LessThan Value:{Quantity:100Mi Percentage:0} GracePeriod:0s MinReclaim:&lt;nil&gt;} {Signal:nodefs.available Operator:LessThan Value:{Quantity:&lt;nil&gt; Percentage:0.1} GracePeriod:0s MinReclaim:&lt;nil&gt;} {Signal:nodefs.inodesFree Operator:LessThan Value:{Quantity:&lt;nil&gt; Percentage:0.05} GracePeriod:0s MinReclaim:&lt;nil&gt;} {Signal:imagefs.available Operator:LessThan Value:{Quantity:&lt;nil&gt; Percentage:0.15} GracePeriod:0s MinReclaim:&lt;nil&gt;}]} QOSReserved:map[] ExperimentalCPUManagerPolicy:static ExperimentalCPUManagerReconcilePeriod:5s ExperimentalPodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms ExperimentalTopologyManagerPolicy:none}
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.454183    5492 fake_topology_manager.go:29] [fake topologymanager] NewFakeManager
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.454189    5492 container_manager_linux.go:305] Creating device plugin manager: true
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.454201    5492 manager.go:126] Creating Device Plugin manager at /var/lib/kubelet/device-plugins/kubelet.sock
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.454215    5492 fake_topology_manager.go:39] [fake topologymanager] AddHintProvider HintProvider:  &amp;{kubelet.sock /var/lib/kubelet/device-plugins/ map[] {0 0} &lt;nil&gt; {{} [0 0 0]} 0x1b64c30 0x72dc9c8 0x1b65500 map[] map[] map[] map[] map[] 0xc000853ce0 [0 1] 0x72dc9c8}
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.454252    5492 cpu_manager.go:131] [cpumanager] detected CPU topology: &amp;{40 20 2 map[0:{0 0 0} 1:{1 1 1} 2:{0 0 2} 3:{1 1 3} 4:{0 0 4} 5:{1 1 5} 6:{0 0 6} 7:{1 1 7} 8:{0 0 8} 9:{1 1 9} 10:{0 0 10} 11:{1 1 11} 12:{0 0 12} 13:{1 1 13} 14:{0 0 14} 15:{1 1 15} 16:{0 0 16} 17:{1 1 17} 18:{0 0 18} 19:{1 1 19} 20:{0 0 0} 21:{1 1 1} 22:{0 0 2} 23:{1 1 3} 24:{0 0 4} 25:{1 1 5} 26:{0 0 6} 27:{1 1 7} 28:{0 0 8} 29:{1 1 9} 30:{0 0 10} 31:{1 1 11} 32:{0 0 12} 33:{1 1 13} 34:{0 0 14} 35:{1 1 15} 36:{0 0 16} 37:{1 1 17} 38:{0 0 18} 39:{1 1 19}]}
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: E0626 19:05:10.454300    5492 container_manager_linux.go:329] failed to initialize cpu manager: [cpumanager] unable to determine reserved CPU resources for static policy
Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: F0626 19:05:10.454309    5492 server.go:273] failed to run Kubelet: [cpumanager] unable to determine reserved CPU resources for static policy
Jun 26 19:05:10 openshift-worker-1.example.com systemd[1]: kubelet.service: Main process exited, code=exited, status=255/n/a
Jun 26 19:05:10 openshift-worker-1.example.com systemd[1]: kubelet.service: Failed with result 'exit-code'.
Jun 26 19:05:10 openshift-worker-1.example.com systemd[1]: Failed to start Kubernetes Kubelet.
Jun 26 19:05:10 openshift-worker-1.example.com systemd[1]: kubelet.service: Consumed 117ms CPU time
</code></pre>
<p>And the node will never become <code>Ready</code> in the node list.</p>
<h2 id="references">References</h2>
<ul>
<li><a href="https://www.redhat.com/en/blog/openshift-container-platform-4-how-does-machine-config-pool-work">https://www.redhat.com/en/blog/openshift-container-platform-4-how-does-machine-config-pool-work</a></li>
<li><a href="https://docs.openshift.com/container-platform/4.4/scalability_and_performance/using-cpu-manager.html">https://docs.openshift.com/container-platform/4.4/scalability_and_performance/using-cpu-manager.html</a></li>
<li><a href="https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/">https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/</a></li>
</ul>
                
                  
                    

<hr>
<div class="md-source-date">
  <small>
    
      Last update: <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">September 13, 2020</span>
    
  </small>
</div>
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        <a href="../alertmanager/" class="md-footer__link md-footer__link--prev" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              AlertManager
            </div>
          </div>
        </a>
      
      
        <a href="../crio-conmon-runc/" class="md-footer__link md-footer__link--next" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Crio vs conmon vs runc
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
        
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": ["toc.integrate"], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}, "search": "../../assets/javascripts/workers/search.fb4a9340.min.js", "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.a1c7c35e.min.js"></script>
      
    
  </body>
</html>