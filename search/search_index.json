{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Blog Use the navigation bar to move around","title":"Home"},{"location":"#blog","text":"Use the navigation bar to move around","title":"Blog"},{"location":"ceph/ceph-manual-test/","text":"Using Ceph with qemu-kvm manually Install Ceph Create a Ceph cluster according to: https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/3/html/container_guide/deploying-red-hat-ceph-storage-in-containers Installing ceph credentials to /etc/ceph Make sure that all required ceph credentials are in /etc/ceph. In this case, I copied them directly from /etc/ceph on one of my monitor nodes. Creating a pool ceph osd pool create rbd-pool 128 rbd pool init rbd-pool Downloading and customizing RHEL qcow2 Download the RHEL qcow2 from https://access.redhat.com , e.g.: https://access.redhat.com/downloads/content/69/ver=/rhel---7/7.7/x86_64/product-software Install libguestfs-tools: yum install libguestfs-tools -y Change the root password of the image: virt-customize -a rhel-server-7.8-beta-1-x86_64-kvm.qcow2 --password root:password # or, if needed: # export LIBGUESTFS_BACKEND=direct ; virt-customize -a rhel-server-7.8-beta-1-x86_64-kvm.qcow2 --password root:password Converting and uploading image into Ceph pool qemu-img convert -f qcow2 -O raw rhel-server-7.8-beta-1-x86_64-kvm.qcow2 rbd:rbd-pool/rhel-server Booting a VM from the raw Ceph image with QEMU-KVM Start a VM that directly uses the uploaded image from the Ceph pool: /usr/libexec/qemu-kvm -drive file=rbd:rbd-pool/rhel-server -nographic -m 1024 To get out of qemu-kvm, type CTRL-a x You should be able to log into the image with credentials: root / password Note: With the RHEL cloud image, the screen will show grub, then go blank for a while. This is normal, just wait for a few seconds. Booting a VM from the raw Ceph image with libvirt See: https://blog.modest-destiny.com/posts/kvm-libvirt-add-ceph-rbd-pool/ [root@undercloud-0 ~]# DISK_SIZE=$(qemu-img info rhel-server-7.8-beta-1-x86_64-kvm.raw | awk -F '[(|)]' '/virtual size/ {print $(NF-1)}' | awk '{print $1}') [root@undercloud-0 ~]# virsh vol-create-as $CEPH_POOL rhel $DISK_SIZE --format raw Vol rhel created [root@undercloud-0 ~]# virsh vol-upload --pool $CEPH_POOL rhel rhel-server-7.8-beta-1-x86_64-kvm.raw error: cannot upload to volume rhel error: this function is not supported by the connection driver: storage pool doesn't support volume upload","title":"Ceph manual test with qemu"},{"location":"ceph/ceph-manual-test/#using-ceph-with-qemu-kvm-manually","text":"","title":"Using Ceph with qemu-kvm manually"},{"location":"ceph/ceph-manual-test/#install-ceph","text":"Create a Ceph cluster according to: https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/3/html/container_guide/deploying-red-hat-ceph-storage-in-containers","title":"Install Ceph"},{"location":"ceph/ceph-manual-test/#installing-ceph-credentials-to-etcceph","text":"Make sure that all required ceph credentials are in /etc/ceph. In this case, I copied them directly from /etc/ceph on one of my monitor nodes.","title":"Installing ceph credentials to /etc/ceph"},{"location":"ceph/ceph-manual-test/#creating-a-pool","text":"ceph osd pool create rbd-pool 128 rbd pool init rbd-pool","title":"Creating a pool"},{"location":"ceph/ceph-manual-test/#downloading-and-customizing-rhel-qcow2","text":"Download the RHEL qcow2 from https://access.redhat.com , e.g.: https://access.redhat.com/downloads/content/69/ver=/rhel---7/7.7/x86_64/product-software Install libguestfs-tools: yum install libguestfs-tools -y Change the root password of the image: virt-customize -a rhel-server-7.8-beta-1-x86_64-kvm.qcow2 --password root:password # or, if needed: # export LIBGUESTFS_BACKEND=direct ; virt-customize -a rhel-server-7.8-beta-1-x86_64-kvm.qcow2 --password root:password","title":"Downloading and customizing RHEL qcow2"},{"location":"ceph/ceph-manual-test/#converting-and-uploading-image-into-ceph-pool","text":"qemu-img convert -f qcow2 -O raw rhel-server-7.8-beta-1-x86_64-kvm.qcow2 rbd:rbd-pool/rhel-server","title":"Converting and uploading image into Ceph pool"},{"location":"ceph/ceph-manual-test/#booting-a-vm-from-the-raw-ceph-image-with-qemu-kvm","text":"Start a VM that directly uses the uploaded image from the Ceph pool: /usr/libexec/qemu-kvm -drive file=rbd:rbd-pool/rhel-server -nographic -m 1024 To get out of qemu-kvm, type CTRL-a x You should be able to log into the image with credentials: root / password Note: With the RHEL cloud image, the screen will show grub, then go blank for a while. This is normal, just wait for a few seconds.","title":"Booting a VM from the raw Ceph image with QEMU-KVM"},{"location":"ceph/ceph-manual-test/#booting-a-vm-from-the-raw-ceph-image-with-libvirt","text":"See: https://blog.modest-destiny.com/posts/kvm-libvirt-add-ceph-rbd-pool/ [root@undercloud-0 ~]# DISK_SIZE=$(qemu-img info rhel-server-7.8-beta-1-x86_64-kvm.raw | awk -F '[(|)]' '/virtual size/ {print $(NF-1)}' | awk '{print $1}') [root@undercloud-0 ~]# virsh vol-create-as $CEPH_POOL rhel $DISK_SIZE --format raw Vol rhel created [root@undercloud-0 ~]# virsh vol-upload --pool $CEPH_POOL rhel rhel-server-7.8-beta-1-x86_64-kvm.raw error: cannot upload to volume rhel error: this function is not supported by the connection driver: storage pool doesn't support volume upload","title":"Booting a VM from the raw Ceph image with libvirt"},{"location":"linux/cgroups/","text":"Quick guide for cgroups What are cgroups? Summary - what is a cgroup cgroups ... stand for control groups handle management, accounting of system resources like CPU, memory, I/O associates a set of tasks with a set of parameters for one or more subsystems on their own allow for simple job tracking combined with other subsystems (so-called resource controllers) allow for resource accounting / monitoring / limiting of resources provided through the cgroupfs pseudo filesystem are organized in hierarchies - hierarchies are defined by creating subdirectories in the cgroup filessystem each process is in exactly one node in each hierarchy (cpu hierarchy, memory hierarchy, ...) limit how much you can use of a system resource (quantity) Definitions - what is a cgroup The following is the definition of cgroups v1 from the kernel documentation: https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt : (...) A *cgroup* associates a set of tasks with a set of parameters for one or more subsystems. A *subsystem* is a module that makes use of the task grouping facilities provided by cgroups to treat groups of tasks in particular ways. A subsystem is typically a \"resource controller\" that schedules a resource or applies per-cgroup limits, but it may be anything that wants to act on a group of processes, e.g. a virtualization subsystem. A *hierarchy* is a set of cgroups arranged in a tree, such that every task in the system is in exactly one of the cgroups in the hierarchy, and a set of subsystems; each subsystem has system-specific state attached to each cgroup in the hierarchy. Each hierarchy has an instance of the cgroup virtual filesystem associated with it. (...) On their own, the only use for cgroups is for simple job tracking. The intention is that other subsystems hook into the generic cgroup support to provide new attributes for cgroups, such as accounting/limiting the resources which processes in a cgroup can access. For example, cpusets (see Documentation/cgroup-v1/cpusets.txt) allow you to associate a set of CPUs and a set of memory nodes with the tasks in each cgroup. (...) man 7 cgroups : (...) Control cgroups, usually referred to as cgroups, are a Linux kernel feature which allow processes to be organized into hierarchical groups whose usage of various types of resources can then be limited and monitored. The kernel's cgroup interface is provided through a pseudo-filesystem called cgroupfs. Grouping is implemented in the core cgroup kernel code, while resource tracking and limits are implemented in a set of per-resource-type subsystems (memory, CPU, and so on). (...) Subsystems are sometimes also known as resource controllers (or simply, controllers). (...) The cgroups for a controller are arranged in a hierarchy. This hierarchy is defined by creating, removing, and renaming subdirectories within the cgroup filesystem. At each level of the hierarchy, attributes (e.g., limits) can be defined. The limits, control, and accounting provided by cgroups generally have effect throughout the subhierarchy underneath the cgroup where the attributes are defined. Thus, for example, the limits placed on a cgroup at a higher level in the hierarchy cannot be exceeded by descendant cgroups. (...) https://lwn.net/Articles/679786/ : The cgroup subsystem and associated controllers handle management and accounting of various system resources like CPU, memory, I/O, and more. Together with the Linux namespace subsystem, which is a bit older (having started around 2002) and is considered a bit more mature (apart, perhaps, from user namespaces, which still raise discussions), these subsystems form the basis of Linux containers. Currently, most projects involving Linux containers, like Docker, LXC, OpenVZ, Kubernetes, and others, are based on both of them. The development of the Linux cgroup subsystem started in 2006 at Google, led primarily by Rohit Seth and Paul Menage. Initially the project was called \"Process Containers\", but later on the name was changed to \"Control Groups\", to avoid confusion with Linux containers, and nowadays everybody calls them \"cgroups\" for short. There are currently 12 cgroup controllers in cgroups v1; all\u2014except one\u2014have existed for several years. The new addition is the PIDs controller, developed by Aditya Kali and merged in kernel 4.3. It allows restricting the number of processes created inside a control group, and it can be used as an anti-fork-bomb solution. The PID space in Linux consists of, at a maximum, about four million PIDs (PID_MAX_LIMIT). Given today's RAM capacities, this limit could easily and quite quickly be exhausted by a fork bomb from within a single container. The PIDs controller is supported by both cgroups v1 and cgroups v2. Over the years, there was a lot of criticism about the implementation of cgroups, which seems to present a number of inconsistencies and a lot of chaos. For example, when creating subgroups (cgroups within cgroups), several cgroup controllers propagate parameters to their immediate subgroups, while other controllers do not. Or, for a different example, some controllers use interface files (such as the cpuset controller's clone_children) that appear in all controllers even though they only affect one. As maintainer Tejun Heo himself has admitted [YouTube], \"design followed implementation\", \"different decisions were taken for different controllers\", and \"sometimes too much flexibility causes a hindrance\". In an LWN article from 2012, it was said that \"control groups are one of those features that kernel developers love to hate.\" The relationship of containers and cgroups Containers are basically just a bunch of cgroups plus namespace isolation (plus some extra features): https://en.wikipedia.org/wiki/LXC : The Linux kernel provides the cgroups functionality that allows limitation and prioritization of resources (CPU, memory, block I/O, network, etc.) without the need for starting any virtual machines, and also namespace isolation functionality that allows complete isolation of an applications' view of the operating environment, including process trees, networking, user IDs and mounted file systems.[3] LXC combines the kernel's cgroups and support for isolated namespaces to provide an isolated environment for applications. Early versions of Docker used LXC as the container execution driver, though LXC was made optional in v0.9 and support was dropped in Docker v1.10. [4] https://en.wikipedia.org/wiki/Docker_(software) : Docker is developed primarily for Linux, where it uses the resource isolation features of the Linux kernel such as cgroups and kernel namespaces, and a union-capable file system such as OverlayFS and others[28] to allow independent \"containers\" to run within a single Linux instance, avoiding the overhead of starting and maintaining virtual machines (VMs).[29] The Linux kernel's support for namespaces mostly[30] isolates an application's view of the operating environment, including process trees, network, user IDs and mounted file systems, while the kernel's cgroups provide resource limiting for memory and CPU.[31] Since version 0.9, Docker includes the libcontainer library as its own way to directly use virtualization facilities provided by the Linux kernel, in addition to using abstracted virtualization interfaces via libvirt, LXC and systemd-nspawn.[13][32][27] cgroups versions cgroup comes in 2 versions. cgroups v2 are to replace cgroups v1 eventually. However, for reasons of backwards compatibility, both will probably be around for a very long time. cgroups v1 have several issues ... uncoordinated development of resource controllers inconsistencies between controllers complex hierarchy management Solution: cgroups v2. man 7 cgroups : (...) The initial release of the cgroups implementation was in Linux 2.6.24. Over time, various cgroup controllers have been added to allow the management of various types of resources. However, the development of these controllers was largely uncoordi\u2010 nated, with the result that many inconsistencies arose between controllers and man\u2010 agement of the cgroup hierarchies became rather complex. (A longer description of these problems can be found in the kernel source file Documentation/cgroup-v2.txt.) (...) Backwards compatibility cgroups v1 is unlikely to be removed cgroups v1 and v2 can coexist cgroups v2 only implemented a subset of v1's functionality users can use resource controllers supported in v2 and use v1 controllers for features which are unsupported in v2 man 7 cgroups : (...) Although cgroups v2 is intended as a replacement for cgroups v1, the older system continues to exist (and for compatibility reasons is unlikely to be removed). Cur\u2010 rently, cgroups v2 implements only a subset of the controllers available in cgroups v1. The two systems are implemented so that both v1 controllers and v2 controllers can be mounted on the same system. Thus, for example, it is possible to use those controllers that are supported under version 2, while also using version 1 con\u2010 trollers where version 2 does not yet support those controllers. The only restric\u2010 tion here is that a controller can't be simultaneously employed in both a cgroups v1 hierarchy and in the cgroups v2 hierarchy. (...) Which version of cgroups are you running? cgroups are mounted as a virtual filesystem. Hence, verify with the mount command which version is currently in use. Default in RHEL 7 RHEL 7 uses cgroups v1: [root@rhospbl-1 ~]# mount | grep cgroup tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,seclabel,mode=755) cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd) cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_prio,net_cls) cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio) cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices) cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb) cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer) cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct,cpu) cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset) cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event) cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory) cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids) [root@rhospbl-1 ~]# uname -a Linux rhospbl-1.openstack.gsslab.rdu2.redhat.com 3.10.0-693.el7.x86_64 #1 SMP Thu Jul 6 19:56:57 EDT 2017 x86_64 x86_64 x86_64 GNU/Linux Default in Fedora 28 [akaris@wks-akaris blog]$ cat /etc/redhat-release Fedora release 28 (Twenty Eight) [akaris@wks-akaris blog]$ mount | grep cgroup tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,seclabel,mode=755) cgroup2 on /sys/fs/cgroup/unified type cgroup2 (rw,nosuid,nodev,noexec,relatime,seclabel,nsdelegate) cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,xattr,name=systemd) cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,cpu,cpuacct) cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,hugetlb) cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,perf_event) cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,memory) cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,pids) cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,freezer) cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,net_cls,net_prio) cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,blkio) cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,devices) cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,cpuset) Default in RHEL 8 [root@rhel8 ~]# cat /etc/redhat-release Red Hat Enterprise Linux release 8.0 Beta (Ootpa) [root@rhel8 ~]# mount | grep cgroup tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,seclabel,mode=755) cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd) cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,freezer) cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,cpu,cpuacct) cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,memory) cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,net_cls,net_prio) cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,pids) cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,blkio) cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,hugetlb) cgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,rdma) cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,devices) cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,cpuset) cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,perf_event) Support for cgroupv2? In RHEL 7 No, cgroups v2 is not in the kernel in RHEL 7: [root@rhospbl-1 ~]# mount -t cgroup2 none /mnt/test mount: unknown filesystem type 'cgroup2' In RHEL 8 Yes: [root@rhel8 ~]# mount cgroup2 -t cgroup2 /mnt/cgroupv2 [root@rhel8 ~]# mount | grep cgroup2 cgroup2 on /mnt/cgroupv2 type cgroup2 (rw,relatime,seclabel) [root@rhel8 ~]# Types of cgroups v1 resource controllers cpu resource controller cpu tracking based on cgroups cpuset resource controller https://www.kernel.org/doc/Documentation/cgroup-v1/hugetlb.txt : [root@overcloud-controller-0 cgroup]# cd /sys/fs/cgroup/cpuset/ [root@overcloud-controller-0 cpuset]# mkdir test [root@overcloud-controller-0 test]# echo 2-3 > cpuset.cpus [root@overcloud-controller-0 test]# cat cpuset.cpus 2-3 [root@overcloud-controller-0 test]# dd if=/dev/zero of=/dev/null & [2] 931866 [root@overcloud-controller-0 test]# taskset -p -c 931866 pid 931866's current affinity list: 0-3 [root@overcloud-controller-0 test]# echo 931866 > tasks [root@overcloud-controller-0 test]# taskset -p -c 931866 pid 931866's current affinity list: 2,3 hugetlb resource controller controls amount of hugepages usable by a process by default, a process can request as many hugepages as it wants Looking at meminfo, we see that 4 hugepages are used: [root@overcloud-computesriov-0 system.slice]# cat /sys/devices/system/node/node?/meminfo | grep -i huge Node 0 AnonHugePages: 4096 kB Node 0 HugePages_Total: 16 Node 0 HugePages_Free: 12 Node 0 HugePages_Surp: 0 Node 1 AnonHugePages: 2048 kB Node 1 HugePages_Total: 16 Node 1 HugePages_Free: 16 Node 1 HugePages_Surp: 0 One way to find out which processes are using hugepages, is to check the hugetlb cgroups: [root@overcloud-computesriov-0 ~]# cd /sys/fs/cgroup/hugetlb [root@overcloud-computesriov-0 hugetlb]# ll total 0 -rw-r--r--. 1 root root 0 Nov 27 06:17 cgroup.clone_children --w--w--w-. 1 root root 0 Nov 27 06:17 cgroup.event_control -rw-r--r--. 1 root root 0 Nov 27 06:17 cgroup.procs -r--r--r--. 1 root root 0 Nov 27 06:17 cgroup.sane_behavior -rw-r--r--. 1 root root 0 Nov 27 06:17 hugetlb.1GB.failcnt -rw-r--r--. 1 root root 0 Nov 27 06:17 hugetlb.1GB.limit_in_bytes -rw-r--r--. 1 root root 0 Nov 27 06:17 hugetlb.1GB.max_usage_in_bytes -r--r--r--. 1 root root 0 Nov 27 06:17 hugetlb.1GB.usage_in_bytes -rw-r--r--. 1 root root 0 Nov 27 06:17 notify_on_release -rw-r--r--. 1 root root 0 Nov 27 06:17 release_agent drwxr-xr-x. 11 root root 0 Nov 27 06:46 system.slice -rw-r--r--. 1 root root 0 Nov 27 06:17 tasks [root@overcloud-computesriov-0 hugetlb]# cat hugetlb.1GB.usage_in_bytes 4294967296 [root@overcloud-computesriov-0 hugetlb]# cd system.slice [root@overcloud-computesriov-0 system.slice]# ll total 0 -rw-r--r--. 1 root root 0 Nov 27 06:25 cgroup.clone_children --w--w--w-. 1 root root 0 Nov 27 06:25 cgroup.event_control -rw-r--r--. 1 root root 0 Nov 27 06:25 cgroup.procs drwxr-xr-x. 2 root root 0 Nov 27 06:46 docker-111c9c039324d640875e550763d6507450cbbd07f6674c3883388839807cd614.scope drwxr-xr-x. 2 root root 0 Nov 27 06:51 docker-1286b301010bb53e3d919616054e645c00a2288b9cdc8235bdb68aa404a0c34b.scope drwxr-xr-x. 2 root root 0 Nov 27 06:46 docker-46ed5e7b2045df285552bde12209717f1601b27e3d6e137ed9122d3d9c519a3d.scope drwxr-xr-x. 2 root root 0 Nov 27 06:51 docker-90030441400dd9536aa33d13d3d5792a4e1f025fb383141cb0f18cfaed260979.scope drwxr-xr-x. 2 root root 0 Nov 27 06:46 docker-a94d9089fac5ed4b1dbe48b0f5460536462c49e0ff14fd4059fbae7a7dbd1b4d.scope drwxr-xr-x. 2 root root 0 Nov 27 06:51 docker-b9ff1a61cc4f144cc2aa16332d8e07248ad71a7263056b0d9cddb7339368457a.scope drwxr-xr-x. 2 root root 0 Nov 27 06:51 docker-c538d8c4e222b977b218746ee9ebef34335d768a364fbe1bfb3e72284d65520a.scope drwxr-xr-x. 2 root root 0 Nov 27 06:51 docker-d2e87b4ec13cc51c9dab3e593ee44051eb243ce684cc300b7e6103e8f35e1320.scope drwxr-xr-x. 2 root root 0 Nov 27 06:51 docker-dce63a96f513527b894bbec6c7f39f40dd2912bdbf4dec0a51b2e59704c03e7b.scope -rw-r--r--. 1 root root 0 Nov 27 06:25 hugetlb.1GB.failcnt -rw-r--r--. 1 root root 0 Nov 27 06:25 hugetlb.1GB.limit_in_bytes -rw-r--r--. 1 root root 0 Nov 27 06:25 hugetlb.1GB.max_usage_in_bytes -r--r--r--. 1 root root 0 Nov 27 06:25 hugetlb.1GB.usage_in_bytes -rw-r--r--. 1 root root 0 Nov 27 06:25 notify_on_release -rw-r--r--. 1 root root 0 Nov 27 06:25 tasks [root@overcloud-computesriov-0 system.slice]# cat hugetlb.1GB.usage_in_bytes 4294967296 [root@overcloud-computesriov-0 system.slice]# find . -name '*usage_in_bytes' ./docker-b9ff1a61cc4f144cc2aa16332d8e07248ad71a7263056b0d9cddb7339368457a.scope/hugetlb.1GB.max_usage_in_bytes ./docker-b9ff1a61cc4f144cc2aa16332d8e07248ad71a7263056b0d9cddb7339368457a.scope/hugetlb.1GB.usage_in_bytes ./docker-dce63a96f513527b894bbec6c7f39f40dd2912bdbf4dec0a51b2e59704c03e7b.scope/hugetlb.1GB.max_usage_in_bytes ./docker-dce63a96f513527b894bbec6c7f39f40dd2912bdbf4dec0a51b2e59704c03e7b.scope/hugetlb.1GB.usage_in_bytes ./docker-c538d8c4e222b977b218746ee9ebef34335d768a364fbe1bfb3e72284d65520a.scope/hugetlb.1GB.max_usage_in_bytes ./docker-c538d8c4e222b977b218746ee9ebef34335d768a364fbe1bfb3e72284d65520a.scope/hugetlb.1GB.usage_in_bytes ./docker-d2e87b4ec13cc51c9dab3e593ee44051eb243ce684cc300b7e6103e8f35e1320.scope/hugetlb.1GB.max_usage_in_bytes ./docker-d2e87b4ec13cc51c9dab3e593ee44051eb243ce684cc300b7e6103e8f35e1320.scope/hugetlb.1GB.usage_in_bytes ./docker-90030441400dd9536aa33d13d3d5792a4e1f025fb383141cb0f18cfaed260979.scope/hugetlb.1GB.max_usage_in_bytes ./docker-90030441400dd9536aa33d13d3d5792a4e1f025fb383141cb0f18cfaed260979.scope/hugetlb.1GB.usage_in_bytes ./docker-1286b301010bb53e3d919616054e645c00a2288b9cdc8235bdb68aa404a0c34b.scope/hugetlb.1GB.max_usage_in_bytes ./docker-1286b301010bb53e3d919616054e645c00a2288b9cdc8235bdb68aa404a0c34b.scope/hugetlb.1GB.usage_in_bytes ./docker-111c9c039324d640875e550763d6507450cbbd07f6674c3883388839807cd614.scope/hugetlb.1GB.max_usage_in_bytes ./docker-111c9c039324d640875e550763d6507450cbbd07f6674c3883388839807cd614.scope/hugetlb.1GB.usage_in_bytes ./docker-a94d9089fac5ed4b1dbe48b0f5460536462c49e0ff14fd4059fbae7a7dbd1b4d.scope/hugetlb.1GB.max_usage_in_bytes ./docker-a94d9089fac5ed4b1dbe48b0f5460536462c49e0ff14fd4059fbae7a7dbd1b4d.scope/hugetlb.1GB.usage_in_bytes ./docker-46ed5e7b2045df285552bde12209717f1601b27e3d6e137ed9122d3d9c519a3d.scope/hugetlb.1GB.max_usage_in_bytes ./docker-46ed5e7b2045df285552bde12209717f1601b27e3d6e137ed9122d3d9c519a3d.scope/hugetlb.1GB.usage_in_bytes ./hugetlb.1GB.max_usage_in_bytes ./hugetlb.1GB.usage_in_bytes [root@overcloud-computesriov-0 system.slice]# find . -name '*usage_in_bytes' | while read line; do echo $line ; cat $line ; done ./docker-b9ff1a61cc4f144cc2aa16332d8e07248ad71a7263056b0d9cddb7339368457a.scope/hugetlb.1GB.max_usage_in_bytes 0 ./docker-b9ff1a61cc4f144cc2aa16332d8e07248ad71a7263056b0d9cddb7339368457a.scope/hugetlb.1GB.usage_in_bytes 0 ./docker-dce63a96f513527b894bbec6c7f39f40dd2912bdbf4dec0a51b2e59704c03e7b.scope/hugetlb.1GB.max_usage_in_bytes 0 ./docker-dce63a96f513527b894bbec6c7f39f40dd2912bdbf4dec0a51b2e59704c03e7b.scope/hugetlb.1GB.usage_in_bytes 0 ./docker-c538d8c4e222b977b218746ee9ebef34335d768a364fbe1bfb3e72284d65520a.scope/hugetlb.1GB.max_usage_in_bytes 0 ./docker-c538d8c4e222b977b218746ee9ebef34335d768a364fbe1bfb3e72284d65520a.scope/hugetlb.1GB.usage_in_bytes 0 ./docker-d2e87b4ec13cc51c9dab3e593ee44051eb243ce684cc300b7e6103e8f35e1320.scope/hugetlb.1GB.max_usage_in_bytes 0 ./docker-d2e87b4ec13cc51c9dab3e593ee44051eb243ce684cc300b7e6103e8f35e1320.scope/hugetlb.1GB.usage_in_bytes 0 ./docker-90030441400dd9536aa33d13d3d5792a4e1f025fb383141cb0f18cfaed260979.scope/hugetlb.1GB.max_usage_in_bytes 0 ./docker-90030441400dd9536aa33d13d3d5792a4e1f025fb383141cb0f18cfaed260979.scope/hugetlb.1GB.usage_in_bytes 0 ./docker-1286b301010bb53e3d919616054e645c00a2288b9cdc8235bdb68aa404a0c34b.scope/hugetlb.1GB.max_usage_in_bytes 0 ./docker-1286b301010bb53e3d919616054e645c00a2288b9cdc8235bdb68aa404a0c34b.scope/hugetlb.1GB.usage_in_bytes 0 ./docker-111c9c039324d640875e550763d6507450cbbd07f6674c3883388839807cd614.scope/hugetlb.1GB.max_usage_in_bytes 0 ./docker-111c9c039324d640875e550763d6507450cbbd07f6674c3883388839807cd614.scope/hugetlb.1GB.usage_in_bytes 0 ./docker-a94d9089fac5ed4b1dbe48b0f5460536462c49e0ff14fd4059fbae7a7dbd1b4d.scope/hugetlb.1GB.max_usage_in_bytes 4294967296 ./docker-a94d9089fac5ed4b1dbe48b0f5460536462c49e0ff14fd4059fbae7a7dbd1b4d.scope/hugetlb.1GB.usage_in_bytes 4294967296 ./docker-46ed5e7b2045df285552bde12209717f1601b27e3d6e137ed9122d3d9c519a3d.scope/hugetlb.1GB.max_usage_in_bytes 0 ./docker-46ed5e7b2045df285552bde12209717f1601b27e3d6e137ed9122d3d9c519a3d.scope/hugetlb.1GB.usage_in_bytes 0 ./hugetlb.1GB.max_usage_in_bytes 4294967296 ./hugetlb.1GB.usage_in_bytes 4294967296 [root@overcloud-computesriov-0 system.slice]# docker ps | grep a94d9089 a94d9089fac5 registry.access.redhat.com/rhosp13/openstack-nova-libvirt:13.0-72 \"kolla_start\" 3 days ago Up 3 days nova_libvirt [root@overcloud-computesriov-0 system.slice]# systemctl status docker-a94d9089fac5ed4b1dbe48b0f5460536462c49e0ff14fd4059fbae7a7dbd1b4d.scope \u25cf docker-a94d9089fac5ed4b1dbe48b0f5460536462c49e0ff14fd4059fbae7a7dbd1b4d.scope - libcontainer container a94d9089fac5ed4b1dbe48b0f5460536462c49e0ff14fd4059fbae7a7dbd1b4d Loaded: loaded (/run/systemd/system/docker-a94d9089fac5ed4b1dbe48b0f5460536462c49e0ff14fd4059fbae7a7dbd1b4d.scope; static; vendor preset: disabled) Drop-In: /run/systemd/system/docker-a94d9089fac5ed4b1dbe48b0f5460536462c49e0ff14fd4059fbae7a7dbd1b4d.scope.d \u2514\u250050-BlockIOAccounting.conf, 50-CPUAccounting.conf, 50-DefaultDependencies.conf, 50-Delegate.conf, 50-Description.conf, 50-MemoryAccounting.conf, 50-Slice.conf Active: active (running) since Tue 2018-11-27 06:46:11 UTC; 3 days ago Tasks: 18 Memory: 14.8M CGroup: /system.slice/docker-a94d9089fac5ed4b1dbe48b0f5460536462c49e0ff14fd4059fbae7a7dbd1b4d.scope \u2514\u250033304 /usr/sbin/libvirtd Nov 27 06:46:11 overcloud-computesriov-0 systemd[1]: Started libcontainer container a94d9089fac5ed4b1dbe48b0f5460536462c49e0ff14fd4059fbae7a7dbd1b4d. Nov 27 06:46:11 overcloud-computesriov-0 sudo[33325]: root : TTY=unknown ; PWD=/ ; USER=root ; COMMAND=/usr/local/bin/kolla_set_configs [root@overcloud-computesriov-0 system.slice]# memory cgroup keep track of pages used by each group allow the OOM (out of memory) killer to trigger on a specific memory cgroup only kernel can \"freeze\" the cgroup blockio cgroup keep track of I/Os for each group throttle each group writes go through page cache unless O_DIRECT is set net_cls cgroup automatically set traffic classs for egress traffic (use tc/iptables) net_prio cgroup automatically set traffic classs for egress traffic (use queuing disciplines) devices cgroup which group can read/write from which device in /dev freezer cgroup used to freeze / stop all processes in a group (SIGSTOP / SIGCONT) mounting, unmounting and comounting cgroup v1 resource controllers man 7 cgroups (...) It is possible to comount multiple controllers against the same hierarchy. For example, here the cpu and cpuacct controllers are comounted against a single hierar\u2010 chy: mount -t cgroup -o cpu,cpuacct none /sys/fs/cgroup/cpu,cpuacct Comounting controllers has the effect that a process is in the same cgroup for all of the comounted controllers. Separately mounting controllers allows a process to be in cgroup /foo1 for one controller while being in /foo2/foo3 for another. It is possible to comount all v1 controllers against the same hierarchy: mount -t cgroup -o all cgroup /sys/fs/cgroup (One can achieve the same result by omitting -o all, since it is the default if no controllers are explicitly specified.) (...) [akaris@wks-akaris blog]$ mount | grep cgroup tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,seclabel,mode=755) cgroup2 on /sys/fs/cgroup/unified type cgroup2 (rw,nosuid,nodev,noexec,relatime,seclabel,nsdelegate) cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,xattr,name=systemd) cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,cpu,cpuacct) cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,hugetlb) cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,perf_event) cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,memory) cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,pids) cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,freezer) cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,net_cls,net_prio) cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,blkio) cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,devices) cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,cpuset) [akaris@wks-akaris blog]$ ls /sys/fs/cgroup blkio cpu cpuacct cpu,cpuacct cpuset devices freezer hugetlb memory net_cls net_cls,net_prio net_prio perf_event pids systemd unified [akaris@wks-akaris blog]$ man 7 cgroup (...) It is not possible to mount the same controller against multiple cgroup hierarchies. For example, it is not possible to mount both the cpu and cpuacct controllers against one hierarchy, and to mount the cpu controller alone against another hierar\u2010 chy. It is possible to create multiple mount points with exactly the same set of comounted controllers. However, in this case all that results is multiple mount points providing a view of the same hierarchy. (...) [root@rhel8 ~]# mount | grep cgroup tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,seclabel,mode=755) cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd) cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,freezer) cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,cpu,cpuacct) cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,memory) cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,net_cls,net_prio) cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,pids) cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,blkio) cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,hugetlb) cgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,rdma) cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,devices) cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,cpuset) cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,perf_event) cgroup2 on /mnt/cgroupv2 type cgroup2 (rw,relatime,seclabel) [root@rhel8 ~]# mount group -t net_cls /sys/fs/cgroup/net_cls mount: /sys/fs/cgroup/net_cls,net_prio: unknown filesystem type 'net_cls'. [root@rhel8 ~]# mount group -t cgroup -o net_cls /sys/fs/cgroup/net_cls mount: /sys/fs/cgroup/net_cls,net_prio: group already mounted or mount point busy. [root@rhel8 ~]# mount group -t cgroup -o net_cls /mnt mount: /mnt: group already mounted or mount point busy. [root@rhel8 ~]# mount group -t cgroup -o net_cls,net_prio /mnt [root@rhel8 ~]# umount /mnt man 7 cgroups > (...) Unmounting v1 controllers A mounted cgroup filesystem can be unmounted using the umount(8) command, as in the following example: umount /sys/fs/cgroup/pids But note well: a cgroup filesystem is unmounted only if it is not busy, that is, it has no child cgroups. If this is not the case, then the only effect of the umount(8) is to make the mount invisible. Thus, to ensure that the mount point is really removed, one must first remove all child cgroups, which in turn can be done only after all member processes have been moved from those cgroups to the root cgroup. (...) Processes in cgroups Processes start in the same cgroups as their parent A process can be moved by: echo $id > /sys/fs/cgroup/.../tasks Listing the cgroups that a process is in https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt : Each task under /proc has an added file named 'cgroup' displaying, for each active hierarchy, the subsystem names and the cgroup name as the path relative to the root of the cgroup file system. Example: [root@rhospbl-1 ~]# cat /proc/$(pidof libvirtd)/cgroup 11:pids:/ 10:memory:/system.slice 9:perf_event:/ 8:cpuset:/ 7:cpuacct,cpu:/system.slice 6:freezer:/ 5:hugetlb:/ 4:devices:/system.slice/libvirtd.service 3:blkio:/system.slice 2:net_prio,net_cls:/ 1:name=systemd:/system.slice/libvirtd.service notify_on_release If the notify_on_release flag is enabled (1) in a cgroup, then whenever the last task in the cgroup leaves (exits or attaches to some other cgroup) and the last child cgroup of that cgroup is removed, then the kernel runs the command specified by the contents of the \"release_agent\" file in that hierarchy's root directory, supplying the pathname (relative to the mount point of the cgroup file system) of the abandoned cgroup. This enables automatic removal of abandoned cgroups. The default value of notify_on_release in the root cgroup at system boot is disabled (0). The default value of other cgroups at creation is the current value of their parents' notify_on_release settings. The default value of a cgroup hierarchy's release_agent path is empty. Example: [root@overcloud-controller-0 ~]# systemctl status session-c2.scope \u25cf session-c2.scope - Session c2 of user rabbitmq Loaded: loaded (/run/systemd/system/session-c2.scope; static; vendor preset: disabled) Drop-In: /run/systemd/system/session-c2.scope.d \u2514\u250050-After-systemd-logind\\x2eservice.conf, 50-After-systemd-user-sessions\\x2eservice.conf, 50-Description.conf, 50-SendSIGHUP.conf, 50-Slice.conf, 50-TasksMax.conf Active: active (abandoned) since Wed 2018-11-07 22:40:43 UTC; 2 weeks 5 days ago CGroup: /user.slice/user-975.slice/session-c2.scope \u2514\u250022384 /usr/lib64/erlang/erts-7.3.1.4/bin/epmd -daemon Warning: Journal has been rotated since unit was started. Log output is incomplete or unavailable. [root@overcloud-controller-0 ~]# cat /sys/fs/cgroup/ blkio/ cpu,cpuacct/ freezer/ net_cls/ perf_event/ cpu/ cpuset/ hugetlb/ net_cls,net_prio/ pids/ cpuacct/ devices/ memory/ net_prio/ systemd/ [root@overcloud-controller-0 ~]# cat /sys/fs/cgroup/ blkio/ cpu,cpuacct/ freezer/ net_cls/ perf_event/ cpu/ cpuset/ hugetlb/ net_cls,net_prio/ pids/ cpuacct/ devices/ memory/ net_prio/ systemd/ [root@overcloud-controller-0 ~]# cat /sys/fs/cgroup/ blkio/ cpu,cpuacct/ freezer/ net_cls/ perf_event/ cpu/ cpuset/ hugetlb/ net_cls,net_prio/ pids/ cpuacct/ devices/ memory/ net_prio/ systemd/ [root@overcloud-controller-0 ~]# cat /sys/fs/cgroup/systemd/ cgroup.clone_children cgroup.procs machine.slice/ release_agent tasks cgroup.event_control cgroup.sane_behavior notify_on_release system.slice/ user.slice/ [root@overcloud-controller-0 ~]# cat /sys/fs/cgroup/systemd/ cgroup.clone_children cgroup.procs machine.slice/ release_agent tasks cgroup.event_control cgroup.sane_behavior notify_on_release system.slice/ user.slice/ [root@overcloud-controller-0 ~]# cat /sys/fs/cgroup/systemd/user.slice/user-975.slice/session-c2.scope/ cgroup.clone_children cgroup.event_control cgroup.procs notify_on_release tasks [root@overcloud-controller-0 ~]# cat /sys/fs/cgroup/systemd/user.slice/user-975.slice/session-c2.scope/notify_on_release 1 Resources https://en.wikipedia.org/wiki/Cgroups https://www.kernel.org/doc/Documentation/cgroup-v1/ https://lwn.net/Articles/679786/ https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/resource_management_guide/index man 7 cgroups https://www.youtube.com/watch?v=sK5i-N34im8","title":"cgroups"},{"location":"linux/cgroups/#quick-guide-for-cgroups","text":"","title":"Quick guide for cgroups"},{"location":"linux/cgroups/#what-are-cgroups","text":"","title":"What are cgroups?"},{"location":"linux/cgroups/#summary-what-is-a-cgroup","text":"cgroups ... stand for control groups handle management, accounting of system resources like CPU, memory, I/O associates a set of tasks with a set of parameters for one or more subsystems on their own allow for simple job tracking combined with other subsystems (so-called resource controllers) allow for resource accounting / monitoring / limiting of resources provided through the cgroupfs pseudo filesystem are organized in hierarchies - hierarchies are defined by creating subdirectories in the cgroup filessystem each process is in exactly one node in each hierarchy (cpu hierarchy, memory hierarchy, ...) limit how much you can use of a system resource (quantity)","title":"Summary - what is a cgroup"},{"location":"linux/cgroups/#definitions-what-is-a-cgroup","text":"The following is the definition of cgroups v1 from the kernel documentation: https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt : (...) A *cgroup* associates a set of tasks with a set of parameters for one or more subsystems. A *subsystem* is a module that makes use of the task grouping facilities provided by cgroups to treat groups of tasks in particular ways. A subsystem is typically a \"resource controller\" that schedules a resource or applies per-cgroup limits, but it may be anything that wants to act on a group of processes, e.g. a virtualization subsystem. A *hierarchy* is a set of cgroups arranged in a tree, such that every task in the system is in exactly one of the cgroups in the hierarchy, and a set of subsystems; each subsystem has system-specific state attached to each cgroup in the hierarchy. Each hierarchy has an instance of the cgroup virtual filesystem associated with it. (...) On their own, the only use for cgroups is for simple job tracking. The intention is that other subsystems hook into the generic cgroup support to provide new attributes for cgroups, such as accounting/limiting the resources which processes in a cgroup can access. For example, cpusets (see Documentation/cgroup-v1/cpusets.txt) allow you to associate a set of CPUs and a set of memory nodes with the tasks in each cgroup. (...) man 7 cgroups : (...) Control cgroups, usually referred to as cgroups, are a Linux kernel feature which allow processes to be organized into hierarchical groups whose usage of various types of resources can then be limited and monitored. The kernel's cgroup interface is provided through a pseudo-filesystem called cgroupfs. Grouping is implemented in the core cgroup kernel code, while resource tracking and limits are implemented in a set of per-resource-type subsystems (memory, CPU, and so on). (...) Subsystems are sometimes also known as resource controllers (or simply, controllers). (...) The cgroups for a controller are arranged in a hierarchy. This hierarchy is defined by creating, removing, and renaming subdirectories within the cgroup filesystem. At each level of the hierarchy, attributes (e.g., limits) can be defined. The limits, control, and accounting provided by cgroups generally have effect throughout the subhierarchy underneath the cgroup where the attributes are defined. Thus, for example, the limits placed on a cgroup at a higher level in the hierarchy cannot be exceeded by descendant cgroups. (...) https://lwn.net/Articles/679786/ : The cgroup subsystem and associated controllers handle management and accounting of various system resources like CPU, memory, I/O, and more. Together with the Linux namespace subsystem, which is a bit older (having started around 2002) and is considered a bit more mature (apart, perhaps, from user namespaces, which still raise discussions), these subsystems form the basis of Linux containers. Currently, most projects involving Linux containers, like Docker, LXC, OpenVZ, Kubernetes, and others, are based on both of them. The development of the Linux cgroup subsystem started in 2006 at Google, led primarily by Rohit Seth and Paul Menage. Initially the project was called \"Process Containers\", but later on the name was changed to \"Control Groups\", to avoid confusion with Linux containers, and nowadays everybody calls them \"cgroups\" for short. There are currently 12 cgroup controllers in cgroups v1; all\u2014except one\u2014have existed for several years. The new addition is the PIDs controller, developed by Aditya Kali and merged in kernel 4.3. It allows restricting the number of processes created inside a control group, and it can be used as an anti-fork-bomb solution. The PID space in Linux consists of, at a maximum, about four million PIDs (PID_MAX_LIMIT). Given today's RAM capacities, this limit could easily and quite quickly be exhausted by a fork bomb from within a single container. The PIDs controller is supported by both cgroups v1 and cgroups v2. Over the years, there was a lot of criticism about the implementation of cgroups, which seems to present a number of inconsistencies and a lot of chaos. For example, when creating subgroups (cgroups within cgroups), several cgroup controllers propagate parameters to their immediate subgroups, while other controllers do not. Or, for a different example, some controllers use interface files (such as the cpuset controller's clone_children) that appear in all controllers even though they only affect one. As maintainer Tejun Heo himself has admitted [YouTube], \"design followed implementation\", \"different decisions were taken for different controllers\", and \"sometimes too much flexibility causes a hindrance\". In an LWN article from 2012, it was said that \"control groups are one of those features that kernel developers love to hate.\"","title":"Definitions - what is a cgroup"},{"location":"linux/cgroups/#the-relationship-of-containers-and-cgroups","text":"Containers are basically just a bunch of cgroups plus namespace isolation (plus some extra features): https://en.wikipedia.org/wiki/LXC : The Linux kernel provides the cgroups functionality that allows limitation and prioritization of resources (CPU, memory, block I/O, network, etc.) without the need for starting any virtual machines, and also namespace isolation functionality that allows complete isolation of an applications' view of the operating environment, including process trees, networking, user IDs and mounted file systems.[3] LXC combines the kernel's cgroups and support for isolated namespaces to provide an isolated environment for applications. Early versions of Docker used LXC as the container execution driver, though LXC was made optional in v0.9 and support was dropped in Docker v1.10. [4] https://en.wikipedia.org/wiki/Docker_(software) : Docker is developed primarily for Linux, where it uses the resource isolation features of the Linux kernel such as cgroups and kernel namespaces, and a union-capable file system such as OverlayFS and others[28] to allow independent \"containers\" to run within a single Linux instance, avoiding the overhead of starting and maintaining virtual machines (VMs).[29] The Linux kernel's support for namespaces mostly[30] isolates an application's view of the operating environment, including process trees, network, user IDs and mounted file systems, while the kernel's cgroups provide resource limiting for memory and CPU.[31] Since version 0.9, Docker includes the libcontainer library as its own way to directly use virtualization facilities provided by the Linux kernel, in addition to using abstracted virtualization interfaces via libvirt, LXC and systemd-nspawn.[13][32][27]","title":"The relationship of containers and cgroups"},{"location":"linux/cgroups/#cgroups-versions","text":"cgroup comes in 2 versions. cgroups v2 are to replace cgroups v1 eventually. However, for reasons of backwards compatibility, both will probably be around for a very long time. cgroups v1 have several issues ... uncoordinated development of resource controllers inconsistencies between controllers complex hierarchy management Solution: cgroups v2. man 7 cgroups : (...) The initial release of the cgroups implementation was in Linux 2.6.24. Over time, various cgroup controllers have been added to allow the management of various types of resources. However, the development of these controllers was largely uncoordi\u2010 nated, with the result that many inconsistencies arose between controllers and man\u2010 agement of the cgroup hierarchies became rather complex. (A longer description of these problems can be found in the kernel source file Documentation/cgroup-v2.txt.) (...)","title":"cgroups versions"},{"location":"linux/cgroups/#backwards-compatibility","text":"cgroups v1 is unlikely to be removed cgroups v1 and v2 can coexist cgroups v2 only implemented a subset of v1's functionality users can use resource controllers supported in v2 and use v1 controllers for features which are unsupported in v2 man 7 cgroups : (...) Although cgroups v2 is intended as a replacement for cgroups v1, the older system continues to exist (and for compatibility reasons is unlikely to be removed). Cur\u2010 rently, cgroups v2 implements only a subset of the controllers available in cgroups v1. The two systems are implemented so that both v1 controllers and v2 controllers can be mounted on the same system. Thus, for example, it is possible to use those controllers that are supported under version 2, while also using version 1 con\u2010 trollers where version 2 does not yet support those controllers. The only restric\u2010 tion here is that a controller can't be simultaneously employed in both a cgroups v1 hierarchy and in the cgroups v2 hierarchy. (...)","title":"Backwards compatibility"},{"location":"linux/cgroups/#which-version-of-cgroups-are-you-running","text":"cgroups are mounted as a virtual filesystem. Hence, verify with the mount command which version is currently in use.","title":"Which version of cgroups are you running?"},{"location":"linux/cgroups/#default-in-rhel-7","text":"RHEL 7 uses cgroups v1: [root@rhospbl-1 ~]# mount | grep cgroup tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,seclabel,mode=755) cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd) cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_prio,net_cls) cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio) cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices) cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb) cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer) cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct,cpu) cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset) cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event) cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory) cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids) [root@rhospbl-1 ~]# uname -a Linux rhospbl-1.openstack.gsslab.rdu2.redhat.com 3.10.0-693.el7.x86_64 #1 SMP Thu Jul 6 19:56:57 EDT 2017 x86_64 x86_64 x86_64 GNU/Linux","title":"Default in RHEL 7"},{"location":"linux/cgroups/#default-in-fedora-28","text":"[akaris@wks-akaris blog]$ cat /etc/redhat-release Fedora release 28 (Twenty Eight) [akaris@wks-akaris blog]$ mount | grep cgroup tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,seclabel,mode=755) cgroup2 on /sys/fs/cgroup/unified type cgroup2 (rw,nosuid,nodev,noexec,relatime,seclabel,nsdelegate) cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,xattr,name=systemd) cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,cpu,cpuacct) cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,hugetlb) cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,perf_event) cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,memory) cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,pids) cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,freezer) cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,net_cls,net_prio) cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,blkio) cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,devices) cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,cpuset)","title":"Default in Fedora 28"},{"location":"linux/cgroups/#default-in-rhel-8","text":"[root@rhel8 ~]# cat /etc/redhat-release Red Hat Enterprise Linux release 8.0 Beta (Ootpa) [root@rhel8 ~]# mount | grep cgroup tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,seclabel,mode=755) cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd) cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,freezer) cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,cpu,cpuacct) cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,memory) cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,net_cls,net_prio) cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,pids) cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,blkio) cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,hugetlb) cgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,rdma) cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,devices) cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,cpuset) cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,perf_event)","title":"Default in RHEL 8"},{"location":"linux/cgroups/#support-for-cgroupv2","text":"","title":"Support for cgroupv2?"},{"location":"linux/cgroups/#in-rhel-7","text":"No, cgroups v2 is not in the kernel in RHEL 7: [root@rhospbl-1 ~]# mount -t cgroup2 none /mnt/test mount: unknown filesystem type 'cgroup2'","title":"In RHEL 7"},{"location":"linux/cgroups/#in-rhel-8","text":"Yes: [root@rhel8 ~]# mount cgroup2 -t cgroup2 /mnt/cgroupv2 [root@rhel8 ~]# mount | grep cgroup2 cgroup2 on /mnt/cgroupv2 type cgroup2 (rw,relatime,seclabel) [root@rhel8 ~]#","title":"In RHEL 8"},{"location":"linux/cgroups/#types-of-cgroups-v1-resource-controllers","text":"","title":"Types of cgroups v1 resource controllers"},{"location":"linux/cgroups/#cpu-resource-controller","text":"cpu tracking based on cgroups","title":"cpu resource controller"},{"location":"linux/cgroups/#cpuset-resource-controller","text":"https://www.kernel.org/doc/Documentation/cgroup-v1/hugetlb.txt : [root@overcloud-controller-0 cgroup]# cd /sys/fs/cgroup/cpuset/ [root@overcloud-controller-0 cpuset]# mkdir test [root@overcloud-controller-0 test]# echo 2-3 > cpuset.cpus [root@overcloud-controller-0 test]# cat cpuset.cpus 2-3 [root@overcloud-controller-0 test]# dd if=/dev/zero of=/dev/null & [2] 931866 [root@overcloud-controller-0 test]# taskset -p -c 931866 pid 931866's current affinity list: 0-3 [root@overcloud-controller-0 test]# echo 931866 > tasks [root@overcloud-controller-0 test]# taskset -p -c 931866 pid 931866's current affinity list: 2,3","title":"cpuset resource controller"},{"location":"linux/cgroups/#hugetlb-resource-controller","text":"controls amount of hugepages usable by a process by default, a process can request as many hugepages as it wants Looking at meminfo, we see that 4 hugepages are used: [root@overcloud-computesriov-0 system.slice]# cat /sys/devices/system/node/node?/meminfo | grep -i huge Node 0 AnonHugePages: 4096 kB Node 0 HugePages_Total: 16 Node 0 HugePages_Free: 12 Node 0 HugePages_Surp: 0 Node 1 AnonHugePages: 2048 kB Node 1 HugePages_Total: 16 Node 1 HugePages_Free: 16 Node 1 HugePages_Surp: 0 One way to find out which processes are using hugepages, is to check the hugetlb cgroups: [root@overcloud-computesriov-0 ~]# cd /sys/fs/cgroup/hugetlb [root@overcloud-computesriov-0 hugetlb]# ll total 0 -rw-r--r--. 1 root root 0 Nov 27 06:17 cgroup.clone_children --w--w--w-. 1 root root 0 Nov 27 06:17 cgroup.event_control -rw-r--r--. 1 root root 0 Nov 27 06:17 cgroup.procs -r--r--r--. 1 root root 0 Nov 27 06:17 cgroup.sane_behavior -rw-r--r--. 1 root root 0 Nov 27 06:17 hugetlb.1GB.failcnt -rw-r--r--. 1 root root 0 Nov 27 06:17 hugetlb.1GB.limit_in_bytes -rw-r--r--. 1 root root 0 Nov 27 06:17 hugetlb.1GB.max_usage_in_bytes -r--r--r--. 1 root root 0 Nov 27 06:17 hugetlb.1GB.usage_in_bytes -rw-r--r--. 1 root root 0 Nov 27 06:17 notify_on_release -rw-r--r--. 1 root root 0 Nov 27 06:17 release_agent drwxr-xr-x. 11 root root 0 Nov 27 06:46 system.slice -rw-r--r--. 1 root root 0 Nov 27 06:17 tasks [root@overcloud-computesriov-0 hugetlb]# cat hugetlb.1GB.usage_in_bytes 4294967296 [root@overcloud-computesriov-0 hugetlb]# cd system.slice [root@overcloud-computesriov-0 system.slice]# ll total 0 -rw-r--r--. 1 root root 0 Nov 27 06:25 cgroup.clone_children --w--w--w-. 1 root root 0 Nov 27 06:25 cgroup.event_control -rw-r--r--. 1 root root 0 Nov 27 06:25 cgroup.procs drwxr-xr-x. 2 root root 0 Nov 27 06:46 docker-111c9c039324d640875e550763d6507450cbbd07f6674c3883388839807cd614.scope drwxr-xr-x. 2 root root 0 Nov 27 06:51 docker-1286b301010bb53e3d919616054e645c00a2288b9cdc8235bdb68aa404a0c34b.scope drwxr-xr-x. 2 root root 0 Nov 27 06:46 docker-46ed5e7b2045df285552bde12209717f1601b27e3d6e137ed9122d3d9c519a3d.scope drwxr-xr-x. 2 root root 0 Nov 27 06:51 docker-90030441400dd9536aa33d13d3d5792a4e1f025fb383141cb0f18cfaed260979.scope drwxr-xr-x. 2 root root 0 Nov 27 06:46 docker-a94d9089fac5ed4b1dbe48b0f5460536462c49e0ff14fd4059fbae7a7dbd1b4d.scope drwxr-xr-x. 2 root root 0 Nov 27 06:51 docker-b9ff1a61cc4f144cc2aa16332d8e07248ad71a7263056b0d9cddb7339368457a.scope drwxr-xr-x. 2 root root 0 Nov 27 06:51 docker-c538d8c4e222b977b218746ee9ebef34335d768a364fbe1bfb3e72284d65520a.scope drwxr-xr-x. 2 root root 0 Nov 27 06:51 docker-d2e87b4ec13cc51c9dab3e593ee44051eb243ce684cc300b7e6103e8f35e1320.scope drwxr-xr-x. 2 root root 0 Nov 27 06:51 docker-dce63a96f513527b894bbec6c7f39f40dd2912bdbf4dec0a51b2e59704c03e7b.scope -rw-r--r--. 1 root root 0 Nov 27 06:25 hugetlb.1GB.failcnt -rw-r--r--. 1 root root 0 Nov 27 06:25 hugetlb.1GB.limit_in_bytes -rw-r--r--. 1 root root 0 Nov 27 06:25 hugetlb.1GB.max_usage_in_bytes -r--r--r--. 1 root root 0 Nov 27 06:25 hugetlb.1GB.usage_in_bytes -rw-r--r--. 1 root root 0 Nov 27 06:25 notify_on_release -rw-r--r--. 1 root root 0 Nov 27 06:25 tasks [root@overcloud-computesriov-0 system.slice]# cat hugetlb.1GB.usage_in_bytes 4294967296 [root@overcloud-computesriov-0 system.slice]# find . -name '*usage_in_bytes' ./docker-b9ff1a61cc4f144cc2aa16332d8e07248ad71a7263056b0d9cddb7339368457a.scope/hugetlb.1GB.max_usage_in_bytes ./docker-b9ff1a61cc4f144cc2aa16332d8e07248ad71a7263056b0d9cddb7339368457a.scope/hugetlb.1GB.usage_in_bytes ./docker-dce63a96f513527b894bbec6c7f39f40dd2912bdbf4dec0a51b2e59704c03e7b.scope/hugetlb.1GB.max_usage_in_bytes ./docker-dce63a96f513527b894bbec6c7f39f40dd2912bdbf4dec0a51b2e59704c03e7b.scope/hugetlb.1GB.usage_in_bytes ./docker-c538d8c4e222b977b218746ee9ebef34335d768a364fbe1bfb3e72284d65520a.scope/hugetlb.1GB.max_usage_in_bytes ./docker-c538d8c4e222b977b218746ee9ebef34335d768a364fbe1bfb3e72284d65520a.scope/hugetlb.1GB.usage_in_bytes ./docker-d2e87b4ec13cc51c9dab3e593ee44051eb243ce684cc300b7e6103e8f35e1320.scope/hugetlb.1GB.max_usage_in_bytes ./docker-d2e87b4ec13cc51c9dab3e593ee44051eb243ce684cc300b7e6103e8f35e1320.scope/hugetlb.1GB.usage_in_bytes ./docker-90030441400dd9536aa33d13d3d5792a4e1f025fb383141cb0f18cfaed260979.scope/hugetlb.1GB.max_usage_in_bytes ./docker-90030441400dd9536aa33d13d3d5792a4e1f025fb383141cb0f18cfaed260979.scope/hugetlb.1GB.usage_in_bytes ./docker-1286b301010bb53e3d919616054e645c00a2288b9cdc8235bdb68aa404a0c34b.scope/hugetlb.1GB.max_usage_in_bytes ./docker-1286b301010bb53e3d919616054e645c00a2288b9cdc8235bdb68aa404a0c34b.scope/hugetlb.1GB.usage_in_bytes ./docker-111c9c039324d640875e550763d6507450cbbd07f6674c3883388839807cd614.scope/hugetlb.1GB.max_usage_in_bytes ./docker-111c9c039324d640875e550763d6507450cbbd07f6674c3883388839807cd614.scope/hugetlb.1GB.usage_in_bytes ./docker-a94d9089fac5ed4b1dbe48b0f5460536462c49e0ff14fd4059fbae7a7dbd1b4d.scope/hugetlb.1GB.max_usage_in_bytes ./docker-a94d9089fac5ed4b1dbe48b0f5460536462c49e0ff14fd4059fbae7a7dbd1b4d.scope/hugetlb.1GB.usage_in_bytes ./docker-46ed5e7b2045df285552bde12209717f1601b27e3d6e137ed9122d3d9c519a3d.scope/hugetlb.1GB.max_usage_in_bytes ./docker-46ed5e7b2045df285552bde12209717f1601b27e3d6e137ed9122d3d9c519a3d.scope/hugetlb.1GB.usage_in_bytes ./hugetlb.1GB.max_usage_in_bytes ./hugetlb.1GB.usage_in_bytes [root@overcloud-computesriov-0 system.slice]# find . -name '*usage_in_bytes' | while read line; do echo $line ; cat $line ; done ./docker-b9ff1a61cc4f144cc2aa16332d8e07248ad71a7263056b0d9cddb7339368457a.scope/hugetlb.1GB.max_usage_in_bytes 0 ./docker-b9ff1a61cc4f144cc2aa16332d8e07248ad71a7263056b0d9cddb7339368457a.scope/hugetlb.1GB.usage_in_bytes 0 ./docker-dce63a96f513527b894bbec6c7f39f40dd2912bdbf4dec0a51b2e59704c03e7b.scope/hugetlb.1GB.max_usage_in_bytes 0 ./docker-dce63a96f513527b894bbec6c7f39f40dd2912bdbf4dec0a51b2e59704c03e7b.scope/hugetlb.1GB.usage_in_bytes 0 ./docker-c538d8c4e222b977b218746ee9ebef34335d768a364fbe1bfb3e72284d65520a.scope/hugetlb.1GB.max_usage_in_bytes 0 ./docker-c538d8c4e222b977b218746ee9ebef34335d768a364fbe1bfb3e72284d65520a.scope/hugetlb.1GB.usage_in_bytes 0 ./docker-d2e87b4ec13cc51c9dab3e593ee44051eb243ce684cc300b7e6103e8f35e1320.scope/hugetlb.1GB.max_usage_in_bytes 0 ./docker-d2e87b4ec13cc51c9dab3e593ee44051eb243ce684cc300b7e6103e8f35e1320.scope/hugetlb.1GB.usage_in_bytes 0 ./docker-90030441400dd9536aa33d13d3d5792a4e1f025fb383141cb0f18cfaed260979.scope/hugetlb.1GB.max_usage_in_bytes 0 ./docker-90030441400dd9536aa33d13d3d5792a4e1f025fb383141cb0f18cfaed260979.scope/hugetlb.1GB.usage_in_bytes 0 ./docker-1286b301010bb53e3d919616054e645c00a2288b9cdc8235bdb68aa404a0c34b.scope/hugetlb.1GB.max_usage_in_bytes 0 ./docker-1286b301010bb53e3d919616054e645c00a2288b9cdc8235bdb68aa404a0c34b.scope/hugetlb.1GB.usage_in_bytes 0 ./docker-111c9c039324d640875e550763d6507450cbbd07f6674c3883388839807cd614.scope/hugetlb.1GB.max_usage_in_bytes 0 ./docker-111c9c039324d640875e550763d6507450cbbd07f6674c3883388839807cd614.scope/hugetlb.1GB.usage_in_bytes 0 ./docker-a94d9089fac5ed4b1dbe48b0f5460536462c49e0ff14fd4059fbae7a7dbd1b4d.scope/hugetlb.1GB.max_usage_in_bytes 4294967296 ./docker-a94d9089fac5ed4b1dbe48b0f5460536462c49e0ff14fd4059fbae7a7dbd1b4d.scope/hugetlb.1GB.usage_in_bytes 4294967296 ./docker-46ed5e7b2045df285552bde12209717f1601b27e3d6e137ed9122d3d9c519a3d.scope/hugetlb.1GB.max_usage_in_bytes 0 ./docker-46ed5e7b2045df285552bde12209717f1601b27e3d6e137ed9122d3d9c519a3d.scope/hugetlb.1GB.usage_in_bytes 0 ./hugetlb.1GB.max_usage_in_bytes 4294967296 ./hugetlb.1GB.usage_in_bytes 4294967296 [root@overcloud-computesriov-0 system.slice]# docker ps | grep a94d9089 a94d9089fac5 registry.access.redhat.com/rhosp13/openstack-nova-libvirt:13.0-72 \"kolla_start\" 3 days ago Up 3 days nova_libvirt [root@overcloud-computesriov-0 system.slice]# systemctl status docker-a94d9089fac5ed4b1dbe48b0f5460536462c49e0ff14fd4059fbae7a7dbd1b4d.scope \u25cf docker-a94d9089fac5ed4b1dbe48b0f5460536462c49e0ff14fd4059fbae7a7dbd1b4d.scope - libcontainer container a94d9089fac5ed4b1dbe48b0f5460536462c49e0ff14fd4059fbae7a7dbd1b4d Loaded: loaded (/run/systemd/system/docker-a94d9089fac5ed4b1dbe48b0f5460536462c49e0ff14fd4059fbae7a7dbd1b4d.scope; static; vendor preset: disabled) Drop-In: /run/systemd/system/docker-a94d9089fac5ed4b1dbe48b0f5460536462c49e0ff14fd4059fbae7a7dbd1b4d.scope.d \u2514\u250050-BlockIOAccounting.conf, 50-CPUAccounting.conf, 50-DefaultDependencies.conf, 50-Delegate.conf, 50-Description.conf, 50-MemoryAccounting.conf, 50-Slice.conf Active: active (running) since Tue 2018-11-27 06:46:11 UTC; 3 days ago Tasks: 18 Memory: 14.8M CGroup: /system.slice/docker-a94d9089fac5ed4b1dbe48b0f5460536462c49e0ff14fd4059fbae7a7dbd1b4d.scope \u2514\u250033304 /usr/sbin/libvirtd Nov 27 06:46:11 overcloud-computesriov-0 systemd[1]: Started libcontainer container a94d9089fac5ed4b1dbe48b0f5460536462c49e0ff14fd4059fbae7a7dbd1b4d. Nov 27 06:46:11 overcloud-computesriov-0 sudo[33325]: root : TTY=unknown ; PWD=/ ; USER=root ; COMMAND=/usr/local/bin/kolla_set_configs [root@overcloud-computesriov-0 system.slice]#","title":"hugetlb resource controller"},{"location":"linux/cgroups/#memory-cgroup","text":"keep track of pages used by each group allow the OOM (out of memory) killer to trigger on a specific memory cgroup only kernel can \"freeze\" the cgroup","title":"memory cgroup"},{"location":"linux/cgroups/#blockio-cgroup","text":"keep track of I/Os for each group throttle each group writes go through page cache unless O_DIRECT is set","title":"blockio cgroup"},{"location":"linux/cgroups/#net_cls-cgroup","text":"automatically set traffic classs for egress traffic (use tc/iptables)","title":"net_cls cgroup"},{"location":"linux/cgroups/#net_prio-cgroup","text":"automatically set traffic classs for egress traffic (use queuing disciplines)","title":"net_prio cgroup"},{"location":"linux/cgroups/#devices-cgroup","text":"which group can read/write from which device in /dev","title":"devices cgroup"},{"location":"linux/cgroups/#freezer-cgroup","text":"used to freeze / stop all processes in a group (SIGSTOP / SIGCONT)","title":"freezer cgroup"},{"location":"linux/cgroups/#mounting-unmounting-and-comounting-cgroup-v1-resource-controllers","text":"man 7 cgroups (...) It is possible to comount multiple controllers against the same hierarchy. For example, here the cpu and cpuacct controllers are comounted against a single hierar\u2010 chy: mount -t cgroup -o cpu,cpuacct none /sys/fs/cgroup/cpu,cpuacct Comounting controllers has the effect that a process is in the same cgroup for all of the comounted controllers. Separately mounting controllers allows a process to be in cgroup /foo1 for one controller while being in /foo2/foo3 for another. It is possible to comount all v1 controllers against the same hierarchy: mount -t cgroup -o all cgroup /sys/fs/cgroup (One can achieve the same result by omitting -o all, since it is the default if no controllers are explicitly specified.) (...) [akaris@wks-akaris blog]$ mount | grep cgroup tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,seclabel,mode=755) cgroup2 on /sys/fs/cgroup/unified type cgroup2 (rw,nosuid,nodev,noexec,relatime,seclabel,nsdelegate) cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,xattr,name=systemd) cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,cpu,cpuacct) cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,hugetlb) cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,perf_event) cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,memory) cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,pids) cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,freezer) cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,net_cls,net_prio) cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,blkio) cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,devices) cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,cpuset) [akaris@wks-akaris blog]$ ls /sys/fs/cgroup blkio cpu cpuacct cpu,cpuacct cpuset devices freezer hugetlb memory net_cls net_cls,net_prio net_prio perf_event pids systemd unified [akaris@wks-akaris blog]$ man 7 cgroup (...) It is not possible to mount the same controller against multiple cgroup hierarchies. For example, it is not possible to mount both the cpu and cpuacct controllers against one hierarchy, and to mount the cpu controller alone against another hierar\u2010 chy. It is possible to create multiple mount points with exactly the same set of comounted controllers. However, in this case all that results is multiple mount points providing a view of the same hierarchy. (...) [root@rhel8 ~]# mount | grep cgroup tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,seclabel,mode=755) cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd) cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,freezer) cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,cpu,cpuacct) cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,memory) cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,net_cls,net_prio) cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,pids) cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,blkio) cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,hugetlb) cgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,rdma) cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,devices) cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,cpuset) cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,perf_event) cgroup2 on /mnt/cgroupv2 type cgroup2 (rw,relatime,seclabel) [root@rhel8 ~]# mount group -t net_cls /sys/fs/cgroup/net_cls mount: /sys/fs/cgroup/net_cls,net_prio: unknown filesystem type 'net_cls'. [root@rhel8 ~]# mount group -t cgroup -o net_cls /sys/fs/cgroup/net_cls mount: /sys/fs/cgroup/net_cls,net_prio: group already mounted or mount point busy. [root@rhel8 ~]# mount group -t cgroup -o net_cls /mnt mount: /mnt: group already mounted or mount point busy. [root@rhel8 ~]# mount group -t cgroup -o net_cls,net_prio /mnt [root@rhel8 ~]# umount /mnt man 7 cgroups > (...) Unmounting v1 controllers A mounted cgroup filesystem can be unmounted using the umount(8) command, as in the following example: umount /sys/fs/cgroup/pids But note well: a cgroup filesystem is unmounted only if it is not busy, that is, it has no child cgroups. If this is not the case, then the only effect of the umount(8) is to make the mount invisible. Thus, to ensure that the mount point is really removed, one must first remove all child cgroups, which in turn can be done only after all member processes have been moved from those cgroups to the root cgroup. (...)","title":"mounting, unmounting and comounting cgroup v1 resource controllers"},{"location":"linux/cgroups/#processes-in-cgroups","text":"Processes start in the same cgroups as their parent A process can be moved by: echo $id > /sys/fs/cgroup/.../tasks","title":"Processes in cgroups"},{"location":"linux/cgroups/#listing-the-cgroups-that-a-process-is-in","text":"https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt : Each task under /proc has an added file named 'cgroup' displaying, for each active hierarchy, the subsystem names and the cgroup name as the path relative to the root of the cgroup file system. Example: [root@rhospbl-1 ~]# cat /proc/$(pidof libvirtd)/cgroup 11:pids:/ 10:memory:/system.slice 9:perf_event:/ 8:cpuset:/ 7:cpuacct,cpu:/system.slice 6:freezer:/ 5:hugetlb:/ 4:devices:/system.slice/libvirtd.service 3:blkio:/system.slice 2:net_prio,net_cls:/ 1:name=systemd:/system.slice/libvirtd.service","title":"Listing the cgroups that a process is in"},{"location":"linux/cgroups/#notify_on_release","text":"If the notify_on_release flag is enabled (1) in a cgroup, then whenever the last task in the cgroup leaves (exits or attaches to some other cgroup) and the last child cgroup of that cgroup is removed, then the kernel runs the command specified by the contents of the \"release_agent\" file in that hierarchy's root directory, supplying the pathname (relative to the mount point of the cgroup file system) of the abandoned cgroup. This enables automatic removal of abandoned cgroups. The default value of notify_on_release in the root cgroup at system boot is disabled (0). The default value of other cgroups at creation is the current value of their parents' notify_on_release settings. The default value of a cgroup hierarchy's release_agent path is empty. Example: [root@overcloud-controller-0 ~]# systemctl status session-c2.scope \u25cf session-c2.scope - Session c2 of user rabbitmq Loaded: loaded (/run/systemd/system/session-c2.scope; static; vendor preset: disabled) Drop-In: /run/systemd/system/session-c2.scope.d \u2514\u250050-After-systemd-logind\\x2eservice.conf, 50-After-systemd-user-sessions\\x2eservice.conf, 50-Description.conf, 50-SendSIGHUP.conf, 50-Slice.conf, 50-TasksMax.conf Active: active (abandoned) since Wed 2018-11-07 22:40:43 UTC; 2 weeks 5 days ago CGroup: /user.slice/user-975.slice/session-c2.scope \u2514\u250022384 /usr/lib64/erlang/erts-7.3.1.4/bin/epmd -daemon Warning: Journal has been rotated since unit was started. Log output is incomplete or unavailable. [root@overcloud-controller-0 ~]# cat /sys/fs/cgroup/ blkio/ cpu,cpuacct/ freezer/ net_cls/ perf_event/ cpu/ cpuset/ hugetlb/ net_cls,net_prio/ pids/ cpuacct/ devices/ memory/ net_prio/ systemd/ [root@overcloud-controller-0 ~]# cat /sys/fs/cgroup/ blkio/ cpu,cpuacct/ freezer/ net_cls/ perf_event/ cpu/ cpuset/ hugetlb/ net_cls,net_prio/ pids/ cpuacct/ devices/ memory/ net_prio/ systemd/ [root@overcloud-controller-0 ~]# cat /sys/fs/cgroup/ blkio/ cpu,cpuacct/ freezer/ net_cls/ perf_event/ cpu/ cpuset/ hugetlb/ net_cls,net_prio/ pids/ cpuacct/ devices/ memory/ net_prio/ systemd/ [root@overcloud-controller-0 ~]# cat /sys/fs/cgroup/systemd/ cgroup.clone_children cgroup.procs machine.slice/ release_agent tasks cgroup.event_control cgroup.sane_behavior notify_on_release system.slice/ user.slice/ [root@overcloud-controller-0 ~]# cat /sys/fs/cgroup/systemd/ cgroup.clone_children cgroup.procs machine.slice/ release_agent tasks cgroup.event_control cgroup.sane_behavior notify_on_release system.slice/ user.slice/ [root@overcloud-controller-0 ~]# cat /sys/fs/cgroup/systemd/user.slice/user-975.slice/session-c2.scope/ cgroup.clone_children cgroup.event_control cgroup.procs notify_on_release tasks [root@overcloud-controller-0 ~]# cat /sys/fs/cgroup/systemd/user.slice/user-975.slice/session-c2.scope/notify_on_release 1","title":"notify_on_release"},{"location":"linux/cgroups/#resources","text":"https://en.wikipedia.org/wiki/Cgroups https://www.kernel.org/doc/Documentation/cgroup-v1/ https://lwn.net/Articles/679786/ https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/resource_management_guide/index man 7 cgroups https://www.youtube.com/watch?v=sK5i-N34im8","title":"Resources"},{"location":"linux/containers/","text":"Linux containers What is a Linux container Introductory read about the components that make up a container: https://medium.com/@nagarwal/understanding-the-docker-internals-7ccb052ce9fe Relations ship between containers, cgroups, SELinux and containers Red Hat Enterprise Linux Atomis Host 7 Overview of Containers in Red Hat Systems - https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html-single/overview_of_containers_in_red_hat_systems/index : Kernel namespaces ensure process isolation and cgroups are employed to control the system resources. SELinux is used to assure separation between the host and the container and also between the individual containers. Management interface forms a higher layer that interacts with the aforementioned kernel components and provides tools for construction and management of containers. Red Hat Enterprise Linux Atomis Host 7 Overview of Containers in Red Hat Systems - https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html-single/overview_of_containers_in_red_hat_systems/index : Namespaces The kernel provides process isolation by creating separate namespaces for containers. Namespaces enable creating an abstraction of a particular global system resource and make it appear as a separated instance to processes within a namespace. Consequently, several containers can use the same resource simultaneously without creating a conflict. (...) Control Groups (cgroups) The kernel uses cgroups to group processes for the purpose of system resource management. Cgroups allocate CPU time, system memory, network bandwidth, or combinations of these among user- defined groups of tasks. In Red Hat Enterprise Linux 7, cgroups are managed with systemd slice, scope, and service units. For more information on cgroups, see the Red Hat Enterprise Linux 7 Resource Management Guide. SELinux SELinux provides secure separation of containers by applying SELinux policy and labels. It integrates with virtual devices by using the sVirt technology. For more information see the Red Hat Enterprise Linux 7 SELinux Users and Administrators Guide. (...) Red Hat Enterprise Linux Atomis Host 7 Overview of Containers in Red Hat Systems - https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html-single/overview_of_containers_in_red_hat_systems/index : 1.3. SECURE CONTAINERS WITH SELINUX From the security point of view, there is a need to isolate the host system from a container and to isolate containers from each other. The kernel features used by containers, namely cgroups and namespaces, by themselves provide a certain level of security. Cgroups ensure that a single container cannot exhaust a large amount of system resources, thus preventing some denial-of-service attacks. By virtue of namespaces, the /dev directory created within a container is private to each container, and therefore unaffected by the host changes. However, this can not prevent a hostile process from breaking out of the container since the entire system is not namespaced or containerized. Another level of separation, provided by SELinux, is therefore needed. Security-Enhanced Linux (SELinux) is an implementation of a mandatory access control (MAC) mechanism, multi-level security (MLS), and multi-category security (MCS) in the Linux kernel. The sVirt project builds upon SELinux and integrates with Libvirt to provide a MAC framework for virtual machines and containers. This architecture provides a secure separation for containers as it prevents root processes within the container from interfering with other processes running outside this container. The containers created with Docker are automatically assigned with an SELinux context specified in the SELinux policy. 5Red Hat Enterprise Linux Atomic Host 7 Overview of Containers in Red Hat Systems By default, containers created with libvirt tools are assigned with the virtd_lxc_t label (execute ps -eZ | grep virtd_lxc_t). You can apply sVirt by setting static or dynamic labeling for processes inside the container. Note You might notice that SELinux appears to be disabled inside the container even though it is running in enforcing mode on host system \u2013 you can verify this by executing the getenforce command on host and in the container. This is to prevent utilities that have SELinux awareness, such as setenforce, to perform any SELinux activity inside the container. Note that if SELinux is disabled or running in permissive mode on the host machine, containers are not separated securely enough. For more information about SELinux, refer to Red Hat Enterprise Linux 7 SELinux Users and Administrators Guide, sVirt is described in Red Hat Enterprise Linux 7 Virtualization Security Guide.","title":"Containers in Linux"},{"location":"linux/containers/#linux-containers","text":"","title":"Linux containers"},{"location":"linux/containers/#what-is-a-linux-container","text":"Introductory read about the components that make up a container: https://medium.com/@nagarwal/understanding-the-docker-internals-7ccb052ce9fe","title":"What is a Linux container"},{"location":"linux/containers/#relations-ship-between-containers-cgroups-selinux-and-containers","text":"Red Hat Enterprise Linux Atomis Host 7 Overview of Containers in Red Hat Systems - https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html-single/overview_of_containers_in_red_hat_systems/index : Kernel namespaces ensure process isolation and cgroups are employed to control the system resources. SELinux is used to assure separation between the host and the container and also between the individual containers. Management interface forms a higher layer that interacts with the aforementioned kernel components and provides tools for construction and management of containers. Red Hat Enterprise Linux Atomis Host 7 Overview of Containers in Red Hat Systems - https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html-single/overview_of_containers_in_red_hat_systems/index : Namespaces The kernel provides process isolation by creating separate namespaces for containers. Namespaces enable creating an abstraction of a particular global system resource and make it appear as a separated instance to processes within a namespace. Consequently, several containers can use the same resource simultaneously without creating a conflict. (...) Control Groups (cgroups) The kernel uses cgroups to group processes for the purpose of system resource management. Cgroups allocate CPU time, system memory, network bandwidth, or combinations of these among user- defined groups of tasks. In Red Hat Enterprise Linux 7, cgroups are managed with systemd slice, scope, and service units. For more information on cgroups, see the Red Hat Enterprise Linux 7 Resource Management Guide. SELinux SELinux provides secure separation of containers by applying SELinux policy and labels. It integrates with virtual devices by using the sVirt technology. For more information see the Red Hat Enterprise Linux 7 SELinux Users and Administrators Guide. (...) Red Hat Enterprise Linux Atomis Host 7 Overview of Containers in Red Hat Systems - https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html-single/overview_of_containers_in_red_hat_systems/index : 1.3. SECURE CONTAINERS WITH SELINUX From the security point of view, there is a need to isolate the host system from a container and to isolate containers from each other. The kernel features used by containers, namely cgroups and namespaces, by themselves provide a certain level of security. Cgroups ensure that a single container cannot exhaust a large amount of system resources, thus preventing some denial-of-service attacks. By virtue of namespaces, the /dev directory created within a container is private to each container, and therefore unaffected by the host changes. However, this can not prevent a hostile process from breaking out of the container since the entire system is not namespaced or containerized. Another level of separation, provided by SELinux, is therefore needed. Security-Enhanced Linux (SELinux) is an implementation of a mandatory access control (MAC) mechanism, multi-level security (MLS), and multi-category security (MCS) in the Linux kernel. The sVirt project builds upon SELinux and integrates with Libvirt to provide a MAC framework for virtual machines and containers. This architecture provides a secure separation for containers as it prevents root processes within the container from interfering with other processes running outside this container. The containers created with Docker are automatically assigned with an SELinux context specified in the SELinux policy. 5Red Hat Enterprise Linux Atomic Host 7 Overview of Containers in Red Hat Systems By default, containers created with libvirt tools are assigned with the virtd_lxc_t label (execute ps -eZ | grep virtd_lxc_t). You can apply sVirt by setting static or dynamic labeling for processes inside the container. Note You might notice that SELinux appears to be disabled inside the container even though it is running in enforcing mode on host system \u2013 you can verify this by executing the getenforce command on host and in the container. This is to prevent utilities that have SELinux awareness, such as setenforce, to perform any SELinux activity inside the container. Note that if SELinux is disabled or running in permissive mode on the host machine, containers are not separated securely enough. For more information about SELinux, refer to Red Hat Enterprise Linux 7 SELinux Users and Administrators Guide, sVirt is described in Red Hat Enterprise Linux 7 Virtualization Security Guide.","title":"Relations ship between containers, cgroups, SELinux and containers"},{"location":"linux/hugepages/","text":"Hugepages Requesting hugepages in C Example 1 Create the code, mmap.c : #include <sys/mman.h> #include <stdio.h> #include <stdlib.h> #define PAGE_SIZE (unsigned int) 1024*1024*1024 #define NUM_PAGES 2 void main() { char * buf = mmap( NULL, NUM_PAGES * PAGE_SIZE, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS | MAP_HUGETLB, -1, 0 ); if (buf == MAP_FAILED) { perror(\"mmap\"); exit(1); } char * line = NULL; size_t size; printf(\"Memory address %p\\n\" \"This will only reserve pages. Execute \\n\" \"grep -R '' /sys/kernel/mm/hugepages/hugepages-1048576kB/\\n\" \"find /sys -name meminfo | xargs grep -i huge\\n\" \"to verify this.\\n\\n\" \"When you are done, please hit return\\n\", buf); getline(&line,&size,stdin); int i; printf(\"Now, actually writing all 0s into the first hugepage\\n\"); for(i = 0; i < PAGE_SIZE; i++) { buf[i] = '0'; } printf(\"Now, verify again\\n\" \"grep -R '' /sys/kernel/mm/hugepages/hugepages-1048576kB/\\n\" \"find /sys -name meminfo | xargs grep -i huge\\n\" \"to verify this.\\n\\n\" \"When you are done, please hit return\\n\", buf); getline(&line,&size,stdin); printf(\"Now, actually writing one 0 into the second hugepage\\n\"); for(; i < PAGE_SIZE + 1; i++) { buf[i] = '0'; } printf(\"Now, verify again\\n\" \"grep -R '' /sys/kernel/mm/hugepages/hugepages-1048576kB/\\n\" \"find /sys -name meminfo | xargs grep -i huge\\n\" \"to verify this.\\n\\n\" \"When you are done, please hit return to end the program\\n\", buf); getline(&line,&size,stdin); } Compile this: gcc mmap.c -o mmap Run the binary and follow the onscreen instructions: [root@dell-r430-30 ~]# ./mmap Memory address 0x2aaac0000000 This will only reserve pages. Execute grep -R '' /sys/kernel/mm/hugepages/hugepages-1048576kB/ find /sys -name meminfo | xargs grep -i huge to verify this. When you are done, please hit return Now, actually writing all 0s into the first hugepage ^C [root@dell-r430-30 ~]# ^C [root@dell-r430-30 ~]# ^C [root@dell-r430-30 ~]# gcc mmap.c -o mmap [root@dell-r430-30 ~]# ./mmap Memory address 0x2aaac0000000 This will only reserve pages. Execute grep -R '' /sys/kernel/mm/hugepages/hugepages-1048576kB/ find /sys -name meminfo | xargs grep -i huge to verify this. When you are done, please hit return Now, actually writing all 0s into the first hugepage Now, verify again grep -R '' /sys/kernel/mm/hugepages/hugepages-1048576kB/ find /sys -name meminfo | xargs grep -i huge to verify this. When you are done, please hit return Now, actually writing one 0 into the second hugepage Now, verify again grep -R '' /sys/kernel/mm/hugepages/hugepages-1048576kB/ find /sys -name meminfo | xargs grep -i huge to verify this. When you are done, please hit return to end the program Verify reserved hugepages and actually allocated hugepages. Also note that writing 1GB to the hugepage actually takes quite some time: [root@dell-r430-30 ~]# grep -R '' /sys/kernel/mm/hugepages/hugepages-1048576kB/ /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_overcommit_hugepages:0 /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages:32 /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages_mempolicy:32 /sys/kernel/mm/hugepages/hugepages-1048576kB/surplus_hugepages:0 /sys/kernel/mm/hugepages/hugepages-1048576kB/resv_hugepages:2 /sys/kernel/mm/hugepages/hugepages-1048576kB/free_hugepages:32 [root@dell-r430-30 ~]# find /sys -name meminfo | xargs grep -i huge /sys/devices/system/node/node0/meminfo:Node 0 AnonHugePages: 0 kB /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Total: 16 /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Free: 16 /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Surp: 0 /sys/devices/system/node/node1/meminfo:Node 1 AnonHugePages: 6144 kB /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Total: 16 /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Free: 16 /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Surp: 0 [root@dell-r430-30 ~]# grep -R '' /sys/kernel/mm/hugepages/hugepages-1048576kB/ /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_overcommit_hugepages:0 /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages:32 /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages_mempolicy:32 /sys/kernel/mm/hugepages/hugepages-1048576kB/surplus_hugepages:0 /sys/kernel/mm/hugepages/hugepages-1048576kB/resv_hugepages:1 /sys/kernel/mm/hugepages/hugepages-1048576kB/free_hugepages:31 [root@dell-r430-30 ~]# find /sys -name meminfo | xargs grep -i huge /sys/devices/system/node/node0/meminfo:Node 0 AnonHugePages: 0 kB /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Total: 16 /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Free: 15 /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Surp: 0 /sys/devices/system/node/node1/meminfo:Node 1 AnonHugePages: 6144 kB /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Total: 16 /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Free: 16 /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Surp: 0 [root@dell-r430-30 ~]# grep -R '' /sys/kernel/mm/hugepages/hugepages-1048576kB/ /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_overcommit_hugepages:0 /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages:32 /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages_mempolicy:32 /sys/kernel/mm/hugepages/hugepages-1048576kB/surplus_hugepages:0 /sys/kernel/mm/hugepages/hugepages-1048576kB/resv_hugepages:0 /sys/kernel/mm/hugepages/hugepages-1048576kB/free_hugepages:30 [root@dell-r430-30 ~]# find /sys -name meminfo | xargs grep -i huge /sys/devices/system/node/node0/meminfo:Node 0 AnonHugePages: 0 kB /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Total: 16 /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Free: 14 /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Surp: 0 /sys/devices/system/node/node1/meminfo:Node 1 AnonHugePages: 6144 kB /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Total: 16 /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Free: 16 /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Surp: 0 [root@dell-r430-30 ~]# find /sys -name meminfo | xargs grep -i huge /sys/devices/system/node/node0/meminfo:Node 0 AnonHugePages: 0 kB /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Total: 16 /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Free: 16 /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Surp: 0 /sys/devices/system/node/node1/meminfo:Node 1 AnonHugePages: 6144 kB /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Total: 16 /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Free: 16 /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Surp: 0 Example 2 - Allocating hugepages right away Instead of having to write to the hugepage to allocate it, we can populate the page right away and it will show up in used pages right away: man mmap (...) MAP_POPULATE (since Linux 2.5.46) Populate (prefault) page tables for a mapping. For a file mapping, this causes read-ahead on the file. Later accesses to the mapping will not be blocked by page faults. MAP_POPULATE is supported for private mappings only since Linux 2.6.23. (...) The application is mmap2.c : #include <sys/mman.h> #include <stdio.h> #include <stdlib.h> #define PAGE_SIZE (unsigned int) 1024*1024*1024 #define NUM_PAGES 2 void main() { char * buf = mmap( NULL, NUM_PAGES * PAGE_SIZE, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS | MAP_HUGETLB | MAP_POPULATE, -1, 0 ); if (buf == MAP_FAILED) { perror(\"mmap\"); exit(1); } char * line = NULL; size_t size; printf(\"Memory address %p\\n\" \"This will only reserve and populate pages. Execute \\n\" \"grep -R '' /sys/kernel/mm/hugepages/hugepages-1048576kB/\\n\" \"find /sys -name meminfo | xargs grep -i huge\\n\" \"to verify this.\\n\\n\" \"When you are done, please hit return\\n\", buf); getline(&line,&size,stdin); } Testing: [root@dell-r430-30 ~]# gcc mmap2.c -o mmap2 [root@dell-r430-30 ~]# ./mmap2 Memory address 0x2aaac0000000 This will only reserve and populate pages. Execute grep -R '' /sys/kernel/mm/hugepages/hugepages-1048576kB/ find /sys -name meminfo | xargs grep -i huge to verify this. When you are done, please hit return [root@dell-r430-30 ~]# find /sys -name meminfo | xargs grep -i huge /sys/devices/system/node/node0/meminfo:Node 0 AnonHugePages: 0 kB /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Total: 16 /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Free: 16 /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Surp: 0 /sys/devices/system/node/node1/meminfo:Node 1 AnonHugePages: 6144 kB /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Total: 16 /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Free: 14 /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Surp: 0 [root@dell-r430-30 ~]# Sharing hugepages between processes In vhost_user, OVS-DPDK and qemu-kvm instances share the same hugepages for DMA copies. https://access.redhat.com/solutions/3394851 . Let's emulate this with 2 sample applications. http://nuncaalaprimera.com/2014/using-hugepage-backed-buffers-in-linux-kernel-driver IPC (Inter Process Communication) Let's start with IPC (Inter Process Communication) via Linux sockets as we'll have to communicate the location of the shared memory from the server process to the client process. ipc.c : /* * * This heavily borrows from examples in * \"The Linux Programming Interface\" * */ #include <stdio.h> #include <stdlib.h> #include <string.h> #include <stdbool.h> #include <sys/un.h> #include <sys/socket.h> #include <sys/ioctl.h> #include <signal.h> #include <sys/types.h> #include <sys/stat.h> #include <fcntl.h> #include <sys/mman.h> #define SOCKNAME \"/tmp/unix.socket\" #define HUGEPAGE_FILE_NAME \"/dev/hugepages/server\" #define NUM_PAGES 2 #define PAGE_SIZE (unsigned long) 1024 * 1024 * 1024 int bind_socket() { int sfd; struct sockaddr_un addr; sfd = socket(AF_UNIX, SOCK_STREAM, 0); if (sfd == -1) return -1; /* Create socket */ memset(&addr, 0, sizeof(struct sockaddr_un)); /* Clear structure */ addr.sun_family = AF_UNIX; /* UNIX domain address */ strncpy(addr.sun_path, SOCKNAME, sizeof(addr.sun_path) - 1); if (bind(sfd, (struct sockaddr *) &addr, sizeof(struct sockaddr_un)) == -1) return -1; return sfd; } int connect_socket() { int sfd; struct sockaddr_un addr; sfd = socket(AF_UNIX, SOCK_STREAM, 0); if (sfd == -1) return -1; /* Create socket */ memset(&addr, 0, sizeof(struct sockaddr_un)); /* Clear structure */ addr.sun_family = AF_UNIX; /* UNIX domain address */ strncpy(addr.sun_path, SOCKNAME, sizeof(addr.sun_path) - 1); if (connect(sfd, (struct sockaddr *) &addr, sizeof(struct sockaddr_un)) == -1) return -1; return sfd; } int share_memory_location(int fd, char * address) { if (listen(fd, 1) == -1) return -1; int cfd; // while(!sigint_received) { cfd = accept(fd, NULL, NULL); printf(\"Client connected\\n\"); if(cfd == -1) return -1; printf(\"Sending '%s' to client\\n\", address); dprintf(cfd, address); close(cfd); // } return 0; } int close_socket(int fd) { close(fd); unlink(SOCKNAME); return 0; } int read_memory_location(int fd, void * memory_location, int buf_size) { int bytes_read; bytes_read = read(fd, memory_location, 100); return 0; } int create_shared_hugepage() { int fd = open(HUGEPAGE_FILE_NAME, O_CREAT | O_RDWR, 0755); if (fd < 0) return -1; char * buf = mmap( (void *)(0x0UL), NUM_PAGES * PAGE_SIZE, PROT_READ | PROT_WRITE, MAP_SHARED | MAP_POPULATE, fd, 0 ); if (buf == MAP_FAILED) { return -1; } buf[0] = 'y'; return fd; } int read_from_shared_hugepage(char * page_location, void * return_buf) { int fd = open(page_location, O_RDWR, 0755); if (fd < 0) return -1; char * buf = mmap( (void *)(0x0UL), NUM_PAGES * PAGE_SIZE, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0 ); if (buf == MAP_FAILED) { return -1; } ((char *) return_buf)[0] = buf[0]; return fd; } int delete_shared_hugepage(int fd) { close(fd); unlink(HUGEPAGE_FILE_NAME); return 0; } int main(int argc , char ** argv) { if(argc < 2) { printf(\"Please provide either server or client as an argument\\n\"); exit(1); } bool server_mode = false; if(strcmp(argv[1], \"server\") == 0) { printf(\"Server mode ...\\n\"); server_mode = true; } else { printf(\"Client mode ...\\n\"); } int fd; if(server_mode) { char * memory_location = HUGEPAGE_FILE_NAME; int hp_fd = create_shared_hugepage(); if(hp_fd == -1) { perror(\"Couldn't allocate hugepage\"); exit(1); } fd = bind_socket(); if(fd == -1) { printf(\"Error binding socket (does %s exist? Delete it!)\\n\", SOCKNAME); perror(\"\"); exit(1); } if(share_memory_location(fd, memory_location) == -1) { perror(\"Error sharing memory location\"); exit(1); } sleep(5); delete_shared_hugepage(hp_fd); close_socket(fd); } else { char mem_loc[100]; fd = connect_socket(); if(read_memory_location(fd, &mem_loc, 100) == -1) { perror(\"Error reading memory location\"); exit(1); } printf(\"Memory location is '%s'\\n\", mem_loc); char hp_content[100]; read_from_shared_hugepage(mem_loc, hp_content); printf(\"Content of first byte at page '%s' is '%s'\\n\", mem_loc, hp_content[0]); } } Resources https://lwn.net/Articles/374424/ https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt","title":"Hugepages"},{"location":"linux/hugepages/#hugepages","text":"","title":"Hugepages"},{"location":"linux/hugepages/#requesting-hugepages-in-c","text":"","title":"Requesting hugepages in C"},{"location":"linux/hugepages/#example-1","text":"Create the code, mmap.c : #include <sys/mman.h> #include <stdio.h> #include <stdlib.h> #define PAGE_SIZE (unsigned int) 1024*1024*1024 #define NUM_PAGES 2 void main() { char * buf = mmap( NULL, NUM_PAGES * PAGE_SIZE, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS | MAP_HUGETLB, -1, 0 ); if (buf == MAP_FAILED) { perror(\"mmap\"); exit(1); } char * line = NULL; size_t size; printf(\"Memory address %p\\n\" \"This will only reserve pages. Execute \\n\" \"grep -R '' /sys/kernel/mm/hugepages/hugepages-1048576kB/\\n\" \"find /sys -name meminfo | xargs grep -i huge\\n\" \"to verify this.\\n\\n\" \"When you are done, please hit return\\n\", buf); getline(&line,&size,stdin); int i; printf(\"Now, actually writing all 0s into the first hugepage\\n\"); for(i = 0; i < PAGE_SIZE; i++) { buf[i] = '0'; } printf(\"Now, verify again\\n\" \"grep -R '' /sys/kernel/mm/hugepages/hugepages-1048576kB/\\n\" \"find /sys -name meminfo | xargs grep -i huge\\n\" \"to verify this.\\n\\n\" \"When you are done, please hit return\\n\", buf); getline(&line,&size,stdin); printf(\"Now, actually writing one 0 into the second hugepage\\n\"); for(; i < PAGE_SIZE + 1; i++) { buf[i] = '0'; } printf(\"Now, verify again\\n\" \"grep -R '' /sys/kernel/mm/hugepages/hugepages-1048576kB/\\n\" \"find /sys -name meminfo | xargs grep -i huge\\n\" \"to verify this.\\n\\n\" \"When you are done, please hit return to end the program\\n\", buf); getline(&line,&size,stdin); } Compile this: gcc mmap.c -o mmap Run the binary and follow the onscreen instructions: [root@dell-r430-30 ~]# ./mmap Memory address 0x2aaac0000000 This will only reserve pages. Execute grep -R '' /sys/kernel/mm/hugepages/hugepages-1048576kB/ find /sys -name meminfo | xargs grep -i huge to verify this. When you are done, please hit return Now, actually writing all 0s into the first hugepage ^C [root@dell-r430-30 ~]# ^C [root@dell-r430-30 ~]# ^C [root@dell-r430-30 ~]# gcc mmap.c -o mmap [root@dell-r430-30 ~]# ./mmap Memory address 0x2aaac0000000 This will only reserve pages. Execute grep -R '' /sys/kernel/mm/hugepages/hugepages-1048576kB/ find /sys -name meminfo | xargs grep -i huge to verify this. When you are done, please hit return Now, actually writing all 0s into the first hugepage Now, verify again grep -R '' /sys/kernel/mm/hugepages/hugepages-1048576kB/ find /sys -name meminfo | xargs grep -i huge to verify this. When you are done, please hit return Now, actually writing one 0 into the second hugepage Now, verify again grep -R '' /sys/kernel/mm/hugepages/hugepages-1048576kB/ find /sys -name meminfo | xargs grep -i huge to verify this. When you are done, please hit return to end the program Verify reserved hugepages and actually allocated hugepages. Also note that writing 1GB to the hugepage actually takes quite some time: [root@dell-r430-30 ~]# grep -R '' /sys/kernel/mm/hugepages/hugepages-1048576kB/ /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_overcommit_hugepages:0 /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages:32 /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages_mempolicy:32 /sys/kernel/mm/hugepages/hugepages-1048576kB/surplus_hugepages:0 /sys/kernel/mm/hugepages/hugepages-1048576kB/resv_hugepages:2 /sys/kernel/mm/hugepages/hugepages-1048576kB/free_hugepages:32 [root@dell-r430-30 ~]# find /sys -name meminfo | xargs grep -i huge /sys/devices/system/node/node0/meminfo:Node 0 AnonHugePages: 0 kB /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Total: 16 /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Free: 16 /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Surp: 0 /sys/devices/system/node/node1/meminfo:Node 1 AnonHugePages: 6144 kB /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Total: 16 /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Free: 16 /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Surp: 0 [root@dell-r430-30 ~]# grep -R '' /sys/kernel/mm/hugepages/hugepages-1048576kB/ /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_overcommit_hugepages:0 /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages:32 /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages_mempolicy:32 /sys/kernel/mm/hugepages/hugepages-1048576kB/surplus_hugepages:0 /sys/kernel/mm/hugepages/hugepages-1048576kB/resv_hugepages:1 /sys/kernel/mm/hugepages/hugepages-1048576kB/free_hugepages:31 [root@dell-r430-30 ~]# find /sys -name meminfo | xargs grep -i huge /sys/devices/system/node/node0/meminfo:Node 0 AnonHugePages: 0 kB /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Total: 16 /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Free: 15 /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Surp: 0 /sys/devices/system/node/node1/meminfo:Node 1 AnonHugePages: 6144 kB /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Total: 16 /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Free: 16 /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Surp: 0 [root@dell-r430-30 ~]# grep -R '' /sys/kernel/mm/hugepages/hugepages-1048576kB/ /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_overcommit_hugepages:0 /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages:32 /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages_mempolicy:32 /sys/kernel/mm/hugepages/hugepages-1048576kB/surplus_hugepages:0 /sys/kernel/mm/hugepages/hugepages-1048576kB/resv_hugepages:0 /sys/kernel/mm/hugepages/hugepages-1048576kB/free_hugepages:30 [root@dell-r430-30 ~]# find /sys -name meminfo | xargs grep -i huge /sys/devices/system/node/node0/meminfo:Node 0 AnonHugePages: 0 kB /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Total: 16 /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Free: 14 /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Surp: 0 /sys/devices/system/node/node1/meminfo:Node 1 AnonHugePages: 6144 kB /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Total: 16 /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Free: 16 /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Surp: 0 [root@dell-r430-30 ~]# find /sys -name meminfo | xargs grep -i huge /sys/devices/system/node/node0/meminfo:Node 0 AnonHugePages: 0 kB /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Total: 16 /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Free: 16 /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Surp: 0 /sys/devices/system/node/node1/meminfo:Node 1 AnonHugePages: 6144 kB /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Total: 16 /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Free: 16 /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Surp: 0","title":"Example 1"},{"location":"linux/hugepages/#example-2-allocating-hugepages-right-away","text":"Instead of having to write to the hugepage to allocate it, we can populate the page right away and it will show up in used pages right away: man mmap (...) MAP_POPULATE (since Linux 2.5.46) Populate (prefault) page tables for a mapping. For a file mapping, this causes read-ahead on the file. Later accesses to the mapping will not be blocked by page faults. MAP_POPULATE is supported for private mappings only since Linux 2.6.23. (...) The application is mmap2.c : #include <sys/mman.h> #include <stdio.h> #include <stdlib.h> #define PAGE_SIZE (unsigned int) 1024*1024*1024 #define NUM_PAGES 2 void main() { char * buf = mmap( NULL, NUM_PAGES * PAGE_SIZE, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS | MAP_HUGETLB | MAP_POPULATE, -1, 0 ); if (buf == MAP_FAILED) { perror(\"mmap\"); exit(1); } char * line = NULL; size_t size; printf(\"Memory address %p\\n\" \"This will only reserve and populate pages. Execute \\n\" \"grep -R '' /sys/kernel/mm/hugepages/hugepages-1048576kB/\\n\" \"find /sys -name meminfo | xargs grep -i huge\\n\" \"to verify this.\\n\\n\" \"When you are done, please hit return\\n\", buf); getline(&line,&size,stdin); } Testing: [root@dell-r430-30 ~]# gcc mmap2.c -o mmap2 [root@dell-r430-30 ~]# ./mmap2 Memory address 0x2aaac0000000 This will only reserve and populate pages. Execute grep -R '' /sys/kernel/mm/hugepages/hugepages-1048576kB/ find /sys -name meminfo | xargs grep -i huge to verify this. When you are done, please hit return [root@dell-r430-30 ~]# find /sys -name meminfo | xargs grep -i huge /sys/devices/system/node/node0/meminfo:Node 0 AnonHugePages: 0 kB /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Total: 16 /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Free: 16 /sys/devices/system/node/node0/meminfo:Node 0 HugePages_Surp: 0 /sys/devices/system/node/node1/meminfo:Node 1 AnonHugePages: 6144 kB /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Total: 16 /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Free: 14 /sys/devices/system/node/node1/meminfo:Node 1 HugePages_Surp: 0 [root@dell-r430-30 ~]#","title":"Example 2 - Allocating hugepages right away"},{"location":"linux/hugepages/#sharing-hugepages-between-processes","text":"In vhost_user, OVS-DPDK and qemu-kvm instances share the same hugepages for DMA copies. https://access.redhat.com/solutions/3394851 . Let's emulate this with 2 sample applications. http://nuncaalaprimera.com/2014/using-hugepage-backed-buffers-in-linux-kernel-driver","title":"Sharing hugepages between processes"},{"location":"linux/hugepages/#ipc-inter-process-communication","text":"Let's start with IPC (Inter Process Communication) via Linux sockets as we'll have to communicate the location of the shared memory from the server process to the client process. ipc.c : /* * * This heavily borrows from examples in * \"The Linux Programming Interface\" * */ #include <stdio.h> #include <stdlib.h> #include <string.h> #include <stdbool.h> #include <sys/un.h> #include <sys/socket.h> #include <sys/ioctl.h> #include <signal.h> #include <sys/types.h> #include <sys/stat.h> #include <fcntl.h> #include <sys/mman.h> #define SOCKNAME \"/tmp/unix.socket\" #define HUGEPAGE_FILE_NAME \"/dev/hugepages/server\" #define NUM_PAGES 2 #define PAGE_SIZE (unsigned long) 1024 * 1024 * 1024 int bind_socket() { int sfd; struct sockaddr_un addr; sfd = socket(AF_UNIX, SOCK_STREAM, 0); if (sfd == -1) return -1; /* Create socket */ memset(&addr, 0, sizeof(struct sockaddr_un)); /* Clear structure */ addr.sun_family = AF_UNIX; /* UNIX domain address */ strncpy(addr.sun_path, SOCKNAME, sizeof(addr.sun_path) - 1); if (bind(sfd, (struct sockaddr *) &addr, sizeof(struct sockaddr_un)) == -1) return -1; return sfd; } int connect_socket() { int sfd; struct sockaddr_un addr; sfd = socket(AF_UNIX, SOCK_STREAM, 0); if (sfd == -1) return -1; /* Create socket */ memset(&addr, 0, sizeof(struct sockaddr_un)); /* Clear structure */ addr.sun_family = AF_UNIX; /* UNIX domain address */ strncpy(addr.sun_path, SOCKNAME, sizeof(addr.sun_path) - 1); if (connect(sfd, (struct sockaddr *) &addr, sizeof(struct sockaddr_un)) == -1) return -1; return sfd; } int share_memory_location(int fd, char * address) { if (listen(fd, 1) == -1) return -1; int cfd; // while(!sigint_received) { cfd = accept(fd, NULL, NULL); printf(\"Client connected\\n\"); if(cfd == -1) return -1; printf(\"Sending '%s' to client\\n\", address); dprintf(cfd, address); close(cfd); // } return 0; } int close_socket(int fd) { close(fd); unlink(SOCKNAME); return 0; } int read_memory_location(int fd, void * memory_location, int buf_size) { int bytes_read; bytes_read = read(fd, memory_location, 100); return 0; } int create_shared_hugepage() { int fd = open(HUGEPAGE_FILE_NAME, O_CREAT | O_RDWR, 0755); if (fd < 0) return -1; char * buf = mmap( (void *)(0x0UL), NUM_PAGES * PAGE_SIZE, PROT_READ | PROT_WRITE, MAP_SHARED | MAP_POPULATE, fd, 0 ); if (buf == MAP_FAILED) { return -1; } buf[0] = 'y'; return fd; } int read_from_shared_hugepage(char * page_location, void * return_buf) { int fd = open(page_location, O_RDWR, 0755); if (fd < 0) return -1; char * buf = mmap( (void *)(0x0UL), NUM_PAGES * PAGE_SIZE, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0 ); if (buf == MAP_FAILED) { return -1; } ((char *) return_buf)[0] = buf[0]; return fd; } int delete_shared_hugepage(int fd) { close(fd); unlink(HUGEPAGE_FILE_NAME); return 0; } int main(int argc , char ** argv) { if(argc < 2) { printf(\"Please provide either server or client as an argument\\n\"); exit(1); } bool server_mode = false; if(strcmp(argv[1], \"server\") == 0) { printf(\"Server mode ...\\n\"); server_mode = true; } else { printf(\"Client mode ...\\n\"); } int fd; if(server_mode) { char * memory_location = HUGEPAGE_FILE_NAME; int hp_fd = create_shared_hugepage(); if(hp_fd == -1) { perror(\"Couldn't allocate hugepage\"); exit(1); } fd = bind_socket(); if(fd == -1) { printf(\"Error binding socket (does %s exist? Delete it!)\\n\", SOCKNAME); perror(\"\"); exit(1); } if(share_memory_location(fd, memory_location) == -1) { perror(\"Error sharing memory location\"); exit(1); } sleep(5); delete_shared_hugepage(hp_fd); close_socket(fd); } else { char mem_loc[100]; fd = connect_socket(); if(read_memory_location(fd, &mem_loc, 100) == -1) { perror(\"Error reading memory location\"); exit(1); } printf(\"Memory location is '%s'\\n\", mem_loc); char hp_content[100]; read_from_shared_hugepage(mem_loc, hp_content); printf(\"Content of first byte at page '%s' is '%s'\\n\", mem_loc, hp_content[0]); } }","title":"IPC (Inter Process Communication)"},{"location":"linux/hugepages/#resources","text":"https://lwn.net/Articles/374424/ https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt","title":"Resources"},{"location":"linux/meson/","text":"meson What is meson? Meson is a new build system, meant to replace make . https://mesonbuild.com/index.html Overview Meson is an open source build system meant to be both extremely fast, and, even more importantly, as user friendly as possible. The main design point of Meson is that every moment a developer spends writing or debugging build definitions is a second wasted. So is every second spent waiting for the build system to actually start compiling code. Features multiplatform support for Linux, macOS, Windows, GCC, Clang, Visual Studio and others supported languages include C, C++, D, Fortran, Java, Rust build definitions in a very readable and user friendly non-Turing complete DSL cross compilation for many operating systems as well as bare metal optimized for extremely fast full and incremental builds without sacrificing correctness built-in multiplatform dependency provider that works together with distro packages fun! Who is using meson? https://mesonbuild.com/Users.html Requirements for meson Meson needs Python 3.5 or higher: https://mesonbuild.com/Getting-meson.html How to get meson For RHEL, it will be released with RHEL 8: https://access.redhat.com/downloads/content/meson/0.45.1-2.el8/noarch/f21541eb/package","title":"meson"},{"location":"linux/meson/#meson","text":"","title":"meson"},{"location":"linux/meson/#what-is-meson","text":"Meson is a new build system, meant to replace make . https://mesonbuild.com/index.html Overview Meson is an open source build system meant to be both extremely fast, and, even more importantly, as user friendly as possible. The main design point of Meson is that every moment a developer spends writing or debugging build definitions is a second wasted. So is every second spent waiting for the build system to actually start compiling code. Features multiplatform support for Linux, macOS, Windows, GCC, Clang, Visual Studio and others supported languages include C, C++, D, Fortran, Java, Rust build definitions in a very readable and user friendly non-Turing complete DSL cross compilation for many operating systems as well as bare metal optimized for extremely fast full and incremental builds without sacrificing correctness built-in multiplatform dependency provider that works together with distro packages fun!","title":"What is meson?"},{"location":"linux/meson/#who-is-using-meson","text":"https://mesonbuild.com/Users.html","title":"Who is using meson?"},{"location":"linux/meson/#requirements-for-meson","text":"Meson needs Python 3.5 or higher: https://mesonbuild.com/Getting-meson.html","title":"Requirements for meson"},{"location":"linux/meson/#how-to-get-meson","text":"For RHEL, it will be released with RHEL 8: https://access.redhat.com/downloads/content/meson/0.45.1-2.el8/noarch/f21541eb/package","title":"How to get meson"},{"location":"linux/namespaces/","text":"Linux namespaces What is a namespace? one important component of containers process within the namespace sees its own isolated instance of a resource the resources are visible only to other processes that are members of the namespace limit what you can view on a system each process is a member of one namespace of each resource namespace resources are: Cgroup, IPC, Network, Mount, PID, User, UTS man 7 namespaces : (...) A namespace wraps a global system resource in an abstraction that makes it appear to the processes within the namespace that they have their own isolated instance of the global resource. Changes to the global resource are visible to other processes that are members of the namespace, but are invisible to other processes. One use of namespaces is to implement containers. (...) unshare - the tool to create new namespaces A child process starts within the same namespaces as its parent. In order to move a process to a new namespace, it needs to be spawned with unshare : man unshare NAME unshare - run program with some namespaces unshared from parent SYNOPSIS unshare [options] [program [arguments]] DESCRIPTION Unshares the indicated namespaces from the parent process and then executes the specified program. If program is not given, then ``${SHELL}'' is run (default: /bin/sh). The namespaces can optionally be made persistent by bind mounting /proc/pid/ns/type files to a filesystem path and entered with nsenter(1) even after the program terminates (except PID namespaces where permanently running init process is required). Once a persistent namespace is no longer needed, it can be unpersisted with umount(8). See the EXAMPLES section for more details. The namespaces to be unshared are indicated via options. Unshareable namespaces are: mount namespace Mounting and unmounting filesystems will not affect the rest of the system, except for filesystems which are explicitly marked as shared (with mount --make-shared; see /proc/self/mountinfo or findmnt -o+PROPAGATION for the shared flags). For further details, see mount_namespaces(7) and the discussion of the CLONE_NEWNS flag in clone(2). unshare since util-linux version 2.27 automatically sets propagation to private in a new mount namespace to make sure that the new namespace is really unshared. It's possible to disable this feature with option --propagation unchanged. Note that private is the kernel default. UTS namespace Setting hostname or domainname will not affect the rest of the system. For further details, see namespaces(7) and the discussion of the CLONE_NEWUTS flag in clone(2). IPC namespace The process will have an independent namespace for POSIX message queues as well as System V message queues, semaphore sets and shared memory segments. For further details, see names\u2010 paces(7) and the discussion of the CLONE_NEWIPC flag in clone(2). network namespace The process will have independent IPv4 and IPv6 stacks, IP routing tables, firewall rules, the /proc/net and /sys/class/net directory trees, sockets, etc. For further details, see names\u2010 paces(7) and the discussion of the CLONE_NEWNET flag in clone(2). PID namespace Children will have a distinct set of PID-to-process mappings from their parent. For further details, see pid_namespaces(7) and the discussion of the CLONE_NEWPID flag in clone(2). cgroup namespace The process will have a virtualized view of /proc/self/cgroup, and new cgroup mounts will be rooted at the namespace cgroup root. For further details, see cgroup_namespaces(7) and the dis\u2010 cussion of the CLONE_NEWCGROUP flag in clone(2). user namespace The process will have a distinct set of UIDs, GIDs and capabilities. For further details, see user_namespaces(7) and the discussion of the CLONE_NEWUSER flag in clone(2). OPTIONS -i, --ipc[=file] Unshare the IPC namespace. If file is specified, then a persistent namespace is created by a bind mount. -m, --mount[=file] Unshare the mount namespace. If file is specified, then a persistent namespace is created by a bind mount. Note that file has to be located on a filesystem with the propagation flag set to private. Use the command findmnt -o+PROPAGATION when not sure about the current setting. See also the examples below. -n, --net[=file] Unshare the network namespace. If file is specified, then a persistent namespace is created by a bind mount. -p, --pid[=file] Unshare the PID namespace. If file is specified then persistent namespace is created by a bind mount. See also the --fork and --mount-proc options. -u, --uts[=file] Unshare the UTS namespace. If file is specified, then a persistent namespace is created by a bind mount. -U, --user[=file] Unshare the user namespace. If file is specified, then a persistent namespace is created by a bind mount. -C, --cgroup[=file] Unshare the cgroup namespace. If file is specified then persistent namespace is created by bind mount. -f, --fork Fork the specified program as a child process of unshare rather than running it directly. This is useful when creating a new PID namespace. (...) unshare uses the unshare system call: [akaris@wks-akaris ~]$ sudo strace -tt -f -s1024 unshare -u /bin/bash 2>&1 | grep -i uts 15:54:50.016682 unshare(CLONE_NEWUTS) = 0 man unshare UNSHARE(2) Linux Programmer's Manual UNSHARE(2) NAME unshare - disassociate parts of the process execution context SYNOPSIS #define _GNU_SOURCE #include <sched.h> int unshare(int flags); DESCRIPTION unshare() allows a process (or thread) to disassociate parts of its execution context that are currently being shared with other processes (or threads). Part of the execu\u2010 tion context, such as the mount namespace, is shared implicitly when a new process is created using fork(2) or vfork(2), while other parts, such as virtual memory, may be shared by explicit request when creating a process or thread using clone(2). The main use of unshare() is to allow a process to control its shared execution context without creating a new process. (...) However, namespaces can also be created by feeding flags to the clone system call: [akaris@wks-akaris ~]$ man clone | egrep '^[ ]+CLONE_NEW' CLONE_NEWCGROUP (since Linux 4.6) CLONE_NEWIPC (since Linux 2.6.19) CLONE_NEWNET (since Linux 2.6.24) CLONE_NEWNS (since Linux 2.4.19) CLONE_NEWPID (since Linux 2.6.24) CLONE_NEWUSER CLONE_NEWUTS (since Linux 2.6.19) CLONE_NEWPID was specified in flags, but the limit on the nesting depth of PID CLONE_NEWUSER was specified in flags, and the call would cause the limit on the CLONE_NEWUTS was specified by an unprivileged process (process without CLONE_NEWUSER was specified in flags and the caller is in a chroot environment CLONE_NEWUSER was specified in flags, and the limit on the number of nested user How to list namespaces Note that when a namespace is unshared, it will show up with a different namespace ID in /proc/$PID/ns/ : [root@wks-akaris akaris]# sudo ls /proc/1/ns/ -al total 0 dr-x--x--x. 2 root root 0 Dec 20 15:47 . dr-xr-xr-x. 9 root root 0 Dec 20 09:59 .. lrwxrwxrwx. 1 root root 0 Dec 20 15:47 cgroup -> 'cgroup:[4026531835]' lrwxrwxrwx. 1 root root 0 Dec 20 15:47 ipc -> 'ipc:[4026531839]' lrwxrwxrwx. 1 root root 0 Dec 20 15:47 mnt -> 'mnt:[4026531840]' lrwxrwxrwx. 1 root root 0 Dec 20 15:47 net -> 'net:[4026532008]' lrwxrwxrwx. 1 root root 0 Dec 20 15:47 pid -> 'pid:[4026531836]' lrwxrwxrwx. 1 root root 0 Dec 20 15:47 pid_for_children -> 'pid:[4026531836]' lrwxrwxrwx. 1 root root 0 Dec 20 15:47 user -> 'user:[4026531837]' lrwxrwxrwx. 1 root root 0 Dec 20 15:47 uts -> 'uts:[4026531838]' [root@wks-akaris akaris]# sudo ls /proc/$$/ns/ -al total 0 dr-x--x--x. 2 root root 0 Dec 20 15:45 . dr-xr-xr-x. 9 root root 0 Dec 20 15:43 .. lrwxrwxrwx. 1 root root 0 Dec 20 15:45 cgroup -> 'cgroup:[4026531835]' lrwxrwxrwx. 1 root root 0 Dec 20 15:45 ipc -> 'ipc:[4026531839]' lrwxrwxrwx. 1 root root 0 Dec 20 15:45 mnt -> 'mnt:[4026531840]' lrwxrwxrwx. 1 root root 0 Dec 20 15:45 net -> 'net:[4026532008]' lrwxrwxrwx. 1 root root 0 Dec 20 15:45 pid -> 'pid:[4026531836]' lrwxrwxrwx. 1 root root 0 Dec 20 15:45 pid_for_children -> 'pid:[4026531836]' lrwxrwxrwx. 1 root root 0 Dec 20 15:45 user -> 'user:[4026531837]' lrwxrwxrwx. 1 root root 0 Dec 20 15:45 uts -> 'uts:[4026532828]' Or, a more compact example of the above: [akaris@wks-akaris ~]$ sudo unshare -m uts /bin/bash [sudo] password for akaris: unshare: failed to execute uts: No such file or directory [akaris@wks-akaris ~]$ sudo unshare -u /bin/bash [root@wks-akaris akaris]# readlink /proc/1/ns/uts uts:[4026531838] [root@wks-akaris akaris]# readlink /proc/$$/ns/uts uts:[4026532828] The easiest and most detailed way to show the namespace hierarchy, though, is lsns : man lsns LSNS(8) System Administration LSNS(8) NAME lsns - list namespaces SYNOPSIS lsns [options] [namespace] DESCRIPTION lsns lists information about all the currently accessible namespaces or about the given namespace. The namespace identifier is an inode number. The default output is subject to change. So whenever possible, you should avoid using default outputs in your scripts. Always explicitly define expected columns by using the --output option together with a columns list in environments where a stable output is required. NSFS column, printed when net is specified for --type option, is special; it uses multi-line cells. Use the option --nowrap is for switching to \",\" separated single-line representation. Note that lsns reads information directly from the /proc filesystem and for non-root users it may return incomplete information. The current /proc filesystem may be unshared and affected by a PID namespace (see unshare --mount-proc for more details). lsns is not able to see persistent namespaces without processes where the namespace instance is held by a bind mount to /proc/pid/ns/type. (...) Example: [akaris@wks-akaris ~]$ sudo lsns -t uts NS TYPE NPROCS PID USER COMMAND 4026531838 uts 288 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 32 [akaris@wks-akaris ~]$ sudo unshare -u /bin/bash [root@wks-akaris akaris]# lsns -t uts NS TYPE NPROCS PID USER COMMAND 4026531838 uts 289 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 32 4026532849 uts 2 13226 root /bin/bash [root@wks-akaris akaris]# unshare -u /bin/bash [root@wks-akaris akaris]# lsns -t uts NS TYPE NPROCS PID USER COMMAND 4026531838 uts 289 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 32 4026532849 uts 1 13226 root /bin/bash 4026532850 uts 2 13257 root /bin/bash [root@wks-akaris akaris]# readlink /proc/$$/ns/uts uts:[4026532850] [root@wks-akaris akaris]# exit exit [root@wks-akaris akaris]# readlink /proc/$$/ns/uts uts:[4026532849] [root@wks-akaris akaris]# exit exit [akaris@wks-akaris ~]$ readlink /proc/$$/ns/uts uts:[4026531838] namespace types UTS namespace UTS stands for the datastructure that stores the identifiers returned by uname. UTS namespaces allow every process to have its own hostname and domain name. Example Create an instance of /bin/bash in its own UTS namespace and assign it its own hostname: [akaris@wks-akaris ~]$ sudo unshare -u /bin/bash [root@wks-akaris akaris]# hostname utsnamespace [root@wks-akaris akaris]# bash [root@utsnamespace akaris]# [root@wks-akaris akaris]# hostname utsnamespace [root@wks-akaris akaris]# uname -a Linux utsnamespace 4.18.12-200.fc28.x86_64 #1 SMP Thu Oct 4 15:46:35 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux Open another CLI on the same system and verify the hostname: [akaris@wks-akaris ~]$ hostname wks-akaris [akaris@wks-akaris ~]$ uname -a Linux wks-akaris 4.18.12-200.fc28.x86_64 #1 SMP Thu Oct 4 15:46:35 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux network namespaces Allow to add a private network stack to processes within a network namespace. Let you move interfaces from one network namespace to another. veth interfaces can connect namespaces together. man ip-netns (...) A network namespace is logically another copy of the network stack, with its own routes, firewall rules, and network devices. By default a process inherits its network namespace from its parent. Initially all the processes share the same default network namespace from the init process. By convention a named network namespace is an object at /var/run/netns/NAME that can be opened. The file descriptor resulting from opening /var/run/netns/NAME refers to the spec\u2010 ified network namespace. Holding that file descriptor open keeps the network namespace alive. The file descriptor can be used with the setns(2) system call to change the network namespace associated with a task. (...) Red Hat Enterprise Linux Atomis Host 7 Overview of Containers in Red Hat Systems: Network namespaces provide isolation of network controllers, system resources associated with networking, firewall and routing tables. This allows container to use separate virtual network stack, loopback device and process space. You can add virtual or real devices to the container, assign them their own IP Addresses and even full iptables rules. You can view the different network settings by executing the ip addr command on the host and inside the container. Example There are 2 ways to generate persistent network namespaces. Either use ip netns add , or use the more inconvenient unshare command: [akaris@wks-akaris ~]$ sudo ip netns add netns1 [akaris@wks-akaris ~]$ sudo ip netns netns1 [akaris@wks-akaris ~]$ sudo mount | grep netns tmpfs on /run/netns type tmpfs (rw,nosuid,nodev,seclabel,mode=755) nsfs on /run/netns/netns1 type nsfs (rw,seclabel) nsfs on /run/netns/netns1 type nsfs (rw,seclabel) [akaris@wks-akaris ~]$ sudo touch /run/netns/netns2 [akaris@wks-akaris ~]$ sudo unshare --net=!$ /bin/bash sudo unshare --net=/run/netns/netns2 /bin/bash [root@wks-akaris akaris]# exit exit [akaris@wks-akaris ~]$ sudo ip netns netns2 netns1 [akaris@wks-akaris ~]$ sudo mount | grep netns tmpfs on /run/netns type tmpfs (rw,nosuid,nodev,seclabel,mode=755) nsfs on /run/netns/netns1 type nsfs (rw,seclabel) nsfs on /run/netns/netns1 type nsfs (rw,seclabel) nsfs on /run/netns/netns2 type nsfs (rw,seclabel) nsfs on /run/netns/netns2 type nsfs (rw,seclabel) In both cases, you will be able to enter the namespace and create interfaces therein: [akaris@wks-akaris ~]$ sudo ip netns exec netns2 /bin/bash [root@wks-akaris akaris]# ip link ls 1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 [root@wks-akaris akaris]# sudo ip link add veth0 type veth peer name veth1 [root@wks-akaris akaris]# ip link ls 1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: veth1@veth0: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether 1e:a0:b3:b7:e6:db brd ff:ff:ff:ff:ff:ff 3: veth0@veth1: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether ee:1a:b2:f0:07:87 brd ff:ff:ff:ff:ff:ff [root@wks-akaris akaris]# exit exit [akaris@wks-akaris ~]$ sudo ip netns exec netns1 /bin/bash [root@wks-akaris akaris]# ip link ls 1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 [root@wks-akaris akaris]# exit exit mount namespaces mount namespaces allow processes to have their own mount points, such as :root fs, /tmp, /proc/, etc. within a mount namespace, it's possible to mount and unmount mount points without affecting any other namespace man unshare (...) mount namespace Mounting and unmounting filesystems will not affect the rest of the system, except for filesystems which are explicitly marked as shared (with mount --make-shared; see /proc/self/mountinfo or findmnt -o+PROPAGATION for the shared flags). For further details, see mount_namespaces(7) and the discussion of the CLONE_NEWNS flag in clone(2). unshare since util-linux version 2.27 automatically sets propagation to private in a new mount namespace to make sure that the new namespace is really unshared. It's possible to disable this feature with option --propagation unchanged. Note that pri\u2010 vate is the kernel default. (...) man mount_namespaces (...) Mount namespaces provide isolation of the list of mount points seen by the processes in each namespace instance. Thus, the processes in each of the mount namespace instances will see distinct single-directory hierarchies. The views provided by the /proc/[pid]/mounts, /proc/[pid]/mountinfo, and /proc/[pid]/mountstats files (all described in proc(5)) correspond to the mount namespace in which the process with the PID [pid] resides. (All of the processes that reside in the same mount namespace will see the same view in these files.) When a process creates a new mount namespace using clone(2) or unshare(2) with the CLONE_NEWNS flag, the mount point list for the new namespace is a copy of the caller's mount point list. Subsequent modifications to the mount point list (mount(2) and umount(2)) in either mount namespace will not (by default) affect the mount point list seen in the other namespace (but see the following discussion of shared subtrees). (...) Red Hat Enterprise Linux Atomis Host 7 Overview of Containers in Red Hat Systems: Mount namespaces isolate the set of file system mount points seen by a group of processes so that processes in different mount namespaces can have different views of the file system hierarchy. With mount namespaces, the mount() and umount() system calls cease to operate on a global set of mount points (visible to all processes) and instead perform operations that affect just the mount namespace associated with the container process. For example, each container can have its own /tmp or /var directory or even have an entirely different userspace. Example [akaris@wks-akaris ~]$ sudo unshare -m /bin/bash [root@wks-akaris akaris]# lsns -t mnt NS TYPE NPROCS PID USER COMMAND 4026531840 mnt 297 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 32 4026531860 mnt 1 33 root kdevtmpfs 4026532186 mnt 1 983 root /usr/lib/systemd/systemd-udevd 4026532508 mnt 1 1261 rtkit /usr/libexec/rtkit-daemon 4026532509 mnt 1 1276 chrony /usr/sbin/chronyd 4026532510 mnt 4 1460 root /usr/sbin/NetworkManager --no-daemon 4026532511 mnt 1 2481 root /usr/libexec/bluetooth/bluetoothd 4026532514 mnt 1 3045 root /usr/libexec/fwupd/fwupd 4026532665 mnt 1 1541 colord /usr/libexec/colord 4026532749 mnt 1 2030 root /usr/libexec/boltd 4026532828 mnt 2 17149 root /bin/bash [root@wks-akaris akaris]# umount -a -l [root@wks-akaris akaris]# ls /dev/ [root@wks-akaris akaris]# ls /proc [root@wks-akaris akaris]# exit exit [akaris@wks-akaris ~]$ ls /dev | head -2 autofs block PID namespaces isolate the process ID number space different processes in different PID namespaces can have the same PID a process can only see processes in its own PID namespace. a process has a PID per namespace The global namespace has a different PID for the same process than the 'custom' namespace man pid_namespaces (...) PID namespaces isolate the process ID number space, meaning that processes in different PID namespaces can have the same PID. PID namespaces allow containers to provide func\u2010 tionality such as suspending/resuming the set of processes in the container and migrating the container to a new host while the processes inside the container maintain the same PIDs. PIDs in a new PID namespace start at 1, somewhat like a standalone system, and calls to fork(2), vfork(2), or clone(2) will produce processes with PIDs that are unique within the namespace. (...) The first process created in a new namespace (i.e., the process created using clone(2) with the CLONE_NEWPID flag, or the first child created by a process after a call to unshare(2) using the CLONE_NEWPID flag) has the PID 1, and is the \"init\" process for the namespace (see init(1)). A child process that is orphaned within the namespace will be reparented to this process rather than init(1) (unless one of the ancestors of the child in the same PID namespace employed the prctl(2) PR_SET_CHILD_SUBREAPER command to mark itself as the reaper of orphaned descendant processes). If the \"init\" process of a PID namespace terminates, the kernel terminates all of the processes in the namespace via a SIGKILL signal. This behavior reflects the fact that the \"init\" process is essential for the correct operation of a PID namespace. In this case, a subsequent fork(2) into this PID namespace fail with the error ENOMEM; it is not possi\u2010 ble to create a new processes in a PID namespace whose \"init\" process has terminated. (...) Red Hat Enterprise Linux Atomis Host 7 Overview of Containers in Red Hat Systems: PID namespaces allow processes in different containers to have the same PID, so each container can have its own init (PID1) process that manages various system initialization tasks as well as containers life cycle. Also, each container has its unique /proc directory. Note that from within the container you can monitor only processes running inside this container. In other words, the container is only aware of its native processes and can not \"see\" the processes running in different parts of the system. On the other hand, the host operating system is aware of processes running inside of the container, but assigns them different PID numbers. For example, run the ps -eZ | grep systemd$ command on host to see all instances of systemd including those running inside of containers. Example Note that PID namespaces need to be unshared with --fork . Also note that this will reset the PID space, however this will not show in ps as the information comes from the /proc file system: [akaris@wks-akaris ~]$ sudo unshare -p --fork /bin/bash [root@wks-akaris akaris]# echo $$ 1 [root@wks-akaris akaris]# ps PID TTY TIME CMD 17365 pts/5 00:00:00 sudo 17367 pts/5 00:00:00 unshare 17368 pts/5 00:00:00 bash 17399 pts/5 00:00:00 ps We need a more complex series of commands to display the actual process IDs in ps - we need to create both a new PID and a new mount namespace and then unmount and remount proc: [akaris@wks-akaris ~]$ sudo unshare -p --fork -m /bin/bash [root@wks-akaris akaris]# umount -l /proc [root@wks-akaris akaris]# mount proc /proc -t proc [root@wks-akaris akaris]# ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.1 0.0 123344 5916 pts/5 S 19:01 0:00 /bin/bash root 31 0.0 0.0 153224 3700 pts/5 R+ 19:01 0:00 ps aux [root@wks-akaris akaris]# user namespaces Allows UID/GID mapping. Within the namespace, a user can have UID 0 (root) but this will be squashed to root outside of the container. man user_namespaces (...) User namespaces isolate security-related identifiers and attributes, in particular, user IDs and group IDs (see credentials(7)), the root directory, keys (see keyrings(7)), and capabilities (see capabilities(7)). A process's user and group IDs can be different inside and outside a user namespace. In particular, a process can have a normal unprivileged user ID outside a user namespace while at the same time having a user ID of 0 inside the namespace; in other words, the process has full privileges for operations inside the user namespace, but is unprivileged for operations outside the namespace. (...) Red Hat Enterprise Linux Atomis Host 7 Overview of Containers in Red Hat Systems: User namespaces are similar to PID namespaces, they allow you to specify a range of host UIDs dedicated to the container. Consequently, a process can have full root privileges for operations inside the container, and at the same time be unprivileged for operations outside the container. For compatibility reasons, user namespaces are turned off in the current version of Red Hat Enterprise Linux 7, but will be enabled in the near future. Example [akaris@wks-akaris ~]$ sudo unshare -U /bin/bash [nfsnobody@wks-akaris akaris]$ id uid=65534(nfsnobody) gid=65534(nfsnobody) groups=65534(nfsnobody) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 [nfsnobody@wks-akaris akaris]$ echo $$ 10086 Open another CLI and run: [akaris@wks-akaris ~]$ sudo newuidmap 10086 0 0 1 [akaris@wks-akaris ~]$ Now, within the namespace, verify what happened: [nfsnobody@wks-akaris akaris]$ id uid=0(root) gid=65534(nfsnobody) groups=65534(nfsnobody) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 [nfsnobody@wks-akaris akaris]$ IPC namespaces Isolates interprocess communication A process / container can have its own semaphores, message queues, shared memory, etc Red Hat Enterprise Linux Atomis Host 7 Overview of Containers in Red Hat Systems: IPC namespaces isolate certain interprocess communication (IPC) resources, such as System V IPC objects and POSIX message queues. This means that two containers can create shared memory segments and semaphores with the same name, but are not able to interact with other containers memory segments or shared memory. cgroup namespaces and their purpose man cgroup_namespaces (...) Among the purposes served by the virtualization provided by cgroup namespaces are the following: * It prevents information leaks whereby cgroup directory paths outside of a container would otherwise be visible to processes in the container. Such leakages could, for example, reveal information about the container framework to containerized applications. * It eases tasks such as container migration. The virtualization provided by cgroup namespaces allows containers to be isolated from knowledge of the pathnames of ancestor cgroups. Without such isolation, the full cgroup pathnames (displayed in /proc/self/cgroups) would need to be replicated on the target system when migrating a container; those pathnames would also need to be unique, so that they don't conflict with other pathnames on the target system. * It allows better confinement of containerized processes, because it is possible to mount the container's cgroup filesystems such that the container processes can't gain access to ancestor cgroup directories. Consider, for example, the following scenario: \u00b7 We have a cgroup directory, /cg/1, that is owned by user ID 9000. \u00b7 We have a process, X, also owned by user ID 9000, that is namespaced under the cgroup /cg/1/2 (i.e., X was placed in a new cgroup namespace via clone(2) or unshare(2) with the CLONE_NEWCGROUP flag). In the absence of cgroup namespacing, because the cgroup directory /cg/1 is owned (and writable) by UID 9000 and process X is also owned by user ID 9000, then process X would be able to modify the contents of cgroups files (i.e., change cgroup settings) not only in /cg/1/2 but also in the ancestor cgroup directory /cg/1. Namespacing process X under the cgroup directory /cg/1/2, in combination with suitable mount operations for the cgroup filesystem (as shown above), prevents it modifying files in /cg/1, since it cannot even see the contents of that directory (or of further removed cgroup ancestor directories). Combined with correct enforcement of hierarchical limits, this prevents process X from escaping the limits imposed by ancestor cgroups. (...) Building containers - the namespace part of it A nice walkthrough can be found in this youtube video: https://youtu.be/sK5i-N34im8?t=2476 Resources manpages: [akaris@wks-akaris blog]$ apropos namespace | grep _namespaces cgroup_namespaces (7) - overview of Linux cgroup namespaces mount_namespaces (7) - overview of Linux mount namespaces network_namespaces (7) - overview of Linux network namespaces pid_namespaces (7) - overview of Linux PID namespaces user_namespaces (7) - overview of Linux user namespaces [akaris@wks-akaris blog]$ man 7 namespaces https://www.youtube.com/watch?v=sK5i-N34im8","title":"namespaces"},{"location":"linux/namespaces/#linux-namespaces","text":"","title":"Linux namespaces"},{"location":"linux/namespaces/#what-is-a-namespace","text":"one important component of containers process within the namespace sees its own isolated instance of a resource the resources are visible only to other processes that are members of the namespace limit what you can view on a system each process is a member of one namespace of each resource namespace resources are: Cgroup, IPC, Network, Mount, PID, User, UTS man 7 namespaces : (...) A namespace wraps a global system resource in an abstraction that makes it appear to the processes within the namespace that they have their own isolated instance of the global resource. Changes to the global resource are visible to other processes that are members of the namespace, but are invisible to other processes. One use of namespaces is to implement containers. (...)","title":"What is a namespace?"},{"location":"linux/namespaces/#unshare-the-tool-to-create-new-namespaces","text":"A child process starts within the same namespaces as its parent. In order to move a process to a new namespace, it needs to be spawned with unshare : man unshare NAME unshare - run program with some namespaces unshared from parent SYNOPSIS unshare [options] [program [arguments]] DESCRIPTION Unshares the indicated namespaces from the parent process and then executes the specified program. If program is not given, then ``${SHELL}'' is run (default: /bin/sh). The namespaces can optionally be made persistent by bind mounting /proc/pid/ns/type files to a filesystem path and entered with nsenter(1) even after the program terminates (except PID namespaces where permanently running init process is required). Once a persistent namespace is no longer needed, it can be unpersisted with umount(8). See the EXAMPLES section for more details. The namespaces to be unshared are indicated via options. Unshareable namespaces are: mount namespace Mounting and unmounting filesystems will not affect the rest of the system, except for filesystems which are explicitly marked as shared (with mount --make-shared; see /proc/self/mountinfo or findmnt -o+PROPAGATION for the shared flags). For further details, see mount_namespaces(7) and the discussion of the CLONE_NEWNS flag in clone(2). unshare since util-linux version 2.27 automatically sets propagation to private in a new mount namespace to make sure that the new namespace is really unshared. It's possible to disable this feature with option --propagation unchanged. Note that private is the kernel default. UTS namespace Setting hostname or domainname will not affect the rest of the system. For further details, see namespaces(7) and the discussion of the CLONE_NEWUTS flag in clone(2). IPC namespace The process will have an independent namespace for POSIX message queues as well as System V message queues, semaphore sets and shared memory segments. For further details, see names\u2010 paces(7) and the discussion of the CLONE_NEWIPC flag in clone(2). network namespace The process will have independent IPv4 and IPv6 stacks, IP routing tables, firewall rules, the /proc/net and /sys/class/net directory trees, sockets, etc. For further details, see names\u2010 paces(7) and the discussion of the CLONE_NEWNET flag in clone(2). PID namespace Children will have a distinct set of PID-to-process mappings from their parent. For further details, see pid_namespaces(7) and the discussion of the CLONE_NEWPID flag in clone(2). cgroup namespace The process will have a virtualized view of /proc/self/cgroup, and new cgroup mounts will be rooted at the namespace cgroup root. For further details, see cgroup_namespaces(7) and the dis\u2010 cussion of the CLONE_NEWCGROUP flag in clone(2). user namespace The process will have a distinct set of UIDs, GIDs and capabilities. For further details, see user_namespaces(7) and the discussion of the CLONE_NEWUSER flag in clone(2). OPTIONS -i, --ipc[=file] Unshare the IPC namespace. If file is specified, then a persistent namespace is created by a bind mount. -m, --mount[=file] Unshare the mount namespace. If file is specified, then a persistent namespace is created by a bind mount. Note that file has to be located on a filesystem with the propagation flag set to private. Use the command findmnt -o+PROPAGATION when not sure about the current setting. See also the examples below. -n, --net[=file] Unshare the network namespace. If file is specified, then a persistent namespace is created by a bind mount. -p, --pid[=file] Unshare the PID namespace. If file is specified then persistent namespace is created by a bind mount. See also the --fork and --mount-proc options. -u, --uts[=file] Unshare the UTS namespace. If file is specified, then a persistent namespace is created by a bind mount. -U, --user[=file] Unshare the user namespace. If file is specified, then a persistent namespace is created by a bind mount. -C, --cgroup[=file] Unshare the cgroup namespace. If file is specified then persistent namespace is created by bind mount. -f, --fork Fork the specified program as a child process of unshare rather than running it directly. This is useful when creating a new PID namespace. (...) unshare uses the unshare system call: [akaris@wks-akaris ~]$ sudo strace -tt -f -s1024 unshare -u /bin/bash 2>&1 | grep -i uts 15:54:50.016682 unshare(CLONE_NEWUTS) = 0 man unshare UNSHARE(2) Linux Programmer's Manual UNSHARE(2) NAME unshare - disassociate parts of the process execution context SYNOPSIS #define _GNU_SOURCE #include <sched.h> int unshare(int flags); DESCRIPTION unshare() allows a process (or thread) to disassociate parts of its execution context that are currently being shared with other processes (or threads). Part of the execu\u2010 tion context, such as the mount namespace, is shared implicitly when a new process is created using fork(2) or vfork(2), while other parts, such as virtual memory, may be shared by explicit request when creating a process or thread using clone(2). The main use of unshare() is to allow a process to control its shared execution context without creating a new process. (...) However, namespaces can also be created by feeding flags to the clone system call: [akaris@wks-akaris ~]$ man clone | egrep '^[ ]+CLONE_NEW' CLONE_NEWCGROUP (since Linux 4.6) CLONE_NEWIPC (since Linux 2.6.19) CLONE_NEWNET (since Linux 2.6.24) CLONE_NEWNS (since Linux 2.4.19) CLONE_NEWPID (since Linux 2.6.24) CLONE_NEWUSER CLONE_NEWUTS (since Linux 2.6.19) CLONE_NEWPID was specified in flags, but the limit on the nesting depth of PID CLONE_NEWUSER was specified in flags, and the call would cause the limit on the CLONE_NEWUTS was specified by an unprivileged process (process without CLONE_NEWUSER was specified in flags and the caller is in a chroot environment CLONE_NEWUSER was specified in flags, and the limit on the number of nested user","title":"unshare - the tool to create new namespaces"},{"location":"linux/namespaces/#how-to-list-namespaces","text":"Note that when a namespace is unshared, it will show up with a different namespace ID in /proc/$PID/ns/ : [root@wks-akaris akaris]# sudo ls /proc/1/ns/ -al total 0 dr-x--x--x. 2 root root 0 Dec 20 15:47 . dr-xr-xr-x. 9 root root 0 Dec 20 09:59 .. lrwxrwxrwx. 1 root root 0 Dec 20 15:47 cgroup -> 'cgroup:[4026531835]' lrwxrwxrwx. 1 root root 0 Dec 20 15:47 ipc -> 'ipc:[4026531839]' lrwxrwxrwx. 1 root root 0 Dec 20 15:47 mnt -> 'mnt:[4026531840]' lrwxrwxrwx. 1 root root 0 Dec 20 15:47 net -> 'net:[4026532008]' lrwxrwxrwx. 1 root root 0 Dec 20 15:47 pid -> 'pid:[4026531836]' lrwxrwxrwx. 1 root root 0 Dec 20 15:47 pid_for_children -> 'pid:[4026531836]' lrwxrwxrwx. 1 root root 0 Dec 20 15:47 user -> 'user:[4026531837]' lrwxrwxrwx. 1 root root 0 Dec 20 15:47 uts -> 'uts:[4026531838]' [root@wks-akaris akaris]# sudo ls /proc/$$/ns/ -al total 0 dr-x--x--x. 2 root root 0 Dec 20 15:45 . dr-xr-xr-x. 9 root root 0 Dec 20 15:43 .. lrwxrwxrwx. 1 root root 0 Dec 20 15:45 cgroup -> 'cgroup:[4026531835]' lrwxrwxrwx. 1 root root 0 Dec 20 15:45 ipc -> 'ipc:[4026531839]' lrwxrwxrwx. 1 root root 0 Dec 20 15:45 mnt -> 'mnt:[4026531840]' lrwxrwxrwx. 1 root root 0 Dec 20 15:45 net -> 'net:[4026532008]' lrwxrwxrwx. 1 root root 0 Dec 20 15:45 pid -> 'pid:[4026531836]' lrwxrwxrwx. 1 root root 0 Dec 20 15:45 pid_for_children -> 'pid:[4026531836]' lrwxrwxrwx. 1 root root 0 Dec 20 15:45 user -> 'user:[4026531837]' lrwxrwxrwx. 1 root root 0 Dec 20 15:45 uts -> 'uts:[4026532828]' Or, a more compact example of the above: [akaris@wks-akaris ~]$ sudo unshare -m uts /bin/bash [sudo] password for akaris: unshare: failed to execute uts: No such file or directory [akaris@wks-akaris ~]$ sudo unshare -u /bin/bash [root@wks-akaris akaris]# readlink /proc/1/ns/uts uts:[4026531838] [root@wks-akaris akaris]# readlink /proc/$$/ns/uts uts:[4026532828] The easiest and most detailed way to show the namespace hierarchy, though, is lsns : man lsns LSNS(8) System Administration LSNS(8) NAME lsns - list namespaces SYNOPSIS lsns [options] [namespace] DESCRIPTION lsns lists information about all the currently accessible namespaces or about the given namespace. The namespace identifier is an inode number. The default output is subject to change. So whenever possible, you should avoid using default outputs in your scripts. Always explicitly define expected columns by using the --output option together with a columns list in environments where a stable output is required. NSFS column, printed when net is specified for --type option, is special; it uses multi-line cells. Use the option --nowrap is for switching to \",\" separated single-line representation. Note that lsns reads information directly from the /proc filesystem and for non-root users it may return incomplete information. The current /proc filesystem may be unshared and affected by a PID namespace (see unshare --mount-proc for more details). lsns is not able to see persistent namespaces without processes where the namespace instance is held by a bind mount to /proc/pid/ns/type. (...) Example: [akaris@wks-akaris ~]$ sudo lsns -t uts NS TYPE NPROCS PID USER COMMAND 4026531838 uts 288 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 32 [akaris@wks-akaris ~]$ sudo unshare -u /bin/bash [root@wks-akaris akaris]# lsns -t uts NS TYPE NPROCS PID USER COMMAND 4026531838 uts 289 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 32 4026532849 uts 2 13226 root /bin/bash [root@wks-akaris akaris]# unshare -u /bin/bash [root@wks-akaris akaris]# lsns -t uts NS TYPE NPROCS PID USER COMMAND 4026531838 uts 289 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 32 4026532849 uts 1 13226 root /bin/bash 4026532850 uts 2 13257 root /bin/bash [root@wks-akaris akaris]# readlink /proc/$$/ns/uts uts:[4026532850] [root@wks-akaris akaris]# exit exit [root@wks-akaris akaris]# readlink /proc/$$/ns/uts uts:[4026532849] [root@wks-akaris akaris]# exit exit [akaris@wks-akaris ~]$ readlink /proc/$$/ns/uts uts:[4026531838]","title":"How to list namespaces"},{"location":"linux/namespaces/#namespace-types","text":"","title":"namespace types"},{"location":"linux/namespaces/#uts-namespace","text":"UTS stands for the datastructure that stores the identifiers returned by uname. UTS namespaces allow every process to have its own hostname and domain name.","title":"UTS namespace"},{"location":"linux/namespaces/#example","text":"Create an instance of /bin/bash in its own UTS namespace and assign it its own hostname: [akaris@wks-akaris ~]$ sudo unshare -u /bin/bash [root@wks-akaris akaris]# hostname utsnamespace [root@wks-akaris akaris]# bash [root@utsnamespace akaris]# [root@wks-akaris akaris]# hostname utsnamespace [root@wks-akaris akaris]# uname -a Linux utsnamespace 4.18.12-200.fc28.x86_64 #1 SMP Thu Oct 4 15:46:35 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux Open another CLI on the same system and verify the hostname: [akaris@wks-akaris ~]$ hostname wks-akaris [akaris@wks-akaris ~]$ uname -a Linux wks-akaris 4.18.12-200.fc28.x86_64 #1 SMP Thu Oct 4 15:46:35 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux","title":"Example"},{"location":"linux/namespaces/#network-namespaces","text":"Allow to add a private network stack to processes within a network namespace. Let you move interfaces from one network namespace to another. veth interfaces can connect namespaces together. man ip-netns (...) A network namespace is logically another copy of the network stack, with its own routes, firewall rules, and network devices. By default a process inherits its network namespace from its parent. Initially all the processes share the same default network namespace from the init process. By convention a named network namespace is an object at /var/run/netns/NAME that can be opened. The file descriptor resulting from opening /var/run/netns/NAME refers to the spec\u2010 ified network namespace. Holding that file descriptor open keeps the network namespace alive. The file descriptor can be used with the setns(2) system call to change the network namespace associated with a task. (...) Red Hat Enterprise Linux Atomis Host 7 Overview of Containers in Red Hat Systems: Network namespaces provide isolation of network controllers, system resources associated with networking, firewall and routing tables. This allows container to use separate virtual network stack, loopback device and process space. You can add virtual or real devices to the container, assign them their own IP Addresses and even full iptables rules. You can view the different network settings by executing the ip addr command on the host and inside the container.","title":"network namespaces"},{"location":"linux/namespaces/#example_1","text":"There are 2 ways to generate persistent network namespaces. Either use ip netns add , or use the more inconvenient unshare command: [akaris@wks-akaris ~]$ sudo ip netns add netns1 [akaris@wks-akaris ~]$ sudo ip netns netns1 [akaris@wks-akaris ~]$ sudo mount | grep netns tmpfs on /run/netns type tmpfs (rw,nosuid,nodev,seclabel,mode=755) nsfs on /run/netns/netns1 type nsfs (rw,seclabel) nsfs on /run/netns/netns1 type nsfs (rw,seclabel) [akaris@wks-akaris ~]$ sudo touch /run/netns/netns2 [akaris@wks-akaris ~]$ sudo unshare --net=!$ /bin/bash sudo unshare --net=/run/netns/netns2 /bin/bash [root@wks-akaris akaris]# exit exit [akaris@wks-akaris ~]$ sudo ip netns netns2 netns1 [akaris@wks-akaris ~]$ sudo mount | grep netns tmpfs on /run/netns type tmpfs (rw,nosuid,nodev,seclabel,mode=755) nsfs on /run/netns/netns1 type nsfs (rw,seclabel) nsfs on /run/netns/netns1 type nsfs (rw,seclabel) nsfs on /run/netns/netns2 type nsfs (rw,seclabel) nsfs on /run/netns/netns2 type nsfs (rw,seclabel) In both cases, you will be able to enter the namespace and create interfaces therein: [akaris@wks-akaris ~]$ sudo ip netns exec netns2 /bin/bash [root@wks-akaris akaris]# ip link ls 1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 [root@wks-akaris akaris]# sudo ip link add veth0 type veth peer name veth1 [root@wks-akaris akaris]# ip link ls 1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: veth1@veth0: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether 1e:a0:b3:b7:e6:db brd ff:ff:ff:ff:ff:ff 3: veth0@veth1: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether ee:1a:b2:f0:07:87 brd ff:ff:ff:ff:ff:ff [root@wks-akaris akaris]# exit exit [akaris@wks-akaris ~]$ sudo ip netns exec netns1 /bin/bash [root@wks-akaris akaris]# ip link ls 1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 [root@wks-akaris akaris]# exit exit","title":"Example"},{"location":"linux/namespaces/#mount-namespaces","text":"mount namespaces allow processes to have their own mount points, such as :root fs, /tmp, /proc/, etc. within a mount namespace, it's possible to mount and unmount mount points without affecting any other namespace man unshare (...) mount namespace Mounting and unmounting filesystems will not affect the rest of the system, except for filesystems which are explicitly marked as shared (with mount --make-shared; see /proc/self/mountinfo or findmnt -o+PROPAGATION for the shared flags). For further details, see mount_namespaces(7) and the discussion of the CLONE_NEWNS flag in clone(2). unshare since util-linux version 2.27 automatically sets propagation to private in a new mount namespace to make sure that the new namespace is really unshared. It's possible to disable this feature with option --propagation unchanged. Note that pri\u2010 vate is the kernel default. (...) man mount_namespaces (...) Mount namespaces provide isolation of the list of mount points seen by the processes in each namespace instance. Thus, the processes in each of the mount namespace instances will see distinct single-directory hierarchies. The views provided by the /proc/[pid]/mounts, /proc/[pid]/mountinfo, and /proc/[pid]/mountstats files (all described in proc(5)) correspond to the mount namespace in which the process with the PID [pid] resides. (All of the processes that reside in the same mount namespace will see the same view in these files.) When a process creates a new mount namespace using clone(2) or unshare(2) with the CLONE_NEWNS flag, the mount point list for the new namespace is a copy of the caller's mount point list. Subsequent modifications to the mount point list (mount(2) and umount(2)) in either mount namespace will not (by default) affect the mount point list seen in the other namespace (but see the following discussion of shared subtrees). (...) Red Hat Enterprise Linux Atomis Host 7 Overview of Containers in Red Hat Systems: Mount namespaces isolate the set of file system mount points seen by a group of processes so that processes in different mount namespaces can have different views of the file system hierarchy. With mount namespaces, the mount() and umount() system calls cease to operate on a global set of mount points (visible to all processes) and instead perform operations that affect just the mount namespace associated with the container process. For example, each container can have its own /tmp or /var directory or even have an entirely different userspace.","title":"mount namespaces"},{"location":"linux/namespaces/#example_2","text":"[akaris@wks-akaris ~]$ sudo unshare -m /bin/bash [root@wks-akaris akaris]# lsns -t mnt NS TYPE NPROCS PID USER COMMAND 4026531840 mnt 297 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 32 4026531860 mnt 1 33 root kdevtmpfs 4026532186 mnt 1 983 root /usr/lib/systemd/systemd-udevd 4026532508 mnt 1 1261 rtkit /usr/libexec/rtkit-daemon 4026532509 mnt 1 1276 chrony /usr/sbin/chronyd 4026532510 mnt 4 1460 root /usr/sbin/NetworkManager --no-daemon 4026532511 mnt 1 2481 root /usr/libexec/bluetooth/bluetoothd 4026532514 mnt 1 3045 root /usr/libexec/fwupd/fwupd 4026532665 mnt 1 1541 colord /usr/libexec/colord 4026532749 mnt 1 2030 root /usr/libexec/boltd 4026532828 mnt 2 17149 root /bin/bash [root@wks-akaris akaris]# umount -a -l [root@wks-akaris akaris]# ls /dev/ [root@wks-akaris akaris]# ls /proc [root@wks-akaris akaris]# exit exit [akaris@wks-akaris ~]$ ls /dev | head -2 autofs block","title":"Example"},{"location":"linux/namespaces/#pid-namespaces","text":"isolate the process ID number space different processes in different PID namespaces can have the same PID a process can only see processes in its own PID namespace. a process has a PID per namespace The global namespace has a different PID for the same process than the 'custom' namespace man pid_namespaces (...) PID namespaces isolate the process ID number space, meaning that processes in different PID namespaces can have the same PID. PID namespaces allow containers to provide func\u2010 tionality such as suspending/resuming the set of processes in the container and migrating the container to a new host while the processes inside the container maintain the same PIDs. PIDs in a new PID namespace start at 1, somewhat like a standalone system, and calls to fork(2), vfork(2), or clone(2) will produce processes with PIDs that are unique within the namespace. (...) The first process created in a new namespace (i.e., the process created using clone(2) with the CLONE_NEWPID flag, or the first child created by a process after a call to unshare(2) using the CLONE_NEWPID flag) has the PID 1, and is the \"init\" process for the namespace (see init(1)). A child process that is orphaned within the namespace will be reparented to this process rather than init(1) (unless one of the ancestors of the child in the same PID namespace employed the prctl(2) PR_SET_CHILD_SUBREAPER command to mark itself as the reaper of orphaned descendant processes). If the \"init\" process of a PID namespace terminates, the kernel terminates all of the processes in the namespace via a SIGKILL signal. This behavior reflects the fact that the \"init\" process is essential for the correct operation of a PID namespace. In this case, a subsequent fork(2) into this PID namespace fail with the error ENOMEM; it is not possi\u2010 ble to create a new processes in a PID namespace whose \"init\" process has terminated. (...) Red Hat Enterprise Linux Atomis Host 7 Overview of Containers in Red Hat Systems: PID namespaces allow processes in different containers to have the same PID, so each container can have its own init (PID1) process that manages various system initialization tasks as well as containers life cycle. Also, each container has its unique /proc directory. Note that from within the container you can monitor only processes running inside this container. In other words, the container is only aware of its native processes and can not \"see\" the processes running in different parts of the system. On the other hand, the host operating system is aware of processes running inside of the container, but assigns them different PID numbers. For example, run the ps -eZ | grep systemd$ command on host to see all instances of systemd including those running inside of containers.","title":"PID namespaces"},{"location":"linux/namespaces/#example_3","text":"Note that PID namespaces need to be unshared with --fork . Also note that this will reset the PID space, however this will not show in ps as the information comes from the /proc file system: [akaris@wks-akaris ~]$ sudo unshare -p --fork /bin/bash [root@wks-akaris akaris]# echo $$ 1 [root@wks-akaris akaris]# ps PID TTY TIME CMD 17365 pts/5 00:00:00 sudo 17367 pts/5 00:00:00 unshare 17368 pts/5 00:00:00 bash 17399 pts/5 00:00:00 ps We need a more complex series of commands to display the actual process IDs in ps - we need to create both a new PID and a new mount namespace and then unmount and remount proc: [akaris@wks-akaris ~]$ sudo unshare -p --fork -m /bin/bash [root@wks-akaris akaris]# umount -l /proc [root@wks-akaris akaris]# mount proc /proc -t proc [root@wks-akaris akaris]# ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.1 0.0 123344 5916 pts/5 S 19:01 0:00 /bin/bash root 31 0.0 0.0 153224 3700 pts/5 R+ 19:01 0:00 ps aux [root@wks-akaris akaris]#","title":"Example"},{"location":"linux/namespaces/#user-namespaces","text":"Allows UID/GID mapping. Within the namespace, a user can have UID 0 (root) but this will be squashed to root outside of the container. man user_namespaces (...) User namespaces isolate security-related identifiers and attributes, in particular, user IDs and group IDs (see credentials(7)), the root directory, keys (see keyrings(7)), and capabilities (see capabilities(7)). A process's user and group IDs can be different inside and outside a user namespace. In particular, a process can have a normal unprivileged user ID outside a user namespace while at the same time having a user ID of 0 inside the namespace; in other words, the process has full privileges for operations inside the user namespace, but is unprivileged for operations outside the namespace. (...) Red Hat Enterprise Linux Atomis Host 7 Overview of Containers in Red Hat Systems: User namespaces are similar to PID namespaces, they allow you to specify a range of host UIDs dedicated to the container. Consequently, a process can have full root privileges for operations inside the container, and at the same time be unprivileged for operations outside the container. For compatibility reasons, user namespaces are turned off in the current version of Red Hat Enterprise Linux 7, but will be enabled in the near future.","title":"user namespaces"},{"location":"linux/namespaces/#example_4","text":"[akaris@wks-akaris ~]$ sudo unshare -U /bin/bash [nfsnobody@wks-akaris akaris]$ id uid=65534(nfsnobody) gid=65534(nfsnobody) groups=65534(nfsnobody) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 [nfsnobody@wks-akaris akaris]$ echo $$ 10086 Open another CLI and run: [akaris@wks-akaris ~]$ sudo newuidmap 10086 0 0 1 [akaris@wks-akaris ~]$ Now, within the namespace, verify what happened: [nfsnobody@wks-akaris akaris]$ id uid=0(root) gid=65534(nfsnobody) groups=65534(nfsnobody) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 [nfsnobody@wks-akaris akaris]$","title":"Example"},{"location":"linux/namespaces/#ipc-namespaces","text":"Isolates interprocess communication A process / container can have its own semaphores, message queues, shared memory, etc Red Hat Enterprise Linux Atomis Host 7 Overview of Containers in Red Hat Systems: IPC namespaces isolate certain interprocess communication (IPC) resources, such as System V IPC objects and POSIX message queues. This means that two containers can create shared memory segments and semaphores with the same name, but are not able to interact with other containers memory segments or shared memory.","title":"IPC namespaces"},{"location":"linux/namespaces/#cgroup-namespaces-and-their-purpose","text":"man cgroup_namespaces (...) Among the purposes served by the virtualization provided by cgroup namespaces are the following: * It prevents information leaks whereby cgroup directory paths outside of a container would otherwise be visible to processes in the container. Such leakages could, for example, reveal information about the container framework to containerized applications. * It eases tasks such as container migration. The virtualization provided by cgroup namespaces allows containers to be isolated from knowledge of the pathnames of ancestor cgroups. Without such isolation, the full cgroup pathnames (displayed in /proc/self/cgroups) would need to be replicated on the target system when migrating a container; those pathnames would also need to be unique, so that they don't conflict with other pathnames on the target system. * It allows better confinement of containerized processes, because it is possible to mount the container's cgroup filesystems such that the container processes can't gain access to ancestor cgroup directories. Consider, for example, the following scenario: \u00b7 We have a cgroup directory, /cg/1, that is owned by user ID 9000. \u00b7 We have a process, X, also owned by user ID 9000, that is namespaced under the cgroup /cg/1/2 (i.e., X was placed in a new cgroup namespace via clone(2) or unshare(2) with the CLONE_NEWCGROUP flag). In the absence of cgroup namespacing, because the cgroup directory /cg/1 is owned (and writable) by UID 9000 and process X is also owned by user ID 9000, then process X would be able to modify the contents of cgroups files (i.e., change cgroup settings) not only in /cg/1/2 but also in the ancestor cgroup directory /cg/1. Namespacing process X under the cgroup directory /cg/1/2, in combination with suitable mount operations for the cgroup filesystem (as shown above), prevents it modifying files in /cg/1, since it cannot even see the contents of that directory (or of further removed cgroup ancestor directories). Combined with correct enforcement of hierarchical limits, this prevents process X from escaping the limits imposed by ancestor cgroups. (...)","title":"cgroup namespaces and their purpose"},{"location":"linux/namespaces/#building-containers-the-namespace-part-of-it","text":"A nice walkthrough can be found in this youtube video: https://youtu.be/sK5i-N34im8?t=2476","title":"Building containers - the namespace part of it"},{"location":"linux/namespaces/#resources","text":"manpages: [akaris@wks-akaris blog]$ apropos namespace | grep _namespaces cgroup_namespaces (7) - overview of Linux cgroup namespaces mount_namespaces (7) - overview of Linux mount namespaces network_namespaces (7) - overview of Linux network namespaces pid_namespaces (7) - overview of Linux PID namespaces user_namespaces (7) - overview of Linux user namespaces [akaris@wks-akaris blog]$ man 7 namespaces https://www.youtube.com/watch?v=sK5i-N34im8","title":"Resources"},{"location":"linux/old_java_version_with_xorgs_in_container/","text":"How to run old Java with xorgs in a container Also see reference: https://adam.younglogic.com/2017/01/gui-applications-container/ Make sure that you're running xorgs and not wayland: https://docs.fedoraproject.org/en-US/quick-docs/configuring-xorg-as-default-gnome-session/ Procedure Open /etc/gdm/custom.conf and uncomment WaylandEnable=false. Add the following line to the [daemon] section: DefaultSession=gnome-xorg.desktop Save the custom.conf file. Build container with buildah: buildah from --name java-container fedora:26 buildah run java-container -- yum install xclock icedtea-web -y buildah commit java-container java-image Disable selinux: sudo setenforce 0 Test container: podman run -ti -e DISPLAY --rm -v /run/user/1000/gdm/Xauthority:/run/user/0/gdm/Xauthority:Z --net=host localhost/java-image javaws xclock If this does not work, check journalctl -f . I got: May 19 13:48:48 linux audit[104503]: AVC avc: denied { connectto } for pid=104503 comm=\"xclock\" path=002F746D702F2E5831312D756E69782F5831 scontext=system_u:system_r:container_t:s0:c616,c783 tcontext=unconfined_u:unconfined_r:xserver_t:s0-s0:c0.c1023 tclass=unix_stream_socket permissive= In order to work around this: echo \"(allow container_t xserver_t (unix_stream_socket (connectto)))\" > mycontainer.cil sudo semodule -i mycontainer.ci Once xclock works, save viewer.jnlp (from the iDrac) to /tmp/viewer.jnlp Exit container and run jviewer: podman run -ti -e DISPLAY --rm -v /run/user/1000/gdm/Xauthority:/run/user/0/gdm/Xauthority:Z --net=host -v /tmp/viewer.jnlp:/root/viewer.jnlp localhost/java-image javaws /root/viewer.jnlp","title":"Old Java inside a container with xorgs"},{"location":"linux/old_java_version_with_xorgs_in_container/#how-to-run-old-java-with-xorgs-in-a-container","text":"Also see reference: https://adam.younglogic.com/2017/01/gui-applications-container/ Make sure that you're running xorgs and not wayland: https://docs.fedoraproject.org/en-US/quick-docs/configuring-xorg-as-default-gnome-session/ Procedure Open /etc/gdm/custom.conf and uncomment WaylandEnable=false. Add the following line to the [daemon] section: DefaultSession=gnome-xorg.desktop Save the custom.conf file. Build container with buildah: buildah from --name java-container fedora:26 buildah run java-container -- yum install xclock icedtea-web -y buildah commit java-container java-image Disable selinux: sudo setenforce 0 Test container: podman run -ti -e DISPLAY --rm -v /run/user/1000/gdm/Xauthority:/run/user/0/gdm/Xauthority:Z --net=host localhost/java-image javaws xclock If this does not work, check journalctl -f . I got: May 19 13:48:48 linux audit[104503]: AVC avc: denied { connectto } for pid=104503 comm=\"xclock\" path=002F746D702F2E5831312D756E69782F5831 scontext=system_u:system_r:container_t:s0:c616,c783 tcontext=unconfined_u:unconfined_r:xserver_t:s0-s0:c0.c1023 tclass=unix_stream_socket permissive= In order to work around this: echo \"(allow container_t xserver_t (unix_stream_socket (connectto)))\" > mycontainer.cil sudo semodule -i mycontainer.ci Once xclock works, save viewer.jnlp (from the iDrac) to /tmp/viewer.jnlp Exit container and run jviewer: podman run -ti -e DISPLAY --rm -v /run/user/1000/gdm/Xauthority:/run/user/0/gdm/Xauthority:Z --net=host -v /tmp/viewer.jnlp:/root/viewer.jnlp localhost/java-image javaws /root/viewer.jnlp","title":"How to run old Java with xorgs in a container"},{"location":"networking/arp_and_the_neighbor_table/","text":"ARP and the neighbor table Document of reference: http://haifux.org/lectures/180/netLec2.pdf Disabling ARP NUD NOARP From http://haifux.org/lectures/180/netLec2.pdf Neighboring Subsystem \u2013 states \u25cf Special states: \u25cf NUD_NOARP \u25cf NUD_PERMANENT \u25cf No state transitions are allowed from these states to another state. This means that if an ARP entry ends up in NUD_NOARP , it will not time out, update or change in any way. An administrator needs to delete the ARP entry for ARP resolution to work again properly for the IP. List NOARP entries: [root@overcloud-novacomputeiha-0 ~]# ip neigh ls nud noarp 172.16.2.9 dev lo lladdr 00:00:00:00:00:00 NOARP 127.0.0.1 dev lo lladdr 00:00:00:00:00:00 NOARP ff02::1:ff14:62d7 dev vlan902 lladdr 33:33:ff:14:62:d7 NOARP ff02::1:ff8d:846d dev vlan903 lladdr 33:33:ff:8d:84:6d NOARP ff02::16 dev ovs-bond-if1 lladdr 33:33:00:00:00:16 NOARP ff02::1:ff9f:c668 dev em1 lladdr 33:33:ff:9f:c6:68 NOARP ff02::1 dev lx-bond0 lladdr 33:33:00:00:00:01 NOARP ff02::16 dev vlan901 lladdr 33:33:00:00:00:16 NOARP ff02::16 dev em1 lladdr 33:33:00:00:00:16 NOARP ff02::16 dev br-ex lladdr 33:33:00:00:00:16 NOARP ff02::1:ff03:a0eb dev vlan901 lladdr 33:33:ff:03:a0:eb NOARP ff02::16 dev vlan902 lladdr 33:33:00:00:00:16 NOARP ff02::2 dev em4 lladdr 33:33:00:00:00:02 NOARP ff02::16 dev veth3 lladdr 33:33:00:00:00:16 NOARP ff02::16 dev vlan903 lladdr 33:33:00:00:00:16 NOARP ff02::16 dev ovs-bond-if0 lladdr 33:33:00:00:00:16 NOARP ff02::1:ff24:4974 dev ovs-bond-if1 lladdr 33:33:ff:24:49:74 NOARP ff02::16 dev vxlan_sys_4789 lladdr 33:33:00:00:00:16 NOARP ff02::1:ffe5:da4c dev br-ex lladdr 33:33:ff:e5:da:4c NOARP ff02::16 dev em4 lladdr 33:33:00:00:00:16 NOARP ff02::2 dev vlan901 lladdr 33:33:00:00:00:02 NOARP ff02::2 dev em1 lladdr 33:33:00:00:00:02 NOARP ff02::2 dev br-ex lladdr 33:33:00:00:00:02 NOARP ff02::1:ff9f:c66b dev em4 lladdr 33:33:ff:9f:c6:6b NOARP ff02::1:ff91:8481 dev ovs-bond-if0 lladdr 33:33:ff:91:84:81 NOARP ff02::2 dev vlan902 lladdr 33:33:00:00:00:02 NOARP ff02::16 dev lx-bond0 lladdr 33:33:00:00:00:16 NOARP ff02::1:ff5f:4750 dev veth3 lladdr 33:33:ff:5f:47:50 NOARP ::1 dev lo lladdr 00:00:00:00:00:00 NOARP ff02::1:ffb7:7124 dev vxlan_sys_4789 lladdr 33:33:ff:b7:71:24 NOARP ff02::2 dev vlan903 lladdr 33:33:00:00:00:02 NOARP ff02::1:ff54:bc0d dev lx-bond0 lladdr 33:33:ff:54:bc:0d NOARP Per interface ARP can be disabled globally on a per interface basis. This may be useful e.g. when using some failover / redundancy solution implemented in user space: [root@overcloud-novacomputeiha-0 ~]# ping 192.168.123.11 PING 192.168.123.11 (192.168.123.11) 56(84) bytes of data. 64 bytes from 192.168.123.11: icmp_seq=1 ttl=64 time=0.342 ms 64 bytes from 192.168.123.11: icmp_seq=2 ttl=64 time=0.181 ms ^C --- 192.168.123.11 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1000ms rtt min/avg/max/mdev = 0.181/0.261/0.342/0.082 ms [root@overcloud-novacomputeiha-0 ~]# ip r g 192.168.123.11 192.168.123.11 dev lx-bond0.905 src 192.168.123.10 cache [root@overcloud-novacomputeiha-0 ~]# ip link set dev lx-bond0.905 arp off [root@overcloud-novacomputeiha-0 ~]# ip neigh ls | grep 192.168.123.11 [root@overcloud-novacomputeiha-0 ~]# ip neigh ls nud noarp | grep 192.168.123.11 [root@overcloud-novacomputeiha-0 ~]# ping 192.168.123.11 PING 192.168.123.11 (192.168.123.11) 56(84) bytes of data. ^C --- 192.168.123.11 ping statistics --- 2 packets transmitted, 0 received, 100% packet loss, time 1001ms The interface will show NOARP : [root@overcloud-novacomputeiha-0 ~]# ip link ls dev lx-bond0.905 31: lx-bond0.905@lx-bond0: <BROADCAST,MULTICAST,NOARP,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 link/ether fe:b8:03:54:bc:0d brd ff:ff:ff:ff:ff:ff This also means that the opposite side cannot resolve the IP address: [root@overcloud-novacomputeiha-0 ~]# ip netns exec test2 ping 192.168.123.10 PING 192.168.123.10 (192.168.123.10) 56(84) bytes of data. ^C --- 192.168.123.10 ping statistics --- 2 packets transmitted, 0 received, 100% packet loss, time 999ms [root@overcloud-novacomputeiha-0 ~]# arp -n | grep 192.168.123 [root@overcloud-novacomputeiha-0 ~]# ip netns exec test2 arp -n | grep 192.168.123 192.168.123.10 (incomplete) veth2 [root@overcloud-novacomputeiha-0 ~]# Per ARP entry Let's set NOARP for 192.168.123.11: [root@overcloud-novacomputeiha-0 ~]# ip neigh ls | grep 192.168.123 192.168.123.11 dev lx-bond0.905 lladdr 22:a6:81:76:8a:78 REACHABLE [root@overcloud-novacomputeiha-0 ~]# ip neigh change 192.168.123.11 dev lx-bond0.905 lladdr 22:a6:81:76:8a:78 nud noarp [root@overcloud-novacomputeiha-0 ~]# ip neigh ls | grep 192.168.123 [root@overcloud-novacomputeiha-0 ~]# ip neigh ls nud noarp | grep 192.168.123 192.168.123.11 dev lx-bond0.905 lladdr 22:a6:81:76:8a:78 NOARP [root@overcloud-novacomputeiha-0 ~]# ping 192.168.123.11 PING 192.168.123.11 (192.168.123.11) 56(84) bytes of data. 64 bytes from 192.168.123.11: icmp_seq=1 ttl=64 time=0.344 ms ^C --- 192.168.123.11 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.344/0.344/0.344/0.000 ms Note that flipping the state for the interface resets the NOARP entry for the neighbor: [root@overcloud-novacomputeiha-0 ~]# ip link set dev lx-bond0.905 arp off [root@overcloud-novacomputeiha-0 ~]# ip neigh ls nud noarp | grep 192.168.123 [root@overcloud-novacomputeiha-0 ~]# ip link set dev lx-bond0.905 arp on [root@overcloud-novacomputeiha-0 ~]# ip neigh ls nud noarp | grep 192.168.123 [root@overcloud-novacomputeiha-0 ~]# ip neigh change 192.168.123.11 dev lx-bond0.905 lladdr 22:a6:81:76:8a:78 nud noarp RTNETLINK answers: No such file or directory [root@overcloud-novacomputeiha-0 ~]# ping 192.168.123.11 PING 192.168.123.11 (192.168.123.11) 56(84) bytes of data. 64 bytes from 192.168.123.11: icmp_seq=1 ttl=64 time=0.480 ms ^C --- 192.168.123.11 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.480/0.480/0.480/0.000 ms [root@overcloud-novacomputeiha-0 ~]# ip neigh change 192.168.123.11 dev lx-bond0.905 lladdr 22:a6:81:76:8a:78 nud noarp [root@overcloud-novacomputeiha-0 ~]# We can see that ARP learning is disabled with the following test. Note that we have to ping from the neighbor. Otherwise, even with correct ARP resolution, this won't work: [root@overcloud-novacomputeiha-0 ~]# ping 192.168.123.11 PING 192.168.123.11 (192.168.123.11) 56(84) bytes of data. ^C --- 192.168.123.11 ping statistics --- 2 packets transmitted, 0 received, 100% packet loss, time 999ms [root@overcloud-novacomputeiha-0 ~]# ip netns exec test2 ip link set dev veth2 address 22:a6:81:76:8a:78 [root@overcloud-novacomputeiha-0 ~]# ping 192.168.123.11 PING 192.168.123.11 (192.168.123.11) 56(84) bytes of data. ^C --- 192.168.123.11 ping statistics --- 5 packets transmitted, 0 received, 100% packet loss, time 4000ms [root@overcloud-novacomputeiha-0 ~]# ip netns exec test2 ping 192.168.123.10 PING 192.168.123.10 (192.168.123.10) 56(84) bytes of data. 64 bytes from 192.168.123.10: icmp_seq=1 ttl=64 time=0.426 ms ^C --- 192.168.123.10 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.426/0.426/0.426/0.000 ms [root@overcloud-novacomputeiha-0 ~]# ip netns exec test2 ip a ls dev veth2 33: veth2@if32: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 22:a6:81:76:8a:78 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 192.168.123.11/24 scope global veth2 valid_lft forever preferred_lft forever inet6 fe80::20a6:81ff:fe76:8a78/64 scope link valid_lft forever preferred_lft forever [root@overcloud-novacomputeiha-0 ~]# ip netns exec test2 ip link set dev veth2 address 22:a6:81:76:8a:79 [root@overcloud-novacomputeiha-0 ~]# ip netns exec test2 ip a ls dev veth2 33: veth2@if32: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 22:a6:81:76:8a:79 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 192.168.123.11/24 scope global veth2 valid_lft forever preferred_lft forever inet6 fe80::20a6:81ff:fe76:8a78/64 scope link valid_lft forever preferred_lft forever [root@overcloud-novacomputeiha-0 ~]# ping 192.168.123.11 PING 192.168.123.11 (192.168.123.11) 56(84) bytes of data. ^C --- 192.168.123.11 ping statistics --- 2 packets transmitted, 0 received, 100% packet loss, time 999ms [root@overcloud-novacomputeiha-0 ~]# ip link set dev lx-bond0.905 arp off [root@overcloud-novacomputeiha-0 ~]# ip link set dev lx-bond0.905 arp on [root@overcloud-novacomputeiha-0 ~]# ping 192.168.123.11 PING 192.168.123.11 (192.168.123.11) 56(84) bytes of data. 64 bytes from 192.168.123.11: icmp_seq=1 ttl=64 time=0.566 ms ^C --- 192.168.123.11 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.566/0.566/0.566/0.000 ms [root@overcloud-novacomputeiha-0 ~]# Here's how it looks with NUD NOARP: [root@overcloud-novacomputeiha-0 ~]# ip neigh change 192.168.123.11 dev lx-bond0.905 lladdr 22:a6:81:76:8a:78 nud noarp [root@overcloud-novacomputeiha-0 ~]# ip netns exec test2 ping 192.168.123.10 PING 192.168.123.10 (192.168.123.10) 56(84) bytes of data. 64 bytes from 192.168.123.10: icmp_seq=1 ttl=64 time=0.175 ms ^C --- 192.168.123.10 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.175/0.175/0.175/0.000 ms [root@overcloud-novacomputeiha-0 ~]# ping 192.168.123.11 PING 192.168.123.11 (192.168.123.11) 56(84) bytes of data. 64 bytes from 192.168.123.11: icmp_seq=1 ttl=64 time=0.229 ms 64 bytes from 192.168.123.11: icmp_seq=2 ttl=64 time=0.190 ms ^C --- 192.168.123.11 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 999ms rtt min/avg/max/mdev = 0.190/0.209/0.229/0.024 ms [root@overcloud-novacomputeiha-0 ~]# ip netns exec test2 ip link set dev veth2 address 22:a6:81:76:8a:79 [root@overcloud-novacomputeiha-0 ~]# ping 192.168.123.11 PING 192.168.123.11 (192.168.123.11) 56(84) bytes of data. ^C --- 192.168.123.11 ping statistics --- 1 packets transmitted, 0 received, 100% packet loss, time 0ms [root@overcloud-novacomputeiha-0 ~]# ip netns exec test2 ping 192.168.123.10 PING 192.168.123.10 (192.168.123.10) 56(84) bytes of data. ^C --- 192.168.123.10 ping statistics --- 2 packets transmitted, 0 received, 100% packet loss, time 999ms [root@overcloud-novacomputeiha-0 ~]# ip link set dev lx-bond0.905 arp off [root@overcloud-novacomputeiha-0 ~]# ip link set dev lx-bond0.905 arp on [root@overcloud-novacomputeiha-0 ~]# ip netns exec test2 ping 192.168.123.10 PING 192.168.123.10 (192.168.123.10) 56(84) bytes of data. 64 bytes from 192.168.123.10: icmp_seq=1 ttl=64 time=0.377 ms ^C --- 192.168.123.10 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.377/0.377/0.377/0.000 ms","title":"ARP and the Neighbor table"},{"location":"networking/arp_and_the_neighbor_table/#arp-and-the-neighbor-table","text":"Document of reference: http://haifux.org/lectures/180/netLec2.pdf","title":"ARP and the neighbor table"},{"location":"networking/arp_and_the_neighbor_table/#disabling-arp","text":"","title":"Disabling ARP"},{"location":"networking/arp_and_the_neighbor_table/#nud-noarp","text":"From http://haifux.org/lectures/180/netLec2.pdf Neighboring Subsystem \u2013 states \u25cf Special states: \u25cf NUD_NOARP \u25cf NUD_PERMANENT \u25cf No state transitions are allowed from these states to another state. This means that if an ARP entry ends up in NUD_NOARP , it will not time out, update or change in any way. An administrator needs to delete the ARP entry for ARP resolution to work again properly for the IP. List NOARP entries: [root@overcloud-novacomputeiha-0 ~]# ip neigh ls nud noarp 172.16.2.9 dev lo lladdr 00:00:00:00:00:00 NOARP 127.0.0.1 dev lo lladdr 00:00:00:00:00:00 NOARP ff02::1:ff14:62d7 dev vlan902 lladdr 33:33:ff:14:62:d7 NOARP ff02::1:ff8d:846d dev vlan903 lladdr 33:33:ff:8d:84:6d NOARP ff02::16 dev ovs-bond-if1 lladdr 33:33:00:00:00:16 NOARP ff02::1:ff9f:c668 dev em1 lladdr 33:33:ff:9f:c6:68 NOARP ff02::1 dev lx-bond0 lladdr 33:33:00:00:00:01 NOARP ff02::16 dev vlan901 lladdr 33:33:00:00:00:16 NOARP ff02::16 dev em1 lladdr 33:33:00:00:00:16 NOARP ff02::16 dev br-ex lladdr 33:33:00:00:00:16 NOARP ff02::1:ff03:a0eb dev vlan901 lladdr 33:33:ff:03:a0:eb NOARP ff02::16 dev vlan902 lladdr 33:33:00:00:00:16 NOARP ff02::2 dev em4 lladdr 33:33:00:00:00:02 NOARP ff02::16 dev veth3 lladdr 33:33:00:00:00:16 NOARP ff02::16 dev vlan903 lladdr 33:33:00:00:00:16 NOARP ff02::16 dev ovs-bond-if0 lladdr 33:33:00:00:00:16 NOARP ff02::1:ff24:4974 dev ovs-bond-if1 lladdr 33:33:ff:24:49:74 NOARP ff02::16 dev vxlan_sys_4789 lladdr 33:33:00:00:00:16 NOARP ff02::1:ffe5:da4c dev br-ex lladdr 33:33:ff:e5:da:4c NOARP ff02::16 dev em4 lladdr 33:33:00:00:00:16 NOARP ff02::2 dev vlan901 lladdr 33:33:00:00:00:02 NOARP ff02::2 dev em1 lladdr 33:33:00:00:00:02 NOARP ff02::2 dev br-ex lladdr 33:33:00:00:00:02 NOARP ff02::1:ff9f:c66b dev em4 lladdr 33:33:ff:9f:c6:6b NOARP ff02::1:ff91:8481 dev ovs-bond-if0 lladdr 33:33:ff:91:84:81 NOARP ff02::2 dev vlan902 lladdr 33:33:00:00:00:02 NOARP ff02::16 dev lx-bond0 lladdr 33:33:00:00:00:16 NOARP ff02::1:ff5f:4750 dev veth3 lladdr 33:33:ff:5f:47:50 NOARP ::1 dev lo lladdr 00:00:00:00:00:00 NOARP ff02::1:ffb7:7124 dev vxlan_sys_4789 lladdr 33:33:ff:b7:71:24 NOARP ff02::2 dev vlan903 lladdr 33:33:00:00:00:02 NOARP ff02::1:ff54:bc0d dev lx-bond0 lladdr 33:33:ff:54:bc:0d NOARP","title":"NUD NOARP"},{"location":"networking/arp_and_the_neighbor_table/#per-interface","text":"ARP can be disabled globally on a per interface basis. This may be useful e.g. when using some failover / redundancy solution implemented in user space: [root@overcloud-novacomputeiha-0 ~]# ping 192.168.123.11 PING 192.168.123.11 (192.168.123.11) 56(84) bytes of data. 64 bytes from 192.168.123.11: icmp_seq=1 ttl=64 time=0.342 ms 64 bytes from 192.168.123.11: icmp_seq=2 ttl=64 time=0.181 ms ^C --- 192.168.123.11 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1000ms rtt min/avg/max/mdev = 0.181/0.261/0.342/0.082 ms [root@overcloud-novacomputeiha-0 ~]# ip r g 192.168.123.11 192.168.123.11 dev lx-bond0.905 src 192.168.123.10 cache [root@overcloud-novacomputeiha-0 ~]# ip link set dev lx-bond0.905 arp off [root@overcloud-novacomputeiha-0 ~]# ip neigh ls | grep 192.168.123.11 [root@overcloud-novacomputeiha-0 ~]# ip neigh ls nud noarp | grep 192.168.123.11 [root@overcloud-novacomputeiha-0 ~]# ping 192.168.123.11 PING 192.168.123.11 (192.168.123.11) 56(84) bytes of data. ^C --- 192.168.123.11 ping statistics --- 2 packets transmitted, 0 received, 100% packet loss, time 1001ms The interface will show NOARP : [root@overcloud-novacomputeiha-0 ~]# ip link ls dev lx-bond0.905 31: lx-bond0.905@lx-bond0: <BROADCAST,MULTICAST,NOARP,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 link/ether fe:b8:03:54:bc:0d brd ff:ff:ff:ff:ff:ff This also means that the opposite side cannot resolve the IP address: [root@overcloud-novacomputeiha-0 ~]# ip netns exec test2 ping 192.168.123.10 PING 192.168.123.10 (192.168.123.10) 56(84) bytes of data. ^C --- 192.168.123.10 ping statistics --- 2 packets transmitted, 0 received, 100% packet loss, time 999ms [root@overcloud-novacomputeiha-0 ~]# arp -n | grep 192.168.123 [root@overcloud-novacomputeiha-0 ~]# ip netns exec test2 arp -n | grep 192.168.123 192.168.123.10 (incomplete) veth2 [root@overcloud-novacomputeiha-0 ~]#","title":"Per interface"},{"location":"networking/arp_and_the_neighbor_table/#per-arp-entry","text":"Let's set NOARP for 192.168.123.11: [root@overcloud-novacomputeiha-0 ~]# ip neigh ls | grep 192.168.123 192.168.123.11 dev lx-bond0.905 lladdr 22:a6:81:76:8a:78 REACHABLE [root@overcloud-novacomputeiha-0 ~]# ip neigh change 192.168.123.11 dev lx-bond0.905 lladdr 22:a6:81:76:8a:78 nud noarp [root@overcloud-novacomputeiha-0 ~]# ip neigh ls | grep 192.168.123 [root@overcloud-novacomputeiha-0 ~]# ip neigh ls nud noarp | grep 192.168.123 192.168.123.11 dev lx-bond0.905 lladdr 22:a6:81:76:8a:78 NOARP [root@overcloud-novacomputeiha-0 ~]# ping 192.168.123.11 PING 192.168.123.11 (192.168.123.11) 56(84) bytes of data. 64 bytes from 192.168.123.11: icmp_seq=1 ttl=64 time=0.344 ms ^C --- 192.168.123.11 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.344/0.344/0.344/0.000 ms Note that flipping the state for the interface resets the NOARP entry for the neighbor: [root@overcloud-novacomputeiha-0 ~]# ip link set dev lx-bond0.905 arp off [root@overcloud-novacomputeiha-0 ~]# ip neigh ls nud noarp | grep 192.168.123 [root@overcloud-novacomputeiha-0 ~]# ip link set dev lx-bond0.905 arp on [root@overcloud-novacomputeiha-0 ~]# ip neigh ls nud noarp | grep 192.168.123 [root@overcloud-novacomputeiha-0 ~]# ip neigh change 192.168.123.11 dev lx-bond0.905 lladdr 22:a6:81:76:8a:78 nud noarp RTNETLINK answers: No such file or directory [root@overcloud-novacomputeiha-0 ~]# ping 192.168.123.11 PING 192.168.123.11 (192.168.123.11) 56(84) bytes of data. 64 bytes from 192.168.123.11: icmp_seq=1 ttl=64 time=0.480 ms ^C --- 192.168.123.11 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.480/0.480/0.480/0.000 ms [root@overcloud-novacomputeiha-0 ~]# ip neigh change 192.168.123.11 dev lx-bond0.905 lladdr 22:a6:81:76:8a:78 nud noarp [root@overcloud-novacomputeiha-0 ~]# We can see that ARP learning is disabled with the following test. Note that we have to ping from the neighbor. Otherwise, even with correct ARP resolution, this won't work: [root@overcloud-novacomputeiha-0 ~]# ping 192.168.123.11 PING 192.168.123.11 (192.168.123.11) 56(84) bytes of data. ^C --- 192.168.123.11 ping statistics --- 2 packets transmitted, 0 received, 100% packet loss, time 999ms [root@overcloud-novacomputeiha-0 ~]# ip netns exec test2 ip link set dev veth2 address 22:a6:81:76:8a:78 [root@overcloud-novacomputeiha-0 ~]# ping 192.168.123.11 PING 192.168.123.11 (192.168.123.11) 56(84) bytes of data. ^C --- 192.168.123.11 ping statistics --- 5 packets transmitted, 0 received, 100% packet loss, time 4000ms [root@overcloud-novacomputeiha-0 ~]# ip netns exec test2 ping 192.168.123.10 PING 192.168.123.10 (192.168.123.10) 56(84) bytes of data. 64 bytes from 192.168.123.10: icmp_seq=1 ttl=64 time=0.426 ms ^C --- 192.168.123.10 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.426/0.426/0.426/0.000 ms [root@overcloud-novacomputeiha-0 ~]# ip netns exec test2 ip a ls dev veth2 33: veth2@if32: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 22:a6:81:76:8a:78 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 192.168.123.11/24 scope global veth2 valid_lft forever preferred_lft forever inet6 fe80::20a6:81ff:fe76:8a78/64 scope link valid_lft forever preferred_lft forever [root@overcloud-novacomputeiha-0 ~]# ip netns exec test2 ip link set dev veth2 address 22:a6:81:76:8a:79 [root@overcloud-novacomputeiha-0 ~]# ip netns exec test2 ip a ls dev veth2 33: veth2@if32: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 22:a6:81:76:8a:79 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 192.168.123.11/24 scope global veth2 valid_lft forever preferred_lft forever inet6 fe80::20a6:81ff:fe76:8a78/64 scope link valid_lft forever preferred_lft forever [root@overcloud-novacomputeiha-0 ~]# ping 192.168.123.11 PING 192.168.123.11 (192.168.123.11) 56(84) bytes of data. ^C --- 192.168.123.11 ping statistics --- 2 packets transmitted, 0 received, 100% packet loss, time 999ms [root@overcloud-novacomputeiha-0 ~]# ip link set dev lx-bond0.905 arp off [root@overcloud-novacomputeiha-0 ~]# ip link set dev lx-bond0.905 arp on [root@overcloud-novacomputeiha-0 ~]# ping 192.168.123.11 PING 192.168.123.11 (192.168.123.11) 56(84) bytes of data. 64 bytes from 192.168.123.11: icmp_seq=1 ttl=64 time=0.566 ms ^C --- 192.168.123.11 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.566/0.566/0.566/0.000 ms [root@overcloud-novacomputeiha-0 ~]# Here's how it looks with NUD NOARP: [root@overcloud-novacomputeiha-0 ~]# ip neigh change 192.168.123.11 dev lx-bond0.905 lladdr 22:a6:81:76:8a:78 nud noarp [root@overcloud-novacomputeiha-0 ~]# ip netns exec test2 ping 192.168.123.10 PING 192.168.123.10 (192.168.123.10) 56(84) bytes of data. 64 bytes from 192.168.123.10: icmp_seq=1 ttl=64 time=0.175 ms ^C --- 192.168.123.10 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.175/0.175/0.175/0.000 ms [root@overcloud-novacomputeiha-0 ~]# ping 192.168.123.11 PING 192.168.123.11 (192.168.123.11) 56(84) bytes of data. 64 bytes from 192.168.123.11: icmp_seq=1 ttl=64 time=0.229 ms 64 bytes from 192.168.123.11: icmp_seq=2 ttl=64 time=0.190 ms ^C --- 192.168.123.11 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 999ms rtt min/avg/max/mdev = 0.190/0.209/0.229/0.024 ms [root@overcloud-novacomputeiha-0 ~]# ip netns exec test2 ip link set dev veth2 address 22:a6:81:76:8a:79 [root@overcloud-novacomputeiha-0 ~]# ping 192.168.123.11 PING 192.168.123.11 (192.168.123.11) 56(84) bytes of data. ^C --- 192.168.123.11 ping statistics --- 1 packets transmitted, 0 received, 100% packet loss, time 0ms [root@overcloud-novacomputeiha-0 ~]# ip netns exec test2 ping 192.168.123.10 PING 192.168.123.10 (192.168.123.10) 56(84) bytes of data. ^C --- 192.168.123.10 ping statistics --- 2 packets transmitted, 0 received, 100% packet loss, time 999ms [root@overcloud-novacomputeiha-0 ~]# ip link set dev lx-bond0.905 arp off [root@overcloud-novacomputeiha-0 ~]# ip link set dev lx-bond0.905 arp on [root@overcloud-novacomputeiha-0 ~]# ip netns exec test2 ping 192.168.123.10 PING 192.168.123.10 (192.168.123.10) 56(84) bytes of data. 64 bytes from 192.168.123.10: icmp_seq=1 ttl=64 time=0.377 ms ^C --- 192.168.123.10 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.377/0.377/0.377/0.000 ms","title":"Per ARP entry"},{"location":"networking/bonding_in_linux/","text":"Bonding in Linux About the different bonding options in Linux Popular bonding options and drivers are: Linux bonding kernel driver To verify: cat /proc/net/bonding/<bond name> Teaming driver where only the essential stuff is in kernel, the rest is userspace ( https://rhelblog.redhat.com/2014/06/23/team-driver/ , https://github.com/jpirko/libteam/wiki/Infrastructure-Specification ) OVS (either bonding PMDs in DPDK or kernel interfaces) http://docs.openvswitch.org/en/latest/topics/bonding/ To verify: ovs-appctl bond/show ; ovs-appctl lacp/show DPDK (there is a bonding PMD)","title":"Bonding in Linux"},{"location":"networking/bonding_in_linux/#bonding-in-linux","text":"","title":"Bonding in Linux"},{"location":"networking/bonding_in_linux/#about-the-different-bonding-options-in-linux","text":"Popular bonding options and drivers are: Linux bonding kernel driver To verify: cat /proc/net/bonding/<bond name> Teaming driver where only the essential stuff is in kernel, the rest is userspace ( https://rhelblog.redhat.com/2014/06/23/team-driver/ , https://github.com/jpirko/libteam/wiki/Infrastructure-Specification ) OVS (either bonding PMDs in DPDK or kernel interfaces) http://docs.openvswitch.org/en/latest/topics/bonding/ To verify: ovs-appctl bond/show ; ovs-appctl lacp/show DPDK (there is a bonding PMD)","title":"About the different bonding options in Linux"},{"location":"networking/geneve_tunneling/","text":"Geneve tunneling vs VXLAN Packet captures VXLAN [root@rhel-test1 ~]# ip link add dev vxlan0 type vxlan remote 192.168.1.12 vni 1234 vxlan: destination port not specified Will use Linux kernel default (non-standard value) Use 'dstport 4789' to get the IANA assigned value Use 'dstport 0' to get default and quiet this message [root@rhel-test1 ~]# ip link del dev vxlan0 [root@rhel-test1 ~]# ip link add dev vxlan0 type vxlan remote 192.168.1.12 vni 1234 dstport 4789 [root@rhel-test1 ~]# ip a a 192.168.124.1/30 dev vxlan0 [root@rhel-test1 ~]# ip link set dev vxlan0 up [root@rhel-test1 ~]# ping 192.168.124.2 PING 192.168.124.2 (192.168.124.2) 56(84) bytes of data. 64 bytes from 192.168.124.2: icmp_seq=1 ttl=64 time=3.02 ms 64 bytes from 192.168.124.2: icmp_seq=2 ttl=64 time=1.87 ms ^C --- 192.168.124.2 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1001ms rtt min/avg/max/mdev = 1.879/2.453/3.027/0.574 ms [root@rhel-test1 ~]# ping 192.168.124.2 PING 192.168.124.2 (192.168.124.2) 56(84) bytes of data. 64 bytes from 192.168.124.2: icmp_seq=1 ttl=64 time=1.15 ms ^C --- 192.168.124.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 1.158/1.158/1.158/0.000 ms [root@rhel-test1 ~]# [root@rhel-test2 ~]# ip link add dev vxlan0 type vxlan remote 192.168.0.12 vni 1234 dstport 4789 [root@rhel-test2 ~]# ip a a 192.168.124.2/30 vxlan0 Error: either \"local\" is duplicate, or \"vxlan0\" is a garbage. [root@rhel-test2 ~]# ip a a 192.168.124.2/30 dev vxlan0 [root@rhel-test2 ~]# ip link set dev vlan0 up Cannot find device \"vlan0\" [root@rhel-test2 ~]# ip link set dev vxlan0 up [root@rhel-test2 ~]# tcpdump -nne -i vxlan0 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on vxlan0, link-type EN10MB (Ethernet), capture size 65535 bytes 13:25:54.405073 b6:37:34:97:b8:e3 > 56:cf:14:90:3b:e0, ethertype IPv4 (0x0800), length 98: 192.168.124.1 > 192.168.124.2: ICMP echo request, id 2192, seq 1, length 64 13:25:54.405131 56:cf:14:90:3b:e0 > b6:37:34:97:b8:e3, ethertype IPv4 (0x0800), length 98: 192.168.124.2 > 192.168.124.1: ICMP echo reply, id 2192, seq 1, length 64 ^C 2 packets captured 2 packets received by filter 0 packets dropped by kernel [root@rhel-test2 ~]# [akaris@wks-akaris geneve]$ tshark -r vxlan.pcap -V frame.number==43 Frame 43: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) Encapsulation type: Ethernet (1) Arrival Time: Mar 6, 2019 13:25:54.545467000 EST [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1551896754.545467000 seconds [Time delta from previous captured frame: 5.489084000 seconds] [Time delta from previous displayed frame: 0.000000000 seconds] [Time since reference or first frame: 61.528243000 seconds] Frame Number: 43 Frame Length: 148 bytes (1184 bits) Capture Length: 148 bytes (1184 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:udp:vxlan:eth:ethertype:ip:icmp:data] Ethernet II, Src: fa:16:3e:40:02:d7 (fa:16:3e:40:02:d7), Dst: fa:16:3e:bf:f2:d4 (fa:16:3e:bf:f2:d4) Destination: fa:16:3e:bf:f2:d4 (fa:16:3e:bf:f2:d4) Address: fa:16:3e:bf:f2:d4 (fa:16:3e:bf:f2:d4) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: fa:16:3e:40:02:d7 (fa:16:3e:40:02:d7) Address: fa:16:3e:40:02:d7 (fa:16:3e:40:02:d7) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 192.168.0.12, Dst: 192.168.1.12 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x00 (DSCP: CS0, ECN: Not-ECT) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Total Length: 134 Identification: 0x45eb (17899) Flags: 0x0000 0... .... .... .... = Reserved bit: Not set .0.. .... .... .... = Don't fragment: Not set ..0. .... .... .... = More fragments: Not set ...0 0000 0000 0000 = Fragment offset: 0 Time to live: 63 Protocol: UDP (17) Header checksum: 0xb313 [validation disabled] [Header checksum status: Unverified] Source: 192.168.0.12 Destination: 192.168.1.12 User Datagram Protocol, Src Port: 34990, Dst Port: 4789 Source Port: 34990 Destination Port: 4789 Length: 114 [Checksum: [missing]] [Checksum Status: Not present] [Stream index: 11] Virtual eXtensible Local Area Network Flags: 0x0800, VXLAN Network ID (VNI) 0... .... .... .... = GBP Extension: Not defined .... .... .0.. .... = Don't Learn: False .... 1... .... .... = VXLAN Network ID (VNI): True .... .... .... 0... = Policy Applied: False .000 .000 0.00 .000 = Reserved(R): 0x0000 Group Policy ID: 0 VXLAN Network Identifier (VNI): 1234 Reserved: 0 Ethernet II, Src: b6:37:34:97:b8:e3 (b6:37:34:97:b8:e3), Dst: 56:cf:14:90:3b:e0 (56:cf:14:90:3b:e0) Destination: 56:cf:14:90:3b:e0 (56:cf:14:90:3b:e0) Address: 56:cf:14:90:3b:e0 (56:cf:14:90:3b:e0) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: b6:37:34:97:b8:e3 (b6:37:34:97:b8:e3) Address: b6:37:34:97:b8:e3 (b6:37:34:97:b8:e3) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 192.168.124.1, Dst: 192.168.124.2 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x00 (DSCP: CS0, ECN: Not-ECT) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Total Length: 84 Identification: 0x3470 (13424) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set ...0 0000 0000 0000 = Fragment offset: 0 Time to live: 64 Protocol: ICMP (1) Header checksum: 0x8ce4 [validation disabled] [Header checksum status: Unverified] Source: 192.168.124.1 Destination: 192.168.124.2 Internet Control Message Protocol Type: 8 (Echo (ping) request) Code: 0 Checksum: 0x7c23 [correct] [Checksum Status: Good] Identifier (BE): 2192 (0x0890) Identifier (LE): 36872 (0x9008) Sequence number (BE): 1 (0x0001) Sequence number (LE): 256 (0x0100) Timestamp from icmp data: Mar 6, 2019 13:25:53.000000000 EST [Timestamp from icmp data (relative): 1.545467000 seconds] Data (48 bytes) 0000 7d 0b 06 00 00 00 00 00 10 11 12 13 14 15 16 17 }............... 0010 18 19 1a 1b 1c 1d 1e 1f 20 21 22 23 24 25 26 27 ........ !\"#$%&' 0020 28 29 2a 2b 2c 2d 2e 2f 30 31 32 33 34 35 36 37 ()*+,-./01234567 Data: 7d0b060000000000101112131415161718191a1b1c1d1e1f... [Length: 48] [akaris@wks-akaris geneve]$ tshark -r vxlan.pcap -V frame.number==44 Frame 44: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) Encapsulation type: Ethernet (1) Arrival Time: Mar 6, 2019 13:25:54.545788000 EST [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1551896754.545788000 seconds [Time delta from previous captured frame: 0.000321000 seconds] [Time delta from previous displayed frame: 0.000000000 seconds] [Time since reference or first frame: 61.528564000 seconds] Frame Number: 44 Frame Length: 148 bytes (1184 bits) Capture Length: 148 bytes (1184 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:udp:vxlan:eth:ethertype:ip:icmp:data] Ethernet II, Src: fa:16:3e:bf:f2:d4 (fa:16:3e:bf:f2:d4), Dst: fa:16:3e:40:02:d7 (fa:16:3e:40:02:d7) Destination: fa:16:3e:40:02:d7 (fa:16:3e:40:02:d7) Address: fa:16:3e:40:02:d7 (fa:16:3e:40:02:d7) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: fa:16:3e:bf:f2:d4 (fa:16:3e:bf:f2:d4) Address: fa:16:3e:bf:f2:d4 (fa:16:3e:bf:f2:d4) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 192.168.1.12, Dst: 192.168.0.12 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x00 (DSCP: CS0, ECN: Not-ECT) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Total Length: 134 Identification: 0xe768 (59240) Flags: 0x0000 0... .... .... .... = Reserved bit: Not set .0.. .... .... .... = Don't fragment: Not set ..0. .... .... .... = More fragments: Not set ...0 0000 0000 0000 = Fragment offset: 0 Time to live: 64 Protocol: UDP (17) Header checksum: 0x1096 [validation disabled] [Header checksum status: Unverified] Source: 192.168.1.12 Destination: 192.168.0.12 User Datagram Protocol, Src Port: 38748, Dst Port: 4789 Source Port: 38748 Destination Port: 4789 Length: 114 [Checksum: [missing]] [Checksum Status: Not present] [Stream index: 12] Virtual eXtensible Local Area Network Flags: 0x0800, VXLAN Network ID (VNI) 0... .... .... .... = GBP Extension: Not defined .... .... .0.. .... = Don't Learn: False .... 1... .... .... = VXLAN Network ID (VNI): True .... .... .... 0... = Policy Applied: False .000 .000 0.00 .000 = Reserved(R): 0x0000 Group Policy ID: 0 VXLAN Network Identifier (VNI): 1234 Reserved: 0 Ethernet II, Src: 56:cf:14:90:3b:e0 (56:cf:14:90:3b:e0), Dst: b6:37:34:97:b8:e3 (b6:37:34:97:b8:e3) Destination: b6:37:34:97:b8:e3 (b6:37:34:97:b8:e3) Address: b6:37:34:97:b8:e3 (b6:37:34:97:b8:e3) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: 56:cf:14:90:3b:e0 (56:cf:14:90:3b:e0) Address: 56:cf:14:90:3b:e0 (56:cf:14:90:3b:e0) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 192.168.124.2, Dst: 192.168.124.1 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x00 (DSCP: CS0, ECN: Not-ECT) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Total Length: 84 Identification: 0x7bc6 (31686) Flags: 0x0000 0... .... .... .... = Reserved bit: Not set .0.. .... .... .... = Don't fragment: Not set ..0. .... .... .... = More fragments: Not set ...0 0000 0000 0000 = Fragment offset: 0 Time to live: 64 Protocol: ICMP (1) Header checksum: 0x858e [validation disabled] [Header checksum status: Unverified] Source: 192.168.124.2 Destination: 192.168.124.1 Internet Control Message Protocol Type: 0 (Echo (ping) reply) Code: 0 Checksum: 0x8423 [correct] [Checksum Status: Good] Identifier (BE): 2192 (0x0890) Identifier (LE): 36872 (0x9008) Sequence number (BE): 1 (0x0001) Sequence number (LE): 256 (0x0100) [Request frame: 43] [Response time: 0.321 ms] Timestamp from icmp data: Mar 6, 2019 13:25:53.000000000 EST [Timestamp from icmp data (relative): 1.545788000 seconds] Data (48 bytes) 0000 7d 0b 06 00 00 00 00 00 10 11 12 13 14 15 16 17 }............... 0010 18 19 1a 1b 1c 1d 1e 1f 20 21 22 23 24 25 26 27 ........ !\"#$%&' 0020 28 29 2a 2b 2c 2d 2e 2f 30 31 32 33 34 35 36 37 ()*+,-./01234567 Data: 7d0b060000000000101112131415161718191a1b1c1d1e1f... [Length: 48] Geneve https://www.ietf.org/id/draft-ietf-nvo3-geneve-10.txt 3.3. UDP Header The use of an encapsulating UDP [RFC0768] header follows the connectionless semantics of Ethernet and IP in addition to providing entropy to routers performing ECMP. The header fields are therefore interpreted as follows: Source port: A source port selected by the originating tunnel endpoint. This source port SHOULD be the same for all packets belonging to a single encapsulated flow to prevent reordering due to the use of different paths. To encourage an even distribution of flows across multiple links, the source port SHOULD be calculated using a hash of the encapsulated packet headers using, for example, a traditional 5-tuple. Since the port represents a flow identifier rather than a true UDP connection, the entire 16-bit range MAY be used to maximize entropy. Dest port: IANA has assigned port 6081 as the fixed well-known destination port for Geneve. Although the well-known value should be used by default, it is RECOMMENDED that implementations make this configurable. The chosen port is used for identification of Geneve packets and MUST NOT be reversed for different ends of a connection as is done with TCP. [cloud-user@rhel-test1 ~]$ sudo -i [root@rhel-test1 ~]# ip link add dev gnv0 type geneve remote 192.168.22.1 vni 1234 [root@rhel-test1 ~]# ip link ls dev gnv0 3: gnv0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT qlen 1000 link/ether ca:02:1d:c7:57:52 brd ff:ff:ff:ff:ff:ff [root@rhel-test1 ~]# ip link del dev gnv0 [root@rhel-test1 ~]# ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1446 qdisc pfifo_fast state UP qlen 1000 link/ether fa:16:3e:4f:be:dc brd ff:ff:ff:ff:ff:ff inet 192.168.0.12/24 brd 192.168.0.255 scope global dynamic eth0 valid_lft 85923sec preferred_lft 85923sec inet6 2000:192:168:1:f816:3eff:fe4f:bedc/64 scope global noprefixroute dynamic valid_lft 86369sec preferred_lft 14369sec inet6 fe80::f816:3eff:fe4f:bedc/64 scope link valid_lft forever preferred_lft forever [root@rhel-test1 ~]# ping 192.168.1.12 PING 192.168.1.12 (192.168.1.12) 56(84) bytes of data. 64 bytes from 192.168.1.12: icmp_seq=1 ttl=63 time=1.64 ms ^C --- 192.168.1.12 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 1.648/1.648/1.648/0.000 ms [root@rhel-test1 ~]# ip link add dev gnv0 type geneve remote 192.168.1.12 vni 1234 [root@rhel-test1 ~]# ip link set dev gnv0 up [root@rhel-test1 ~]# ip a a dev gnv0 192.168.123.1/30 [root@rhel-test1 ~]# ping 192.168.123.2 PING 192.168.123.2 (192.168.123.2) 56(84) bytes of data. 64 bytes from 192.168.123.2: icmp_seq=1 ttl=64 time=3.30 ms 64 bytes from 192.168.123.2: icmp_seq=2 ttl=64 time=1.62 ms 64 bytes from 192.168.123.2: icmp_seq=3 ttl=64 time=1.61 ms ^C --- 192.168.123.2 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2004ms rtt min/avg/max/mdev = 1.614/2.180/3.308/0.798 ms [root@rhel-test1 ~]# sudo -i[cloud-user@rhel-test2 ~]$ sudo -i [root@rhel-test2 ~]# ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1446 qdisc pfifo_fast state UP qlen 1000 link/ether fa:16:3e:bf:f2:d4 brd ff:ff:ff:ff:ff:ff inet 192.168.1.12/24 brd 192.168.1.255 scope global dynamic eth0 valid_lft 86302sec preferred_lft 86302sec inet6 fe80::f816:3eff:febf:f2d4/64 scope link valid_lft forever preferred_lft forever [root@rhel-test2 ~]# ip link add dev gnv0 type geneve remote 192.168.0.12 vni 1234 [root@rhel-test2 ~]# ip link set dev gnv0 up [root@rhel-test2 ~]# ip a a dev gnv0 192.168.123.2/30 [root@rhel-test2 ~]# [root@overcloud-compute-0 ~]# ip link ls | grep tap 22: tapf1d88f44-52: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1446 qdisc pfifo_fast master qbrf1d88f44-52 state UNKNOWN mode DEFAULT group default qlen 1000 [root@overcloud-compute-0 ~]# tcpdump -nne -itapf1d88f44-52 not port 22 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on tapf1d88f44-52, link-type EN10MB (Ethernet), capture size 262144 bytes 18:14:08.648156 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype ARP (0x0806), length 42: Request who-has 192.168.0.12 tell 192.168.0.1, length 28 18:14:08.648706 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype ARP (0x0806), length 42: Reply 192.168.0.12 is-at fa:16:3e:4f:be:dc, length 28 18:14:24.204432 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype IPv4 (0x0800), length 140: 192.168.0.12.36852 > 192.168.1.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): fa:5f:19:75:3b:53 > 33:33:00:00:00:16, ethertype IPv6 (0x86dd), length 90: :: > ff02::16: HBH ICMP6, multicast listener report v2, 1 group record(s), length 28 18:14:24.205955 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 168: 192.168.1.12 > 192.168.0.12: ICMP 192.168.1.12 udp port 6081 unreachable, length 134 18:14:24.288637 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype IPv4 (0x0800), length 140: 192.168.0.12.36852 > 192.168.1.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): fa:5f:19:75:3b:53 > 33:33:00:00:00:16, ethertype IPv6 (0x86dd), length 90: :: > ff02::16: HBH ICMP6, multicast listener report v2, 1 group record(s), length 28 18:14:24.289879 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 168: 192.168.1.12 > 192.168.0.12: ICMP 192.168.1.12 udp port 6081 unreachable, length 134 18:14:24.481721 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype IPv4 (0x0800), length 128: 192.168.0.12.18960 > 192.168.1.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): fa:5f:19:75:3b:53 > 33:33:ff:75:3b:53, ethertype IPv6 (0x86dd), length 78: :: > ff02::1:ff75:3b53: ICMP6, neighbor solicitation, who has fe80::f85f:19ff:fe75:3b53, length 24 18:14:24.483052 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 156: 192.168.1.12 > 192.168.0.12: ICMP 192.168.1.12 udp port 6081 unreachable, length 122 18:14:25.485623 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype IPv4 (0x0800), length 140: 192.168.0.12.15830 > 192.168.1.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): fa:5f:19:75:3b:53 > 33:33:00:00:00:16, ethertype IPv6 (0x86dd), length 90: fe80::f85f:19ff:fe75:3b53 > ff02::16: HBH ICMP6, multicast listener report v2, 1 group record(s), length 28 18:14:25.485733 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype IPv4 (0x0800), length 120: 192.168.0.12.13198 > 192.168.1.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): fa:5f:19:75:3b:53 > 33:33:00:00:00:02, ethertype IPv6 (0x86dd), length 70: fe80::f85f:19ff:fe75:3b53 > ff02::2: ICMP6, router solicitation, length 16 18:14:25.487070 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 168: 192.168.1.12 > 192.168.0.12: ICMP 192.168.1.12 udp port 6081 unreachable, length 134 18:14:25.487083 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 148: 192.168.1.12 > 192.168.0.12: ICMP 192.168.1.12 udp port 6081 unreachable, length 114 18:14:26.400718 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype IPv4 (0x0800), length 140: 192.168.0.12.15830 > 192.168.1.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): fa:5f:19:75:3b:53 > 33:33:00:00:00:16, ethertype IPv6 (0x86dd), length 90: fe80::f85f:19ff:fe75:3b53 > ff02::16: HBH ICMP6, multicast listener report v2, 1 group record(s), length 28 18:14:26.402262 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 168: 192.168.1.12 > 192.168.0.12: ICMP 192.168.1.12 udp port 6081 unreachable, length 134 18:14:29.488599 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype IPv4 (0x0800), length 120: 192.168.0.12.13198 > 192.168.1.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): fa:5f:19:75:3b:53 > 33:33:00:00:00:02, ethertype IPv6 (0x86dd), length 70: fe80::f85f:19ff:fe75:3b53 > ff02::2: ICMP6, router solicitation, length 16 18:14:29.489939 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 148: 192.168.1.12 > 192.168.0.12: ICMP 192.168.1.12 udp port 6081 unreachable, length 114 18:14:33.496721 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype IPv4 (0x0800), length 120: 192.168.0.12.13198 > 192.168.1.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): fa:5f:19:75:3b:53 > 33:33:00:00:00:02, ethertype IPv6 (0x86dd), length 70: fe80::f85f:19ff:fe75:3b53 > ff02::2: ICMP6, router solicitation, length 16 18:14:33.498447 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 148: 192.168.1.12 > 192.168.0.12: ICMP 192.168.1.12 udp port 6081 unreachable, length 114 18:14:40.076232 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 140: 192.168.1.12.278 > 192.168.0.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): 6e:51:d3:23:5a:f1 > 33:33:00:00:00:16, ethertype IPv6 (0x86dd), length 90: :: > ff02::16: HBH ICMP6, multicast listener report v2, 1 group record(s), length 28 18:14:40.217265 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 140: 192.168.1.12.278 > 192.168.0.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): 6e:51:d3:23:5a:f1 > 33:33:00:00:00:16, ethertype IPv6 (0x86dd), length 90: :: > ff02::16: HBH ICMP6, multicast listener report v2, 1 group record(s), length 28 18:14:40.625369 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 128: 192.168.1.12.47096 > 192.168.0.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): 6e:51:d3:23:5a:f1 > 33:33:ff:23:5a:f1, ethertype IPv6 (0x86dd), length 78: :: > ff02::1:ff23:5af1: ICMP6, neighbor solicitation, who has fe80::6c51:d3ff:fe23:5af1, length 24 18:14:41.627556 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 140: 192.168.1.12.55971 > 192.168.0.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): 6e:51:d3:23:5a:f1 > 33:33:00:00:00:16, ethertype IPv6 (0x86dd), length 90: fe80::6c51:d3ff:fe23:5af1 > ff02::16: HBH ICMP6, multicast listener report v2, 1 group record(s), length 28 18:14:41.627589 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 120: 192.168.1.12.16772 > 192.168.0.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): 6e:51:d3:23:5a:f1 > 33:33:00:00:00:02, ethertype IPv6 (0x86dd), length 70: fe80::6c51:d3ff:fe23:5af1 > ff02::2: ICMP6, router solicitation, length 16 18:14:42.169268 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 140: 192.168.1.12.55971 > 192.168.0.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): 6e:51:d3:23:5a:f1 > 33:33:00:00:00:16, ethertype IPv6 (0x86dd), length 90: fe80::6c51:d3ff:fe23:5af1 > ff02::16: HBH ICMP6, multicast listener report v2, 1 group record(s), length 28 18:14:45.081086 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype ARP (0x0806), length 42: Request who-has 192.168.0.12 tell 192.168.0.1, length 28 18:14:45.081657 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype ARP (0x0806), length 42: Reply 192.168.0.12 is-at fa:16:3e:4f:be:dc, length 28 18:14:45.631348 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 120: 192.168.1.12.16772 > 192.168.0.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): 6e:51:d3:23:5a:f1 > 33:33:00:00:00:02, ethertype IPv6 (0x86dd), length 70: fe80::6c51:d3ff:fe23:5af1 > ff02::2: ICMP6, router solicitation, length 16 18:14:49.639089 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 120: 192.168.1.12.16772 > 192.168.0.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): 6e:51:d3:23:5a:f1 > 33:33:00:00:00:02, ethertype IPv6 (0x86dd), length 70: fe80::6c51:d3ff:fe23:5af1 > ff02::2: ICMP6, router solicitation, length 16 18:14:52.450778 fa:16:3e:37:aa:90 > 33:33:00:00:00:01, ethertype IPv6 (0x86dd), length 118: fe80::f816:3eff:fe37:aa90 > ff02::1: ICMP6, router advertisement, length 64 18:15:20.233535 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype ARP (0x0806), length 42: Request who-has 192.168.0.12 tell 192.168.0.1, length 28 18:15:20.234077 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype ARP (0x0806), length 42: Reply 192.168.0.12 is-at fa:16:3e:4f:be:dc, length 28 18:15:28.043937 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype IPv4 (0x0800), length 92: 192.168.0.12.29278 > 192.168.1.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): fa:5f:19:75:3b:53 > ff:ff:ff:ff:ff:ff, ethertype ARP (0x0806), length 42: Request who-has 192.168.123.2 tell 192.168.123.1, length 28 18:15:28.045199 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 92: 192.168.1.12.28059 > 192.168.0.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): 6e:51:d3:23:5a:f1 > fa:5f:19:75:3b:53, ethertype ARP (0x0806), length 42: Reply 192.168.123.2 is-at 6e:51:d3:23:5a:f1, length 28 18:15:28.045900 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype IPv4 (0x0800), length 148: 192.168.0.12.62602 > 192.168.1.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): fa:5f:19:75:3b:53 > 6e:51:d3:23:5a:f1, ethertype IPv4 (0x0800), length 98: 192.168.123.1 > 192.168.123.2: ICMP echo request, id 2135, seq 1, length 64 18:15:28.046928 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 148: 192.168.1.12.59192 > 192.168.0.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): 6e:51:d3:23:5a:f1 > fa:5f:19:75:3b:53, ethertype IPv4 (0x0800), length 98: 192.168.123.2 > 192.168.123.1: ICMP echo reply, id 2135, seq 1, length 64 18:15:29.045996 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype IPv4 (0x0800), length 148: 192.168.0.12.62602 > 192.168.1.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): fa:5f:19:75:3b:53 > 6e:51:d3:23:5a:f1, ethertype IPv4 (0x0800), length 98: 192.168.123.1 > 192.168.123.2: ICMP echo request, id 2135, seq 2, length 64 18:15:29.047241 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 148: 192.168.1.12.59192 > 192.168.0.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): 6e:51:d3:23:5a:f1 > fa:5f:19:75:3b:53, ethertype IPv4 (0x0800), length 98: 192.168.123.2 > 192.168.123.1: ICMP echo reply, id 2135, seq 2, length 64 18:15:30.048222 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype IPv4 (0x0800), length 148: 192.168.0.12.62602 > 192.168.1.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): fa:5f:19:75:3b:53 > 6e:51:d3:23:5a:f1, ethertype IPv4 (0x0800), length 98: 192.168.123.1 > 192.168.123.2: ICMP echo request, id 2135, seq 3, length 64 18:15:30.049395 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 148: 192.168.1.12.59192 > 192.168.0.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): 6e:51:d3:23:5a:f1 > fa:5f:19:75:3b:53, ethertype IPv4 (0x0800), length 98: 192.168.123.2 > 192.168.123.1: ICMP echo reply, id 2135, seq 3, length 64 ^C 39 packets captured 39 packets received by filter 0 packets dropped by kernel [root@overcloud-compute-0 ~]# [akaris@wks-akaris geneve]$ tshark -r geneve.pcap -V frame.number==102 Frame 102: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) Encapsulation type: Ethernet (1) Arrival Time: Mar 6, 2019 13:15:28.046414000 EST [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1551896128.046414000 seconds [Time delta from previous captured frame: 0.001384000 seconds] [Time delta from previous displayed frame: 0.000000000 seconds] [Time since reference or first frame: 67.862074000 seconds] Frame Number: 102 Frame Length: 148 bytes (1184 bits) Capture Length: 148 bytes (1184 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:udp:geneve:eth:ethertype:ip:icmp:data] Ethernet II, Src: fa:16:3e:40:02:d7 (fa:16:3e:40:02:d7), Dst: fa:16:3e:bf:f2:d4 (fa:16:3e:bf:f2:d4) Destination: fa:16:3e:bf:f2:d4 (fa:16:3e:bf:f2:d4) Address: fa:16:3e:bf:f2:d4 (fa:16:3e:bf:f2:d4) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: fa:16:3e:40:02:d7 (fa:16:3e:40:02:d7) Address: fa:16:3e:40:02:d7 (fa:16:3e:40:02:d7) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 192.168.0.12, Dst: 192.168.1.12 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x00 (DSCP: CS0, ECN: Not-ECT) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Total Length: 134 Identification: 0xa840 (43072) Flags: 0x0000 0... .... .... .... = Reserved bit: Not set .0.. .... .... .... = Don't fragment: Not set ..0. .... .... .... = More fragments: Not set ...0 0000 0000 0000 = Fragment offset: 0 Time to live: 63 Protocol: UDP (17) Header checksum: 0x50be [validation disabled] [Header checksum status: Unverified] Source: 192.168.0.12 Destination: 192.168.1.12 User Datagram Protocol, Src Port: 62602, Dst Port: 6081 Source Port: 62602 Destination Port: 6081 Length: 114 Checksum: 0x4569 [unverified] [Checksum Status: Unverified] [Stream index: 18] Generic Network Virtualization Encapsulation, VNI: 0x0004d2 Version: 0 Length: 0 bytes Flags: 0x00 0... .... = Operations, Administration and Management Frame: False .0.. .... = Critical Options Present: False ..00 0000 = Reserved: False Protocol Type: Transparent Ethernet bridging (0x6558) Virtual Network Identifier (VNI): 0x0004d2 Ethernet II, Src: fa:5f:19:75:3b:53 (fa:5f:19:75:3b:53), Dst: 6e:51:d3:23:5a:f1 (6e:51:d3:23:5a:f1) Destination: 6e:51:d3:23:5a:f1 (6e:51:d3:23:5a:f1) Address: 6e:51:d3:23:5a:f1 (6e:51:d3:23:5a:f1) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: fa:5f:19:75:3b:53 (fa:5f:19:75:3b:53) Address: fa:5f:19:75:3b:53 (fa:5f:19:75:3b:53) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 192.168.123.1, Dst: 192.168.123.2 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x00 (DSCP: CS0, ECN: Not-ECT) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Total Length: 84 Identification: 0x0cba (3258) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set ...0 0000 0000 0000 = Fragment offset: 0 Time to live: 64 Protocol: ICMP (1) Header checksum: 0xb69a [validation disabled] [Header checksum status: Unverified] Source: 192.168.123.1 Destination: 192.168.123.2 Internet Control Message Protocol Type: 8 (Echo (ping) request) Code: 0 Checksum: 0xa3c1 [correct] [Checksum Status: Good] Identifier (BE): 2135 (0x0857) Identifier (LE): 22280 (0x5708) Sequence number (BE): 1 (0x0001) Sequence number (LE): 256 (0x0100) Timestamp from icmp data: Mar 6, 2019 13:15:26.000000000 EST [Timestamp from icmp data (relative): 2.046414000 seconds] Data (48 bytes) 0000 c1 a8 0d 00 00 00 00 00 10 11 12 13 14 15 16 17 ................ 0010 18 19 1a 1b 1c 1d 1e 1f 20 21 22 23 24 25 26 27 ........ !\"#$%&' 0020 28 29 2a 2b 2c 2d 2e 2f 30 31 32 33 34 35 36 37 ()*+,-./01234567 Data: c1a80d0000000000101112131415161718191a1b1c1d1e1f... [Length: 48] [akaris@wks-akaris geneve]$ [akaris@wks-akaris geneve]$ tshark -r geneve.pcap -V frame.number==103 Frame 103: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) Encapsulation type: Ethernet (1) Arrival Time: Mar 6, 2019 13:15:28.046802000 EST [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1551896128.046802000 seconds [Time delta from previous captured frame: 0.000388000 seconds] [Time delta from previous displayed frame: 0.000000000 seconds] [Time since reference or first frame: 67.862462000 seconds] Frame Number: 103 Frame Length: 148 bytes (1184 bits) Capture Length: 148 bytes (1184 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:udp:geneve:eth:ethertype:ip:icmp:data] Ethernet II, Src: fa:16:3e:bf:f2:d4 (fa:16:3e:bf:f2:d4), Dst: fa:16:3e:40:02:d7 (fa:16:3e:40:02:d7) Destination: fa:16:3e:40:02:d7 (fa:16:3e:40:02:d7) Address: fa:16:3e:40:02:d7 (fa:16:3e:40:02:d7) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: fa:16:3e:bf:f2:d4 (fa:16:3e:bf:f2:d4) Address: fa:16:3e:bf:f2:d4 (fa:16:3e:bf:f2:d4) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 192.168.1.12, Dst: 192.168.0.12 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x00 (DSCP: CS0, ECN: Not-ECT) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Total Length: 134 Identification: 0x5bdb (23515) Flags: 0x0000 0... .... .... .... = Reserved bit: Not set .0.. .... .... .... = Don't fragment: Not set ..0. .... .... .... = More fragments: Not set ...0 0000 0000 0000 = Fragment offset: 0 Time to live: 64 Protocol: UDP (17) Header checksum: 0x9c23 [validation disabled] [Header checksum status: Unverified] Source: 192.168.1.12 Destination: 192.168.0.12 User Datagram Protocol, Src Port: 59192, Dst Port: 6081 Source Port: 59192 Destination Port: 6081 Length: 114 Checksum: 0x52bb [unverified] [Checksum Status: Unverified] [Stream index: 19] Generic Network Virtualization Encapsulation, VNI: 0x0004d2 Version: 0 Length: 0 bytes Flags: 0x00 0... .... = Operations, Administration and Management Frame: False .0.. .... = Critical Options Present: False ..00 0000 = Reserved: False Protocol Type: Transparent Ethernet bridging (0x6558) Virtual Network Identifier (VNI): 0x0004d2 Ethernet II, Src: 6e:51:d3:23:5a:f1 (6e:51:d3:23:5a:f1), Dst: fa:5f:19:75:3b:53 (fa:5f:19:75:3b:53) Destination: fa:5f:19:75:3b:53 (fa:5f:19:75:3b:53) Address: fa:5f:19:75:3b:53 (fa:5f:19:75:3b:53) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: 6e:51:d3:23:5a:f1 (6e:51:d3:23:5a:f1) Address: 6e:51:d3:23:5a:f1 (6e:51:d3:23:5a:f1) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 192.168.123.2, Dst: 192.168.123.1 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x00 (DSCP: CS0, ECN: Not-ECT) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Total Length: 84 Identification: 0x7754 (30548) Flags: 0x0000 0... .... .... .... = Reserved bit: Not set .0.. .... .... .... = Don't fragment: Not set ..0. .... .... .... = More fragments: Not set ...0 0000 0000 0000 = Fragment offset: 0 Time to live: 64 Protocol: ICMP (1) Header checksum: 0x8c00 [validation disabled] [Header checksum status: Unverified] Source: 192.168.123.2 Destination: 192.168.123.1 Internet Control Message Protocol Type: 0 (Echo (ping) reply) Code: 0 Checksum: 0xabc1 [correct] [Checksum Status: Good] Identifier (BE): 2135 (0x0857) Identifier (LE): 22280 (0x5708) Sequence number (BE): 1 (0x0001) Sequence number (LE): 256 (0x0100) [Request frame: 102] [Response time: 0.388 ms] Timestamp from icmp data: Mar 6, 2019 13:15:26.000000000 EST [Timestamp from icmp data (relative): 2.046802000 seconds] Data (48 bytes) 0000 c1 a8 0d 00 00 00 00 00 10 11 12 13 14 15 16 17 ................ 0010 18 19 1a 1b 1c 1d 1e 1f 20 21 22 23 24 25 26 27 ........ !\"#$%&' 0020 28 29 2a 2b 2c 2d 2e 2f 30 31 32 33 34 35 36 37 ()*+,-./01234567 Data: c1a80d0000000000101112131415161718191a1b1c1d1e1f... [Length: 48]","title":"Geneve tunneling"},{"location":"networking/geneve_tunneling/#geneve-tunneling-vs-vxlan","text":"","title":"Geneve tunneling vs VXLAN"},{"location":"networking/geneve_tunneling/#packet-captures","text":"","title":"Packet captures"},{"location":"networking/geneve_tunneling/#vxlan","text":"[root@rhel-test1 ~]# ip link add dev vxlan0 type vxlan remote 192.168.1.12 vni 1234 vxlan: destination port not specified Will use Linux kernel default (non-standard value) Use 'dstport 4789' to get the IANA assigned value Use 'dstport 0' to get default and quiet this message [root@rhel-test1 ~]# ip link del dev vxlan0 [root@rhel-test1 ~]# ip link add dev vxlan0 type vxlan remote 192.168.1.12 vni 1234 dstport 4789 [root@rhel-test1 ~]# ip a a 192.168.124.1/30 dev vxlan0 [root@rhel-test1 ~]# ip link set dev vxlan0 up [root@rhel-test1 ~]# ping 192.168.124.2 PING 192.168.124.2 (192.168.124.2) 56(84) bytes of data. 64 bytes from 192.168.124.2: icmp_seq=1 ttl=64 time=3.02 ms 64 bytes from 192.168.124.2: icmp_seq=2 ttl=64 time=1.87 ms ^C --- 192.168.124.2 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1001ms rtt min/avg/max/mdev = 1.879/2.453/3.027/0.574 ms [root@rhel-test1 ~]# ping 192.168.124.2 PING 192.168.124.2 (192.168.124.2) 56(84) bytes of data. 64 bytes from 192.168.124.2: icmp_seq=1 ttl=64 time=1.15 ms ^C --- 192.168.124.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 1.158/1.158/1.158/0.000 ms [root@rhel-test1 ~]# [root@rhel-test2 ~]# ip link add dev vxlan0 type vxlan remote 192.168.0.12 vni 1234 dstport 4789 [root@rhel-test2 ~]# ip a a 192.168.124.2/30 vxlan0 Error: either \"local\" is duplicate, or \"vxlan0\" is a garbage. [root@rhel-test2 ~]# ip a a 192.168.124.2/30 dev vxlan0 [root@rhel-test2 ~]# ip link set dev vlan0 up Cannot find device \"vlan0\" [root@rhel-test2 ~]# ip link set dev vxlan0 up [root@rhel-test2 ~]# tcpdump -nne -i vxlan0 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on vxlan0, link-type EN10MB (Ethernet), capture size 65535 bytes 13:25:54.405073 b6:37:34:97:b8:e3 > 56:cf:14:90:3b:e0, ethertype IPv4 (0x0800), length 98: 192.168.124.1 > 192.168.124.2: ICMP echo request, id 2192, seq 1, length 64 13:25:54.405131 56:cf:14:90:3b:e0 > b6:37:34:97:b8:e3, ethertype IPv4 (0x0800), length 98: 192.168.124.2 > 192.168.124.1: ICMP echo reply, id 2192, seq 1, length 64 ^C 2 packets captured 2 packets received by filter 0 packets dropped by kernel [root@rhel-test2 ~]# [akaris@wks-akaris geneve]$ tshark -r vxlan.pcap -V frame.number==43 Frame 43: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) Encapsulation type: Ethernet (1) Arrival Time: Mar 6, 2019 13:25:54.545467000 EST [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1551896754.545467000 seconds [Time delta from previous captured frame: 5.489084000 seconds] [Time delta from previous displayed frame: 0.000000000 seconds] [Time since reference or first frame: 61.528243000 seconds] Frame Number: 43 Frame Length: 148 bytes (1184 bits) Capture Length: 148 bytes (1184 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:udp:vxlan:eth:ethertype:ip:icmp:data] Ethernet II, Src: fa:16:3e:40:02:d7 (fa:16:3e:40:02:d7), Dst: fa:16:3e:bf:f2:d4 (fa:16:3e:bf:f2:d4) Destination: fa:16:3e:bf:f2:d4 (fa:16:3e:bf:f2:d4) Address: fa:16:3e:bf:f2:d4 (fa:16:3e:bf:f2:d4) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: fa:16:3e:40:02:d7 (fa:16:3e:40:02:d7) Address: fa:16:3e:40:02:d7 (fa:16:3e:40:02:d7) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 192.168.0.12, Dst: 192.168.1.12 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x00 (DSCP: CS0, ECN: Not-ECT) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Total Length: 134 Identification: 0x45eb (17899) Flags: 0x0000 0... .... .... .... = Reserved bit: Not set .0.. .... .... .... = Don't fragment: Not set ..0. .... .... .... = More fragments: Not set ...0 0000 0000 0000 = Fragment offset: 0 Time to live: 63 Protocol: UDP (17) Header checksum: 0xb313 [validation disabled] [Header checksum status: Unverified] Source: 192.168.0.12 Destination: 192.168.1.12 User Datagram Protocol, Src Port: 34990, Dst Port: 4789 Source Port: 34990 Destination Port: 4789 Length: 114 [Checksum: [missing]] [Checksum Status: Not present] [Stream index: 11] Virtual eXtensible Local Area Network Flags: 0x0800, VXLAN Network ID (VNI) 0... .... .... .... = GBP Extension: Not defined .... .... .0.. .... = Don't Learn: False .... 1... .... .... = VXLAN Network ID (VNI): True .... .... .... 0... = Policy Applied: False .000 .000 0.00 .000 = Reserved(R): 0x0000 Group Policy ID: 0 VXLAN Network Identifier (VNI): 1234 Reserved: 0 Ethernet II, Src: b6:37:34:97:b8:e3 (b6:37:34:97:b8:e3), Dst: 56:cf:14:90:3b:e0 (56:cf:14:90:3b:e0) Destination: 56:cf:14:90:3b:e0 (56:cf:14:90:3b:e0) Address: 56:cf:14:90:3b:e0 (56:cf:14:90:3b:e0) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: b6:37:34:97:b8:e3 (b6:37:34:97:b8:e3) Address: b6:37:34:97:b8:e3 (b6:37:34:97:b8:e3) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 192.168.124.1, Dst: 192.168.124.2 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x00 (DSCP: CS0, ECN: Not-ECT) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Total Length: 84 Identification: 0x3470 (13424) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set ...0 0000 0000 0000 = Fragment offset: 0 Time to live: 64 Protocol: ICMP (1) Header checksum: 0x8ce4 [validation disabled] [Header checksum status: Unverified] Source: 192.168.124.1 Destination: 192.168.124.2 Internet Control Message Protocol Type: 8 (Echo (ping) request) Code: 0 Checksum: 0x7c23 [correct] [Checksum Status: Good] Identifier (BE): 2192 (0x0890) Identifier (LE): 36872 (0x9008) Sequence number (BE): 1 (0x0001) Sequence number (LE): 256 (0x0100) Timestamp from icmp data: Mar 6, 2019 13:25:53.000000000 EST [Timestamp from icmp data (relative): 1.545467000 seconds] Data (48 bytes) 0000 7d 0b 06 00 00 00 00 00 10 11 12 13 14 15 16 17 }............... 0010 18 19 1a 1b 1c 1d 1e 1f 20 21 22 23 24 25 26 27 ........ !\"#$%&' 0020 28 29 2a 2b 2c 2d 2e 2f 30 31 32 33 34 35 36 37 ()*+,-./01234567 Data: 7d0b060000000000101112131415161718191a1b1c1d1e1f... [Length: 48] [akaris@wks-akaris geneve]$ tshark -r vxlan.pcap -V frame.number==44 Frame 44: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) Encapsulation type: Ethernet (1) Arrival Time: Mar 6, 2019 13:25:54.545788000 EST [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1551896754.545788000 seconds [Time delta from previous captured frame: 0.000321000 seconds] [Time delta from previous displayed frame: 0.000000000 seconds] [Time since reference or first frame: 61.528564000 seconds] Frame Number: 44 Frame Length: 148 bytes (1184 bits) Capture Length: 148 bytes (1184 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:udp:vxlan:eth:ethertype:ip:icmp:data] Ethernet II, Src: fa:16:3e:bf:f2:d4 (fa:16:3e:bf:f2:d4), Dst: fa:16:3e:40:02:d7 (fa:16:3e:40:02:d7) Destination: fa:16:3e:40:02:d7 (fa:16:3e:40:02:d7) Address: fa:16:3e:40:02:d7 (fa:16:3e:40:02:d7) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: fa:16:3e:bf:f2:d4 (fa:16:3e:bf:f2:d4) Address: fa:16:3e:bf:f2:d4 (fa:16:3e:bf:f2:d4) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 192.168.1.12, Dst: 192.168.0.12 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x00 (DSCP: CS0, ECN: Not-ECT) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Total Length: 134 Identification: 0xe768 (59240) Flags: 0x0000 0... .... .... .... = Reserved bit: Not set .0.. .... .... .... = Don't fragment: Not set ..0. .... .... .... = More fragments: Not set ...0 0000 0000 0000 = Fragment offset: 0 Time to live: 64 Protocol: UDP (17) Header checksum: 0x1096 [validation disabled] [Header checksum status: Unverified] Source: 192.168.1.12 Destination: 192.168.0.12 User Datagram Protocol, Src Port: 38748, Dst Port: 4789 Source Port: 38748 Destination Port: 4789 Length: 114 [Checksum: [missing]] [Checksum Status: Not present] [Stream index: 12] Virtual eXtensible Local Area Network Flags: 0x0800, VXLAN Network ID (VNI) 0... .... .... .... = GBP Extension: Not defined .... .... .0.. .... = Don't Learn: False .... 1... .... .... = VXLAN Network ID (VNI): True .... .... .... 0... = Policy Applied: False .000 .000 0.00 .000 = Reserved(R): 0x0000 Group Policy ID: 0 VXLAN Network Identifier (VNI): 1234 Reserved: 0 Ethernet II, Src: 56:cf:14:90:3b:e0 (56:cf:14:90:3b:e0), Dst: b6:37:34:97:b8:e3 (b6:37:34:97:b8:e3) Destination: b6:37:34:97:b8:e3 (b6:37:34:97:b8:e3) Address: b6:37:34:97:b8:e3 (b6:37:34:97:b8:e3) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: 56:cf:14:90:3b:e0 (56:cf:14:90:3b:e0) Address: 56:cf:14:90:3b:e0 (56:cf:14:90:3b:e0) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 192.168.124.2, Dst: 192.168.124.1 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x00 (DSCP: CS0, ECN: Not-ECT) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Total Length: 84 Identification: 0x7bc6 (31686) Flags: 0x0000 0... .... .... .... = Reserved bit: Not set .0.. .... .... .... = Don't fragment: Not set ..0. .... .... .... = More fragments: Not set ...0 0000 0000 0000 = Fragment offset: 0 Time to live: 64 Protocol: ICMP (1) Header checksum: 0x858e [validation disabled] [Header checksum status: Unverified] Source: 192.168.124.2 Destination: 192.168.124.1 Internet Control Message Protocol Type: 0 (Echo (ping) reply) Code: 0 Checksum: 0x8423 [correct] [Checksum Status: Good] Identifier (BE): 2192 (0x0890) Identifier (LE): 36872 (0x9008) Sequence number (BE): 1 (0x0001) Sequence number (LE): 256 (0x0100) [Request frame: 43] [Response time: 0.321 ms] Timestamp from icmp data: Mar 6, 2019 13:25:53.000000000 EST [Timestamp from icmp data (relative): 1.545788000 seconds] Data (48 bytes) 0000 7d 0b 06 00 00 00 00 00 10 11 12 13 14 15 16 17 }............... 0010 18 19 1a 1b 1c 1d 1e 1f 20 21 22 23 24 25 26 27 ........ !\"#$%&' 0020 28 29 2a 2b 2c 2d 2e 2f 30 31 32 33 34 35 36 37 ()*+,-./01234567 Data: 7d0b060000000000101112131415161718191a1b1c1d1e1f... [Length: 48]","title":"VXLAN"},{"location":"networking/geneve_tunneling/#geneve","text":"https://www.ietf.org/id/draft-ietf-nvo3-geneve-10.txt 3.3. UDP Header The use of an encapsulating UDP [RFC0768] header follows the connectionless semantics of Ethernet and IP in addition to providing entropy to routers performing ECMP. The header fields are therefore interpreted as follows: Source port: A source port selected by the originating tunnel endpoint. This source port SHOULD be the same for all packets belonging to a single encapsulated flow to prevent reordering due to the use of different paths. To encourage an even distribution of flows across multiple links, the source port SHOULD be calculated using a hash of the encapsulated packet headers using, for example, a traditional 5-tuple. Since the port represents a flow identifier rather than a true UDP connection, the entire 16-bit range MAY be used to maximize entropy. Dest port: IANA has assigned port 6081 as the fixed well-known destination port for Geneve. Although the well-known value should be used by default, it is RECOMMENDED that implementations make this configurable. The chosen port is used for identification of Geneve packets and MUST NOT be reversed for different ends of a connection as is done with TCP. [cloud-user@rhel-test1 ~]$ sudo -i [root@rhel-test1 ~]# ip link add dev gnv0 type geneve remote 192.168.22.1 vni 1234 [root@rhel-test1 ~]# ip link ls dev gnv0 3: gnv0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT qlen 1000 link/ether ca:02:1d:c7:57:52 brd ff:ff:ff:ff:ff:ff [root@rhel-test1 ~]# ip link del dev gnv0 [root@rhel-test1 ~]# ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1446 qdisc pfifo_fast state UP qlen 1000 link/ether fa:16:3e:4f:be:dc brd ff:ff:ff:ff:ff:ff inet 192.168.0.12/24 brd 192.168.0.255 scope global dynamic eth0 valid_lft 85923sec preferred_lft 85923sec inet6 2000:192:168:1:f816:3eff:fe4f:bedc/64 scope global noprefixroute dynamic valid_lft 86369sec preferred_lft 14369sec inet6 fe80::f816:3eff:fe4f:bedc/64 scope link valid_lft forever preferred_lft forever [root@rhel-test1 ~]# ping 192.168.1.12 PING 192.168.1.12 (192.168.1.12) 56(84) bytes of data. 64 bytes from 192.168.1.12: icmp_seq=1 ttl=63 time=1.64 ms ^C --- 192.168.1.12 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 1.648/1.648/1.648/0.000 ms [root@rhel-test1 ~]# ip link add dev gnv0 type geneve remote 192.168.1.12 vni 1234 [root@rhel-test1 ~]# ip link set dev gnv0 up [root@rhel-test1 ~]# ip a a dev gnv0 192.168.123.1/30 [root@rhel-test1 ~]# ping 192.168.123.2 PING 192.168.123.2 (192.168.123.2) 56(84) bytes of data. 64 bytes from 192.168.123.2: icmp_seq=1 ttl=64 time=3.30 ms 64 bytes from 192.168.123.2: icmp_seq=2 ttl=64 time=1.62 ms 64 bytes from 192.168.123.2: icmp_seq=3 ttl=64 time=1.61 ms ^C --- 192.168.123.2 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2004ms rtt min/avg/max/mdev = 1.614/2.180/3.308/0.798 ms [root@rhel-test1 ~]# sudo -i[cloud-user@rhel-test2 ~]$ sudo -i [root@rhel-test2 ~]# ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1446 qdisc pfifo_fast state UP qlen 1000 link/ether fa:16:3e:bf:f2:d4 brd ff:ff:ff:ff:ff:ff inet 192.168.1.12/24 brd 192.168.1.255 scope global dynamic eth0 valid_lft 86302sec preferred_lft 86302sec inet6 fe80::f816:3eff:febf:f2d4/64 scope link valid_lft forever preferred_lft forever [root@rhel-test2 ~]# ip link add dev gnv0 type geneve remote 192.168.0.12 vni 1234 [root@rhel-test2 ~]# ip link set dev gnv0 up [root@rhel-test2 ~]# ip a a dev gnv0 192.168.123.2/30 [root@rhel-test2 ~]# [root@overcloud-compute-0 ~]# ip link ls | grep tap 22: tapf1d88f44-52: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1446 qdisc pfifo_fast master qbrf1d88f44-52 state UNKNOWN mode DEFAULT group default qlen 1000 [root@overcloud-compute-0 ~]# tcpdump -nne -itapf1d88f44-52 not port 22 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on tapf1d88f44-52, link-type EN10MB (Ethernet), capture size 262144 bytes 18:14:08.648156 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype ARP (0x0806), length 42: Request who-has 192.168.0.12 tell 192.168.0.1, length 28 18:14:08.648706 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype ARP (0x0806), length 42: Reply 192.168.0.12 is-at fa:16:3e:4f:be:dc, length 28 18:14:24.204432 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype IPv4 (0x0800), length 140: 192.168.0.12.36852 > 192.168.1.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): fa:5f:19:75:3b:53 > 33:33:00:00:00:16, ethertype IPv6 (0x86dd), length 90: :: > ff02::16: HBH ICMP6, multicast listener report v2, 1 group record(s), length 28 18:14:24.205955 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 168: 192.168.1.12 > 192.168.0.12: ICMP 192.168.1.12 udp port 6081 unreachable, length 134 18:14:24.288637 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype IPv4 (0x0800), length 140: 192.168.0.12.36852 > 192.168.1.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): fa:5f:19:75:3b:53 > 33:33:00:00:00:16, ethertype IPv6 (0x86dd), length 90: :: > ff02::16: HBH ICMP6, multicast listener report v2, 1 group record(s), length 28 18:14:24.289879 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 168: 192.168.1.12 > 192.168.0.12: ICMP 192.168.1.12 udp port 6081 unreachable, length 134 18:14:24.481721 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype IPv4 (0x0800), length 128: 192.168.0.12.18960 > 192.168.1.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): fa:5f:19:75:3b:53 > 33:33:ff:75:3b:53, ethertype IPv6 (0x86dd), length 78: :: > ff02::1:ff75:3b53: ICMP6, neighbor solicitation, who has fe80::f85f:19ff:fe75:3b53, length 24 18:14:24.483052 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 156: 192.168.1.12 > 192.168.0.12: ICMP 192.168.1.12 udp port 6081 unreachable, length 122 18:14:25.485623 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype IPv4 (0x0800), length 140: 192.168.0.12.15830 > 192.168.1.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): fa:5f:19:75:3b:53 > 33:33:00:00:00:16, ethertype IPv6 (0x86dd), length 90: fe80::f85f:19ff:fe75:3b53 > ff02::16: HBH ICMP6, multicast listener report v2, 1 group record(s), length 28 18:14:25.485733 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype IPv4 (0x0800), length 120: 192.168.0.12.13198 > 192.168.1.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): fa:5f:19:75:3b:53 > 33:33:00:00:00:02, ethertype IPv6 (0x86dd), length 70: fe80::f85f:19ff:fe75:3b53 > ff02::2: ICMP6, router solicitation, length 16 18:14:25.487070 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 168: 192.168.1.12 > 192.168.0.12: ICMP 192.168.1.12 udp port 6081 unreachable, length 134 18:14:25.487083 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 148: 192.168.1.12 > 192.168.0.12: ICMP 192.168.1.12 udp port 6081 unreachable, length 114 18:14:26.400718 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype IPv4 (0x0800), length 140: 192.168.0.12.15830 > 192.168.1.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): fa:5f:19:75:3b:53 > 33:33:00:00:00:16, ethertype IPv6 (0x86dd), length 90: fe80::f85f:19ff:fe75:3b53 > ff02::16: HBH ICMP6, multicast listener report v2, 1 group record(s), length 28 18:14:26.402262 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 168: 192.168.1.12 > 192.168.0.12: ICMP 192.168.1.12 udp port 6081 unreachable, length 134 18:14:29.488599 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype IPv4 (0x0800), length 120: 192.168.0.12.13198 > 192.168.1.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): fa:5f:19:75:3b:53 > 33:33:00:00:00:02, ethertype IPv6 (0x86dd), length 70: fe80::f85f:19ff:fe75:3b53 > ff02::2: ICMP6, router solicitation, length 16 18:14:29.489939 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 148: 192.168.1.12 > 192.168.0.12: ICMP 192.168.1.12 udp port 6081 unreachable, length 114 18:14:33.496721 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype IPv4 (0x0800), length 120: 192.168.0.12.13198 > 192.168.1.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): fa:5f:19:75:3b:53 > 33:33:00:00:00:02, ethertype IPv6 (0x86dd), length 70: fe80::f85f:19ff:fe75:3b53 > ff02::2: ICMP6, router solicitation, length 16 18:14:33.498447 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 148: 192.168.1.12 > 192.168.0.12: ICMP 192.168.1.12 udp port 6081 unreachable, length 114 18:14:40.076232 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 140: 192.168.1.12.278 > 192.168.0.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): 6e:51:d3:23:5a:f1 > 33:33:00:00:00:16, ethertype IPv6 (0x86dd), length 90: :: > ff02::16: HBH ICMP6, multicast listener report v2, 1 group record(s), length 28 18:14:40.217265 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 140: 192.168.1.12.278 > 192.168.0.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): 6e:51:d3:23:5a:f1 > 33:33:00:00:00:16, ethertype IPv6 (0x86dd), length 90: :: > ff02::16: HBH ICMP6, multicast listener report v2, 1 group record(s), length 28 18:14:40.625369 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 128: 192.168.1.12.47096 > 192.168.0.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): 6e:51:d3:23:5a:f1 > 33:33:ff:23:5a:f1, ethertype IPv6 (0x86dd), length 78: :: > ff02::1:ff23:5af1: ICMP6, neighbor solicitation, who has fe80::6c51:d3ff:fe23:5af1, length 24 18:14:41.627556 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 140: 192.168.1.12.55971 > 192.168.0.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): 6e:51:d3:23:5a:f1 > 33:33:00:00:00:16, ethertype IPv6 (0x86dd), length 90: fe80::6c51:d3ff:fe23:5af1 > ff02::16: HBH ICMP6, multicast listener report v2, 1 group record(s), length 28 18:14:41.627589 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 120: 192.168.1.12.16772 > 192.168.0.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): 6e:51:d3:23:5a:f1 > 33:33:00:00:00:02, ethertype IPv6 (0x86dd), length 70: fe80::6c51:d3ff:fe23:5af1 > ff02::2: ICMP6, router solicitation, length 16 18:14:42.169268 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 140: 192.168.1.12.55971 > 192.168.0.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): 6e:51:d3:23:5a:f1 > 33:33:00:00:00:16, ethertype IPv6 (0x86dd), length 90: fe80::6c51:d3ff:fe23:5af1 > ff02::16: HBH ICMP6, multicast listener report v2, 1 group record(s), length 28 18:14:45.081086 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype ARP (0x0806), length 42: Request who-has 192.168.0.12 tell 192.168.0.1, length 28 18:14:45.081657 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype ARP (0x0806), length 42: Reply 192.168.0.12 is-at fa:16:3e:4f:be:dc, length 28 18:14:45.631348 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 120: 192.168.1.12.16772 > 192.168.0.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): 6e:51:d3:23:5a:f1 > 33:33:00:00:00:02, ethertype IPv6 (0x86dd), length 70: fe80::6c51:d3ff:fe23:5af1 > ff02::2: ICMP6, router solicitation, length 16 18:14:49.639089 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 120: 192.168.1.12.16772 > 192.168.0.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): 6e:51:d3:23:5a:f1 > 33:33:00:00:00:02, ethertype IPv6 (0x86dd), length 70: fe80::6c51:d3ff:fe23:5af1 > ff02::2: ICMP6, router solicitation, length 16 18:14:52.450778 fa:16:3e:37:aa:90 > 33:33:00:00:00:01, ethertype IPv6 (0x86dd), length 118: fe80::f816:3eff:fe37:aa90 > ff02::1: ICMP6, router advertisement, length 64 18:15:20.233535 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype ARP (0x0806), length 42: Request who-has 192.168.0.12 tell 192.168.0.1, length 28 18:15:20.234077 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype ARP (0x0806), length 42: Reply 192.168.0.12 is-at fa:16:3e:4f:be:dc, length 28 18:15:28.043937 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype IPv4 (0x0800), length 92: 192.168.0.12.29278 > 192.168.1.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): fa:5f:19:75:3b:53 > ff:ff:ff:ff:ff:ff, ethertype ARP (0x0806), length 42: Request who-has 192.168.123.2 tell 192.168.123.1, length 28 18:15:28.045199 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 92: 192.168.1.12.28059 > 192.168.0.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): 6e:51:d3:23:5a:f1 > fa:5f:19:75:3b:53, ethertype ARP (0x0806), length 42: Reply 192.168.123.2 is-at 6e:51:d3:23:5a:f1, length 28 18:15:28.045900 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype IPv4 (0x0800), length 148: 192.168.0.12.62602 > 192.168.1.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): fa:5f:19:75:3b:53 > 6e:51:d3:23:5a:f1, ethertype IPv4 (0x0800), length 98: 192.168.123.1 > 192.168.123.2: ICMP echo request, id 2135, seq 1, length 64 18:15:28.046928 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 148: 192.168.1.12.59192 > 192.168.0.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): 6e:51:d3:23:5a:f1 > fa:5f:19:75:3b:53, ethertype IPv4 (0x0800), length 98: 192.168.123.2 > 192.168.123.1: ICMP echo reply, id 2135, seq 1, length 64 18:15:29.045996 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype IPv4 (0x0800), length 148: 192.168.0.12.62602 > 192.168.1.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): fa:5f:19:75:3b:53 > 6e:51:d3:23:5a:f1, ethertype IPv4 (0x0800), length 98: 192.168.123.1 > 192.168.123.2: ICMP echo request, id 2135, seq 2, length 64 18:15:29.047241 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 148: 192.168.1.12.59192 > 192.168.0.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): 6e:51:d3:23:5a:f1 > fa:5f:19:75:3b:53, ethertype IPv4 (0x0800), length 98: 192.168.123.2 > 192.168.123.1: ICMP echo reply, id 2135, seq 2, length 64 18:15:30.048222 fa:16:3e:4f:be:dc > fa:16:3e:cd:6b:54, ethertype IPv4 (0x0800), length 148: 192.168.0.12.62602 > 192.168.1.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): fa:5f:19:75:3b:53 > 6e:51:d3:23:5a:f1, ethertype IPv4 (0x0800), length 98: 192.168.123.1 > 192.168.123.2: ICMP echo request, id 2135, seq 3, length 64 18:15:30.049395 fa:16:3e:cd:6b:54 > fa:16:3e:4f:be:dc, ethertype IPv4 (0x0800), length 148: 192.168.1.12.59192 > 192.168.0.12.6081: Geneve, Flags [none], vni 0x4d2, proto TEB (0x6558): 6e:51:d3:23:5a:f1 > fa:5f:19:75:3b:53, ethertype IPv4 (0x0800), length 98: 192.168.123.2 > 192.168.123.1: ICMP echo reply, id 2135, seq 3, length 64 ^C 39 packets captured 39 packets received by filter 0 packets dropped by kernel [root@overcloud-compute-0 ~]# [akaris@wks-akaris geneve]$ tshark -r geneve.pcap -V frame.number==102 Frame 102: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) Encapsulation type: Ethernet (1) Arrival Time: Mar 6, 2019 13:15:28.046414000 EST [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1551896128.046414000 seconds [Time delta from previous captured frame: 0.001384000 seconds] [Time delta from previous displayed frame: 0.000000000 seconds] [Time since reference or first frame: 67.862074000 seconds] Frame Number: 102 Frame Length: 148 bytes (1184 bits) Capture Length: 148 bytes (1184 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:udp:geneve:eth:ethertype:ip:icmp:data] Ethernet II, Src: fa:16:3e:40:02:d7 (fa:16:3e:40:02:d7), Dst: fa:16:3e:bf:f2:d4 (fa:16:3e:bf:f2:d4) Destination: fa:16:3e:bf:f2:d4 (fa:16:3e:bf:f2:d4) Address: fa:16:3e:bf:f2:d4 (fa:16:3e:bf:f2:d4) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: fa:16:3e:40:02:d7 (fa:16:3e:40:02:d7) Address: fa:16:3e:40:02:d7 (fa:16:3e:40:02:d7) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 192.168.0.12, Dst: 192.168.1.12 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x00 (DSCP: CS0, ECN: Not-ECT) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Total Length: 134 Identification: 0xa840 (43072) Flags: 0x0000 0... .... .... .... = Reserved bit: Not set .0.. .... .... .... = Don't fragment: Not set ..0. .... .... .... = More fragments: Not set ...0 0000 0000 0000 = Fragment offset: 0 Time to live: 63 Protocol: UDP (17) Header checksum: 0x50be [validation disabled] [Header checksum status: Unverified] Source: 192.168.0.12 Destination: 192.168.1.12 User Datagram Protocol, Src Port: 62602, Dst Port: 6081 Source Port: 62602 Destination Port: 6081 Length: 114 Checksum: 0x4569 [unverified] [Checksum Status: Unverified] [Stream index: 18] Generic Network Virtualization Encapsulation, VNI: 0x0004d2 Version: 0 Length: 0 bytes Flags: 0x00 0... .... = Operations, Administration and Management Frame: False .0.. .... = Critical Options Present: False ..00 0000 = Reserved: False Protocol Type: Transparent Ethernet bridging (0x6558) Virtual Network Identifier (VNI): 0x0004d2 Ethernet II, Src: fa:5f:19:75:3b:53 (fa:5f:19:75:3b:53), Dst: 6e:51:d3:23:5a:f1 (6e:51:d3:23:5a:f1) Destination: 6e:51:d3:23:5a:f1 (6e:51:d3:23:5a:f1) Address: 6e:51:d3:23:5a:f1 (6e:51:d3:23:5a:f1) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: fa:5f:19:75:3b:53 (fa:5f:19:75:3b:53) Address: fa:5f:19:75:3b:53 (fa:5f:19:75:3b:53) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 192.168.123.1, Dst: 192.168.123.2 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x00 (DSCP: CS0, ECN: Not-ECT) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Total Length: 84 Identification: 0x0cba (3258) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set ...0 0000 0000 0000 = Fragment offset: 0 Time to live: 64 Protocol: ICMP (1) Header checksum: 0xb69a [validation disabled] [Header checksum status: Unverified] Source: 192.168.123.1 Destination: 192.168.123.2 Internet Control Message Protocol Type: 8 (Echo (ping) request) Code: 0 Checksum: 0xa3c1 [correct] [Checksum Status: Good] Identifier (BE): 2135 (0x0857) Identifier (LE): 22280 (0x5708) Sequence number (BE): 1 (0x0001) Sequence number (LE): 256 (0x0100) Timestamp from icmp data: Mar 6, 2019 13:15:26.000000000 EST [Timestamp from icmp data (relative): 2.046414000 seconds] Data (48 bytes) 0000 c1 a8 0d 00 00 00 00 00 10 11 12 13 14 15 16 17 ................ 0010 18 19 1a 1b 1c 1d 1e 1f 20 21 22 23 24 25 26 27 ........ !\"#$%&' 0020 28 29 2a 2b 2c 2d 2e 2f 30 31 32 33 34 35 36 37 ()*+,-./01234567 Data: c1a80d0000000000101112131415161718191a1b1c1d1e1f... [Length: 48] [akaris@wks-akaris geneve]$ [akaris@wks-akaris geneve]$ tshark -r geneve.pcap -V frame.number==103 Frame 103: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) Encapsulation type: Ethernet (1) Arrival Time: Mar 6, 2019 13:15:28.046802000 EST [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1551896128.046802000 seconds [Time delta from previous captured frame: 0.000388000 seconds] [Time delta from previous displayed frame: 0.000000000 seconds] [Time since reference or first frame: 67.862462000 seconds] Frame Number: 103 Frame Length: 148 bytes (1184 bits) Capture Length: 148 bytes (1184 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:udp:geneve:eth:ethertype:ip:icmp:data] Ethernet II, Src: fa:16:3e:bf:f2:d4 (fa:16:3e:bf:f2:d4), Dst: fa:16:3e:40:02:d7 (fa:16:3e:40:02:d7) Destination: fa:16:3e:40:02:d7 (fa:16:3e:40:02:d7) Address: fa:16:3e:40:02:d7 (fa:16:3e:40:02:d7) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: fa:16:3e:bf:f2:d4 (fa:16:3e:bf:f2:d4) Address: fa:16:3e:bf:f2:d4 (fa:16:3e:bf:f2:d4) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 192.168.1.12, Dst: 192.168.0.12 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x00 (DSCP: CS0, ECN: Not-ECT) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Total Length: 134 Identification: 0x5bdb (23515) Flags: 0x0000 0... .... .... .... = Reserved bit: Not set .0.. .... .... .... = Don't fragment: Not set ..0. .... .... .... = More fragments: Not set ...0 0000 0000 0000 = Fragment offset: 0 Time to live: 64 Protocol: UDP (17) Header checksum: 0x9c23 [validation disabled] [Header checksum status: Unverified] Source: 192.168.1.12 Destination: 192.168.0.12 User Datagram Protocol, Src Port: 59192, Dst Port: 6081 Source Port: 59192 Destination Port: 6081 Length: 114 Checksum: 0x52bb [unverified] [Checksum Status: Unverified] [Stream index: 19] Generic Network Virtualization Encapsulation, VNI: 0x0004d2 Version: 0 Length: 0 bytes Flags: 0x00 0... .... = Operations, Administration and Management Frame: False .0.. .... = Critical Options Present: False ..00 0000 = Reserved: False Protocol Type: Transparent Ethernet bridging (0x6558) Virtual Network Identifier (VNI): 0x0004d2 Ethernet II, Src: 6e:51:d3:23:5a:f1 (6e:51:d3:23:5a:f1), Dst: fa:5f:19:75:3b:53 (fa:5f:19:75:3b:53) Destination: fa:5f:19:75:3b:53 (fa:5f:19:75:3b:53) Address: fa:5f:19:75:3b:53 (fa:5f:19:75:3b:53) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: 6e:51:d3:23:5a:f1 (6e:51:d3:23:5a:f1) Address: 6e:51:d3:23:5a:f1 (6e:51:d3:23:5a:f1) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 192.168.123.2, Dst: 192.168.123.1 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x00 (DSCP: CS0, ECN: Not-ECT) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Total Length: 84 Identification: 0x7754 (30548) Flags: 0x0000 0... .... .... .... = Reserved bit: Not set .0.. .... .... .... = Don't fragment: Not set ..0. .... .... .... = More fragments: Not set ...0 0000 0000 0000 = Fragment offset: 0 Time to live: 64 Protocol: ICMP (1) Header checksum: 0x8c00 [validation disabled] [Header checksum status: Unverified] Source: 192.168.123.2 Destination: 192.168.123.1 Internet Control Message Protocol Type: 0 (Echo (ping) reply) Code: 0 Checksum: 0xabc1 [correct] [Checksum Status: Good] Identifier (BE): 2135 (0x0857) Identifier (LE): 22280 (0x5708) Sequence number (BE): 1 (0x0001) Sequence number (LE): 256 (0x0100) [Request frame: 102] [Response time: 0.388 ms] Timestamp from icmp data: Mar 6, 2019 13:15:26.000000000 EST [Timestamp from icmp data (relative): 2.046802000 seconds] Data (48 bytes) 0000 c1 a8 0d 00 00 00 00 00 10 11 12 13 14 15 16 17 ................ 0010 18 19 1a 1b 1c 1d 1e 1f 20 21 22 23 24 25 26 27 ........ !\"#$%&' 0020 28 29 2a 2b 2c 2d 2e 2f 30 31 32 33 34 35 36 37 ()*+,-./01234567 Data: c1a80d0000000000101112131415161718191a1b1c1d1e1f... [Length: 48]","title":"Geneve"},{"location":"networking/juniper_x520/","text":"Juniper commands How to label interfaces on a Juniper switch with X520 cards Enable console logging on the Juniper switch: root@-sw02> monitor start messages | match ifOperStatus Disable console logging on the Juniper switch: root@-sw02> monitor stop messages Flap server interface: [root@02 ~]# ethtool -r p1p3 This will log the following in the switch: root@-sw02> Aug 9 05:40:40 -sw02 mib2d[2112]: SNMP_TRAP_LINK_DOWN: ifIndex 521, ifAdminStatus up(1), ifOperStatus down(2), ifName xe-0/0/2 Aug 9 05:40:40 -sw02 mib2d[2112]: SNMP_TRAP_LINK_UP: ifIndex 521, ifAdminStatus up(1), ifOperStatus up(1), ifName xe-0/0/2 Now, one can set an interface description: root@-sw02> edit Entering configuration mode {master:0}[edit] root@-sw02# set interfaces xe-0/0/2 description \"02 p1p3\" root@-sw02# commit configuration check succeeds commit complete {master:0}[edit] root@-sw02# exit Exiting configuration mode {master:0} root@-sw02> show interfaces descriptions Interface Admin Link Description xe-0/0/0 up up 01 p1p3 xe-0/0/1 up up 01 p1p4 xe-0/0/2 up up 02 p1p3 How to configure interface VLANs Access VLANs {master:0}[edit] root@-sw02# set interfaces xe-0/0/0 unit 0 family ethernet-switching vlan members vlan259 {master:0}[edit] root@-sw02# commit {master:0}[edit] root@-sw02# exit Verify: {master:0} root@-sw02> show vlans brief Routing instance VLAN name Tag Interfaces default-switch default 1 default-switch native_vlan515 515 default-switch vlan250 250 default-switch vlan251 251 default-switch vlan252 252 default-switch vlan253 253 default-switch vlan254 254 default-switch vlan255 255 default-switch vlan256 256 default-switch vlan257 257 default-switch vlan258 258 default-switch vlan259 259 xe-0/0/0.0* {master:0} Trunks root@-sw02> edit Entering configuration mode The configuration has been changed but not committed {master:0}[edit] root@-sw02# set interfaces xe-0/0/2 unit 0 family ethernet-switching vlan members [ 251 257-259 ] {master:0}[edit] root@-sw02# set interfaces xe-0/0/2 unit 0 family ethernet-switching interface-mode trunk {master:0}[edit] root@-sw02# set interfaces xe-0/0/2 native-vlan-id 259 {master:0}[edit] root@-sw02# commit [edit interfaces xe-0/0/2 unit 0 family] 'ethernet-switching' Family ethernet-switching and rest of the families are mutually exclusive error: commit failed: (statements constraint check failed) {master:0}[edit] root@-sw02# show interfaces xe-0/0/2 description \"02 p1p3\"; native-vlan-id 259; unit 0 { family inet { dhcp { vendor-id Juniper-qfx5100-48s-6q; } } ## ## Warning: Family ethernet-switching and rest of the families are mutually exclusive ## family ethernet-switching { interface-mode trunk; vlan { members [ 251 257-259 ]; } } } {master:0}[edit] root@-sw02# delete interfaces xe-0/0/2 unit 0 family inet {master:0}[edit] root@-sw02# commit configuration check succeeds commit complete {master:0}[edit] root@-sw02# exit Exiting configuration mode {master:0} Verification: root@-sw02> show vlans brief Routing instance VLAN name Tag Interfaces default-switch default 1 default-switch native_vlan515 515 default-switch vlan250 250 default-switch vlan251 251 xe-0/0/2.0 default-switch vlan252 252 default-switch vlan253 253 default-switch vlan254 254 default-switch vlan255 255 default-switch vlan256 256 default-switch vlan257 257 xe-0/0/2.0 default-switch vlan258 258 xe-0/0/2.0 default-switch vlan259 259 xe-0/0/0.0* xe-0/0/2.0 {master:0} root@-sw02>","title":"Configure Juniper switch"},{"location":"networking/juniper_x520/#juniper-commands","text":"","title":"Juniper commands"},{"location":"networking/juniper_x520/#how-to-label-interfaces-on-a-juniper-switch-with-x520-cards","text":"Enable console logging on the Juniper switch: root@-sw02> monitor start messages | match ifOperStatus Disable console logging on the Juniper switch: root@-sw02> monitor stop messages Flap server interface: [root@02 ~]# ethtool -r p1p3 This will log the following in the switch: root@-sw02> Aug 9 05:40:40 -sw02 mib2d[2112]: SNMP_TRAP_LINK_DOWN: ifIndex 521, ifAdminStatus up(1), ifOperStatus down(2), ifName xe-0/0/2 Aug 9 05:40:40 -sw02 mib2d[2112]: SNMP_TRAP_LINK_UP: ifIndex 521, ifAdminStatus up(1), ifOperStatus up(1), ifName xe-0/0/2 Now, one can set an interface description: root@-sw02> edit Entering configuration mode {master:0}[edit] root@-sw02# set interfaces xe-0/0/2 description \"02 p1p3\" root@-sw02# commit configuration check succeeds commit complete {master:0}[edit] root@-sw02# exit Exiting configuration mode {master:0} root@-sw02> show interfaces descriptions Interface Admin Link Description xe-0/0/0 up up 01 p1p3 xe-0/0/1 up up 01 p1p4 xe-0/0/2 up up 02 p1p3","title":"How to label interfaces on a Juniper switch with X520 cards"},{"location":"networking/juniper_x520/#how-to-configure-interface-vlans","text":"","title":"How to configure interface VLANs"},{"location":"networking/juniper_x520/#access-vlans","text":"{master:0}[edit] root@-sw02# set interfaces xe-0/0/0 unit 0 family ethernet-switching vlan members vlan259 {master:0}[edit] root@-sw02# commit {master:0}[edit] root@-sw02# exit Verify: {master:0} root@-sw02> show vlans brief Routing instance VLAN name Tag Interfaces default-switch default 1 default-switch native_vlan515 515 default-switch vlan250 250 default-switch vlan251 251 default-switch vlan252 252 default-switch vlan253 253 default-switch vlan254 254 default-switch vlan255 255 default-switch vlan256 256 default-switch vlan257 257 default-switch vlan258 258 default-switch vlan259 259 xe-0/0/0.0* {master:0}","title":"Access VLANs"},{"location":"networking/juniper_x520/#trunks","text":"root@-sw02> edit Entering configuration mode The configuration has been changed but not committed {master:0}[edit] root@-sw02# set interfaces xe-0/0/2 unit 0 family ethernet-switching vlan members [ 251 257-259 ] {master:0}[edit] root@-sw02# set interfaces xe-0/0/2 unit 0 family ethernet-switching interface-mode trunk {master:0}[edit] root@-sw02# set interfaces xe-0/0/2 native-vlan-id 259 {master:0}[edit] root@-sw02# commit [edit interfaces xe-0/0/2 unit 0 family] 'ethernet-switching' Family ethernet-switching and rest of the families are mutually exclusive error: commit failed: (statements constraint check failed) {master:0}[edit] root@-sw02# show interfaces xe-0/0/2 description \"02 p1p3\"; native-vlan-id 259; unit 0 { family inet { dhcp { vendor-id Juniper-qfx5100-48s-6q; } } ## ## Warning: Family ethernet-switching and rest of the families are mutually exclusive ## family ethernet-switching { interface-mode trunk; vlan { members [ 251 257-259 ]; } } } {master:0}[edit] root@-sw02# delete interfaces xe-0/0/2 unit 0 family inet {master:0}[edit] root@-sw02# commit configuration check succeeds commit complete {master:0}[edit] root@-sw02# exit Exiting configuration mode {master:0} Verification: root@-sw02> show vlans brief Routing instance VLAN name Tag Interfaces default-switch default 1 default-switch native_vlan515 515 default-switch vlan250 250 default-switch vlan251 251 xe-0/0/2.0 default-switch vlan252 252 default-switch vlan253 253 default-switch vlan254 254 default-switch vlan255 255 default-switch vlan256 256 default-switch vlan257 257 xe-0/0/2.0 default-switch vlan258 258 xe-0/0/2.0 default-switch vlan259 259 xe-0/0/0.0* xe-0/0/2.0 {master:0} root@-sw02>","title":"Trunks"},{"location":"networking/netlink/","text":"Quick guide for Netlink What is Netlink? A means to exchange networking information between kernel and userspace: The Netlink socket family is a Linux kernel interface used for inter-process communication (IPC) between both the kernel and userspace processes, and between different userspace processes, in a way similar to the Unix domain sockets. Similarly to the Unix domain sockets, and unlike INET sockets, Netlink communication cannot traverse host boundaries. However, while the Unix domain sockets use the file system namespace, Netlink processes are usually addressed by process identifiers (PIDs). [3] Netlink is designed and used for transferring miscellaneous networking information between the kernel space and userspace processes. Networking utilities, such as the iproute2 family and the utilities used for configuring mac80211-based wireless drivers, use Netlink to communicate with the Linux kernel from userspace. Netlink provides a standard socket-based interface for userspace processes, and a kernel-side API for internal use by kernel modules. Originally, Netlink used the AF_NETLINK socket family. https://en.wikipedia.org/wiki/Netlink Netlink is defined and thoroughly described in RFC3549 The idea is to keep code that's critical for performance in the kernel and to move anything related to control and management in user-space ( https://www.linuxjournal.com/article/7356 ). Netlink socket is a special IPC used for transferring information between kernel and user-space processes. It provides a full-duplex communication link between the two by way of standard socket APIs for user-space processes and a special kernel API for kernel modules. Netlink socket uses the address family AF_NETLINK, as compared to AF_INET used by TCP/IP socket. Each netlink socket feature defines its own protocol type in the kernel header file include/linux/netlink.h. https://www.linuxjournal.com/article/7356 Who uses Netlink? Examples Just a few examples: * iproute2 [root@dell-r430-30 ~]# strace -e trace=network /usr/sbin/ip route socket(AF_NETLINK, SOCK_RAW|SOCK_CLOEXEC, NETLINK_ROUTE) = 3 setsockopt(3, SOL_SOCKET, SO_SNDBUF, [32768], 4) = 0 setsockopt(3, SOL_SOCKET, SO_RCVBUF, [1048576], 4) = 0 bind(3, {sa_family=AF_NETLINK, pid=0, groups=00000000}, 12) = 0 getsockname(3, {sa_family=AF_NETLINK, pid=161345, groups=00000000}, [12]) = 0 sendto(3, \"(\\0\\0\\0\\32\\0\\1\\3\\330A\\367[\\0\\0\\0\\0\\2\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\"..., 40, 0, NULL, 0) = 40 recvmsg(3, {msg_name(12)={sa_family=AF_NETLINK, pid=0, groups=00000000}, msg_iov(1)=[{\"4\\0\\0\\0\\30\\0\\2\\0\\330A\\367[Av\\2\\0\\2\\0\\0\\0\\376\\3\\0\\1\\0\\0\\0\\0\\10\\0\\17\\0\"..., 32768}], msg_controllen=0, msg_flags=0}, 0) = 1312 socket(AF_LOCAL, SOCK_DGRAM|SOCK_CLOEXEC, 0) = 4 default via 10.10.99.254 dev br-redhat socket(AF_LOCAL, SOCK_DGRAM|SOCK_CLOEXEC, 0) = 4 10.10.96.0/22 dev br-redhat proto kernel scope link src 10.10.96.194 socket(AF_LOCAL, SOCK_DGRAM|SOCK_CLOEXEC, 0) = 4 169.254.0.0/16 dev br-redhat scope link metric 1008 socket(AF_LOCAL, SOCK_DGRAM|SOCK_CLOEXEC, 0) = 4 169.254.0.0/16 dev br-provis scope link metric 1009 socket(AF_LOCAL, SOCK_DGRAM|SOCK_CLOEXEC, 0) = 4 169.254.0.0/16 dev br-trunk1 scope link metric 1010 socket(AF_LOCAL, SOCK_DGRAM|SOCK_CLOEXEC, 0) = 4 169.254.0.0/16 dev br-stonith scope link metric 1011 socket(AF_LOCAL, SOCK_DGRAM|SOCK_CLOEXEC, 0) = 4 169.254.0.0/16 dev br-trunk2 scope link metric 1012 socket(AF_LOCAL, SOCK_DGRAM|SOCK_CLOEXEC, 0) = 4 192.168.111.0/24 dev br-stonith proto kernel scope link src 192.168.111.1 socket(AF_LOCAL, SOCK_DGRAM|SOCK_CLOEXEC, 0) = 4 192.168.122.0/24 dev virbr0 proto kernel scope link src 192.168.122.1 recvmsg(3, {msg_name(12)={sa_family=AF_NETLINK, pid=0, groups=00000000}, msg_iov(1)=[{\"\\24\\0\\0\\0\\3\\0\\2\\0\\330A\\367[Av\\2\\0\\0\\0\\0\\0\", 32768}], msg_controllen=0, msg_flags=0}, 0) = 20 +++ exited with 0 +++ OVS Socket programming Let's do a little detour towards socket programming, as this will teach the basics for Netlink programming. C Resources The Linux Programming Reference is an excellent book for C programming with Linux. Program Let's start with a generic example for IPC via sockets in C. The following contains an example for a UDP socket: https://github.com/andreaskaris/blog/blob/master/messenger.c Compile: [akaris@wks-akaris c]$ gcc messenger.c -o messenger [akaris@wks-akaris c]$ Then run, in one CLI the receiver: [akaris@wks-akaris c]$ ./messenger receiver This node is a receiver Starting receiver Received datagram from 127.0.0.1 Datagram content: MSG Received datagram from 127.0.0.1 Datagram content: MSG In the other CLI the sender: [akaris@wks-akaris c]$ ./messenger sender This node is a sender Starting sender [akaris@wks-akaris c]$ ./messenger sender This node is a sender Starting sender Python Resources https://wiki.python.org/moin/UdpCommunication Program Let's start with a generic example for IPC via sockets in Python. The following contains an example for a UDP socket: https://github.com/andreaskaris/blog/blob/master/messenger.py Run the server: [akaris@wks-akaris python]$ ./messenger.py rec This node is a receiver Starting receiver Received datagram from ('127.0.0.1', 58172) Datagram content: b'MSG' Received datagram from ('127.0.0.1', 39735) Datagram content: b'MSG' Run the client in another CLI: [akaris@wks-akaris python]$ ./messenger.py sender This node is a sender Starting sender [akaris@wks-akaris python]$ ./messenger.py sender This node is a sender Starting sender [akaris@wks-akaris python]$ How to use Netlink - programming example https://www.linuxjournal.com/article/7356 has a walkthrough for using Netlink in C. https://www.linuxjournal.com/article/8498 as well. C The following C application monitors any route changes that happen in the kernel: https://github.com/andreaskaris/blog/blob/master/netlink.c Compile the code: gcc netlink.c -o netlink Start the application and while the application is running, modify the routing table: [akaris@wks-akaris python]$ sudo ip r a 4.3.2.0/30 via 127.0.0.1 [akaris@wks-akaris python]$ sudo ip r d 4.3.2.0/30 via 127.0.0.1 The application will yield the following. We can see live when the route is added and when it is deleted: [akaris@wks-akaris c]$ ./netlink Starting netlink receiverd nlmsg_len: 60, nlmsg_type 24, nlmsg_flags: 1536, nlmsg_seq: 1543015716, nlmsg_pid: 15333 Netmask: 30 nlmsg_len: 60, nlmsg_type 25, nlmsg_flags: 0, nlmsg_seq: 1543015717, nlmsg_pid: 15343 Netmask: 30 Note that a more complete example can be found here: https://gist.github.com/cl4u2/5204374 Python The following python application monitors any route changes that happen in the kernel: https://github.com/andreaskaris/blog/blob/master/netlink.py Start the application and while the application is running, modify the routing table: [akaris@wks-akaris python]$ sudo ip r a 4.3.2.0/30 via 127.0.0.1 [akaris@wks-akaris python]$ sudo ip r d 4.3.2.0/30 via 127.0.0.1 The application will yield the following. We can see live when the route is added and when it is deleted: [akaris@wks-akaris python]$ ./netlink.py Netlink header: 60 RTM_NEWROUTE 1536 1543012056 13037 Netlink payload: 2 30 0 0 254 3 0 1 0 Netmask: 30 IP: b'\\x04\\x03\\x02\\x00' Netlink header: 60 RTM_DELROUTE 0 1543012061 13047 Netlink payload: 2 30 0 0 254 3 0 1 0 Netmask: 30 IP: b'\\x04\\x03\\x02\\x00' strace'ing the application reveals even more about the message structure: [akaris@wks-akaris python]$ strace -e trace=network ./netlink.py socket(AF_NETLINK, SOCK_RAW|SOCK_CLOEXEC, NETLINK_ROUTE) = 3 bind(3, {sa_family=AF_NETLINK, nl_pid=13307, nl_groups=0x000040}, 12) = 0 recvfrom(3, {{len=60, type=RTM_NEWROUTE, flags=NLM_F_EXCL|NLM_F_CREATE, seq=1543012305, pid=13310}, {rtm_family=AF_INET, rtm_dst_len=30, rtm_src_len=0, rtm_tos=0, rtm_table=RT_TABLE_MAIN, rtm_protocol=RTPROT_BOOT, rtm_scope=RT_SCOPE_UNIVERSE, rtm_type=RTN_UNICAST, rtm_flags=0}, [{{nla_len=8, nla_type=RTA_TABLE}, RT_TABLE_MAIN}, {{nla_len=8, nla_type=RTA_DST}, 4.3.2.0}, {{nla_len=8, nla_type=RTA_GATEWAY}, 127.0.0.1}, {{nla_len=8, nla_type=RTA_OIF}, if_nametoindex(\"lo\")}]}, 65535, 0, NULL, NULL) = 60 Netlink header: 60 RTM_NEWROUTE 1536 1543012305 13310 Netlink payload: 2 30 0 0 254 3 0 1 0 Netmask: 30 IP: b'\\x04\\x03\\x02\\x00' recvfrom(3, {{len=60, type=RTM_DELROUTE, flags=0, seq=1543012307, pid=13320}, {rtm_family=AF_INET, rtm_dst_len=30, rtm_src_len=0, rtm_tos=0, rtm_table=RT_TABLE_MAIN, rtm_protocol=RTPROT_BOOT, rtm_scope=RT_SCOPE_UNIVERSE, rtm_type=RTN_UNICAST, rtm_flags=0}, [{{nla_len=8, nla_type=RTA_TABLE}, RT_TABLE_MAIN}, {{nla_len=8, nla_type=RTA_DST}, 4.3.2.0}, {{nla_len=8, nla_type=RTA_GATEWAY}, 127.0.0.1}, {{nla_len=8, nla_type=RTA_OIF}, if_nametoindex(\"lo\")}]}, 65535, 0, NULL, NULL) = 60 Netlink header: 60 RTM_DELROUTE 0 1543012307 13320 Netlink payload: 2 30 0 0 254 3 0 1 0 Netmask: 30 IP: b'\\x04\\x03\\x02\\x00' recvfrom(3, Resources https://en.wikipedia.org/wiki/Netlink https://tools.ietf.org/html/rfc3549 https://www.linuxjournal.com/article/7356 http://inai.de/documents/Netlink_Protocol.pdf","title":"Netlink"},{"location":"networking/netlink/#quick-guide-for-netlink","text":"","title":"Quick guide for Netlink"},{"location":"networking/netlink/#what-is-netlink","text":"A means to exchange networking information between kernel and userspace: The Netlink socket family is a Linux kernel interface used for inter-process communication (IPC) between both the kernel and userspace processes, and between different userspace processes, in a way similar to the Unix domain sockets. Similarly to the Unix domain sockets, and unlike INET sockets, Netlink communication cannot traverse host boundaries. However, while the Unix domain sockets use the file system namespace, Netlink processes are usually addressed by process identifiers (PIDs). [3] Netlink is designed and used for transferring miscellaneous networking information between the kernel space and userspace processes. Networking utilities, such as the iproute2 family and the utilities used for configuring mac80211-based wireless drivers, use Netlink to communicate with the Linux kernel from userspace. Netlink provides a standard socket-based interface for userspace processes, and a kernel-side API for internal use by kernel modules. Originally, Netlink used the AF_NETLINK socket family. https://en.wikipedia.org/wiki/Netlink Netlink is defined and thoroughly described in RFC3549 The idea is to keep code that's critical for performance in the kernel and to move anything related to control and management in user-space ( https://www.linuxjournal.com/article/7356 ). Netlink socket is a special IPC used for transferring information between kernel and user-space processes. It provides a full-duplex communication link between the two by way of standard socket APIs for user-space processes and a special kernel API for kernel modules. Netlink socket uses the address family AF_NETLINK, as compared to AF_INET used by TCP/IP socket. Each netlink socket feature defines its own protocol type in the kernel header file include/linux/netlink.h. https://www.linuxjournal.com/article/7356","title":"What is Netlink?"},{"location":"networking/netlink/#who-uses-netlink-examples","text":"Just a few examples: * iproute2 [root@dell-r430-30 ~]# strace -e trace=network /usr/sbin/ip route socket(AF_NETLINK, SOCK_RAW|SOCK_CLOEXEC, NETLINK_ROUTE) = 3 setsockopt(3, SOL_SOCKET, SO_SNDBUF, [32768], 4) = 0 setsockopt(3, SOL_SOCKET, SO_RCVBUF, [1048576], 4) = 0 bind(3, {sa_family=AF_NETLINK, pid=0, groups=00000000}, 12) = 0 getsockname(3, {sa_family=AF_NETLINK, pid=161345, groups=00000000}, [12]) = 0 sendto(3, \"(\\0\\0\\0\\32\\0\\1\\3\\330A\\367[\\0\\0\\0\\0\\2\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\"..., 40, 0, NULL, 0) = 40 recvmsg(3, {msg_name(12)={sa_family=AF_NETLINK, pid=0, groups=00000000}, msg_iov(1)=[{\"4\\0\\0\\0\\30\\0\\2\\0\\330A\\367[Av\\2\\0\\2\\0\\0\\0\\376\\3\\0\\1\\0\\0\\0\\0\\10\\0\\17\\0\"..., 32768}], msg_controllen=0, msg_flags=0}, 0) = 1312 socket(AF_LOCAL, SOCK_DGRAM|SOCK_CLOEXEC, 0) = 4 default via 10.10.99.254 dev br-redhat socket(AF_LOCAL, SOCK_DGRAM|SOCK_CLOEXEC, 0) = 4 10.10.96.0/22 dev br-redhat proto kernel scope link src 10.10.96.194 socket(AF_LOCAL, SOCK_DGRAM|SOCK_CLOEXEC, 0) = 4 169.254.0.0/16 dev br-redhat scope link metric 1008 socket(AF_LOCAL, SOCK_DGRAM|SOCK_CLOEXEC, 0) = 4 169.254.0.0/16 dev br-provis scope link metric 1009 socket(AF_LOCAL, SOCK_DGRAM|SOCK_CLOEXEC, 0) = 4 169.254.0.0/16 dev br-trunk1 scope link metric 1010 socket(AF_LOCAL, SOCK_DGRAM|SOCK_CLOEXEC, 0) = 4 169.254.0.0/16 dev br-stonith scope link metric 1011 socket(AF_LOCAL, SOCK_DGRAM|SOCK_CLOEXEC, 0) = 4 169.254.0.0/16 dev br-trunk2 scope link metric 1012 socket(AF_LOCAL, SOCK_DGRAM|SOCK_CLOEXEC, 0) = 4 192.168.111.0/24 dev br-stonith proto kernel scope link src 192.168.111.1 socket(AF_LOCAL, SOCK_DGRAM|SOCK_CLOEXEC, 0) = 4 192.168.122.0/24 dev virbr0 proto kernel scope link src 192.168.122.1 recvmsg(3, {msg_name(12)={sa_family=AF_NETLINK, pid=0, groups=00000000}, msg_iov(1)=[{\"\\24\\0\\0\\0\\3\\0\\2\\0\\330A\\367[Av\\2\\0\\0\\0\\0\\0\", 32768}], msg_controllen=0, msg_flags=0}, 0) = 20 +++ exited with 0 +++ OVS","title":"Who uses Netlink? Examples"},{"location":"networking/netlink/#socket-programming","text":"Let's do a little detour towards socket programming, as this will teach the basics for Netlink programming.","title":"Socket programming"},{"location":"networking/netlink/#c","text":"","title":"C"},{"location":"networking/netlink/#resources","text":"The Linux Programming Reference is an excellent book for C programming with Linux.","title":"Resources"},{"location":"networking/netlink/#program","text":"Let's start with a generic example for IPC via sockets in C. The following contains an example for a UDP socket: https://github.com/andreaskaris/blog/blob/master/messenger.c Compile: [akaris@wks-akaris c]$ gcc messenger.c -o messenger [akaris@wks-akaris c]$ Then run, in one CLI the receiver: [akaris@wks-akaris c]$ ./messenger receiver This node is a receiver Starting receiver Received datagram from 127.0.0.1 Datagram content: MSG Received datagram from 127.0.0.1 Datagram content: MSG In the other CLI the sender: [akaris@wks-akaris c]$ ./messenger sender This node is a sender Starting sender [akaris@wks-akaris c]$ ./messenger sender This node is a sender Starting sender","title":"Program"},{"location":"networking/netlink/#python","text":"","title":"Python"},{"location":"networking/netlink/#resources_1","text":"https://wiki.python.org/moin/UdpCommunication","title":"Resources"},{"location":"networking/netlink/#program_1","text":"Let's start with a generic example for IPC via sockets in Python. The following contains an example for a UDP socket: https://github.com/andreaskaris/blog/blob/master/messenger.py Run the server: [akaris@wks-akaris python]$ ./messenger.py rec This node is a receiver Starting receiver Received datagram from ('127.0.0.1', 58172) Datagram content: b'MSG' Received datagram from ('127.0.0.1', 39735) Datagram content: b'MSG' Run the client in another CLI: [akaris@wks-akaris python]$ ./messenger.py sender This node is a sender Starting sender [akaris@wks-akaris python]$ ./messenger.py sender This node is a sender Starting sender [akaris@wks-akaris python]$","title":"Program"},{"location":"networking/netlink/#how-to-use-netlink-programming-example","text":"https://www.linuxjournal.com/article/7356 has a walkthrough for using Netlink in C. https://www.linuxjournal.com/article/8498 as well.","title":"How to use Netlink - programming example"},{"location":"networking/netlink/#c_1","text":"The following C application monitors any route changes that happen in the kernel: https://github.com/andreaskaris/blog/blob/master/netlink.c Compile the code: gcc netlink.c -o netlink Start the application and while the application is running, modify the routing table: [akaris@wks-akaris python]$ sudo ip r a 4.3.2.0/30 via 127.0.0.1 [akaris@wks-akaris python]$ sudo ip r d 4.3.2.0/30 via 127.0.0.1 The application will yield the following. We can see live when the route is added and when it is deleted: [akaris@wks-akaris c]$ ./netlink Starting netlink receiverd nlmsg_len: 60, nlmsg_type 24, nlmsg_flags: 1536, nlmsg_seq: 1543015716, nlmsg_pid: 15333 Netmask: 30 nlmsg_len: 60, nlmsg_type 25, nlmsg_flags: 0, nlmsg_seq: 1543015717, nlmsg_pid: 15343 Netmask: 30 Note that a more complete example can be found here: https://gist.github.com/cl4u2/5204374","title":"C"},{"location":"networking/netlink/#python_1","text":"The following python application monitors any route changes that happen in the kernel: https://github.com/andreaskaris/blog/blob/master/netlink.py Start the application and while the application is running, modify the routing table: [akaris@wks-akaris python]$ sudo ip r a 4.3.2.0/30 via 127.0.0.1 [akaris@wks-akaris python]$ sudo ip r d 4.3.2.0/30 via 127.0.0.1 The application will yield the following. We can see live when the route is added and when it is deleted: [akaris@wks-akaris python]$ ./netlink.py Netlink header: 60 RTM_NEWROUTE 1536 1543012056 13037 Netlink payload: 2 30 0 0 254 3 0 1 0 Netmask: 30 IP: b'\\x04\\x03\\x02\\x00' Netlink header: 60 RTM_DELROUTE 0 1543012061 13047 Netlink payload: 2 30 0 0 254 3 0 1 0 Netmask: 30 IP: b'\\x04\\x03\\x02\\x00' strace'ing the application reveals even more about the message structure: [akaris@wks-akaris python]$ strace -e trace=network ./netlink.py socket(AF_NETLINK, SOCK_RAW|SOCK_CLOEXEC, NETLINK_ROUTE) = 3 bind(3, {sa_family=AF_NETLINK, nl_pid=13307, nl_groups=0x000040}, 12) = 0 recvfrom(3, {{len=60, type=RTM_NEWROUTE, flags=NLM_F_EXCL|NLM_F_CREATE, seq=1543012305, pid=13310}, {rtm_family=AF_INET, rtm_dst_len=30, rtm_src_len=0, rtm_tos=0, rtm_table=RT_TABLE_MAIN, rtm_protocol=RTPROT_BOOT, rtm_scope=RT_SCOPE_UNIVERSE, rtm_type=RTN_UNICAST, rtm_flags=0}, [{{nla_len=8, nla_type=RTA_TABLE}, RT_TABLE_MAIN}, {{nla_len=8, nla_type=RTA_DST}, 4.3.2.0}, {{nla_len=8, nla_type=RTA_GATEWAY}, 127.0.0.1}, {{nla_len=8, nla_type=RTA_OIF}, if_nametoindex(\"lo\")}]}, 65535, 0, NULL, NULL) = 60 Netlink header: 60 RTM_NEWROUTE 1536 1543012305 13310 Netlink payload: 2 30 0 0 254 3 0 1 0 Netmask: 30 IP: b'\\x04\\x03\\x02\\x00' recvfrom(3, {{len=60, type=RTM_DELROUTE, flags=0, seq=1543012307, pid=13320}, {rtm_family=AF_INET, rtm_dst_len=30, rtm_src_len=0, rtm_tos=0, rtm_table=RT_TABLE_MAIN, rtm_protocol=RTPROT_BOOT, rtm_scope=RT_SCOPE_UNIVERSE, rtm_type=RTN_UNICAST, rtm_flags=0}, [{{nla_len=8, nla_type=RTA_TABLE}, RT_TABLE_MAIN}, {{nla_len=8, nla_type=RTA_DST}, 4.3.2.0}, {{nla_len=8, nla_type=RTA_GATEWAY}, 127.0.0.1}, {{nla_len=8, nla_type=RTA_OIF}, if_nametoindex(\"lo\")}]}, 65535, 0, NULL, NULL) = 60 Netlink header: 60 RTM_DELROUTE 0 1543012307 13320 Netlink payload: 2 30 0 0 254 3 0 1 0 Netmask: 30 IP: b'\\x04\\x03\\x02\\x00' recvfrom(3,","title":"Python"},{"location":"networking/netlink/#resources_2","text":"https://en.wikipedia.org/wiki/Netlink https://tools.ietf.org/html/rfc3549 https://www.linuxjournal.com/article/7356 http://inai.de/documents/Netlink_Protocol.pdf","title":"Resources"},{"location":"networking/ovn-kind/","text":"OVN kind and the Sources Based on: https://github.com/ovn-org/ovn-kubernetes/blob/master/docs/kind.md https://github.com/ovn-org/ovn-kubernetes And most instructions are from my colleagues Tim Rozet and Robin Cernin. Prerequisites Spawn a RHEL 7.8 or above virtual machine and install: yum install docker -y yum install python3 -y yum install git -y yum install python3-pip -y python3 -m pip install j2cli sudo update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 1 curl -O https://golang.org/dl/go1.14.6.linux-amd64.tar.gz -L tar -C /usr/local -xzf go1.14.6.linux-amd64.tar.gz cat <<'EOF' | tee /etc/profile.d/go.sh #!/bin/bash export PATH=$PATH:/usr/local/go/bin EOF chmod +x /etc/profile.d/go.sh source /etc/profile.d/go.sh Download kind: curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.8.1/kind-linux-amd64 chmod +x ./kind For further details, see https://kind.sigs.k8s.io/docs/user/quick-start/ Download ovn-kubernetes: git clone https://github.com/ovn-org/ovn-kubernetes cd ovn-kubernetes/contrib/ Now, run: ./kind.sh -ho Wait until it finishes. Now, go to the home folder and generate kubeconfig: cd ~ kind get kubeconfig --name ovn > .kube/config Verify: kubectl get nodes NAME STATUS ROLES AGE VERSION ovn-control-plane Ready master 15m v1.18.2 ovn-worker Ready <none> 14m v1.18.2 ovn-worker2 Ready <none> 14m v1.18.2 Get your Worker node IPs: # kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME ovn-control-plane Ready master 15m v1.18.2 172.18.0.4 <none> Ubuntu 20.04 LTS 4.18.0-193.13.2.el8_2.x86_64 containerd://1.3.3-14-g449e9269 ovn-worker Ready <none> 15m v1.18.2 172.18.0.3 <none> Ubuntu 20.04 LTS 4.18.0-193.13.2.el8_2.x86_64 containerd://1.3.3-14-g449e9269 ovn-worker2 Ready <none> 15m v1.18.2 172.18.0.2 <none> Ubuntu 20.04 LTS 4.18.0-193.13.2.el8_2.x86_64 containerd://1.3.3-14-g449e9269 Get the worker pod subnets: [root@kind ~]# kubectl get nodes | tail -n +2 | awk '{print $1}' | while read node; do echo === $node === ; kubectl describe node $node | grep PodCIDR ; done === ovn-control-plane === PodCIDR: 10.244.0.0/24 PodCIDRs: 10.244.0.0/24 === ovn-worker === PodCIDR: 10.244.1.0/24 PodCIDRs: 10.244.1.0/24 === ovn-worker2 === PodCIDR: 10.244.2.0/24 PodCIDRs: 10.244.2.0/24 Create CentOS to act as GW: docker run --name gw -d --privileged --network kind -it centos /bin/bash Now connecto to the GW node: docker exec -it gw /bin/bash If your worker node IPs and pod subnets are different from what was listed above, make sure to adjust the following. It's important to route traffic for the node's pod subnet via the node's internal IP. cat <<'EOF' | tee vxlan.sh #!/bin/bash echo \"VIP\" ip add a 9.0.0.1 dev lo echo \"ovn-workers\" # https://joejulian.name/post/how-to-configure-linux-vxlans-with-multiple-unicast-endpoints/ ip link add vxlan0 type vxlan dev eth0 id 4097 dstport 4789 bridge fdb append to 00:00:00:00:00:00 dst 172.18.0.3 dev vxlan0 bridge fdb append to 00:00:00:00:00:00 dst 172.18.0.2 dev vxlan0 ip route add 10.244.0.0/16 dev vxlan0 EOF chmod +x vxlan.sh ./vxlan.sh Now create a namespace with annotations like: apiVersion: v1 kind: Namespace metadata: name: exgw2 annotations: k8s.ovn.org/hybrid-overlay-external-gw: 9.0.0.1 k8s.ovn.org/hybrid-overlay-vtep: 172.18.0.5 #replace this with your simulated ip Execute: kubectl apply -f file.yaml Now create pods on each worker node in the namespace: cat <<'EOF' | oc apply -f - apiVersion: v1 kind: Pod metadata: name: dnsutils namespace: exgw2 spec: containers: - name: dnsutils01 image: gcr.io/kubernetes-e2e-test-images/dnsutils:1.3 command: - sleep - \"3600\" imagePullPolicy: IfNotPresent restartPolicy: Always nodeName: ovn-worker --- apiVersion: v1 kind: Pod metadata: name: dnsutils02 namespace: exgw2 spec: containers: - name: dnsutils image: gcr.io/kubernetes-e2e-test-images/dnsutils:1.3 command: - sleep - \"3600\" imagePullPolicy: IfNotPresent restartPolicy: Always nodeName: ovn-worker2 EOF Execute: [root@kind ~]# kubectl get pods -n exgw2 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES dnsutils 1/1 Running 0 20m 10.244.0.7 ovn-worker <none> <none> dnsutils02 1/1 Running 0 20m 10.244.2.6 ovn-worker2 <none> <none> Verify from the pods: [root@kind ~]# kubectl exec -it dnsutils02 /bin/sh kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead. / # ping 9.0.0.1 PING 9.0.0.1 (9.0.0.1): 56 data bytes 64 bytes from 9.0.0.1: seq=0 ttl=64 time=0.998 ms ^C --- 9.0.0.1 ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 0.998/0.998/0.998 ms / # [root@kind ~]# kubectl exec -it dnsutils /bin/sh kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead. / # ping 9.0.0.1 PING 9.0.0.1 (9.0.0.1): 56 data bytes 64 bytes from 9.0.0.1: seq=0 ttl=64 time=1.686 ms 64 bytes from 9.0.0.1: seq=1 ttl=64 time=0.667 ms ^C --- 9.0.0.1 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.667/1.176/1.686 ms / # Verify the HybridOverlay is enabled in NS: # kubectl get ns exgw2 -o yaml apiVersion: v1 kind: Namespace metadata: annotations: k8s.ovn.org/hybrid-overlay-external-gw: 9.0.0.1 k8s.ovn.org/hybrid-overlay-vtep: 172.18.0.5","title":"OVN kind"},{"location":"networking/ovn-kind/#ovn-kind-and-the","text":"","title":"OVN kind and the"},{"location":"networking/ovn-kind/#sources","text":"Based on: https://github.com/ovn-org/ovn-kubernetes/blob/master/docs/kind.md https://github.com/ovn-org/ovn-kubernetes And most instructions are from my colleagues Tim Rozet and Robin Cernin.","title":"Sources"},{"location":"networking/ovn-kind/#prerequisites","text":"Spawn a RHEL 7.8 or above virtual machine and install: yum install docker -y yum install python3 -y yum install git -y yum install python3-pip -y python3 -m pip install j2cli sudo update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 1 curl -O https://golang.org/dl/go1.14.6.linux-amd64.tar.gz -L tar -C /usr/local -xzf go1.14.6.linux-amd64.tar.gz cat <<'EOF' | tee /etc/profile.d/go.sh #!/bin/bash export PATH=$PATH:/usr/local/go/bin EOF chmod +x /etc/profile.d/go.sh source /etc/profile.d/go.sh Download kind: curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.8.1/kind-linux-amd64 chmod +x ./kind For further details, see https://kind.sigs.k8s.io/docs/user/quick-start/ Download ovn-kubernetes: git clone https://github.com/ovn-org/ovn-kubernetes cd ovn-kubernetes/contrib/ Now, run: ./kind.sh -ho Wait until it finishes. Now, go to the home folder and generate kubeconfig: cd ~ kind get kubeconfig --name ovn > .kube/config Verify: kubectl get nodes NAME STATUS ROLES AGE VERSION ovn-control-plane Ready master 15m v1.18.2 ovn-worker Ready <none> 14m v1.18.2 ovn-worker2 Ready <none> 14m v1.18.2 Get your Worker node IPs: # kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME ovn-control-plane Ready master 15m v1.18.2 172.18.0.4 <none> Ubuntu 20.04 LTS 4.18.0-193.13.2.el8_2.x86_64 containerd://1.3.3-14-g449e9269 ovn-worker Ready <none> 15m v1.18.2 172.18.0.3 <none> Ubuntu 20.04 LTS 4.18.0-193.13.2.el8_2.x86_64 containerd://1.3.3-14-g449e9269 ovn-worker2 Ready <none> 15m v1.18.2 172.18.0.2 <none> Ubuntu 20.04 LTS 4.18.0-193.13.2.el8_2.x86_64 containerd://1.3.3-14-g449e9269 Get the worker pod subnets: [root@kind ~]# kubectl get nodes | tail -n +2 | awk '{print $1}' | while read node; do echo === $node === ; kubectl describe node $node | grep PodCIDR ; done === ovn-control-plane === PodCIDR: 10.244.0.0/24 PodCIDRs: 10.244.0.0/24 === ovn-worker === PodCIDR: 10.244.1.0/24 PodCIDRs: 10.244.1.0/24 === ovn-worker2 === PodCIDR: 10.244.2.0/24 PodCIDRs: 10.244.2.0/24 Create CentOS to act as GW: docker run --name gw -d --privileged --network kind -it centos /bin/bash Now connecto to the GW node: docker exec -it gw /bin/bash If your worker node IPs and pod subnets are different from what was listed above, make sure to adjust the following. It's important to route traffic for the node's pod subnet via the node's internal IP. cat <<'EOF' | tee vxlan.sh #!/bin/bash echo \"VIP\" ip add a 9.0.0.1 dev lo echo \"ovn-workers\" # https://joejulian.name/post/how-to-configure-linux-vxlans-with-multiple-unicast-endpoints/ ip link add vxlan0 type vxlan dev eth0 id 4097 dstport 4789 bridge fdb append to 00:00:00:00:00:00 dst 172.18.0.3 dev vxlan0 bridge fdb append to 00:00:00:00:00:00 dst 172.18.0.2 dev vxlan0 ip route add 10.244.0.0/16 dev vxlan0 EOF chmod +x vxlan.sh ./vxlan.sh Now create a namespace with annotations like: apiVersion: v1 kind: Namespace metadata: name: exgw2 annotations: k8s.ovn.org/hybrid-overlay-external-gw: 9.0.0.1 k8s.ovn.org/hybrid-overlay-vtep: 172.18.0.5 #replace this with your simulated ip Execute: kubectl apply -f file.yaml Now create pods on each worker node in the namespace: cat <<'EOF' | oc apply -f - apiVersion: v1 kind: Pod metadata: name: dnsutils namespace: exgw2 spec: containers: - name: dnsutils01 image: gcr.io/kubernetes-e2e-test-images/dnsutils:1.3 command: - sleep - \"3600\" imagePullPolicy: IfNotPresent restartPolicy: Always nodeName: ovn-worker --- apiVersion: v1 kind: Pod metadata: name: dnsutils02 namespace: exgw2 spec: containers: - name: dnsutils image: gcr.io/kubernetes-e2e-test-images/dnsutils:1.3 command: - sleep - \"3600\" imagePullPolicy: IfNotPresent restartPolicy: Always nodeName: ovn-worker2 EOF Execute: [root@kind ~]# kubectl get pods -n exgw2 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES dnsutils 1/1 Running 0 20m 10.244.0.7 ovn-worker <none> <none> dnsutils02 1/1 Running 0 20m 10.244.2.6 ovn-worker2 <none> <none> Verify from the pods: [root@kind ~]# kubectl exec -it dnsutils02 /bin/sh kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead. / # ping 9.0.0.1 PING 9.0.0.1 (9.0.0.1): 56 data bytes 64 bytes from 9.0.0.1: seq=0 ttl=64 time=0.998 ms ^C --- 9.0.0.1 ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 0.998/0.998/0.998 ms / # [root@kind ~]# kubectl exec -it dnsutils /bin/sh kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead. / # ping 9.0.0.1 PING 9.0.0.1 (9.0.0.1): 56 data bytes 64 bytes from 9.0.0.1: seq=0 ttl=64 time=1.686 ms 64 bytes from 9.0.0.1: seq=1 ttl=64 time=0.667 ms ^C --- 9.0.0.1 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.667/1.176/1.686 ms / # Verify the HybridOverlay is enabled in NS: # kubectl get ns exgw2 -o yaml apiVersion: v1 kind: Namespace metadata: annotations: k8s.ovn.org/hybrid-overlay-external-gw: 9.0.0.1 k8s.ovn.org/hybrid-overlay-vtep: 172.18.0.5","title":"Prerequisites"},{"location":"networking/ovn_standalone_on_fedora31/","text":"OVN standalone on Fedora 31 Base setup Single node DB on ovn1 Configuration and installation The following starts an OVN cluster on Fedora 31 with 3 nodes. Non-clustered DB with ovn1 as the DB node, and ovn2 and ovn3 join ovn1's DB. All nodes have 2 interfaces. One management interface on eth0. DNS names ovn1, ovn2, ovn3 map to the IP addresses on eth0. Interface eth1 has no IP addresses and will be used for the dataplane (attached to br-provider). IP address configuration: [root@ovn1 ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=\"eth0\" BOOTPROTO=\"static\" ONBOOT=\"yes\" TYPE=\"Ethernet\" PREFIX=24 IPADDR=192.168.122.201 GATEWAY=192.168.122.1 DNS1=192.168.122.1 [root@ovn2 ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=\"eth0\" BOOTPROTO=\"static\" ONBOOT=\"yes\" TYPE=\"Ethernet\" PREFIX=24 IPADDR=192.168.122.202 GATEWAY=192.168.122.1 DNS1=192.168.122.1 [root@ovn3 ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=\"eth0\" BOOTPROTO=\"static\" ONBOOT=\"yes\" TYPE=\"Ethernet\" PREFIX=24 IPADDR=192.168.122.203 GATEWAY=192.168.122.1 DNS1=192.168.122.1 Hostname to IP address mapping: # cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.122.201 ovn1 # eth0 192.168.122.202 ovn2 # eth0 192.168.122.203 ovn3 # eth0 Disable NM control over eth1 on all hosts: echo \"NM_CONTROLLED=no\" > /etc/sysconfig/network-scripts/ifcfg-eth1 Install Open vSwitch and OVN on all nodes: yum install ovn -y yum install ovn-central -y yum install ovn-host -y Enable Open vSwitch and ovn-controller on all hosts and start right away: systemctl enable --now openvswitch systemctl enable --now ovn-controller On the master nodes that are to hold the ovn-northd \"control plane\", execute: echo 'OVN_NORTHD_OPTS=\"--db-nb-addr=ovn1 --db-nb-create-insecure-remote=yes --db-sb-addr=ovn1 --db-sb-create-insecure-remote=yes --db-nb-cluster-local-addr=ovn1 --db-sb-cluster-local-addr=ovn1 --ovn-northd-nb-db=tcp:ovn1:6641 --ovn-northd-sb-db=tcp:ovn1:6642\"' >> /etc/sysconfig/ovn systemctl enable --now ovn-northd Note that the above will start a single node cluster. The OVN man page contains an example for a full 3 node clustered DB: http://www.openvswitch.org/support/dist-docs/ovn-ctl.8.txt Now, configure OVS to connect to the OVN DBs. Note that geneve / vxlan tunnel needs IP addresses, DNS entries do not work: On master node ovn1: # [root@ovn1 ~]# ovs-vsctl set open . external-ids:ovn-remote=tcp:ovn1:6642 ovs-vsctl set open . external-ids:ovn-encap-type=geneve # could also be: ovs-vsctl set open . external-ids:ovn-encap-type=geneve,vxlan ovs-vsctl set open . external-ids:ovn-encap-ip=192.168.122.201 ovs-vsctl set open . external-ids:ovn-bridge=br-int On slave node ovn2: # [root@ovn2 ~]# ovs-vsctl set open . external-ids:ovn-remote=tcp:ovn1:6642 ovs-vsctl set open . external-ids:ovn-encap-ip=192.168.122.202 ovs-vsctl set open . external-ids:ovn-encap-type=geneve # could also be: ovs-vsctl set open . external-ids:ovn-encap-type=geneve,vxlan ovs-vsctl set open . external-ids:ovn-bridge=br-int On slave node ovn3: # [root@ovn3 ~]# ovs-vsctl set open . external-ids:ovn-remote=tcp:ovn1:6642 ovs-vsctl set open . external-ids:ovn-encap-ip=192.168.122.203 ovs-vsctl set open . external-ids:ovn-encap-type=geneve # could also be: ovs-vsctl set open . external-ids:ovn-encap-type=geneve,vxlan ovs-vsctl set open . external-ids:ovn-bridge=br-int Verification Verify on ovn1: ovs-vsctl show ovn-sbctl show ovn2: ovs-vsctl show timeout 2 ovn-sbctl show timeout 2 ovn-nbctl show ovn3: ovs-vsctl show timeout 2 ovn-sbctl show timeout 2 ovn-nbctl show [root@ovn1 ~]# ovs-vsctl show 64003a9a-1f2a-403c-8d21-cd187d2f717c Bridge br-int fail_mode: secure Port ovn-b5337e-0 Interface ovn-b5337e-0 type: geneve options: {csum=\"true\", key=flow, remote_ip=\"192.168.122.203\"} Port br-int Interface br-int type: internal Port ovn-8b9ad2-0 Interface ovn-8b9ad2-0 type: geneve options: {csum=\"true\", key=flow, remote_ip=\"192.168.122.202\"} ovs_version: \"2.13.0\" [root@ovn1 ~]# ovn-sbctl show Chassis \"b5337e05-2495-4bf0-8728-25840472baa4\" hostname: ovn3 Encap geneve ip: \"192.168.122.203\" options: {csum=\"true\"} Chassis \"c3f90802-4fd5-44ad-a338-154a9150e46f\" hostname: ovn1 Encap geneve ip: \"192.168.122.201\" options: {csum=\"true\"} Chassis \"8b9ad2e3-c1bc-4ea2-973e-a8bd1d38e502\" hostname: ovn2 Encap geneve ip: \"192.168.122.202\" options: {csum=\"true\"} [root@ovn2 ~]# ovs-vsctl show 33e3d97e-ee6a-4eeb-ac2a-188744895be7 Bridge br-int fail_mode: secure Port ovn-c3f908-0 Interface ovn-c3f908-0 type: geneve options: {csum=\"true\", key=flow, remote_ip=\"192.168.122.201\"} Port br-int Interface br-int type: internal Port ovn-b5337e-0 Interface ovn-b5337e-0 type: geneve options: {csum=\"true\", key=flow, remote_ip=\"192.168.122.203\"} ovs_version: \"2.13.0\" [root@ovn2 ~]# timeout 2 ovn-sbctl show 2020-07-21T11:13:15Z|00001|fatal_signal|WARN|terminating with signal 15 (Terminated) [root@ovn2 ~]# timeout 2 ovn-nbctl show ovn-nbctl: unix:/var/run/ovn/ovnnb_db.sock: database connection failed (No such file or directory) [root@ovn2 ~]# [root@ovn3 ~]# ovs-vsctl show a03dc1d7-a6f7-4400-b696-59ccda197a19 Bridge br-int fail_mode: secure Port ovn-c3f908-0 Interface ovn-c3f908-0 type: geneve options: {csum=\"true\", key=flow, remote_ip=\"192.168.122.201\"} Port br-int Interface br-int type: internal Port ovn-8b9ad2-0 Interface ovn-8b9ad2-0 type: geneve options: {csum=\"true\", key=flow, remote_ip=\"192.168.122.202\"} ovs_version: \"2.13.0\" [root@ovn3 ~]# timeout 2 ovn-sbctl show 2020-07-21T11:13:21Z|00001|fatal_signal|WARN|terminating with signal 15 (Terminated) [root@ovn3 ~]# timeout 2 ovn-nbctl show ovn-nbctl: unix:/var/run/ovn/ovnnb_db.sock: database connection failed (No such file or directory) [root@ovn3 ~]# Adding virtual network with one virtual switch s0 On ovn1, configure a logical switch and port: ovn-nbctl ls-add s0 ovn-nbctl lsp-add s0 port01 ovn-nbctl lsp-set-addresses port01 00:00:00:00:00:01 ovn-nbctl lsp-add s0 port02 ovn-nbctl lsp-set-addresses port02 00:00:00:00:00:02 Now, \"real\" ports need to be wired to the above ports. Note that the logical port name has to match the external_ids:iface-id identifier. If we added ovn-nbctl lsp-add s0 foo instead of port0 , then we would have to set ovs-vsctl set Interface port0 external_ids:iface-id=foo on ovn2. Note that we need to know the MAC address of that port. Therefore, when creating the veth, we are making sure to create it with the correct MAC address. On ovn2, execute: ip link add name veth01 type veth peer name port01 ip netns add ns0 ip link set dev veth01 netns ns0 ip netns exec ns0 ip link set dev lo up ip netns exec ns0 ip link set dev veth01 up ip netns exec ns0 ip link set veth01 address 00:00:00:00:00:01 ip netns exec ns0 ip address add 10.0.0.1/24 dev veth01 ip link set dev port01 up ovs-vsctl add-port br-int port01 ovs-vsctl set Interface port01 external_ids:iface-id=port01 On ovn3, execute: ip link add name veth02 type veth peer name port02 ip netns add ns0 ip link set dev veth02 netns ns0 ip netns exec ns0 ip link set dev lo up ip netns exec ns0 ip link set dev veth02 up ip netns exec ns0 ip link set veth02 address 00:00:00:00:00:02 ip netns exec ns0 ip address add 10.0.0.2/24 dev veth02 ip link set dev port02 up ovs-vsctl add-port br-int port02 external_ids:iface-id=port02 ovs-vsctl set Interface port02 external_ids:iface-id=port02 Verify the new configuration on ovn1: ovn-nbctl show ovn-sbctl show [root@ovn1 ~]# ovn-nbctl show switch 40f56f4d-db18-4d77-bb8b-c93e5179d346 (s0) port port02 addresses: [\"00:00:00:00:00:02\"] port port01 addresses: [\"00:00:00:00:00:01\"] [root@ovn1 ~]# ovn-sbctl show Chassis \"b5337e05-2495-4bf0-8728-25840472baa4\" hostname: ovn3 Encap geneve ip: \"192.168.122.203\" options: {csum=\"true\"} Port_Binding port02 Chassis \"c3f90802-4fd5-44ad-a338-154a9150e46f\" hostname: ovn1 Encap geneve ip: \"192.168.122.201\" options: {csum=\"true\"} Chassis \"8b9ad2e3-c1bc-4ea2-973e-a8bd1d38e502\" hostname: ovn2 Encap geneve ip: \"192.168.122.202\" options: {csum=\"true\"} Port_Binding port01 Note: The southbound database now shows port bindings. Verify logical flows: ovn-sbctl lflow-list]# [root@ovn1 ~]# ovn-sbctl lflow-list Datapath: \"s0\" (63c97e9f-194e-49b1-b075-661cd2132895) Pipeline: ingress table=0 (ls_in_port_sec_l2 ), priority=100 , match=(eth.src[40]), action=(drop;) table=0 (ls_in_port_sec_l2 ), priority=100 , match=(vlan.present), action=(drop;) table=0 (ls_in_port_sec_l2 ), priority=50 , match=(inport == \"port01\"), action=(next;) table=0 (ls_in_port_sec_l2 ), priority=50 , match=(inport == \"port02\"), action=(next;) table=1 (ls_in_port_sec_ip ), priority=0 , match=(1), action=(next;) table=2 (ls_in_port_sec_nd ), priority=0 , match=(1), action=(next;) table=3 (ls_in_pre_acl ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=3 (ls_in_pre_acl ), priority=0 , match=(1), action=(next;) table=4 (ls_in_pre_lb ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=4 (ls_in_pre_lb ), priority=110 , match=(nd || nd_rs || nd_ra), action=(next;) table=4 (ls_in_pre_lb ), priority=0 , match=(1), action=(next;) table=5 (ls_in_pre_stateful ), priority=100 , match=(reg0[0] == 1), action=(ct_next;) table=5 (ls_in_pre_stateful ), priority=0 , match=(1), action=(next;) table=6 (ls_in_acl ), priority=34000, match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=6 (ls_in_acl ), priority=0 , match=(1), action=(next;) table=7 (ls_in_qos_mark ), priority=0 , match=(1), action=(next;) table=8 (ls_in_qos_meter ), priority=0 , match=(1), action=(next;) table=9 (ls_in_lb ), priority=0 , match=(1), action=(next;) table=10(ls_in_stateful ), priority=100 , match=(reg0[1] == 1), action=(ct_commit(ct_label=0/1); next;) table=10(ls_in_stateful ), priority=100 , match=(reg0[2] == 1), action=(ct_lb;) table=10(ls_in_stateful ), priority=0 , match=(1), action=(next;) table=11(ls_in_pre_hairpin ), priority=0 , match=(1), action=(next;) table=12(ls_in_hairpin ), priority=1 , match=(reg0[6] == 1), action=(eth.dst <-> eth.src;outport = inport;flags.loopback = 1;output;) table=12(ls_in_hairpin ), priority=0 , match=(1), action=(next;) table=13(ls_in_arp_rsp ), priority=0 , match=(1), action=(next;) table=14(ls_in_dhcp_options ), priority=0 , match=(1), action=(next;) table=15(ls_in_dhcp_response), priority=0 , match=(1), action=(next;) table=16(ls_in_dns_lookup ), priority=0 , match=(1), action=(next;) table=17(ls_in_dns_response ), priority=0 , match=(1), action=(next;) table=18(ls_in_external_port), priority=0 , match=(1), action=(next;) table=19(ls_in_l2_lkup ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(handle_svc_check(inport);) table=19(ls_in_l2_lkup ), priority=70 , match=(eth.mcast), action=(outport = \"_MC_flood\"; output;) table=19(ls_in_l2_lkup ), priority=50 , match=(eth.dst == 00:00:00:00:00:01), action=(outport = \"port01\"; output;) table=19(ls_in_l2_lkup ), priority=50 , match=(eth.dst == 00:00:00:00:00:02), action=(outport = \"port02\"; output;) Datapath: \"s0\" (63c97e9f-194e-49b1-b075-661cd2132895) Pipeline: egress table=0 (ls_out_pre_lb ), priority=110 , match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=0 (ls_out_pre_lb ), priority=110 , match=(nd || nd_rs || nd_ra), action=(next;) table=0 (ls_out_pre_lb ), priority=0 , match=(1), action=(next;) table=1 (ls_out_pre_acl ), priority=110 , match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=1 (ls_out_pre_acl ), priority=0 , match=(1), action=(next;) table=2 (ls_out_pre_stateful), priority=100 , match=(reg0[0] == 1), action=(ct_next;) table=2 (ls_out_pre_stateful), priority=0 , match=(1), action=(next;) table=3 (ls_out_lb ), priority=0 , match=(1), action=(next;) table=4 (ls_out_acl ), priority=34000, match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=4 (ls_out_acl ), priority=0 , match=(1), action=(next;) table=5 (ls_out_qos_mark ), priority=0 , match=(1), action=(next;) table=6 (ls_out_qos_meter ), priority=0 , match=(1), action=(next;) table=7 (ls_out_stateful ), priority=100 , match=(reg0[1] == 1), action=(ct_commit(ct_label=0/1); next;) table=7 (ls_out_stateful ), priority=100 , match=(reg0[2] == 1), action=(ct_lb;) table=7 (ls_out_stateful ), priority=0 , match=(1), action=(next;) table=8 (ls_out_port_sec_ip ), priority=0 , match=(1), action=(next;) table=9 (ls_out_port_sec_l2 ), priority=100 , match=(eth.mcast), action=(output;) table=9 (ls_out_port_sec_l2 ), priority=50 , match=(outport == \"port01\"), action=(output;) table=9 (ls_out_port_sec_l2 ), priority=50 , match=(outport == \"port02\"), action=(output;) Last but not least, run a test ping between the namespaces on both hosts: [root@ovn2 ~]# ip netns exec ns0 ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 8: veth01@if7: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 00:00:00:00:00:01 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.0.0.1/24 scope global veth01 valid_lft forever preferred_lft forever inet6 fe80::200:ff:fe00:1/64 scope link valid_lft forever preferred_lft forever [root@ovn2 ~]# ip netns exec ns0 ping 10.0.0.2 -c1 -W1 PING 10.0.0.2 (10.0.0.2) 56(84) bytes of data. 64 bytes from 10.0.0.2: icmp_seq=1 ttl=64 time=1.41 ms --- 10.0.0.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 1.413/1.413/1.413/0.000 ms [root@ovn2 ~]# [root@ovn3 ~]# ip netns exec ns0 ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 8: veth02@if7: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 00:00:00:00:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.0.0.2/24 scope global veth02 valid_lft forever preferred_lft forever inet6 fe80::200:ff:fe00:2/64 scope link valid_lft forever preferred_lft forever [root@ovn3 ~]# ip netns exec ns0 ping 10.0.0.1 -c1 -W1 PING 10.0.0.1 (10.0.0.1) 56(84) bytes of data. 64 bytes from 10.0.0.1: icmp_seq=1 ttl=64 time=0.886 ms --- 10.0.0.1 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.886/0.886/0.886/0.000 ms [root@ovn3 ~]# Adding virtual network with one virtual switch s1 On ovn1, configure a logical switch and port: ovn-nbctl ls-add s1 ovn-nbctl lsp-add s1 port11 ovn-nbctl lsp-set-addresses port11 00:00:00:00:01:01 ovn-nbctl lsp-add s1 port12 ovn-nbctl lsp-set-addresses port12 00:00:00:00:01:02 Now, \"real\" ports need to be wired to the above ports. Note that the logical port name has to match the external_ids:iface-id identifier. If we added ovn-nbctl lsp-add s1 foo instead of port0 , then we would have to set ovs-vsctl set Interface port0 external_ids:iface-id=foo on ovn2. Note that we need to know the MAC address of that port. Therefore, when creating the veth, we are making sure to create it with the correct MAC address. On ovn2, execute: ip link add name veth11 type veth peer name port11 ip netns add ns1 ip link set dev veth11 netns ns1 ip netns exec ns1 ip link set dev lo up ip netns exec ns1 ip link set dev veth11 up ip netns exec ns1 ip link set veth11 address 00:00:00:00:01:01 ip netns exec ns1 ip address add 10.0.1.1/24 dev veth11 ip link set dev port11 up ovs-vsctl add-port br-int port11 ovs-vsctl set Interface port11 external_ids:iface-id=port11 On ovn3, execute: ip link add name veth12 type veth peer name port12 ip netns add ns1 ip link set dev veth12 netns ns1 ip netns exec ns1 ip link set dev lo up ip netns exec ns1 ip link set dev veth12 up ip netns exec ns1 ip link set veth12 address 00:00:00:00:01:02 ip netns exec ns1 ip address add 10.0.1.2/24 dev veth12 ip link set dev port12 up ovs-vsctl add-port br-int port12 external_ids:iface-id=port12 ovs-vsctl set Interface port12 external_ids:iface-id=port12 Verify the new configuration on ovn1: ovn-nbctl show ovn-sbctl show [root@ovn1 ~]# ovn-nbctl show switch 722300a5-73e1-42de-ae19-b7997f1f9a86 (s1) port port12 addresses: [\"00:00:00:00:01:02\"] port port11 addresses: [\"00:00:00:00:01:01\"] switch 40f56f4d-db18-4d77-bb8b-c93e5179d346 (s0) port port02 addresses: [\"00:00:00:00:00:02\"] port port01 addresses: [\"00:00:00:00:00:01\"] [root@ovn1 ~]# ovn-sbctl show Chassis \"b5337e05-2495-4bf0-8728-25840472baa4\" hostname: ovn3 Encap geneve ip: \"192.168.122.203\" options: {csum=\"true\"} Port_Binding port12 Port_Binding port02 Chassis \"c3f90802-4fd5-44ad-a338-154a9150e46f\" hostname: ovn1 Encap geneve ip: \"192.168.122.201\" options: {csum=\"true\"} Chassis \"8b9ad2e3-c1bc-4ea2-973e-a8bd1d38e502\" hostname: ovn2 Encap geneve ip: \"192.168.122.202\" options: {csum=\"true\"} Port_Binding port01 Port_Binding port11 Note: The southbound database now shows port bindings. Verify logical flows: ovn-sbctl lflow-list [root@ovn1 ~]# ovn-sbctl lflow-list Datapath: \"s0\" (63c97e9f-194e-49b1-b075-661cd2132895) Pipeline: ingress table=0 (ls_in_port_sec_l2 ), priority=100 , match=(eth.src[40]), action=(drop;) table=0 (ls_in_port_sec_l2 ), priority=100 , match=(vlan.present), action=(drop;) table=0 (ls_in_port_sec_l2 ), priority=50 , match=(inport == \"port01\"), action=(next;) table=0 (ls_in_port_sec_l2 ), priority=50 , match=(inport == \"port02\"), action=(next;) table=1 (ls_in_port_sec_ip ), priority=0 , match=(1), action=(next;) table=2 (ls_in_port_sec_nd ), priority=0 , match=(1), action=(next;) table=3 (ls_in_pre_acl ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=3 (ls_in_pre_acl ), priority=0 , match=(1), action=(next;) table=4 (ls_in_pre_lb ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=4 (ls_in_pre_lb ), priority=110 , match=(nd || nd_rs || nd_ra), action=(next;) table=4 (ls_in_pre_lb ), priority=0 , match=(1), action=(next;) table=5 (ls_in_pre_stateful ), priority=100 , match=(reg0[0] == 1), action=(ct_next;) table=5 (ls_in_pre_stateful ), priority=0 , match=(1), action=(next;) table=6 (ls_in_acl ), priority=34000, match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=6 (ls_in_acl ), priority=0 , match=(1), action=(next;) table=7 (ls_in_qos_mark ), priority=0 , match=(1), action=(next;) table=8 (ls_in_qos_meter ), priority=0 , match=(1), action=(next;) table=9 (ls_in_lb ), priority=0 , match=(1), action=(next;) table=10(ls_in_stateful ), priority=100 , match=(reg0[1] == 1), action=(ct_commit(ct_label=0/1); next;) table=10(ls_in_stateful ), priority=100 , match=(reg0[2] == 1), action=(ct_lb;) table=10(ls_in_stateful ), priority=0 , match=(1), action=(next;) table=11(ls_in_pre_hairpin ), priority=0 , match=(1), action=(next;) table=12(ls_in_hairpin ), priority=1 , match=(reg0[6] == 1), action=(eth.dst <-> eth.src;outport = inport;flags.loopback = 1;output;) table=12(ls_in_hairpin ), priority=0 , match=(1), action=(next;) table=13(ls_in_arp_rsp ), priority=0 , match=(1), action=(next;) table=14(ls_in_dhcp_options ), priority=0 , match=(1), action=(next;) table=15(ls_in_dhcp_response), priority=0 , match=(1), action=(next;) table=16(ls_in_dns_lookup ), priority=0 , match=(1), action=(next;) table=17(ls_in_dns_response ), priority=0 , match=(1), action=(next;) table=18(ls_in_external_port), priority=0 , match=(1), action=(next;) table=19(ls_in_l2_lkup ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(handle_svc_check(inport);) table=19(ls_in_l2_lkup ), priority=70 , match=(eth.mcast), action=(outport = \"_MC_flood\"; output;) table=19(ls_in_l2_lkup ), priority=50 , match=(eth.dst == 00:00:00:00:00:01), action=(outport = \"port01\"; output;) table=19(ls_in_l2_lkup ), priority=50 , match=(eth.dst == 00:00:00:00:00:02), action=(outport = \"port02\"; output;) Datapath: \"s0\" (63c97e9f-194e-49b1-b075-661cd2132895) Pipeline: egress table=0 (ls_out_pre_lb ), priority=110 , match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=0 (ls_out_pre_lb ), priority=110 , match=(nd || nd_rs || nd_ra), action=(next;) table=0 (ls_out_pre_lb ), priority=0 , match=(1), action=(next;) table=1 (ls_out_pre_acl ), priority=110 , match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=1 (ls_out_pre_acl ), priority=0 , match=(1), action=(next;) table=2 (ls_out_pre_stateful), priority=100 , match=(reg0[0] == 1), action=(ct_next;) table=2 (ls_out_pre_stateful), priority=0 , match=(1), action=(next;) table=3 (ls_out_lb ), priority=0 , match=(1), action=(next;) table=4 (ls_out_acl ), priority=34000, match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=4 (ls_out_acl ), priority=0 , match=(1), action=(next;) table=5 (ls_out_qos_mark ), priority=0 , match=(1), action=(next;) table=6 (ls_out_qos_meter ), priority=0 , match=(1), action=(next;) table=7 (ls_out_stateful ), priority=100 , match=(reg0[1] == 1), action=(ct_commit(ct_label=0/1); next;) table=7 (ls_out_stateful ), priority=100 , match=(reg0[2] == 1), action=(ct_lb;) table=7 (ls_out_stateful ), priority=0 , match=(1), action=(next;) table=8 (ls_out_port_sec_ip ), priority=0 , match=(1), action=(next;) table=9 (ls_out_port_sec_l2 ), priority=100 , match=(eth.mcast), action=(output;) table=9 (ls_out_port_sec_l2 ), priority=50 , match=(outport == \"port01\"), action=(output;) table=9 (ls_out_port_sec_l2 ), priority=50 , match=(outport == \"port02\"), action=(output;) Datapath: \"s1\" (7ba84139-1ab7-47fe-874c-6956be31ab0a) Pipeline: ingress table=0 (ls_in_port_sec_l2 ), priority=100 , match=(eth.src[40]), action=(drop;) table=0 (ls_in_port_sec_l2 ), priority=100 , match=(vlan.present), action=(drop;) table=0 (ls_in_port_sec_l2 ), priority=50 , match=(inport == \"port11\"), action=(next;) table=0 (ls_in_port_sec_l2 ), priority=50 , match=(inport == \"port12\"), action=(next;) table=1 (ls_in_port_sec_ip ), priority=0 , match=(1), action=(next;) table=2 (ls_in_port_sec_nd ), priority=0 , match=(1), action=(next;) table=3 (ls_in_pre_acl ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=3 (ls_in_pre_acl ), priority=0 , match=(1), action=(next;) table=4 (ls_in_pre_lb ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=4 (ls_in_pre_lb ), priority=110 , match=(nd || nd_rs || nd_ra), action=(next;) table=4 (ls_in_pre_lb ), priority=0 , match=(1), action=(next;) table=5 (ls_in_pre_stateful ), priority=100 , match=(reg0[0] == 1), action=(ct_next;) table=5 (ls_in_pre_stateful ), priority=0 , match=(1), action=(next;) table=6 (ls_in_acl ), priority=34000, match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=6 (ls_in_acl ), priority=0 , match=(1), action=(next;) table=7 (ls_in_qos_mark ), priority=0 , match=(1), action=(next;) table=8 (ls_in_qos_meter ), priority=0 , match=(1), action=(next;) table=9 (ls_in_lb ), priority=0 , match=(1), action=(next;) table=10(ls_in_stateful ), priority=100 , match=(reg0[1] == 1), action=(ct_commit(ct_label=0/1); next;) table=10(ls_in_stateful ), priority=100 , match=(reg0[2] == 1), action=(ct_lb;) table=10(ls_in_stateful ), priority=0 , match=(1), action=(next;) table=11(ls_in_pre_hairpin ), priority=0 , match=(1), action=(next;) table=12(ls_in_hairpin ), priority=1 , match=(reg0[6] == 1), action=(eth.dst <-> eth.src;outport = inport;flags.loopback = 1;output;) table=12(ls_in_hairpin ), priority=0 , match=(1), action=(next;) table=13(ls_in_arp_rsp ), priority=0 , match=(1), action=(next;) table=14(ls_in_dhcp_options ), priority=0 , match=(1), action=(next;) table=15(ls_in_dhcp_response), priority=0 , match=(1), action=(next;) table=16(ls_in_dns_lookup ), priority=0 , match=(1), action=(next;) table=17(ls_in_dns_response ), priority=0 , match=(1), action=(next;) table=18(ls_in_external_port), priority=0 , match=(1), action=(next;) table=19(ls_in_l2_lkup ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(handle_svc_check(inport);) table=19(ls_in_l2_lkup ), priority=70 , match=(eth.mcast), action=(outport = \"_MC_flood\"; output;) table=19(ls_in_l2_lkup ), priority=50 , match=(eth.dst == 00:00:00:00:01:01), action=(outport = \"port11\"; output;) table=19(ls_in_l2_lkup ), priority=50 , match=(eth.dst == 00:00:00:00:01:02), action=(outport = \"port12\"; output;) Datapath: \"s1\" (7ba84139-1ab7-47fe-874c-6956be31ab0a) Pipeline: egress table=0 (ls_out_pre_lb ), priority=110 , match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=0 (ls_out_pre_lb ), priority=110 , match=(nd || nd_rs || nd_ra), action=(next;) table=0 (ls_out_pre_lb ), priority=0 , match=(1), action=(next;) table=1 (ls_out_pre_acl ), priority=110 , match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=1 (ls_out_pre_acl ), priority=0 , match=(1), action=(next;) table=2 (ls_out_pre_stateful), priority=100 , match=(reg0[0] == 1), action=(ct_next;) table=2 (ls_out_pre_stateful), priority=0 , match=(1), action=(next;) table=3 (ls_out_lb ), priority=0 , match=(1), action=(next;) table=4 (ls_out_acl ), priority=34000, match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=4 (ls_out_acl ), priority=0 , match=(1), action=(next;) table=5 (ls_out_qos_mark ), priority=0 , match=(1), action=(next;) table=6 (ls_out_qos_meter ), priority=0 , match=(1), action=(next;) table=7 (ls_out_stateful ), priority=100 , match=(reg0[1] == 1), action=(ct_commit(ct_label=0/1); next;) table=7 (ls_out_stateful ), priority=100 , match=(reg0[2] == 1), action=(ct_lb;) table=7 (ls_out_stateful ), priority=0 , match=(1), action=(next;) table=8 (ls_out_port_sec_ip ), priority=0 , match=(1), action=(next;) table=9 (ls_out_port_sec_l2 ), priority=100 , match=(eth.mcast), action=(output;) table=9 (ls_out_port_sec_l2 ), priority=50 , match=(outport == \"port11\"), action=(output;) table=9 (ls_out_port_sec_l2 ), priority=50 , match=(outport == \"port12\"), action=(output;) Last but not least, run a test ping between the namespaces on both hosts: [root@ovn2 ~]# ip netns exec ns1 ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 10: veth11@if9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 00:00:00:00:01:01 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.0.1.1/24 scope global veth11 valid_lft forever preferred_lft forever inet6 fe80::200:ff:fe00:101/64 scope link valid_lft forever preferred_lft forever [root@ovn2 ~]# ip netns exec ns1 ping 10.0.1.2 -c1 -W1 PING 10.0.1.2 (10.0.1.2) 56(84) bytes of data. 64 bytes from 10.0.1.2: icmp_seq=1 ttl=64 time=1.83 ms --- 10.0.1.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 1.833/1.833/1.833/0.000 ms [root@ovn2 ~]# [root@ovn3 ~]# ip netns exec ns1 ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 10: veth12@if9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 00:00:00:00:01:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.0.1.2/24 scope global veth12 valid_lft forever preferred_lft forever inet6 fe80::200:ff:fe00:102/64 scope link valid_lft forever preferred_lft forever [root@ovn3 ~]# ip netns exec ns1 ping 10.0.1.1 -c1 -W1 PING 10.0.1.1 (10.0.1.1) 56(84) bytes of data. 64 bytes from 10.0.1.1: icmp_seq=1 ttl=64 time=0.916 ms --- 10.0.1.1 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.916/0.916/0.916/0.000 ms [root@ovn3 ~]# Adding virtual router r1 ## Introduction http://www.openvswitch.org/support/dist-docs/ovn-architecture.7.html Logical Routers and Logical Patch Ports Typically logical routers and logical patch ports do not have a physi\u2010 cal location and effectively reside on every hypervisor. This is the case for logical patch ports between logical routers and logical switches behind those logical routers, to which VMs (and VIFs) attach. Consider a packet sent from one virtual machine or container to another VM or container that resides on a different subnet. The packet will traverse tables 0 to 65 as described in the previous section Architec\u2010 tural Physical Life Cycle of a Packet, using the logical datapath rep\u2010 resenting the logical switch that the sender is attached to. At table 32, the packet will use the fallback flow that resubmits locally to ta\u2010 ble 33 on the same hypervisor. In this case, all of the processing from table 0 to table 65 occurs on the hypervisor where the sender resides. When the packet reaches table 65, the logical egress port is a logical patch port. The implementation in table 65 differs depending on the OVS version, although the observed behavior is meant to be the same: \u2022 In OVS versions 2.6 and earlier, table 65 outputs to an OVS patch port that represents the logical patch port. The packet re-enters the OpenFlow flow table from the OVS patch port\u2019s peer in table 0, which identifies the logi\u2010 cal datapath and logical input port based on the OVS patch port\u2019s OpenFlow port number. \u2022 In OVS versions 2.7 and later, the packet is cloned and resubmitted directly to the first OpenFlow flow table in the ingress pipeline, setting the logical ingress port to the peer logical patch port, and using the peer logical patch port\u2019s logical datapath (that represents the logi\u2010 cal router). The packet re-enters the ingress pipeline in order to traverse tables 8 to 65 again, this time using the logical datapath representing the log\u2010 ical router. The processing continues as described in the previous sec\u2010 tion Architectural Physical Life Cycle of a Packet. When the packet reachs table 65, the logical egress port will once again be a logical patch port. In the same manner as described above, this logical patch port will cause the packet to be resubmitted to OpenFlow tables 8 to 65, this time using the logical datapath representing the logical switch that the destination VM or container is attached to. The packet traverses tables 8 to 65 a third and final time. If the des\u2010 tination VM or container resides on a remote hypervisor, then table 32 will send the packet on a tunnel port from the sender\u2019s hypervisor to the remote hypervisor. Finally table 65 will output the packet directly to the destination VM or container. The following sections describe two exceptions, where logical routers and/or logical patch ports are associated with a physical location. Setup Configure namespace routes for virtual routing - on both hosts, run: ip netns exec ns0 ip r add default via 10.0.0.254 ip netns exec ns1 ip r add default via 10.0.1.254 Now, set up the OVN northbound db - router and router ports: ovn-nbctl lr-add r1 ovn-nbctl lrp-add r1 r1s0 00:00:00:00:00:ff 10.0.0.254/24 ovn-nbctl lsp-add s0 s0r1 ovn-nbctl lsp-set-addresses s0r1 00:00:00:00:00:ff ovn-nbctl lsp-set-options s0r1 router-port=r1s0 ovn-nbctl lsp-set-type s0r1 router ovn-nbctl lrp-add r1 r1s1 00:00:00:00:01:ff 10.0.1.254/24 ovn-nbctl lsp-add s1 s1r1 ovn-nbctl lsp-set-addresses s1r1 00:00:00:00:01:ff ovn-nbctl lsp-set-options s1r1 router-port=r1s1 ovn-nbctl lsp-set-type s1r1 router Verify the new configuration: ovn-nbctl show ovn-sbctl show [root@ovn1 ~]# ovn-nbctl show switch 722300a5-73e1-42de-ae19-b7997f1f9a86 (s1) port port12 addresses: [\"00:00:00:00:01:02\"] port s1r1 type: router addresses: [\"00:00:00:00:01:ff\"] router-port: r1s1 port port11 addresses: [\"00:00:00:00:01:01\"] switch 40f56f4d-db18-4d77-bb8b-c93e5179d346 (s0) port s0r1 type: router addresses: [\"00:00:00:00:00:ff\"] router-port: r1s0 port port02 addresses: [\"00:00:00:00:00:02\"] port port01 addresses: [\"00:00:00:00:00:01\"] router 3affb86c-5f1a-461d-bd07-b4a9580ed9bf (r1) port r1s0 mac: \"00:00:00:00:00:ff\" networks: [\"10.0.0.254/24\"] port r1s1 mac: \"00:00:00:00:01:ff\" networks: [\"10.0.1.254/24\"] [root@ovn1 ~]# ovn-sbctl show Chassis \"b5337e05-2495-4bf0-8728-25840472baa4\" hostname: ovn3 Encap geneve ip: \"192.168.122.203\" options: {csum=\"true\"} Port_Binding port12 Port_Binding port02 Chassis \"c3f90802-4fd5-44ad-a338-154a9150e46f\" hostname: ovn1 Encap geneve ip: \"192.168.122.201\" options: {csum=\"true\"} Chassis \"8b9ad2e3-c1bc-4ea2-973e-a8bd1d38e502\" hostname: ovn2 Encap geneve ip: \"192.168.122.202\" options: {csum=\"true\"} Port_Binding port01 Port_Binding port11 Verify logical flows: ovn-sbctl lflow-list [root@ovn1 ~]# ovn-sbctl lflow-list Datapath: \"s0\" (63c97e9f-194e-49b1-b075-661cd2132895) Pipeline: ingress table=0 (ls_in_port_sec_l2 ), priority=100 , match=(eth.src[40]), action=(drop;) table=0 (ls_in_port_sec_l2 ), priority=100 , match=(vlan.present), action=(drop;) table=0 (ls_in_port_sec_l2 ), priority=50 , match=(inport == \"port01\"), action=(next;) table=0 (ls_in_port_sec_l2 ), priority=50 , match=(inport == \"port02\"), action=(next;) table=0 (ls_in_port_sec_l2 ), priority=50 , match=(inport == \"s0r1\"), action=(next;) table=1 (ls_in_port_sec_ip ), priority=0 , match=(1), action=(next;) table=2 (ls_in_port_sec_nd ), priority=0 , match=(1), action=(next;) table=3 (ls_in_pre_acl ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=3 (ls_in_pre_acl ), priority=0 , match=(1), action=(next;) table=4 (ls_in_pre_lb ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=4 (ls_in_pre_lb ), priority=110 , match=(nd || nd_rs || nd_ra), action=(next;) table=4 (ls_in_pre_lb ), priority=0 , match=(1), action=(next;) table=5 (ls_in_pre_stateful ), priority=100 , match=(reg0[0] == 1), action=(ct_next;) table=5 (ls_in_pre_stateful ), priority=0 , match=(1), action=(next;) table=6 (ls_in_acl ), priority=34000, match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=6 (ls_in_acl ), priority=0 , match=(1), action=(next;) table=7 (ls_in_qos_mark ), priority=0 , match=(1), action=(next;) table=8 (ls_in_qos_meter ), priority=0 , match=(1), action=(next;) table=9 (ls_in_lb ), priority=0 , match=(1), action=(next;) table=10(ls_in_stateful ), priority=100 , match=(reg0[1] == 1), action=(ct_commit(ct_label=0/1); next;) table=10(ls_in_stateful ), priority=100 , match=(reg0[2] == 1), action=(ct_lb;) table=10(ls_in_stateful ), priority=0 , match=(1), action=(next;) table=11(ls_in_pre_hairpin ), priority=0 , match=(1), action=(next;) table=12(ls_in_hairpin ), priority=1 , match=(reg0[6] == 1), action=(eth.dst <-> eth.src;outport = inport;flags.loopback = 1;output;) table=12(ls_in_hairpin ), priority=0 , match=(1), action=(next;) table=13(ls_in_arp_rsp ), priority=0 , match=(1), action=(next;) table=14(ls_in_dhcp_options ), priority=0 , match=(1), action=(next;) table=15(ls_in_dhcp_response), priority=0 , match=(1), action=(next;) table=16(ls_in_dns_lookup ), priority=0 , match=(1), action=(next;) table=17(ls_in_dns_response ), priority=0 , match=(1), action=(next;) table=18(ls_in_external_port), priority=0 , match=(1), action=(next;) table=19(ls_in_l2_lkup ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(handle_svc_check(inport);) table=19(ls_in_l2_lkup ), priority=80 , match=(eth.src == { 00:00:00:00:00:ff} && (arp.op == 1 || nd_ns)), action=(outport = \"_MC_flood\"; output;) table=19(ls_in_l2_lkup ), priority=75 , match=(flags[1] == 0 && arp.op == 1 && arp.tpa == { 10.0.0.254}), action=(outport = \"s0r1\"; output;) table=19(ls_in_l2_lkup ), priority=75 , match=(flags[1] == 0 && nd_ns && nd.target == { fe80::200:ff:fe00:ff}), action=(outport = \"s0r1\"; output;) table=19(ls_in_l2_lkup ), priority=70 , match=(eth.mcast), action=(outport = \"_MC_flood\"; output;) table=19(ls_in_l2_lkup ), priority=50 , match=(eth.dst == 00:00:00:00:00:01), action=(outport = \"port01\"; output;) table=19(ls_in_l2_lkup ), priority=50 , match=(eth.dst == 00:00:00:00:00:02), action=(outport = \"port02\"; output;) table=19(ls_in_l2_lkup ), priority=50 , match=(eth.dst == 00:00:00:00:00:ff), action=(outport = \"s0r1\"; output;) Datapath: \"s0\" (63c97e9f-194e-49b1-b075-661cd2132895) Pipeline: egress table=0 (ls_out_pre_lb ), priority=110 , match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=0 (ls_out_pre_lb ), priority=110 , match=(nd || nd_rs || nd_ra), action=(next;) table=0 (ls_out_pre_lb ), priority=0 , match=(1), action=(next;) table=1 (ls_out_pre_acl ), priority=110 , match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=1 (ls_out_pre_acl ), priority=0 , match=(1), action=(next;) table=2 (ls_out_pre_stateful), priority=100 , match=(reg0[0] == 1), action=(ct_next;) table=2 (ls_out_pre_stateful), priority=0 , match=(1), action=(next;) table=3 (ls_out_lb ), priority=0 , match=(1), action=(next;) table=4 (ls_out_acl ), priority=34000, match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=4 (ls_out_acl ), priority=0 , match=(1), action=(next;) table=5 (ls_out_qos_mark ), priority=0 , match=(1), action=(next;) table=6 (ls_out_qos_meter ), priority=0 , match=(1), action=(next;) table=7 (ls_out_stateful ), priority=100 , match=(reg0[1] == 1), action=(ct_commit(ct_label=0/1); next;) table=7 (ls_out_stateful ), priority=100 , match=(reg0[2] == 1), action=(ct_lb;) table=7 (ls_out_stateful ), priority=0 , match=(1), action=(next;) table=8 (ls_out_port_sec_ip ), priority=0 , match=(1), action=(next;) table=9 (ls_out_port_sec_l2 ), priority=100 , match=(eth.mcast), action=(output;) table=9 (ls_out_port_sec_l2 ), priority=50 , match=(outport == \"port01\"), action=(output;) table=9 (ls_out_port_sec_l2 ), priority=50 , match=(outport == \"port02\"), action=(output;) table=9 (ls_out_port_sec_l2 ), priority=50 , match=(outport == \"s0r1\"), action=(output;) Datapath: \"s1\" (7ba84139-1ab7-47fe-874c-6956be31ab0a) Pipeline: ingress table=0 (ls_in_port_sec_l2 ), priority=100 , match=(eth.src[40]), action=(drop;) table=0 (ls_in_port_sec_l2 ), priority=100 , match=(vlan.present), action=(drop;) table=0 (ls_in_port_sec_l2 ), priority=50 , match=(inport == \"port11\"), action=(next;) table=0 (ls_in_port_sec_l2 ), priority=50 , match=(inport == \"port12\"), action=(next;) table=0 (ls_in_port_sec_l2 ), priority=50 , match=(inport == \"s1r1\"), action=(next;) table=1 (ls_in_port_sec_ip ), priority=0 , match=(1), action=(next;) table=2 (ls_in_port_sec_nd ), priority=0 , match=(1), action=(next;) table=3 (ls_in_pre_acl ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=3 (ls_in_pre_acl ), priority=0 , match=(1), action=(next;) table=4 (ls_in_pre_lb ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=4 (ls_in_pre_lb ), priority=110 , match=(nd || nd_rs || nd_ra), action=(next;) table=4 (ls_in_pre_lb ), priority=0 , match=(1), action=(next;) table=5 (ls_in_pre_stateful ), priority=100 , match=(reg0[0] == 1), action=(ct_next;) table=5 (ls_in_pre_stateful ), priority=0 , match=(1), action=(next;) table=6 (ls_in_acl ), priority=34000, match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=6 (ls_in_acl ), priority=0 , match=(1), action=(next;) table=7 (ls_in_qos_mark ), priority=0 , match=(1), action=(next;) table=8 (ls_in_qos_meter ), priority=0 , match=(1), action=(next;) table=9 (ls_in_lb ), priority=0 , match=(1), action=(next;) table=10(ls_in_stateful ), priority=100 , match=(reg0[1] == 1), action=(ct_commit(ct_label=0/1); next;) table=10(ls_in_stateful ), priority=100 , match=(reg0[2] == 1), action=(ct_lb;) table=10(ls_in_stateful ), priority=0 , match=(1), action=(next;) table=11(ls_in_pre_hairpin ), priority=0 , match=(1), action=(next;) table=12(ls_in_hairpin ), priority=1 , match=(reg0[6] == 1), action=(eth.dst <-> eth.src;outport = inport;flags.loopback = 1;output;) table=12(ls_in_hairpin ), priority=0 , match=(1), action=(next;) table=13(ls_in_arp_rsp ), priority=0 , match=(1), action=(next;) table=14(ls_in_dhcp_options ), priority=0 , match=(1), action=(next;) table=15(ls_in_dhcp_response), priority=0 , match=(1), action=(next;) table=16(ls_in_dns_lookup ), priority=0 , match=(1), action=(next;) table=17(ls_in_dns_response ), priority=0 , match=(1), action=(next;) table=18(ls_in_external_port), priority=0 , match=(1), action=(next;) table=19(ls_in_l2_lkup ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(handle_svc_check(inport);) table=19(ls_in_l2_lkup ), priority=80 , match=(eth.src == { 00:00:00:00:01:ff} && (arp.op == 1 || nd_ns)), action=(outport = \"_MC_flood\"; output;) table=19(ls_in_l2_lkup ), priority=75 , match=(flags[1] == 0 && arp.op == 1 && arp.tpa == { 10.0.1.254}), action=(outport = \"s1r1\"; output;) table=19(ls_in_l2_lkup ), priority=75 , match=(flags[1] == 0 && nd_ns && nd.target == { fe80::200:ff:fe00:1ff}), action=(outport = \"s1r1\"; output;) table=19(ls_in_l2_lkup ), priority=70 , match=(eth.mcast), action=(outport = \"_MC_flood\"; output;) table=19(ls_in_l2_lkup ), priority=50 , match=(eth.dst == 00:00:00:00:01:01), action=(outport = \"port11\"; output;) table=19(ls_in_l2_lkup ), priority=50 , match=(eth.dst == 00:00:00:00:01:02), action=(outport = \"port12\"; output;) table=19(ls_in_l2_lkup ), priority=50 , match=(eth.dst == 00:00:00:00:01:ff), action=(outport = \"s1r1\"; output;) Datapath: \"s1\" (7ba84139-1ab7-47fe-874c-6956be31ab0a) Pipeline: egress table=0 (ls_out_pre_lb ), priority=110 , match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=0 (ls_out_pre_lb ), priority=110 , match=(nd || nd_rs || nd_ra), action=(next;) table=0 (ls_out_pre_lb ), priority=0 , match=(1), action=(next;) table=1 (ls_out_pre_acl ), priority=110 , match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=1 (ls_out_pre_acl ), priority=0 , match=(1), action=(next;) table=2 (ls_out_pre_stateful), priority=100 , match=(reg0[0] == 1), action=(ct_next;) table=2 (ls_out_pre_stateful), priority=0 , match=(1), action=(next;) table=3 (ls_out_lb ), priority=0 , match=(1), action=(next;) table=4 (ls_out_acl ), priority=34000, match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=4 (ls_out_acl ), priority=0 , match=(1), action=(next;) table=5 (ls_out_qos_mark ), priority=0 , match=(1), action=(next;) table=6 (ls_out_qos_meter ), priority=0 , match=(1), action=(next;) table=7 (ls_out_stateful ), priority=100 , match=(reg0[1] == 1), action=(ct_commit(ct_label=0/1); next;) table=7 (ls_out_stateful ), priority=100 , match=(reg0[2] == 1), action=(ct_lb;) table=7 (ls_out_stateful ), priority=0 , match=(1), action=(next;) table=8 (ls_out_port_sec_ip ), priority=0 , match=(1), action=(next;) table=9 (ls_out_port_sec_l2 ), priority=100 , match=(eth.mcast), action=(output;) table=9 (ls_out_port_sec_l2 ), priority=50 , match=(outport == \"port11\"), action=(output;) table=9 (ls_out_port_sec_l2 ), priority=50 , match=(outport == \"port12\"), action=(output;) table=9 (ls_out_port_sec_l2 ), priority=50 , match=(outport == \"s1r1\"), action=(output;) Datapath: \"r1\" (df1fbb81-4fde-4585-a7bd-5e03b7961947) Pipeline: ingress table=0 (lr_in_admission ), priority=100 , match=(vlan.present || eth.src[40]), action=(drop;) table=0 (lr_in_admission ), priority=50 , match=(eth.dst == 00:00:00:00:00:ff && inport == \"r1s0\"), action=(next;) table=0 (lr_in_admission ), priority=50 , match=(eth.dst == 00:00:00:00:01:ff && inport == \"r1s1\"), action=(next;) table=0 (lr_in_admission ), priority=50 , match=(eth.mcast && inport == \"r1s0\"), action=(next;) table=0 (lr_in_admission ), priority=50 , match=(eth.mcast && inport == \"r1s1\"), action=(next;) table=1 (lr_in_lookup_neighbor), priority=100 , match=(arp.op == 2), action=(reg9[2] = lookup_arp(inport, arp.spa, arp.sha); next;) table=1 (lr_in_lookup_neighbor), priority=100 , match=(inport == \"r1s0\" && arp.spa == 10.0.0.0/24 && arp.op == 1), action=(reg9[2] = lookup_arp(inport, arp.spa, arp.sha); next;) table=1 (lr_in_lookup_neighbor), priority=100 , match=(inport == \"r1s1\" && arp.spa == 10.0.1.0/24 && arp.op == 1), action=(reg9[2] = lookup_arp(inport, arp.spa, arp.sha); next;) table=1 (lr_in_lookup_neighbor), priority=100 , match=(nd_na), action=(reg9[2] = lookup_nd(inport, nd.target, nd.tll); next;) table=1 (lr_in_lookup_neighbor), priority=100 , match=(nd_ns), action=(reg9[2] = lookup_nd(inport, ip6.src, nd.sll); next;) table=1 (lr_in_lookup_neighbor), priority=0 , match=(1), action=(reg9[3] = 1; next;) table=2 (lr_in_learn_neighbor), priority=100 , match=(reg9[3] == 1 || reg9[2] == 1), action=(next;) table=2 (lr_in_learn_neighbor), priority=90 , match=(arp), action=(put_arp(inport, arp.spa, arp.sha); next;) table=2 (lr_in_learn_neighbor), priority=90 , match=(nd_na), action=(put_nd(inport, nd.target, nd.tll); next;) table=2 (lr_in_learn_neighbor), priority=90 , match=(nd_ns), action=(put_nd(inport, ip6.src, nd.sll); next;) table=3 (lr_in_ip_input ), priority=100 , match=(ip4.src == {10.0.0.254, 10.0.0.255} && reg9[0] == 0), action=(drop;) table=3 (lr_in_ip_input ), priority=100 , match=(ip4.src == {10.0.1.254, 10.0.1.255} && reg9[0] == 0), action=(drop;) table=3 (lr_in_ip_input ), priority=100 , match=(ip4.src_mcast ||ip4.src == 255.255.255.255 || ip4.src == 127.0.0.0/8 || ip4.dst == 127.0.0.0/8 || ip4.src == 0.0.0.0/8 || ip4.dst == 0.0.0.0/8), action=(drop;) table=3 (lr_in_ip_input ), priority=100 , match=(ip6.dst == fe80::200:ff:fe00:1ff && udp.src == 547 && udp.dst == 546), action=(reg0 = 0; handle_dhcpv6_reply;) table=3 (lr_in_ip_input ), priority=100 , match=(ip6.dst == fe80::200:ff:fe00:ff && udp.src == 547 && udp.dst == 546), action=(reg0 = 0; handle_dhcpv6_reply;) table=3 (lr_in_ip_input ), priority=90 , match=(inport == \"r1s0\" && arp.spa == 10.0.0.0/24 && arp.tpa == 10.0.0.254 && arp.op == 1), action=(eth.dst = eth.src; eth.src = 00:00:00:00:00:ff; arp.op = 2; /* ARP reply */ arp.tha = arp.sha; arp.sha = 00:00:00:00:00:ff; arp.tpa = arp.spa; arp.spa = 10.0.0.254; outport = \"r1s0\"; flags.loopback = 1; output;) table=3 (lr_in_ip_input ), priority=90 , match=(inport == \"r1s0\" && nd_ns && ip6.dst == {fe80::200:ff:fe00:ff, ff02::1:ff00:ff} && nd.target == fe80::200:ff:fe00:ff), action=(nd_na_router { eth.src = 00:00:00:00:00:ff; ip6.src = fe80::200:ff:fe00:ff; nd.target = fe80::200:ff:fe00:ff; nd.tll = 00:00:00:00:00:ff; outport = inport; flags.loopback = 1; output; };) table=3 (lr_in_ip_input ), priority=90 , match=(inport == \"r1s1\" && arp.spa == 10.0.1.0/24 && arp.tpa == 10.0.1.254 && arp.op == 1), action=(eth.dst = eth.src; eth.src = 00:00:00:00:01:ff; arp.op = 2; /* ARP reply */ arp.tha = arp.sha; arp.sha = 00:00:00:00:01:ff; arp.tpa = arp.spa; arp.spa = 10.0.1.254; outport = \"r1s1\"; flags.loopback = 1; output;) table=3 (lr_in_ip_input ), priority=90 , match=(inport == \"r1s1\" && nd_ns && ip6.dst == {fe80::200:ff:fe00:1ff, ff02::1:ff00:1ff} && nd.target == fe80::200:ff:fe00:1ff), action=(nd_na_router { eth.src = 00:00:00:00:01:ff; ip6.src = fe80::200:ff:fe00:1ff; nd.target = fe80::200:ff:fe00:1ff; nd.tll = 00:00:00:00:01:ff; outport = inport; flags.loopback = 1; output; };) table=3 (lr_in_ip_input ), priority=90 , match=(ip4.dst == 10.0.0.254 && icmp4.type == 8 && icmp4.code == 0), action=(ip4.dst <-> ip4.src; ip.ttl = 255; icmp4.type = 0; flags.loopback = 1; next; ) table=3 (lr_in_ip_input ), priority=90 , match=(ip4.dst == 10.0.1.254 && icmp4.type == 8 && icmp4.code == 0), action=(ip4.dst <-> ip4.src; ip.ttl = 255; icmp4.type = 0; flags.loopback = 1; next; ) table=3 (lr_in_ip_input ), priority=90 , match=(ip6.dst == fe80::200:ff:fe00:1ff && icmp6.type == 128 && icmp6.code == 0), action=(ip6.dst <-> ip6.src; ip.ttl = 255; icmp6.type = 129; flags.loopback = 1; next; ) table=3 (lr_in_ip_input ), priority=90 , match=(ip6.dst == fe80::200:ff:fe00:ff && icmp6.type == 128 && icmp6.code == 0), action=(ip6.dst <-> ip6.src; ip.ttl = 255; icmp6.type = 129; flags.loopback = 1; next; ) table=3 (lr_in_ip_input ), priority=85 , match=(arp || nd), action=(drop;) table=3 (lr_in_ip_input ), priority=84 , match=(nd_rs || nd_ra), action=(next;) table=3 (lr_in_ip_input ), priority=83 , match=(ip6.mcast_rsvd), action=(drop;) table=3 (lr_in_ip_input ), priority=82 , match=(ip4.mcast || ip6.mcast), action=(drop;) table=3 (lr_in_ip_input ), priority=80 , match=(ip4 && ip4.dst == 10.0.0.254 && !ip.later_frag && tcp), action=(tcp_reset {eth.dst <-> eth.src; ip4.dst <-> ip4.src; next; };) table=3 (lr_in_ip_input ), priority=80 , match=(ip4 && ip4.dst == 10.0.0.254 && !ip.later_frag && udp), action=(icmp4 {eth.dst <-> eth.src; ip4.dst <-> ip4.src; ip.ttl = 255; icmp4.type = 3; icmp4.code = 3; next; };) table=3 (lr_in_ip_input ), priority=80 , match=(ip4 && ip4.dst == 10.0.1.254 && !ip.later_frag && tcp), action=(tcp_reset {eth.dst <-> eth.src; ip4.dst <-> ip4.src; next; };) table=3 (lr_in_ip_input ), priority=80 , match=(ip4 && ip4.dst == 10.0.1.254 && !ip.later_frag && udp), action=(icmp4 {eth.dst <-> eth.src; ip4.dst <-> ip4.src; ip.ttl = 255; icmp4.type = 3; icmp4.code = 3; next; };) table=3 (lr_in_ip_input ), priority=80 , match=(ip6 && ip6.dst == fe80::200:ff:fe00:1ff && !ip.later_frag && tcp), action=(tcp_reset {eth.dst <-> eth.src; ip6.dst <-> ip6.src; next; };) table=3 (lr_in_ip_input ), priority=80 , match=(ip6 && ip6.dst == fe80::200:ff:fe00:1ff && !ip.later_frag && udp), action=(icmp6 {eth.dst <-> eth.src; ip6.dst <-> ip6.src; ip.ttl = 255; icmp6.type = 1; icmp6.code = 4; next; };) table=3 (lr_in_ip_input ), priority=80 , match=(ip6 && ip6.dst == fe80::200:ff:fe00:ff && !ip.later_frag && tcp), action=(tcp_reset {eth.dst <-> eth.src; ip6.dst <-> ip6.src; next; };) table=3 (lr_in_ip_input ), priority=80 , match=(ip6 && ip6.dst == fe80::200:ff:fe00:ff && !ip.later_frag && udp), action=(icmp6 {eth.dst <-> eth.src; ip6.dst <-> ip6.src; ip.ttl = 255; icmp6.type = 1; icmp6.code = 4; next; };) table=3 (lr_in_ip_input ), priority=70 , match=(ip4 && ip4.dst == 10.0.0.254 && !ip.later_frag), action=(icmp4 {eth.dst <-> eth.src; ip4.dst <-> ip4.src; ip.ttl = 255; icmp4.type = 3; icmp4.code = 2; next; };) table=3 (lr_in_ip_input ), priority=70 , match=(ip4 && ip4.dst == 10.0.1.254 && !ip.later_frag), action=(icmp4 {eth.dst <-> eth.src; ip4.dst <-> ip4.src; ip.ttl = 255; icmp4.type = 3; icmp4.code = 2; next; };) table=3 (lr_in_ip_input ), priority=70 , match=(ip6 && ip6.dst == fe80::200:ff:fe00:1ff && !ip.later_frag), action=(icmp6 {eth.dst <-> eth.src; ip6.dst <-> ip6.src; ip.ttl = 255; icmp6.type = 1; icmp6.code = 3; next; };) table=3 (lr_in_ip_input ), priority=70 , match=(ip6 && ip6.dst == fe80::200:ff:fe00:ff && !ip.later_frag), action=(icmp6 {eth.dst <-> eth.src; ip6.dst <-> ip6.src; ip.ttl = 255; icmp6.type = 1; icmp6.code = 3; next; };) table=3 (lr_in_ip_input ), priority=60 , match=(ip4.dst == {10.0.0.254} || ip6.dst == {fe80::200:ff:fe00:ff}), action=(drop;) table=3 (lr_in_ip_input ), priority=60 , match=(ip4.dst == {10.0.1.254} || ip6.dst == {fe80::200:ff:fe00:1ff}), action=(drop;) table=3 (lr_in_ip_input ), priority=50 , match=(eth.bcast), action=(drop;) table=3 (lr_in_ip_input ), priority=40 , match=(inport == \"r1s0\" && ip4 && ip.ttl == {0, 1} && !ip.later_frag), action=(icmp4 {eth.dst <-> eth.src; icmp4.type = 11; /* Time exceeded */ icmp4.code = 0; /* TTL exceeded in transit */ ip4.dst = ip4.src; ip4.src = 10.0.0.254; ip.ttl = 255; next; };) table=3 (lr_in_ip_input ), priority=40 , match=(inport == \"r1s1\" && ip4 && ip.ttl == {0, 1} && !ip.later_frag), action=(icmp4 {eth.dst <-> eth.src; icmp4.type = 11; /* Time exceeded */ icmp4.code = 0; /* TTL exceeded in transit */ ip4.dst = ip4.src; ip4.src = 10.0.1.254; ip.ttl = 255; next; };) table=3 (lr_in_ip_input ), priority=30 , match=(ip4 && ip.ttl == {0, 1}), action=(drop;) table=3 (lr_in_ip_input ), priority=0 , match=(1), action=(next;) table=4 (lr_in_defrag ), priority=0 , match=(1), action=(next;) table=5 (lr_in_unsnat ), priority=0 , match=(1), action=(next;) table=6 (lr_in_dnat ), priority=0 , match=(1), action=(next;) table=7 (lr_in_nd_ra_options), priority=0 , match=(1), action=(next;) table=8 (lr_in_nd_ra_response), priority=0 , match=(1), action=(next;) table=9 (lr_in_ip_routing ), priority=550 , match=(nd_rs || nd_ra), action=(drop;) table=9 (lr_in_ip_routing ), priority=129 , match=(inport == \"r1s0\" && ip6.dst == fe80::/64), action=(ip.ttl--; reg8[0..15] = 0; xxreg0 = ip6.dst; xxreg1 = fe80::200:ff:fe00:ff; eth.src = 00:00:00:00:00:ff; outport = \"r1s0\"; flags.loopback = 1; next;) table=9 (lr_in_ip_routing ), priority=129 , match=(inport == \"r1s1\" && ip6.dst == fe80::/64), action=(ip.ttl--; reg8[0..15] = 0; xxreg0 = ip6.dst; xxreg1 = fe80::200:ff:fe00:1ff; eth.src = 00:00:00:00:01:ff; outport = \"r1s1\"; flags.loopback = 1; next;) table=9 (lr_in_ip_routing ), priority=49 , match=(ip4.dst == 10.0.0.0/24), action=(ip.ttl--; reg8[0..15] = 0; reg0 = ip4.dst; reg1 = 10.0.0.254; eth.src = 00:00:00:00:00:ff; outport = \"r1s0\"; flags.loopback = 1; next;) table=9 (lr_in_ip_routing ), priority=49 , match=(ip4.dst == 10.0.1.0/24), action=(ip.ttl--; reg8[0..15] = 0; reg0 = ip4.dst; reg1 = 10.0.1.254; eth.src = 00:00:00:00:01:ff; outport = \"r1s1\"; flags.loopback = 1; next;) table=10(lr_in_ip_routing_ecmp), priority=150 , match=(reg8[0..15] == 0), action=(next;) table=11(lr_in_policy ), priority=0 , match=(1), action=(next;) table=12(lr_in_arp_resolve ), priority=500 , match=(ip4.mcast || ip6.mcast), action=(next;) table=12(lr_in_arp_resolve ), priority=0 , match=(ip4), action=(get_arp(outport, reg0); next;) table=12(lr_in_arp_resolve ), priority=0 , match=(ip6), action=(get_nd(outport, xxreg0); next;) table=13(lr_in_chk_pkt_len ), priority=0 , match=(1), action=(next;) table=14(lr_in_larger_pkts ), priority=0 , match=(1), action=(next;) table=15(lr_in_gw_redirect ), priority=0 , match=(1), action=(next;) table=16(lr_in_arp_request ), priority=100 , match=(eth.dst == 00:00:00:00:00:00 && ip4), action=(arp { eth.dst = ff:ff:ff:ff:ff:ff; arp.spa = reg1; arp.tpa = reg0; arp.op = 1; output; };) table=16(lr_in_arp_request ), priority=100 , match=(eth.dst == 00:00:00:00:00:00 && ip6), action=(nd_ns { nd.target = xxreg0; output; };) table=16(lr_in_arp_request ), priority=0 , match=(1), action=(output;) Datapath: \"r1\" (df1fbb81-4fde-4585-a7bd-5e03b7961947) Pipeline: egress table=0 (lr_out_undnat ), priority=0 , match=(1), action=(next;) table=1 (lr_out_snat ), priority=120 , match=(nd_ns), action=(next;) table=1 (lr_out_snat ), priority=0 , match=(1), action=(next;) table=2 (lr_out_egr_loop ), priority=0 , match=(1), action=(next;) table=3 (lr_out_delivery ), priority=100 , match=(outport == \"r1s0\"), action=(output;) table=3 (lr_out_delivery ), priority=100 , match=(outport == \"r1s1\"), action=(output;) Last but not least, run a test ping between the namespaces on both hosts: [root@ovn2 ~]# ip netns exec ns0 ping 10.0.1.1 -c1 -W1 [root@ovn2 ~]# ip netns exec ns0 ping 10.0.1.2 -c1 -W1 [root@ovn2 ~]# ip netns exec ns0 ping 10.0.1.1 -c1 -W1 PING 10.0.1.1 (10.0.1.1) 56(84) bytes of data. 64 bytes from 10.0.1.1: icmp_seq=1 ttl=63 time=78.8 ms --- 10.0.1.1 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 78.799/78.799/78.799/0.000 ms [root@ovn2 ~]# ip netns exec ns0 ping 10.0.1.2 -c1 -W1 PING 10.0.1.2 (10.0.1.2) 56(84) bytes of data. 64 bytes from 10.0.1.2: icmp_seq=1 ttl=63 time=21.4 ms --- 10.0.1.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 21.413/21.413/21.413/0.000 ms [root@ovn2 ~]# Adding external gateway router, non-distributed Introduction http://www.openvswitch.org/support/dist-docs/ovn-architecture.7.html Gateway Routers A gateway router is a logical router that is bound to a physical loca\u2010 tion. This includes all of the logical patch ports of the logical router, as well as all of the peer logical patch ports on logical switches. In the OVN Southbound database, the Port_Binding entries for these logical patch ports use the type l3gateway rather than patch, in order to distinguish that these logical patch ports are bound to a chassis. When a hypervisor processes a packet on a logical datapath representing a logical switch, and the logical egress port is a l3gateway port rep\u2010 resenting connectivity to a gateway router, the packet will match a flow in table 32 that sends the packet on a tunnel port to the chassis where the gateway router resides. This processing in table 32 is done in the same manner as for VIFs. Gateway routers are typically used in between distributed logical routers and physical networks. The distributed logical router and the logical switches behind it, to which VMs and containers attach, effec\u2010 tively reside on each hypervisor. The distributed router and the gate\u2010 way router are connected by another logical switch, sometimes referred to as a join logical switch. On the other side, the gateway router con\u2010 nects to another logical switch that has a localnet port connecting to the physical network. When using gateway routers, DNAT and SNAT rules are associated with the gateway router, which provides a central location that can handle one- to-many SNAT (aka IP masquerading). Setup Add provider bridge mapping on ovn1: # ovn1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=provider:br-provider ovs-vsctl --may-exist add-br br-provider ovs-vsctl --may-exist add-port br-provider eth1 Bind logical router rg (router gateway) to chassis ovn1 and assign an internal IP pointing towards the VMs: chassis=$(ovn-sbctl find chassis hostname=ovn1 | awk '/^name/ {print $NF}' | sed 's/\"//g') ovn-nbctl create Logical_Router name=rg options:chassis=$chassis ovn-nbctl lrp-add rg rgsj 00:00:00:01:00:01 10.1.0.1/30 Create switch sj (switch join) and add the router port of rg to it: ovn-nbctl ls-add sj ovn-nbctl lsp-add sj sjrg ovn-nbctl lsp-set-type sjrg router ovn-nbctl lsp-set-addresses sjrg 00:00:00:01:00:01 ovn-nbctl lsp-set-options sjrg router-port=rgsj Add r1 port to the join switch: ovn-nbctl lrp-add r1 r1sj 00:00:00:01:00:02 10.1.0.2/30 ovn-nbctl lsp-add sj sjr1 ovn-nbctl lsp-set-type sjr1 router ovn-nbctl lsp-set-addresses sjr1 00:00:00:01:00:02 ovn-nbctl lsp-set-options sjr1 router-port=r1sj Add routes - router gateway towards our 2 private subnets via r1 and r1 default route towards rg: ovn-nbctl lr-route-add rg \"10.0.0.0/24\" 10.1.0.2 ovn-nbctl lr-route-add rg \"10.0.1.0/24\" 10.1.0.2 ovn-nbctl lr-route-add r1 \"0.0.0.0/0\" 10.1.0.1 Verify routes: [root@ovn1 ~]# ovn-nbctl lr-route-list rg IPv4 Routes 10.0.0.0/24 10.1.0.2 dst-ip 10.0.1.0/24 10.1.0.2 dst-ip [root@ovn1 ~]# ovn-nbctl lr-route-list r1 IPv4 Routes 0.0.0.0/0 10.1.0.1 dst-ip [root@ovn1 ~]# ovn-nbctl list Logical_Router_Static_Route _uuid : 9ddd9b58-12c2-43c8-afe2-c20cb71aa824 external_ids : {} ip_prefix : \"0.0.0.0/0\" nexthop : \"10.1.0.1\" output_port : [] policy : [] _uuid : 5e915985-d98c-48bf-ba35-25a10c150bb1 external_ids : {} ip_prefix : \"10.0.0.0/24\" nexthop : \"10.1.0.2\" output_port : [] policy : [] _uuid : a73f670f-d435-471f-9273-58a6b8767f5f external_ids : {} ip_prefix : \"10.0.1.0/24\" nexthop : \"10.1.0.2\" output_port : [] policy : [] Add an external port to rg: ovn-nbctl lrp-add rg rgsp 00:00:00:02:00:ff 10.2.0.254/24 Add provider network switch (sp): ovn-nbctl ls-add sp ovn-nbctl lsp-add sp sprg ovn-nbctl lsp-set-type sprg router ovn-nbctl lsp-set-addresses sprg 00:00:00:02:00:ff ovn-nbctl lsp-set-options sprg router-port=rgsp Add localnet port (eth1) to provider switch (sp): ovn-nbctl lsp-add sp sp-localnet ovn-nbctl lsp-set-addresses sp-localnet unknown ovn-nbctl lsp-set-type sp-localnet localnet ovn-nbctl lsp-set-options sp-localnet network_name=provider Verify on ovn1: ovs-vsctl show ovn-nbctl show ovn-sbctl show [root@ovn1 ~]# ovs-vsctl show 64003a9a-1f2a-403c-8d21-cd187d2f717c Bridge br-int fail_mode: secure Port ovn-8b9ad2-0 Interface ovn-8b9ad2-0 type: geneve options: {csum=\"true\", key=flow, remote_ip=\"192.168.122.202\"} Port ovn-b5337e-0 Interface ovn-b5337e-0 type: geneve options: {csum=\"true\", key=flow, remote_ip=\"192.168.122.203\"} Port br-int Interface br-int type: internal Port patch-br-int-to-sp-localnet Interface patch-br-int-to-sp-localnet type: patch options: {peer=patch-sp-localnet-to-br-int} Bridge br-provider Port eth1 Interface eth1 Port patch-sp-localnet-to-br-int Interface patch-sp-localnet-to-br-int type: patch options: {peer=patch-br-int-to-sp-localnet} Port br-provider Interface br-provider type: internal ovs_version: \"2.13.0\" [root@ovn1 ~]# ovn-nbctl show switch c11aeac8-3469-444d-819e-cd0f50437175 (sp) port sp-localnet type: localnet addresses: [\"unknown\"] port sprg type: router addresses: [\"00:00:00:02:00:ff\"] router-port: rgsp switch 40f56f4d-db18-4d77-bb8b-c93e5179d346 (s0) port s0r1 type: router addresses: [\"00:00:00:00:00:ff\"] router-port: r1s0 port port02 addresses: [\"00:00:00:00:00:02\"] port port01 addresses: [\"00:00:00:00:00:01\"] switch fe514df1-5e66-4962-9962-0ce69436eaf7 (sj) port sjrg type: router addresses: [\"00:00:00:01:00:01\"] router-port: rgsj port sjr1 type: router addresses: [\"00:00:00:01:00:02\"] router-port: r1sj switch 722300a5-73e1-42de-ae19-b7997f1f9a86 (s1) port port12 addresses: [\"00:00:00:00:01:02\"] port s1r1 type: router addresses: [\"00:00:00:00:01:ff\"] router-port: r1s1 port port11 addresses: [\"00:00:00:00:01:01\"] router 3affb86c-5f1a-461d-bd07-b4a9580ed9bf (r1) port r1s0 mac: \"00:00:00:00:00:ff\" networks: [\"10.0.0.254/24\"] port r1sj mac: \"00:00:00:01:00:02\" networks: [\"10.1.0.2/30\"] port r1s1 mac: \"00:00:00:00:01:ff\" networks: [\"10.0.1.254/24\"] router d041e224-b40b-46fd-8888-df5d93362579 (rg) port rgsj mac: \"00:00:00:01:00:01\" networks: [\"10.1.0.1/30\"] port rgsp mac: \"00:00:00:02:00:ff\" networks: [\"10.2.0.254/24\"] [root@ovn1 ~]# ovn-sbctl show Chassis \"b5337e05-2495-4bf0-8728-25840472baa4\" hostname: ovn3 Encap geneve ip: \"192.168.122.203\" options: {csum=\"true\"} Port_Binding port12 Port_Binding port02 Chassis \"c3f90802-4fd5-44ad-a338-154a9150e46f\" hostname: ovn1 Encap geneve ip: \"192.168.122.201\" options: {csum=\"true\"} Port_Binding sjrg Port_Binding rgsp Port_Binding sprg Port_Binding rgsj Chassis \"8b9ad2e3-c1bc-4ea2-973e-a8bd1d38e502\" hostname: ovn2 Encap geneve ip: \"192.168.122.202\" options: {csum=\"true\"} Port_Binding port11 Port_Binding port01 Connect to ovn2 and simulate an external node: nmcli connection delete 76505bbf-7aef-32f0-bae6-9497dcf93d2e ip a a dev eth1 10.2.0.10/24 ip link set dev eth1 up ip route add 10.0.1.0/24 via 10.2.0.254 ip route add 10.1.0.0/24 via 10.2.0.254 ip route add 10.0.0.0/24 via 10.2.0.254 Verfify: [root@ovn2 ~]# ping 10.0.0.1 PING 10.0.0.1 (10.0.0.1) 56(84) bytes of data. 64 bytes from 10.0.0.1: icmp_seq=1 ttl=62 time=3.64 ms ^C --- 10.0.0.1 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 3.642/3.642/3.642/0.000 ms [root@ovn2 ~]# ping 10.0.0.1 -c1 -W1 PING 10.0.0.1 (10.0.0.1) 56(84) bytes of data. 64 bytes from 10.0.0.1: icmp_seq=1 ttl=62 time=1.73 ms --- 10.0.0.1 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 1.734/1.734/1.734/0.000 ms [root@ovn2 ~]# traceroute -n -I 10.0.0.1 traceroute to 10.0.0.1 (10.0.0.1), 30 hops max, 60 byte packets 1 * * * 2 * * * 3 10.0.0.1 10.736 ms 10.726 ms 10.716 ms [root@ovn2 ~]# ip netns exec ns0 ping 10.2.0.10 -c1 -W1 PING 10.2.0.10 (10.2.0.10) 56(84) bytes of data. 64 bytes from 10.2.0.10: icmp_seq=1 ttl=62 time=1.62 ms --- 10.2.0.10 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 1.615/1.615/1.615/0.000 ms [root@ovn2 ~]# ip netns exec ns0 traceroute -n -I 10.2.0.10 traceroute to 10.2.0.10 (10.2.0.10), 30 hops max, 60 byte packets 1 10.0.0.254 2.164 ms 1.773 ms 2.921 ms 2 * * * 3 10.2.0.10 5.078 ms * 4.308 ms [root@ovn2 ~]# ping 10.0.1.2 PING 10.0.1.2 (10.0.1.2) 56(84) bytes of data. 64 bytes from 10.0.1.2: icmp_seq=1 ttl=62 time=4.91 ms 64 bytes from 10.0.1.2: icmp_seq=2 ttl=62 time=1.82 ms ^C --- 10.0.1.2 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1001ms rtt min/avg/max/mdev = 1.815/3.360/4.906/1.545 ms [root@ovn2 ~]# Inspecting tunnel encapsulations Introduction http://www.openvswitch.org/support/dist-docs/ovn-architecture.7.html DESIGN DECISIONS Tunnel Encapsulations OVN annotates logical network packets that it sends from one hypervisor to another with the following three pieces of metadata, which are en\u2010 coded in an encapsulation-specific fashion: \u2022 24-bit logical datapath identifier, from the tunnel_key column in the OVN Southbound Datapath_Binding table. \u2022 15-bit logical ingress port identifier. ID 0 is reserved for internal use within OVN. IDs 1 through 32767, inclu\u2010 sive, may be assigned to logical ports (see the tun\u2010 nel_key column in the OVN Southbound Port_Binding table). \u2022 16-bit logical egress port identifier. IDs 0 through 32767 have the same meaning as for logical ingress ports. IDs 32768 through 65535, inclusive, may be assigned to logical multicast groups (see the tunnel_key column in the OVN Southbound Multicast_Group table). For hypervisor-to-hypervisor traffic, OVN supports only Geneve and STT encapsulations, for the following reasons: \u2022 Only STT and Geneve support the large amounts of metadata (over 32 bits per packet) that OVN uses (as described above). \u2022 STT and Geneve use randomized UDP or TCP source ports that allows efficient distribution among multiple paths in environments that use ECMP in their underlay. \u2022 NICs are available to offload STT and Geneve encapsula\u2010 tion and decapsulation. Due to its flexibility, the preferred encapsulation between hypervisors is Geneve. For Geneve encapsulation, OVN transmits the logical datapath identifier in the Geneve VNI. OVN transmits the logical ingress and logical egress ports in a TLV with class 0x0102, type 0x80, and a 32-bit value encoded as follows, from MSB to LSB: 1 15 16 +---+------------+-----------+ |rsv|ingress port|egress port| +---+------------+-----------+ 0 Environments whose NICs lack Geneve offload may prefer STT encapsula\u2010 tion for performance reasons. For STT encapsulation, OVN encodes all three pieces of logical metadata in the STT 64-bit tunnel ID as fol\u2010 lows, from MSB to LSB: 9 15 16 24 +--------+------------+-----------+--------+ |reserved|ingress port|egress port|datapath| +--------+------------+-----------+--------+ 0 For connecting to gateways, in addition to Geneve and STT, OVN supports VXLAN, because only VXLAN support is common on top-of-rack (ToR) switches. Currently, gateways have a feature set that matches the capa\u2010 bilities as defined by the VTEP schema, so fewer bits of metadata are necessary. In the future, gateways that do not support encapsulations with large amounts of metadata may continue to have a reduced feature set. Distributed gateway router https://developers.redhat.com/blog/2018/11/08/how-to-create-an-open-virtual-network-distributed-gateway-router/ Resources https://blog.scottlowe.org/2016/12/09/using-ovn-with-kvm-libvirt/ https://baturin.org/docs/iproute2/ http://dani.foroselectronica.es/multinode-ovn-setup-509/ https://hustcat.github.io/ovn-gateway-practice/ http://www.openvswitch.org/support/dist-docs/ovn-architecture.7.html https://www.redhat.com/en/blog/what-geneve https://tools.ietf.org/html/draft-davie-stt-08 https://tools.ietf.org/html/draft-ietf-nvo3-geneve-16 https://developers.redhat.com/blog/2018/11/08/how-to-create-an-open-virtual-network-distributed-gateway-router/ http://docs.openvswitch.org/en/latest/intro/install/bash-completion/ https://blog.russellbryant.net/2017/05/30/ovn-geneve-vs-vxlan-does-it-matter/","title":"OVN standalone on Fedora 31"},{"location":"networking/ovn_standalone_on_fedora31/#ovn-standalone-on-fedora-31","text":"","title":"OVN standalone on Fedora 31"},{"location":"networking/ovn_standalone_on_fedora31/#base-setup","text":"","title":"Base setup"},{"location":"networking/ovn_standalone_on_fedora31/#single-node-db-on-ovn1","text":"","title":"Single node DB on ovn1"},{"location":"networking/ovn_standalone_on_fedora31/#configuration-and-installation","text":"The following starts an OVN cluster on Fedora 31 with 3 nodes. Non-clustered DB with ovn1 as the DB node, and ovn2 and ovn3 join ovn1's DB. All nodes have 2 interfaces. One management interface on eth0. DNS names ovn1, ovn2, ovn3 map to the IP addresses on eth0. Interface eth1 has no IP addresses and will be used for the dataplane (attached to br-provider). IP address configuration: [root@ovn1 ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=\"eth0\" BOOTPROTO=\"static\" ONBOOT=\"yes\" TYPE=\"Ethernet\" PREFIX=24 IPADDR=192.168.122.201 GATEWAY=192.168.122.1 DNS1=192.168.122.1 [root@ovn2 ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=\"eth0\" BOOTPROTO=\"static\" ONBOOT=\"yes\" TYPE=\"Ethernet\" PREFIX=24 IPADDR=192.168.122.202 GATEWAY=192.168.122.1 DNS1=192.168.122.1 [root@ovn3 ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=\"eth0\" BOOTPROTO=\"static\" ONBOOT=\"yes\" TYPE=\"Ethernet\" PREFIX=24 IPADDR=192.168.122.203 GATEWAY=192.168.122.1 DNS1=192.168.122.1 Hostname to IP address mapping: # cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.122.201 ovn1 # eth0 192.168.122.202 ovn2 # eth0 192.168.122.203 ovn3 # eth0 Disable NM control over eth1 on all hosts: echo \"NM_CONTROLLED=no\" > /etc/sysconfig/network-scripts/ifcfg-eth1 Install Open vSwitch and OVN on all nodes: yum install ovn -y yum install ovn-central -y yum install ovn-host -y Enable Open vSwitch and ovn-controller on all hosts and start right away: systemctl enable --now openvswitch systemctl enable --now ovn-controller On the master nodes that are to hold the ovn-northd \"control plane\", execute: echo 'OVN_NORTHD_OPTS=\"--db-nb-addr=ovn1 --db-nb-create-insecure-remote=yes --db-sb-addr=ovn1 --db-sb-create-insecure-remote=yes --db-nb-cluster-local-addr=ovn1 --db-sb-cluster-local-addr=ovn1 --ovn-northd-nb-db=tcp:ovn1:6641 --ovn-northd-sb-db=tcp:ovn1:6642\"' >> /etc/sysconfig/ovn systemctl enable --now ovn-northd Note that the above will start a single node cluster. The OVN man page contains an example for a full 3 node clustered DB: http://www.openvswitch.org/support/dist-docs/ovn-ctl.8.txt Now, configure OVS to connect to the OVN DBs. Note that geneve / vxlan tunnel needs IP addresses, DNS entries do not work: On master node ovn1: # [root@ovn1 ~]# ovs-vsctl set open . external-ids:ovn-remote=tcp:ovn1:6642 ovs-vsctl set open . external-ids:ovn-encap-type=geneve # could also be: ovs-vsctl set open . external-ids:ovn-encap-type=geneve,vxlan ovs-vsctl set open . external-ids:ovn-encap-ip=192.168.122.201 ovs-vsctl set open . external-ids:ovn-bridge=br-int On slave node ovn2: # [root@ovn2 ~]# ovs-vsctl set open . external-ids:ovn-remote=tcp:ovn1:6642 ovs-vsctl set open . external-ids:ovn-encap-ip=192.168.122.202 ovs-vsctl set open . external-ids:ovn-encap-type=geneve # could also be: ovs-vsctl set open . external-ids:ovn-encap-type=geneve,vxlan ovs-vsctl set open . external-ids:ovn-bridge=br-int On slave node ovn3: # [root@ovn3 ~]# ovs-vsctl set open . external-ids:ovn-remote=tcp:ovn1:6642 ovs-vsctl set open . external-ids:ovn-encap-ip=192.168.122.203 ovs-vsctl set open . external-ids:ovn-encap-type=geneve # could also be: ovs-vsctl set open . external-ids:ovn-encap-type=geneve,vxlan ovs-vsctl set open . external-ids:ovn-bridge=br-int","title":"Configuration and installation"},{"location":"networking/ovn_standalone_on_fedora31/#verification","text":"Verify on ovn1: ovs-vsctl show ovn-sbctl show ovn2: ovs-vsctl show timeout 2 ovn-sbctl show timeout 2 ovn-nbctl show ovn3: ovs-vsctl show timeout 2 ovn-sbctl show timeout 2 ovn-nbctl show [root@ovn1 ~]# ovs-vsctl show 64003a9a-1f2a-403c-8d21-cd187d2f717c Bridge br-int fail_mode: secure Port ovn-b5337e-0 Interface ovn-b5337e-0 type: geneve options: {csum=\"true\", key=flow, remote_ip=\"192.168.122.203\"} Port br-int Interface br-int type: internal Port ovn-8b9ad2-0 Interface ovn-8b9ad2-0 type: geneve options: {csum=\"true\", key=flow, remote_ip=\"192.168.122.202\"} ovs_version: \"2.13.0\" [root@ovn1 ~]# ovn-sbctl show Chassis \"b5337e05-2495-4bf0-8728-25840472baa4\" hostname: ovn3 Encap geneve ip: \"192.168.122.203\" options: {csum=\"true\"} Chassis \"c3f90802-4fd5-44ad-a338-154a9150e46f\" hostname: ovn1 Encap geneve ip: \"192.168.122.201\" options: {csum=\"true\"} Chassis \"8b9ad2e3-c1bc-4ea2-973e-a8bd1d38e502\" hostname: ovn2 Encap geneve ip: \"192.168.122.202\" options: {csum=\"true\"} [root@ovn2 ~]# ovs-vsctl show 33e3d97e-ee6a-4eeb-ac2a-188744895be7 Bridge br-int fail_mode: secure Port ovn-c3f908-0 Interface ovn-c3f908-0 type: geneve options: {csum=\"true\", key=flow, remote_ip=\"192.168.122.201\"} Port br-int Interface br-int type: internal Port ovn-b5337e-0 Interface ovn-b5337e-0 type: geneve options: {csum=\"true\", key=flow, remote_ip=\"192.168.122.203\"} ovs_version: \"2.13.0\" [root@ovn2 ~]# timeout 2 ovn-sbctl show 2020-07-21T11:13:15Z|00001|fatal_signal|WARN|terminating with signal 15 (Terminated) [root@ovn2 ~]# timeout 2 ovn-nbctl show ovn-nbctl: unix:/var/run/ovn/ovnnb_db.sock: database connection failed (No such file or directory) [root@ovn2 ~]# [root@ovn3 ~]# ovs-vsctl show a03dc1d7-a6f7-4400-b696-59ccda197a19 Bridge br-int fail_mode: secure Port ovn-c3f908-0 Interface ovn-c3f908-0 type: geneve options: {csum=\"true\", key=flow, remote_ip=\"192.168.122.201\"} Port br-int Interface br-int type: internal Port ovn-8b9ad2-0 Interface ovn-8b9ad2-0 type: geneve options: {csum=\"true\", key=flow, remote_ip=\"192.168.122.202\"} ovs_version: \"2.13.0\" [root@ovn3 ~]# timeout 2 ovn-sbctl show 2020-07-21T11:13:21Z|00001|fatal_signal|WARN|terminating with signal 15 (Terminated) [root@ovn3 ~]# timeout 2 ovn-nbctl show ovn-nbctl: unix:/var/run/ovn/ovnnb_db.sock: database connection failed (No such file or directory) [root@ovn3 ~]#","title":"Verification"},{"location":"networking/ovn_standalone_on_fedora31/#adding-virtual-network-with-one-virtual-switch-s0","text":"On ovn1, configure a logical switch and port: ovn-nbctl ls-add s0 ovn-nbctl lsp-add s0 port01 ovn-nbctl lsp-set-addresses port01 00:00:00:00:00:01 ovn-nbctl lsp-add s0 port02 ovn-nbctl lsp-set-addresses port02 00:00:00:00:00:02 Now, \"real\" ports need to be wired to the above ports. Note that the logical port name has to match the external_ids:iface-id identifier. If we added ovn-nbctl lsp-add s0 foo instead of port0 , then we would have to set ovs-vsctl set Interface port0 external_ids:iface-id=foo on ovn2. Note that we need to know the MAC address of that port. Therefore, when creating the veth, we are making sure to create it with the correct MAC address. On ovn2, execute: ip link add name veth01 type veth peer name port01 ip netns add ns0 ip link set dev veth01 netns ns0 ip netns exec ns0 ip link set dev lo up ip netns exec ns0 ip link set dev veth01 up ip netns exec ns0 ip link set veth01 address 00:00:00:00:00:01 ip netns exec ns0 ip address add 10.0.0.1/24 dev veth01 ip link set dev port01 up ovs-vsctl add-port br-int port01 ovs-vsctl set Interface port01 external_ids:iface-id=port01 On ovn3, execute: ip link add name veth02 type veth peer name port02 ip netns add ns0 ip link set dev veth02 netns ns0 ip netns exec ns0 ip link set dev lo up ip netns exec ns0 ip link set dev veth02 up ip netns exec ns0 ip link set veth02 address 00:00:00:00:00:02 ip netns exec ns0 ip address add 10.0.0.2/24 dev veth02 ip link set dev port02 up ovs-vsctl add-port br-int port02 external_ids:iface-id=port02 ovs-vsctl set Interface port02 external_ids:iface-id=port02 Verify the new configuration on ovn1: ovn-nbctl show ovn-sbctl show [root@ovn1 ~]# ovn-nbctl show switch 40f56f4d-db18-4d77-bb8b-c93e5179d346 (s0) port port02 addresses: [\"00:00:00:00:00:02\"] port port01 addresses: [\"00:00:00:00:00:01\"] [root@ovn1 ~]# ovn-sbctl show Chassis \"b5337e05-2495-4bf0-8728-25840472baa4\" hostname: ovn3 Encap geneve ip: \"192.168.122.203\" options: {csum=\"true\"} Port_Binding port02 Chassis \"c3f90802-4fd5-44ad-a338-154a9150e46f\" hostname: ovn1 Encap geneve ip: \"192.168.122.201\" options: {csum=\"true\"} Chassis \"8b9ad2e3-c1bc-4ea2-973e-a8bd1d38e502\" hostname: ovn2 Encap geneve ip: \"192.168.122.202\" options: {csum=\"true\"} Port_Binding port01 Note: The southbound database now shows port bindings. Verify logical flows: ovn-sbctl lflow-list]# [root@ovn1 ~]# ovn-sbctl lflow-list Datapath: \"s0\" (63c97e9f-194e-49b1-b075-661cd2132895) Pipeline: ingress table=0 (ls_in_port_sec_l2 ), priority=100 , match=(eth.src[40]), action=(drop;) table=0 (ls_in_port_sec_l2 ), priority=100 , match=(vlan.present), action=(drop;) table=0 (ls_in_port_sec_l2 ), priority=50 , match=(inport == \"port01\"), action=(next;) table=0 (ls_in_port_sec_l2 ), priority=50 , match=(inport == \"port02\"), action=(next;) table=1 (ls_in_port_sec_ip ), priority=0 , match=(1), action=(next;) table=2 (ls_in_port_sec_nd ), priority=0 , match=(1), action=(next;) table=3 (ls_in_pre_acl ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=3 (ls_in_pre_acl ), priority=0 , match=(1), action=(next;) table=4 (ls_in_pre_lb ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=4 (ls_in_pre_lb ), priority=110 , match=(nd || nd_rs || nd_ra), action=(next;) table=4 (ls_in_pre_lb ), priority=0 , match=(1), action=(next;) table=5 (ls_in_pre_stateful ), priority=100 , match=(reg0[0] == 1), action=(ct_next;) table=5 (ls_in_pre_stateful ), priority=0 , match=(1), action=(next;) table=6 (ls_in_acl ), priority=34000, match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=6 (ls_in_acl ), priority=0 , match=(1), action=(next;) table=7 (ls_in_qos_mark ), priority=0 , match=(1), action=(next;) table=8 (ls_in_qos_meter ), priority=0 , match=(1), action=(next;) table=9 (ls_in_lb ), priority=0 , match=(1), action=(next;) table=10(ls_in_stateful ), priority=100 , match=(reg0[1] == 1), action=(ct_commit(ct_label=0/1); next;) table=10(ls_in_stateful ), priority=100 , match=(reg0[2] == 1), action=(ct_lb;) table=10(ls_in_stateful ), priority=0 , match=(1), action=(next;) table=11(ls_in_pre_hairpin ), priority=0 , match=(1), action=(next;) table=12(ls_in_hairpin ), priority=1 , match=(reg0[6] == 1), action=(eth.dst <-> eth.src;outport = inport;flags.loopback = 1;output;) table=12(ls_in_hairpin ), priority=0 , match=(1), action=(next;) table=13(ls_in_arp_rsp ), priority=0 , match=(1), action=(next;) table=14(ls_in_dhcp_options ), priority=0 , match=(1), action=(next;) table=15(ls_in_dhcp_response), priority=0 , match=(1), action=(next;) table=16(ls_in_dns_lookup ), priority=0 , match=(1), action=(next;) table=17(ls_in_dns_response ), priority=0 , match=(1), action=(next;) table=18(ls_in_external_port), priority=0 , match=(1), action=(next;) table=19(ls_in_l2_lkup ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(handle_svc_check(inport);) table=19(ls_in_l2_lkup ), priority=70 , match=(eth.mcast), action=(outport = \"_MC_flood\"; output;) table=19(ls_in_l2_lkup ), priority=50 , match=(eth.dst == 00:00:00:00:00:01), action=(outport = \"port01\"; output;) table=19(ls_in_l2_lkup ), priority=50 , match=(eth.dst == 00:00:00:00:00:02), action=(outport = \"port02\"; output;) Datapath: \"s0\" (63c97e9f-194e-49b1-b075-661cd2132895) Pipeline: egress table=0 (ls_out_pre_lb ), priority=110 , match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=0 (ls_out_pre_lb ), priority=110 , match=(nd || nd_rs || nd_ra), action=(next;) table=0 (ls_out_pre_lb ), priority=0 , match=(1), action=(next;) table=1 (ls_out_pre_acl ), priority=110 , match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=1 (ls_out_pre_acl ), priority=0 , match=(1), action=(next;) table=2 (ls_out_pre_stateful), priority=100 , match=(reg0[0] == 1), action=(ct_next;) table=2 (ls_out_pre_stateful), priority=0 , match=(1), action=(next;) table=3 (ls_out_lb ), priority=0 , match=(1), action=(next;) table=4 (ls_out_acl ), priority=34000, match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=4 (ls_out_acl ), priority=0 , match=(1), action=(next;) table=5 (ls_out_qos_mark ), priority=0 , match=(1), action=(next;) table=6 (ls_out_qos_meter ), priority=0 , match=(1), action=(next;) table=7 (ls_out_stateful ), priority=100 , match=(reg0[1] == 1), action=(ct_commit(ct_label=0/1); next;) table=7 (ls_out_stateful ), priority=100 , match=(reg0[2] == 1), action=(ct_lb;) table=7 (ls_out_stateful ), priority=0 , match=(1), action=(next;) table=8 (ls_out_port_sec_ip ), priority=0 , match=(1), action=(next;) table=9 (ls_out_port_sec_l2 ), priority=100 , match=(eth.mcast), action=(output;) table=9 (ls_out_port_sec_l2 ), priority=50 , match=(outport == \"port01\"), action=(output;) table=9 (ls_out_port_sec_l2 ), priority=50 , match=(outport == \"port02\"), action=(output;) Last but not least, run a test ping between the namespaces on both hosts: [root@ovn2 ~]# ip netns exec ns0 ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 8: veth01@if7: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 00:00:00:00:00:01 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.0.0.1/24 scope global veth01 valid_lft forever preferred_lft forever inet6 fe80::200:ff:fe00:1/64 scope link valid_lft forever preferred_lft forever [root@ovn2 ~]# ip netns exec ns0 ping 10.0.0.2 -c1 -W1 PING 10.0.0.2 (10.0.0.2) 56(84) bytes of data. 64 bytes from 10.0.0.2: icmp_seq=1 ttl=64 time=1.41 ms --- 10.0.0.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 1.413/1.413/1.413/0.000 ms [root@ovn2 ~]# [root@ovn3 ~]# ip netns exec ns0 ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 8: veth02@if7: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 00:00:00:00:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.0.0.2/24 scope global veth02 valid_lft forever preferred_lft forever inet6 fe80::200:ff:fe00:2/64 scope link valid_lft forever preferred_lft forever [root@ovn3 ~]# ip netns exec ns0 ping 10.0.0.1 -c1 -W1 PING 10.0.0.1 (10.0.0.1) 56(84) bytes of data. 64 bytes from 10.0.0.1: icmp_seq=1 ttl=64 time=0.886 ms --- 10.0.0.1 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.886/0.886/0.886/0.000 ms [root@ovn3 ~]#","title":"Adding virtual network with one virtual switch s0"},{"location":"networking/ovn_standalone_on_fedora31/#adding-virtual-network-with-one-virtual-switch-s1","text":"On ovn1, configure a logical switch and port: ovn-nbctl ls-add s1 ovn-nbctl lsp-add s1 port11 ovn-nbctl lsp-set-addresses port11 00:00:00:00:01:01 ovn-nbctl lsp-add s1 port12 ovn-nbctl lsp-set-addresses port12 00:00:00:00:01:02 Now, \"real\" ports need to be wired to the above ports. Note that the logical port name has to match the external_ids:iface-id identifier. If we added ovn-nbctl lsp-add s1 foo instead of port0 , then we would have to set ovs-vsctl set Interface port0 external_ids:iface-id=foo on ovn2. Note that we need to know the MAC address of that port. Therefore, when creating the veth, we are making sure to create it with the correct MAC address. On ovn2, execute: ip link add name veth11 type veth peer name port11 ip netns add ns1 ip link set dev veth11 netns ns1 ip netns exec ns1 ip link set dev lo up ip netns exec ns1 ip link set dev veth11 up ip netns exec ns1 ip link set veth11 address 00:00:00:00:01:01 ip netns exec ns1 ip address add 10.0.1.1/24 dev veth11 ip link set dev port11 up ovs-vsctl add-port br-int port11 ovs-vsctl set Interface port11 external_ids:iface-id=port11 On ovn3, execute: ip link add name veth12 type veth peer name port12 ip netns add ns1 ip link set dev veth12 netns ns1 ip netns exec ns1 ip link set dev lo up ip netns exec ns1 ip link set dev veth12 up ip netns exec ns1 ip link set veth12 address 00:00:00:00:01:02 ip netns exec ns1 ip address add 10.0.1.2/24 dev veth12 ip link set dev port12 up ovs-vsctl add-port br-int port12 external_ids:iface-id=port12 ovs-vsctl set Interface port12 external_ids:iface-id=port12 Verify the new configuration on ovn1: ovn-nbctl show ovn-sbctl show [root@ovn1 ~]# ovn-nbctl show switch 722300a5-73e1-42de-ae19-b7997f1f9a86 (s1) port port12 addresses: [\"00:00:00:00:01:02\"] port port11 addresses: [\"00:00:00:00:01:01\"] switch 40f56f4d-db18-4d77-bb8b-c93e5179d346 (s0) port port02 addresses: [\"00:00:00:00:00:02\"] port port01 addresses: [\"00:00:00:00:00:01\"] [root@ovn1 ~]# ovn-sbctl show Chassis \"b5337e05-2495-4bf0-8728-25840472baa4\" hostname: ovn3 Encap geneve ip: \"192.168.122.203\" options: {csum=\"true\"} Port_Binding port12 Port_Binding port02 Chassis \"c3f90802-4fd5-44ad-a338-154a9150e46f\" hostname: ovn1 Encap geneve ip: \"192.168.122.201\" options: {csum=\"true\"} Chassis \"8b9ad2e3-c1bc-4ea2-973e-a8bd1d38e502\" hostname: ovn2 Encap geneve ip: \"192.168.122.202\" options: {csum=\"true\"} Port_Binding port01 Port_Binding port11 Note: The southbound database now shows port bindings. Verify logical flows: ovn-sbctl lflow-list [root@ovn1 ~]# ovn-sbctl lflow-list Datapath: \"s0\" (63c97e9f-194e-49b1-b075-661cd2132895) Pipeline: ingress table=0 (ls_in_port_sec_l2 ), priority=100 , match=(eth.src[40]), action=(drop;) table=0 (ls_in_port_sec_l2 ), priority=100 , match=(vlan.present), action=(drop;) table=0 (ls_in_port_sec_l2 ), priority=50 , match=(inport == \"port01\"), action=(next;) table=0 (ls_in_port_sec_l2 ), priority=50 , match=(inport == \"port02\"), action=(next;) table=1 (ls_in_port_sec_ip ), priority=0 , match=(1), action=(next;) table=2 (ls_in_port_sec_nd ), priority=0 , match=(1), action=(next;) table=3 (ls_in_pre_acl ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=3 (ls_in_pre_acl ), priority=0 , match=(1), action=(next;) table=4 (ls_in_pre_lb ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=4 (ls_in_pre_lb ), priority=110 , match=(nd || nd_rs || nd_ra), action=(next;) table=4 (ls_in_pre_lb ), priority=0 , match=(1), action=(next;) table=5 (ls_in_pre_stateful ), priority=100 , match=(reg0[0] == 1), action=(ct_next;) table=5 (ls_in_pre_stateful ), priority=0 , match=(1), action=(next;) table=6 (ls_in_acl ), priority=34000, match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=6 (ls_in_acl ), priority=0 , match=(1), action=(next;) table=7 (ls_in_qos_mark ), priority=0 , match=(1), action=(next;) table=8 (ls_in_qos_meter ), priority=0 , match=(1), action=(next;) table=9 (ls_in_lb ), priority=0 , match=(1), action=(next;) table=10(ls_in_stateful ), priority=100 , match=(reg0[1] == 1), action=(ct_commit(ct_label=0/1); next;) table=10(ls_in_stateful ), priority=100 , match=(reg0[2] == 1), action=(ct_lb;) table=10(ls_in_stateful ), priority=0 , match=(1), action=(next;) table=11(ls_in_pre_hairpin ), priority=0 , match=(1), action=(next;) table=12(ls_in_hairpin ), priority=1 , match=(reg0[6] == 1), action=(eth.dst <-> eth.src;outport = inport;flags.loopback = 1;output;) table=12(ls_in_hairpin ), priority=0 , match=(1), action=(next;) table=13(ls_in_arp_rsp ), priority=0 , match=(1), action=(next;) table=14(ls_in_dhcp_options ), priority=0 , match=(1), action=(next;) table=15(ls_in_dhcp_response), priority=0 , match=(1), action=(next;) table=16(ls_in_dns_lookup ), priority=0 , match=(1), action=(next;) table=17(ls_in_dns_response ), priority=0 , match=(1), action=(next;) table=18(ls_in_external_port), priority=0 , match=(1), action=(next;) table=19(ls_in_l2_lkup ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(handle_svc_check(inport);) table=19(ls_in_l2_lkup ), priority=70 , match=(eth.mcast), action=(outport = \"_MC_flood\"; output;) table=19(ls_in_l2_lkup ), priority=50 , match=(eth.dst == 00:00:00:00:00:01), action=(outport = \"port01\"; output;) table=19(ls_in_l2_lkup ), priority=50 , match=(eth.dst == 00:00:00:00:00:02), action=(outport = \"port02\"; output;) Datapath: \"s0\" (63c97e9f-194e-49b1-b075-661cd2132895) Pipeline: egress table=0 (ls_out_pre_lb ), priority=110 , match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=0 (ls_out_pre_lb ), priority=110 , match=(nd || nd_rs || nd_ra), action=(next;) table=0 (ls_out_pre_lb ), priority=0 , match=(1), action=(next;) table=1 (ls_out_pre_acl ), priority=110 , match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=1 (ls_out_pre_acl ), priority=0 , match=(1), action=(next;) table=2 (ls_out_pre_stateful), priority=100 , match=(reg0[0] == 1), action=(ct_next;) table=2 (ls_out_pre_stateful), priority=0 , match=(1), action=(next;) table=3 (ls_out_lb ), priority=0 , match=(1), action=(next;) table=4 (ls_out_acl ), priority=34000, match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=4 (ls_out_acl ), priority=0 , match=(1), action=(next;) table=5 (ls_out_qos_mark ), priority=0 , match=(1), action=(next;) table=6 (ls_out_qos_meter ), priority=0 , match=(1), action=(next;) table=7 (ls_out_stateful ), priority=100 , match=(reg0[1] == 1), action=(ct_commit(ct_label=0/1); next;) table=7 (ls_out_stateful ), priority=100 , match=(reg0[2] == 1), action=(ct_lb;) table=7 (ls_out_stateful ), priority=0 , match=(1), action=(next;) table=8 (ls_out_port_sec_ip ), priority=0 , match=(1), action=(next;) table=9 (ls_out_port_sec_l2 ), priority=100 , match=(eth.mcast), action=(output;) table=9 (ls_out_port_sec_l2 ), priority=50 , match=(outport == \"port01\"), action=(output;) table=9 (ls_out_port_sec_l2 ), priority=50 , match=(outport == \"port02\"), action=(output;) Datapath: \"s1\" (7ba84139-1ab7-47fe-874c-6956be31ab0a) Pipeline: ingress table=0 (ls_in_port_sec_l2 ), priority=100 , match=(eth.src[40]), action=(drop;) table=0 (ls_in_port_sec_l2 ), priority=100 , match=(vlan.present), action=(drop;) table=0 (ls_in_port_sec_l2 ), priority=50 , match=(inport == \"port11\"), action=(next;) table=0 (ls_in_port_sec_l2 ), priority=50 , match=(inport == \"port12\"), action=(next;) table=1 (ls_in_port_sec_ip ), priority=0 , match=(1), action=(next;) table=2 (ls_in_port_sec_nd ), priority=0 , match=(1), action=(next;) table=3 (ls_in_pre_acl ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=3 (ls_in_pre_acl ), priority=0 , match=(1), action=(next;) table=4 (ls_in_pre_lb ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=4 (ls_in_pre_lb ), priority=110 , match=(nd || nd_rs || nd_ra), action=(next;) table=4 (ls_in_pre_lb ), priority=0 , match=(1), action=(next;) table=5 (ls_in_pre_stateful ), priority=100 , match=(reg0[0] == 1), action=(ct_next;) table=5 (ls_in_pre_stateful ), priority=0 , match=(1), action=(next;) table=6 (ls_in_acl ), priority=34000, match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=6 (ls_in_acl ), priority=0 , match=(1), action=(next;) table=7 (ls_in_qos_mark ), priority=0 , match=(1), action=(next;) table=8 (ls_in_qos_meter ), priority=0 , match=(1), action=(next;) table=9 (ls_in_lb ), priority=0 , match=(1), action=(next;) table=10(ls_in_stateful ), priority=100 , match=(reg0[1] == 1), action=(ct_commit(ct_label=0/1); next;) table=10(ls_in_stateful ), priority=100 , match=(reg0[2] == 1), action=(ct_lb;) table=10(ls_in_stateful ), priority=0 , match=(1), action=(next;) table=11(ls_in_pre_hairpin ), priority=0 , match=(1), action=(next;) table=12(ls_in_hairpin ), priority=1 , match=(reg0[6] == 1), action=(eth.dst <-> eth.src;outport = inport;flags.loopback = 1;output;) table=12(ls_in_hairpin ), priority=0 , match=(1), action=(next;) table=13(ls_in_arp_rsp ), priority=0 , match=(1), action=(next;) table=14(ls_in_dhcp_options ), priority=0 , match=(1), action=(next;) table=15(ls_in_dhcp_response), priority=0 , match=(1), action=(next;) table=16(ls_in_dns_lookup ), priority=0 , match=(1), action=(next;) table=17(ls_in_dns_response ), priority=0 , match=(1), action=(next;) table=18(ls_in_external_port), priority=0 , match=(1), action=(next;) table=19(ls_in_l2_lkup ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(handle_svc_check(inport);) table=19(ls_in_l2_lkup ), priority=70 , match=(eth.mcast), action=(outport = \"_MC_flood\"; output;) table=19(ls_in_l2_lkup ), priority=50 , match=(eth.dst == 00:00:00:00:01:01), action=(outport = \"port11\"; output;) table=19(ls_in_l2_lkup ), priority=50 , match=(eth.dst == 00:00:00:00:01:02), action=(outport = \"port12\"; output;) Datapath: \"s1\" (7ba84139-1ab7-47fe-874c-6956be31ab0a) Pipeline: egress table=0 (ls_out_pre_lb ), priority=110 , match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=0 (ls_out_pre_lb ), priority=110 , match=(nd || nd_rs || nd_ra), action=(next;) table=0 (ls_out_pre_lb ), priority=0 , match=(1), action=(next;) table=1 (ls_out_pre_acl ), priority=110 , match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=1 (ls_out_pre_acl ), priority=0 , match=(1), action=(next;) table=2 (ls_out_pre_stateful), priority=100 , match=(reg0[0] == 1), action=(ct_next;) table=2 (ls_out_pre_stateful), priority=0 , match=(1), action=(next;) table=3 (ls_out_lb ), priority=0 , match=(1), action=(next;) table=4 (ls_out_acl ), priority=34000, match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=4 (ls_out_acl ), priority=0 , match=(1), action=(next;) table=5 (ls_out_qos_mark ), priority=0 , match=(1), action=(next;) table=6 (ls_out_qos_meter ), priority=0 , match=(1), action=(next;) table=7 (ls_out_stateful ), priority=100 , match=(reg0[1] == 1), action=(ct_commit(ct_label=0/1); next;) table=7 (ls_out_stateful ), priority=100 , match=(reg0[2] == 1), action=(ct_lb;) table=7 (ls_out_stateful ), priority=0 , match=(1), action=(next;) table=8 (ls_out_port_sec_ip ), priority=0 , match=(1), action=(next;) table=9 (ls_out_port_sec_l2 ), priority=100 , match=(eth.mcast), action=(output;) table=9 (ls_out_port_sec_l2 ), priority=50 , match=(outport == \"port11\"), action=(output;) table=9 (ls_out_port_sec_l2 ), priority=50 , match=(outport == \"port12\"), action=(output;) Last but not least, run a test ping between the namespaces on both hosts: [root@ovn2 ~]# ip netns exec ns1 ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 10: veth11@if9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 00:00:00:00:01:01 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.0.1.1/24 scope global veth11 valid_lft forever preferred_lft forever inet6 fe80::200:ff:fe00:101/64 scope link valid_lft forever preferred_lft forever [root@ovn2 ~]# ip netns exec ns1 ping 10.0.1.2 -c1 -W1 PING 10.0.1.2 (10.0.1.2) 56(84) bytes of data. 64 bytes from 10.0.1.2: icmp_seq=1 ttl=64 time=1.83 ms --- 10.0.1.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 1.833/1.833/1.833/0.000 ms [root@ovn2 ~]# [root@ovn3 ~]# ip netns exec ns1 ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 10: veth12@if9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 00:00:00:00:01:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.0.1.2/24 scope global veth12 valid_lft forever preferred_lft forever inet6 fe80::200:ff:fe00:102/64 scope link valid_lft forever preferred_lft forever [root@ovn3 ~]# ip netns exec ns1 ping 10.0.1.1 -c1 -W1 PING 10.0.1.1 (10.0.1.1) 56(84) bytes of data. 64 bytes from 10.0.1.1: icmp_seq=1 ttl=64 time=0.916 ms --- 10.0.1.1 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.916/0.916/0.916/0.000 ms [root@ovn3 ~]#","title":"Adding virtual network with one virtual switch s1"},{"location":"networking/ovn_standalone_on_fedora31/#adding-virtual-router-r1","text":"","title":"Adding virtual router r1 ##"},{"location":"networking/ovn_standalone_on_fedora31/#introduction","text":"http://www.openvswitch.org/support/dist-docs/ovn-architecture.7.html Logical Routers and Logical Patch Ports Typically logical routers and logical patch ports do not have a physi\u2010 cal location and effectively reside on every hypervisor. This is the case for logical patch ports between logical routers and logical switches behind those logical routers, to which VMs (and VIFs) attach. Consider a packet sent from one virtual machine or container to another VM or container that resides on a different subnet. The packet will traverse tables 0 to 65 as described in the previous section Architec\u2010 tural Physical Life Cycle of a Packet, using the logical datapath rep\u2010 resenting the logical switch that the sender is attached to. At table 32, the packet will use the fallback flow that resubmits locally to ta\u2010 ble 33 on the same hypervisor. In this case, all of the processing from table 0 to table 65 occurs on the hypervisor where the sender resides. When the packet reaches table 65, the logical egress port is a logical patch port. The implementation in table 65 differs depending on the OVS version, although the observed behavior is meant to be the same: \u2022 In OVS versions 2.6 and earlier, table 65 outputs to an OVS patch port that represents the logical patch port. The packet re-enters the OpenFlow flow table from the OVS patch port\u2019s peer in table 0, which identifies the logi\u2010 cal datapath and logical input port based on the OVS patch port\u2019s OpenFlow port number. \u2022 In OVS versions 2.7 and later, the packet is cloned and resubmitted directly to the first OpenFlow flow table in the ingress pipeline, setting the logical ingress port to the peer logical patch port, and using the peer logical patch port\u2019s logical datapath (that represents the logi\u2010 cal router). The packet re-enters the ingress pipeline in order to traverse tables 8 to 65 again, this time using the logical datapath representing the log\u2010 ical router. The processing continues as described in the previous sec\u2010 tion Architectural Physical Life Cycle of a Packet. When the packet reachs table 65, the logical egress port will once again be a logical patch port. In the same manner as described above, this logical patch port will cause the packet to be resubmitted to OpenFlow tables 8 to 65, this time using the logical datapath representing the logical switch that the destination VM or container is attached to. The packet traverses tables 8 to 65 a third and final time. If the des\u2010 tination VM or container resides on a remote hypervisor, then table 32 will send the packet on a tunnel port from the sender\u2019s hypervisor to the remote hypervisor. Finally table 65 will output the packet directly to the destination VM or container. The following sections describe two exceptions, where logical routers and/or logical patch ports are associated with a physical location.","title":"Introduction"},{"location":"networking/ovn_standalone_on_fedora31/#setup","text":"Configure namespace routes for virtual routing - on both hosts, run: ip netns exec ns0 ip r add default via 10.0.0.254 ip netns exec ns1 ip r add default via 10.0.1.254 Now, set up the OVN northbound db - router and router ports: ovn-nbctl lr-add r1 ovn-nbctl lrp-add r1 r1s0 00:00:00:00:00:ff 10.0.0.254/24 ovn-nbctl lsp-add s0 s0r1 ovn-nbctl lsp-set-addresses s0r1 00:00:00:00:00:ff ovn-nbctl lsp-set-options s0r1 router-port=r1s0 ovn-nbctl lsp-set-type s0r1 router ovn-nbctl lrp-add r1 r1s1 00:00:00:00:01:ff 10.0.1.254/24 ovn-nbctl lsp-add s1 s1r1 ovn-nbctl lsp-set-addresses s1r1 00:00:00:00:01:ff ovn-nbctl lsp-set-options s1r1 router-port=r1s1 ovn-nbctl lsp-set-type s1r1 router Verify the new configuration: ovn-nbctl show ovn-sbctl show [root@ovn1 ~]# ovn-nbctl show switch 722300a5-73e1-42de-ae19-b7997f1f9a86 (s1) port port12 addresses: [\"00:00:00:00:01:02\"] port s1r1 type: router addresses: [\"00:00:00:00:01:ff\"] router-port: r1s1 port port11 addresses: [\"00:00:00:00:01:01\"] switch 40f56f4d-db18-4d77-bb8b-c93e5179d346 (s0) port s0r1 type: router addresses: [\"00:00:00:00:00:ff\"] router-port: r1s0 port port02 addresses: [\"00:00:00:00:00:02\"] port port01 addresses: [\"00:00:00:00:00:01\"] router 3affb86c-5f1a-461d-bd07-b4a9580ed9bf (r1) port r1s0 mac: \"00:00:00:00:00:ff\" networks: [\"10.0.0.254/24\"] port r1s1 mac: \"00:00:00:00:01:ff\" networks: [\"10.0.1.254/24\"] [root@ovn1 ~]# ovn-sbctl show Chassis \"b5337e05-2495-4bf0-8728-25840472baa4\" hostname: ovn3 Encap geneve ip: \"192.168.122.203\" options: {csum=\"true\"} Port_Binding port12 Port_Binding port02 Chassis \"c3f90802-4fd5-44ad-a338-154a9150e46f\" hostname: ovn1 Encap geneve ip: \"192.168.122.201\" options: {csum=\"true\"} Chassis \"8b9ad2e3-c1bc-4ea2-973e-a8bd1d38e502\" hostname: ovn2 Encap geneve ip: \"192.168.122.202\" options: {csum=\"true\"} Port_Binding port01 Port_Binding port11 Verify logical flows: ovn-sbctl lflow-list [root@ovn1 ~]# ovn-sbctl lflow-list Datapath: \"s0\" (63c97e9f-194e-49b1-b075-661cd2132895) Pipeline: ingress table=0 (ls_in_port_sec_l2 ), priority=100 , match=(eth.src[40]), action=(drop;) table=0 (ls_in_port_sec_l2 ), priority=100 , match=(vlan.present), action=(drop;) table=0 (ls_in_port_sec_l2 ), priority=50 , match=(inport == \"port01\"), action=(next;) table=0 (ls_in_port_sec_l2 ), priority=50 , match=(inport == \"port02\"), action=(next;) table=0 (ls_in_port_sec_l2 ), priority=50 , match=(inport == \"s0r1\"), action=(next;) table=1 (ls_in_port_sec_ip ), priority=0 , match=(1), action=(next;) table=2 (ls_in_port_sec_nd ), priority=0 , match=(1), action=(next;) table=3 (ls_in_pre_acl ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=3 (ls_in_pre_acl ), priority=0 , match=(1), action=(next;) table=4 (ls_in_pre_lb ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=4 (ls_in_pre_lb ), priority=110 , match=(nd || nd_rs || nd_ra), action=(next;) table=4 (ls_in_pre_lb ), priority=0 , match=(1), action=(next;) table=5 (ls_in_pre_stateful ), priority=100 , match=(reg0[0] == 1), action=(ct_next;) table=5 (ls_in_pre_stateful ), priority=0 , match=(1), action=(next;) table=6 (ls_in_acl ), priority=34000, match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=6 (ls_in_acl ), priority=0 , match=(1), action=(next;) table=7 (ls_in_qos_mark ), priority=0 , match=(1), action=(next;) table=8 (ls_in_qos_meter ), priority=0 , match=(1), action=(next;) table=9 (ls_in_lb ), priority=0 , match=(1), action=(next;) table=10(ls_in_stateful ), priority=100 , match=(reg0[1] == 1), action=(ct_commit(ct_label=0/1); next;) table=10(ls_in_stateful ), priority=100 , match=(reg0[2] == 1), action=(ct_lb;) table=10(ls_in_stateful ), priority=0 , match=(1), action=(next;) table=11(ls_in_pre_hairpin ), priority=0 , match=(1), action=(next;) table=12(ls_in_hairpin ), priority=1 , match=(reg0[6] == 1), action=(eth.dst <-> eth.src;outport = inport;flags.loopback = 1;output;) table=12(ls_in_hairpin ), priority=0 , match=(1), action=(next;) table=13(ls_in_arp_rsp ), priority=0 , match=(1), action=(next;) table=14(ls_in_dhcp_options ), priority=0 , match=(1), action=(next;) table=15(ls_in_dhcp_response), priority=0 , match=(1), action=(next;) table=16(ls_in_dns_lookup ), priority=0 , match=(1), action=(next;) table=17(ls_in_dns_response ), priority=0 , match=(1), action=(next;) table=18(ls_in_external_port), priority=0 , match=(1), action=(next;) table=19(ls_in_l2_lkup ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(handle_svc_check(inport);) table=19(ls_in_l2_lkup ), priority=80 , match=(eth.src == { 00:00:00:00:00:ff} && (arp.op == 1 || nd_ns)), action=(outport = \"_MC_flood\"; output;) table=19(ls_in_l2_lkup ), priority=75 , match=(flags[1] == 0 && arp.op == 1 && arp.tpa == { 10.0.0.254}), action=(outport = \"s0r1\"; output;) table=19(ls_in_l2_lkup ), priority=75 , match=(flags[1] == 0 && nd_ns && nd.target == { fe80::200:ff:fe00:ff}), action=(outport = \"s0r1\"; output;) table=19(ls_in_l2_lkup ), priority=70 , match=(eth.mcast), action=(outport = \"_MC_flood\"; output;) table=19(ls_in_l2_lkup ), priority=50 , match=(eth.dst == 00:00:00:00:00:01), action=(outport = \"port01\"; output;) table=19(ls_in_l2_lkup ), priority=50 , match=(eth.dst == 00:00:00:00:00:02), action=(outport = \"port02\"; output;) table=19(ls_in_l2_lkup ), priority=50 , match=(eth.dst == 00:00:00:00:00:ff), action=(outport = \"s0r1\"; output;) Datapath: \"s0\" (63c97e9f-194e-49b1-b075-661cd2132895) Pipeline: egress table=0 (ls_out_pre_lb ), priority=110 , match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=0 (ls_out_pre_lb ), priority=110 , match=(nd || nd_rs || nd_ra), action=(next;) table=0 (ls_out_pre_lb ), priority=0 , match=(1), action=(next;) table=1 (ls_out_pre_acl ), priority=110 , match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=1 (ls_out_pre_acl ), priority=0 , match=(1), action=(next;) table=2 (ls_out_pre_stateful), priority=100 , match=(reg0[0] == 1), action=(ct_next;) table=2 (ls_out_pre_stateful), priority=0 , match=(1), action=(next;) table=3 (ls_out_lb ), priority=0 , match=(1), action=(next;) table=4 (ls_out_acl ), priority=34000, match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=4 (ls_out_acl ), priority=0 , match=(1), action=(next;) table=5 (ls_out_qos_mark ), priority=0 , match=(1), action=(next;) table=6 (ls_out_qos_meter ), priority=0 , match=(1), action=(next;) table=7 (ls_out_stateful ), priority=100 , match=(reg0[1] == 1), action=(ct_commit(ct_label=0/1); next;) table=7 (ls_out_stateful ), priority=100 , match=(reg0[2] == 1), action=(ct_lb;) table=7 (ls_out_stateful ), priority=0 , match=(1), action=(next;) table=8 (ls_out_port_sec_ip ), priority=0 , match=(1), action=(next;) table=9 (ls_out_port_sec_l2 ), priority=100 , match=(eth.mcast), action=(output;) table=9 (ls_out_port_sec_l2 ), priority=50 , match=(outport == \"port01\"), action=(output;) table=9 (ls_out_port_sec_l2 ), priority=50 , match=(outport == \"port02\"), action=(output;) table=9 (ls_out_port_sec_l2 ), priority=50 , match=(outport == \"s0r1\"), action=(output;) Datapath: \"s1\" (7ba84139-1ab7-47fe-874c-6956be31ab0a) Pipeline: ingress table=0 (ls_in_port_sec_l2 ), priority=100 , match=(eth.src[40]), action=(drop;) table=0 (ls_in_port_sec_l2 ), priority=100 , match=(vlan.present), action=(drop;) table=0 (ls_in_port_sec_l2 ), priority=50 , match=(inport == \"port11\"), action=(next;) table=0 (ls_in_port_sec_l2 ), priority=50 , match=(inport == \"port12\"), action=(next;) table=0 (ls_in_port_sec_l2 ), priority=50 , match=(inport == \"s1r1\"), action=(next;) table=1 (ls_in_port_sec_ip ), priority=0 , match=(1), action=(next;) table=2 (ls_in_port_sec_nd ), priority=0 , match=(1), action=(next;) table=3 (ls_in_pre_acl ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=3 (ls_in_pre_acl ), priority=0 , match=(1), action=(next;) table=4 (ls_in_pre_lb ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=4 (ls_in_pre_lb ), priority=110 , match=(nd || nd_rs || nd_ra), action=(next;) table=4 (ls_in_pre_lb ), priority=0 , match=(1), action=(next;) table=5 (ls_in_pre_stateful ), priority=100 , match=(reg0[0] == 1), action=(ct_next;) table=5 (ls_in_pre_stateful ), priority=0 , match=(1), action=(next;) table=6 (ls_in_acl ), priority=34000, match=(eth.dst == fa:bb:da:7b:a8:58), action=(next;) table=6 (ls_in_acl ), priority=0 , match=(1), action=(next;) table=7 (ls_in_qos_mark ), priority=0 , match=(1), action=(next;) table=8 (ls_in_qos_meter ), priority=0 , match=(1), action=(next;) table=9 (ls_in_lb ), priority=0 , match=(1), action=(next;) table=10(ls_in_stateful ), priority=100 , match=(reg0[1] == 1), action=(ct_commit(ct_label=0/1); next;) table=10(ls_in_stateful ), priority=100 , match=(reg0[2] == 1), action=(ct_lb;) table=10(ls_in_stateful ), priority=0 , match=(1), action=(next;) table=11(ls_in_pre_hairpin ), priority=0 , match=(1), action=(next;) table=12(ls_in_hairpin ), priority=1 , match=(reg0[6] == 1), action=(eth.dst <-> eth.src;outport = inport;flags.loopback = 1;output;) table=12(ls_in_hairpin ), priority=0 , match=(1), action=(next;) table=13(ls_in_arp_rsp ), priority=0 , match=(1), action=(next;) table=14(ls_in_dhcp_options ), priority=0 , match=(1), action=(next;) table=15(ls_in_dhcp_response), priority=0 , match=(1), action=(next;) table=16(ls_in_dns_lookup ), priority=0 , match=(1), action=(next;) table=17(ls_in_dns_response ), priority=0 , match=(1), action=(next;) table=18(ls_in_external_port), priority=0 , match=(1), action=(next;) table=19(ls_in_l2_lkup ), priority=110 , match=(eth.dst == fa:bb:da:7b:a8:58), action=(handle_svc_check(inport);) table=19(ls_in_l2_lkup ), priority=80 , match=(eth.src == { 00:00:00:00:01:ff} && (arp.op == 1 || nd_ns)), action=(outport = \"_MC_flood\"; output;) table=19(ls_in_l2_lkup ), priority=75 , match=(flags[1] == 0 && arp.op == 1 && arp.tpa == { 10.0.1.254}), action=(outport = \"s1r1\"; output;) table=19(ls_in_l2_lkup ), priority=75 , match=(flags[1] == 0 && nd_ns && nd.target == { fe80::200:ff:fe00:1ff}), action=(outport = \"s1r1\"; output;) table=19(ls_in_l2_lkup ), priority=70 , match=(eth.mcast), action=(outport = \"_MC_flood\"; output;) table=19(ls_in_l2_lkup ), priority=50 , match=(eth.dst == 00:00:00:00:01:01), action=(outport = \"port11\"; output;) table=19(ls_in_l2_lkup ), priority=50 , match=(eth.dst == 00:00:00:00:01:02), action=(outport = \"port12\"; output;) table=19(ls_in_l2_lkup ), priority=50 , match=(eth.dst == 00:00:00:00:01:ff), action=(outport = \"s1r1\"; output;) Datapath: \"s1\" (7ba84139-1ab7-47fe-874c-6956be31ab0a) Pipeline: egress table=0 (ls_out_pre_lb ), priority=110 , match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=0 (ls_out_pre_lb ), priority=110 , match=(nd || nd_rs || nd_ra), action=(next;) table=0 (ls_out_pre_lb ), priority=0 , match=(1), action=(next;) table=1 (ls_out_pre_acl ), priority=110 , match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=1 (ls_out_pre_acl ), priority=0 , match=(1), action=(next;) table=2 (ls_out_pre_stateful), priority=100 , match=(reg0[0] == 1), action=(ct_next;) table=2 (ls_out_pre_stateful), priority=0 , match=(1), action=(next;) table=3 (ls_out_lb ), priority=0 , match=(1), action=(next;) table=4 (ls_out_acl ), priority=34000, match=(eth.src == fa:bb:da:7b:a8:58), action=(next;) table=4 (ls_out_acl ), priority=0 , match=(1), action=(next;) table=5 (ls_out_qos_mark ), priority=0 , match=(1), action=(next;) table=6 (ls_out_qos_meter ), priority=0 , match=(1), action=(next;) table=7 (ls_out_stateful ), priority=100 , match=(reg0[1] == 1), action=(ct_commit(ct_label=0/1); next;) table=7 (ls_out_stateful ), priority=100 , match=(reg0[2] == 1), action=(ct_lb;) table=7 (ls_out_stateful ), priority=0 , match=(1), action=(next;) table=8 (ls_out_port_sec_ip ), priority=0 , match=(1), action=(next;) table=9 (ls_out_port_sec_l2 ), priority=100 , match=(eth.mcast), action=(output;) table=9 (ls_out_port_sec_l2 ), priority=50 , match=(outport == \"port11\"), action=(output;) table=9 (ls_out_port_sec_l2 ), priority=50 , match=(outport == \"port12\"), action=(output;) table=9 (ls_out_port_sec_l2 ), priority=50 , match=(outport == \"s1r1\"), action=(output;) Datapath: \"r1\" (df1fbb81-4fde-4585-a7bd-5e03b7961947) Pipeline: ingress table=0 (lr_in_admission ), priority=100 , match=(vlan.present || eth.src[40]), action=(drop;) table=0 (lr_in_admission ), priority=50 , match=(eth.dst == 00:00:00:00:00:ff && inport == \"r1s0\"), action=(next;) table=0 (lr_in_admission ), priority=50 , match=(eth.dst == 00:00:00:00:01:ff && inport == \"r1s1\"), action=(next;) table=0 (lr_in_admission ), priority=50 , match=(eth.mcast && inport == \"r1s0\"), action=(next;) table=0 (lr_in_admission ), priority=50 , match=(eth.mcast && inport == \"r1s1\"), action=(next;) table=1 (lr_in_lookup_neighbor), priority=100 , match=(arp.op == 2), action=(reg9[2] = lookup_arp(inport, arp.spa, arp.sha); next;) table=1 (lr_in_lookup_neighbor), priority=100 , match=(inport == \"r1s0\" && arp.spa == 10.0.0.0/24 && arp.op == 1), action=(reg9[2] = lookup_arp(inport, arp.spa, arp.sha); next;) table=1 (lr_in_lookup_neighbor), priority=100 , match=(inport == \"r1s1\" && arp.spa == 10.0.1.0/24 && arp.op == 1), action=(reg9[2] = lookup_arp(inport, arp.spa, arp.sha); next;) table=1 (lr_in_lookup_neighbor), priority=100 , match=(nd_na), action=(reg9[2] = lookup_nd(inport, nd.target, nd.tll); next;) table=1 (lr_in_lookup_neighbor), priority=100 , match=(nd_ns), action=(reg9[2] = lookup_nd(inport, ip6.src, nd.sll); next;) table=1 (lr_in_lookup_neighbor), priority=0 , match=(1), action=(reg9[3] = 1; next;) table=2 (lr_in_learn_neighbor), priority=100 , match=(reg9[3] == 1 || reg9[2] == 1), action=(next;) table=2 (lr_in_learn_neighbor), priority=90 , match=(arp), action=(put_arp(inport, arp.spa, arp.sha); next;) table=2 (lr_in_learn_neighbor), priority=90 , match=(nd_na), action=(put_nd(inport, nd.target, nd.tll); next;) table=2 (lr_in_learn_neighbor), priority=90 , match=(nd_ns), action=(put_nd(inport, ip6.src, nd.sll); next;) table=3 (lr_in_ip_input ), priority=100 , match=(ip4.src == {10.0.0.254, 10.0.0.255} && reg9[0] == 0), action=(drop;) table=3 (lr_in_ip_input ), priority=100 , match=(ip4.src == {10.0.1.254, 10.0.1.255} && reg9[0] == 0), action=(drop;) table=3 (lr_in_ip_input ), priority=100 , match=(ip4.src_mcast ||ip4.src == 255.255.255.255 || ip4.src == 127.0.0.0/8 || ip4.dst == 127.0.0.0/8 || ip4.src == 0.0.0.0/8 || ip4.dst == 0.0.0.0/8), action=(drop;) table=3 (lr_in_ip_input ), priority=100 , match=(ip6.dst == fe80::200:ff:fe00:1ff && udp.src == 547 && udp.dst == 546), action=(reg0 = 0; handle_dhcpv6_reply;) table=3 (lr_in_ip_input ), priority=100 , match=(ip6.dst == fe80::200:ff:fe00:ff && udp.src == 547 && udp.dst == 546), action=(reg0 = 0; handle_dhcpv6_reply;) table=3 (lr_in_ip_input ), priority=90 , match=(inport == \"r1s0\" && arp.spa == 10.0.0.0/24 && arp.tpa == 10.0.0.254 && arp.op == 1), action=(eth.dst = eth.src; eth.src = 00:00:00:00:00:ff; arp.op = 2; /* ARP reply */ arp.tha = arp.sha; arp.sha = 00:00:00:00:00:ff; arp.tpa = arp.spa; arp.spa = 10.0.0.254; outport = \"r1s0\"; flags.loopback = 1; output;) table=3 (lr_in_ip_input ), priority=90 , match=(inport == \"r1s0\" && nd_ns && ip6.dst == {fe80::200:ff:fe00:ff, ff02::1:ff00:ff} && nd.target == fe80::200:ff:fe00:ff), action=(nd_na_router { eth.src = 00:00:00:00:00:ff; ip6.src = fe80::200:ff:fe00:ff; nd.target = fe80::200:ff:fe00:ff; nd.tll = 00:00:00:00:00:ff; outport = inport; flags.loopback = 1; output; };) table=3 (lr_in_ip_input ), priority=90 , match=(inport == \"r1s1\" && arp.spa == 10.0.1.0/24 && arp.tpa == 10.0.1.254 && arp.op == 1), action=(eth.dst = eth.src; eth.src = 00:00:00:00:01:ff; arp.op = 2; /* ARP reply */ arp.tha = arp.sha; arp.sha = 00:00:00:00:01:ff; arp.tpa = arp.spa; arp.spa = 10.0.1.254; outport = \"r1s1\"; flags.loopback = 1; output;) table=3 (lr_in_ip_input ), priority=90 , match=(inport == \"r1s1\" && nd_ns && ip6.dst == {fe80::200:ff:fe00:1ff, ff02::1:ff00:1ff} && nd.target == fe80::200:ff:fe00:1ff), action=(nd_na_router { eth.src = 00:00:00:00:01:ff; ip6.src = fe80::200:ff:fe00:1ff; nd.target = fe80::200:ff:fe00:1ff; nd.tll = 00:00:00:00:01:ff; outport = inport; flags.loopback = 1; output; };) table=3 (lr_in_ip_input ), priority=90 , match=(ip4.dst == 10.0.0.254 && icmp4.type == 8 && icmp4.code == 0), action=(ip4.dst <-> ip4.src; ip.ttl = 255; icmp4.type = 0; flags.loopback = 1; next; ) table=3 (lr_in_ip_input ), priority=90 , match=(ip4.dst == 10.0.1.254 && icmp4.type == 8 && icmp4.code == 0), action=(ip4.dst <-> ip4.src; ip.ttl = 255; icmp4.type = 0; flags.loopback = 1; next; ) table=3 (lr_in_ip_input ), priority=90 , match=(ip6.dst == fe80::200:ff:fe00:1ff && icmp6.type == 128 && icmp6.code == 0), action=(ip6.dst <-> ip6.src; ip.ttl = 255; icmp6.type = 129; flags.loopback = 1; next; ) table=3 (lr_in_ip_input ), priority=90 , match=(ip6.dst == fe80::200:ff:fe00:ff && icmp6.type == 128 && icmp6.code == 0), action=(ip6.dst <-> ip6.src; ip.ttl = 255; icmp6.type = 129; flags.loopback = 1; next; ) table=3 (lr_in_ip_input ), priority=85 , match=(arp || nd), action=(drop;) table=3 (lr_in_ip_input ), priority=84 , match=(nd_rs || nd_ra), action=(next;) table=3 (lr_in_ip_input ), priority=83 , match=(ip6.mcast_rsvd), action=(drop;) table=3 (lr_in_ip_input ), priority=82 , match=(ip4.mcast || ip6.mcast), action=(drop;) table=3 (lr_in_ip_input ), priority=80 , match=(ip4 && ip4.dst == 10.0.0.254 && !ip.later_frag && tcp), action=(tcp_reset {eth.dst <-> eth.src; ip4.dst <-> ip4.src; next; };) table=3 (lr_in_ip_input ), priority=80 , match=(ip4 && ip4.dst == 10.0.0.254 && !ip.later_frag && udp), action=(icmp4 {eth.dst <-> eth.src; ip4.dst <-> ip4.src; ip.ttl = 255; icmp4.type = 3; icmp4.code = 3; next; };) table=3 (lr_in_ip_input ), priority=80 , match=(ip4 && ip4.dst == 10.0.1.254 && !ip.later_frag && tcp), action=(tcp_reset {eth.dst <-> eth.src; ip4.dst <-> ip4.src; next; };) table=3 (lr_in_ip_input ), priority=80 , match=(ip4 && ip4.dst == 10.0.1.254 && !ip.later_frag && udp), action=(icmp4 {eth.dst <-> eth.src; ip4.dst <-> ip4.src; ip.ttl = 255; icmp4.type = 3; icmp4.code = 3; next; };) table=3 (lr_in_ip_input ), priority=80 , match=(ip6 && ip6.dst == fe80::200:ff:fe00:1ff && !ip.later_frag && tcp), action=(tcp_reset {eth.dst <-> eth.src; ip6.dst <-> ip6.src; next; };) table=3 (lr_in_ip_input ), priority=80 , match=(ip6 && ip6.dst == fe80::200:ff:fe00:1ff && !ip.later_frag && udp), action=(icmp6 {eth.dst <-> eth.src; ip6.dst <-> ip6.src; ip.ttl = 255; icmp6.type = 1; icmp6.code = 4; next; };) table=3 (lr_in_ip_input ), priority=80 , match=(ip6 && ip6.dst == fe80::200:ff:fe00:ff && !ip.later_frag && tcp), action=(tcp_reset {eth.dst <-> eth.src; ip6.dst <-> ip6.src; next; };) table=3 (lr_in_ip_input ), priority=80 , match=(ip6 && ip6.dst == fe80::200:ff:fe00:ff && !ip.later_frag && udp), action=(icmp6 {eth.dst <-> eth.src; ip6.dst <-> ip6.src; ip.ttl = 255; icmp6.type = 1; icmp6.code = 4; next; };) table=3 (lr_in_ip_input ), priority=70 , match=(ip4 && ip4.dst == 10.0.0.254 && !ip.later_frag), action=(icmp4 {eth.dst <-> eth.src; ip4.dst <-> ip4.src; ip.ttl = 255; icmp4.type = 3; icmp4.code = 2; next; };) table=3 (lr_in_ip_input ), priority=70 , match=(ip4 && ip4.dst == 10.0.1.254 && !ip.later_frag), action=(icmp4 {eth.dst <-> eth.src; ip4.dst <-> ip4.src; ip.ttl = 255; icmp4.type = 3; icmp4.code = 2; next; };) table=3 (lr_in_ip_input ), priority=70 , match=(ip6 && ip6.dst == fe80::200:ff:fe00:1ff && !ip.later_frag), action=(icmp6 {eth.dst <-> eth.src; ip6.dst <-> ip6.src; ip.ttl = 255; icmp6.type = 1; icmp6.code = 3; next; };) table=3 (lr_in_ip_input ), priority=70 , match=(ip6 && ip6.dst == fe80::200:ff:fe00:ff && !ip.later_frag), action=(icmp6 {eth.dst <-> eth.src; ip6.dst <-> ip6.src; ip.ttl = 255; icmp6.type = 1; icmp6.code = 3; next; };) table=3 (lr_in_ip_input ), priority=60 , match=(ip4.dst == {10.0.0.254} || ip6.dst == {fe80::200:ff:fe00:ff}), action=(drop;) table=3 (lr_in_ip_input ), priority=60 , match=(ip4.dst == {10.0.1.254} || ip6.dst == {fe80::200:ff:fe00:1ff}), action=(drop;) table=3 (lr_in_ip_input ), priority=50 , match=(eth.bcast), action=(drop;) table=3 (lr_in_ip_input ), priority=40 , match=(inport == \"r1s0\" && ip4 && ip.ttl == {0, 1} && !ip.later_frag), action=(icmp4 {eth.dst <-> eth.src; icmp4.type = 11; /* Time exceeded */ icmp4.code = 0; /* TTL exceeded in transit */ ip4.dst = ip4.src; ip4.src = 10.0.0.254; ip.ttl = 255; next; };) table=3 (lr_in_ip_input ), priority=40 , match=(inport == \"r1s1\" && ip4 && ip.ttl == {0, 1} && !ip.later_frag), action=(icmp4 {eth.dst <-> eth.src; icmp4.type = 11; /* Time exceeded */ icmp4.code = 0; /* TTL exceeded in transit */ ip4.dst = ip4.src; ip4.src = 10.0.1.254; ip.ttl = 255; next; };) table=3 (lr_in_ip_input ), priority=30 , match=(ip4 && ip.ttl == {0, 1}), action=(drop;) table=3 (lr_in_ip_input ), priority=0 , match=(1), action=(next;) table=4 (lr_in_defrag ), priority=0 , match=(1), action=(next;) table=5 (lr_in_unsnat ), priority=0 , match=(1), action=(next;) table=6 (lr_in_dnat ), priority=0 , match=(1), action=(next;) table=7 (lr_in_nd_ra_options), priority=0 , match=(1), action=(next;) table=8 (lr_in_nd_ra_response), priority=0 , match=(1), action=(next;) table=9 (lr_in_ip_routing ), priority=550 , match=(nd_rs || nd_ra), action=(drop;) table=9 (lr_in_ip_routing ), priority=129 , match=(inport == \"r1s0\" && ip6.dst == fe80::/64), action=(ip.ttl--; reg8[0..15] = 0; xxreg0 = ip6.dst; xxreg1 = fe80::200:ff:fe00:ff; eth.src = 00:00:00:00:00:ff; outport = \"r1s0\"; flags.loopback = 1; next;) table=9 (lr_in_ip_routing ), priority=129 , match=(inport == \"r1s1\" && ip6.dst == fe80::/64), action=(ip.ttl--; reg8[0..15] = 0; xxreg0 = ip6.dst; xxreg1 = fe80::200:ff:fe00:1ff; eth.src = 00:00:00:00:01:ff; outport = \"r1s1\"; flags.loopback = 1; next;) table=9 (lr_in_ip_routing ), priority=49 , match=(ip4.dst == 10.0.0.0/24), action=(ip.ttl--; reg8[0..15] = 0; reg0 = ip4.dst; reg1 = 10.0.0.254; eth.src = 00:00:00:00:00:ff; outport = \"r1s0\"; flags.loopback = 1; next;) table=9 (lr_in_ip_routing ), priority=49 , match=(ip4.dst == 10.0.1.0/24), action=(ip.ttl--; reg8[0..15] = 0; reg0 = ip4.dst; reg1 = 10.0.1.254; eth.src = 00:00:00:00:01:ff; outport = \"r1s1\"; flags.loopback = 1; next;) table=10(lr_in_ip_routing_ecmp), priority=150 , match=(reg8[0..15] == 0), action=(next;) table=11(lr_in_policy ), priority=0 , match=(1), action=(next;) table=12(lr_in_arp_resolve ), priority=500 , match=(ip4.mcast || ip6.mcast), action=(next;) table=12(lr_in_arp_resolve ), priority=0 , match=(ip4), action=(get_arp(outport, reg0); next;) table=12(lr_in_arp_resolve ), priority=0 , match=(ip6), action=(get_nd(outport, xxreg0); next;) table=13(lr_in_chk_pkt_len ), priority=0 , match=(1), action=(next;) table=14(lr_in_larger_pkts ), priority=0 , match=(1), action=(next;) table=15(lr_in_gw_redirect ), priority=0 , match=(1), action=(next;) table=16(lr_in_arp_request ), priority=100 , match=(eth.dst == 00:00:00:00:00:00 && ip4), action=(arp { eth.dst = ff:ff:ff:ff:ff:ff; arp.spa = reg1; arp.tpa = reg0; arp.op = 1; output; };) table=16(lr_in_arp_request ), priority=100 , match=(eth.dst == 00:00:00:00:00:00 && ip6), action=(nd_ns { nd.target = xxreg0; output; };) table=16(lr_in_arp_request ), priority=0 , match=(1), action=(output;) Datapath: \"r1\" (df1fbb81-4fde-4585-a7bd-5e03b7961947) Pipeline: egress table=0 (lr_out_undnat ), priority=0 , match=(1), action=(next;) table=1 (lr_out_snat ), priority=120 , match=(nd_ns), action=(next;) table=1 (lr_out_snat ), priority=0 , match=(1), action=(next;) table=2 (lr_out_egr_loop ), priority=0 , match=(1), action=(next;) table=3 (lr_out_delivery ), priority=100 , match=(outport == \"r1s0\"), action=(output;) table=3 (lr_out_delivery ), priority=100 , match=(outport == \"r1s1\"), action=(output;) Last but not least, run a test ping between the namespaces on both hosts: [root@ovn2 ~]# ip netns exec ns0 ping 10.0.1.1 -c1 -W1 [root@ovn2 ~]# ip netns exec ns0 ping 10.0.1.2 -c1 -W1 [root@ovn2 ~]# ip netns exec ns0 ping 10.0.1.1 -c1 -W1 PING 10.0.1.1 (10.0.1.1) 56(84) bytes of data. 64 bytes from 10.0.1.1: icmp_seq=1 ttl=63 time=78.8 ms --- 10.0.1.1 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 78.799/78.799/78.799/0.000 ms [root@ovn2 ~]# ip netns exec ns0 ping 10.0.1.2 -c1 -W1 PING 10.0.1.2 (10.0.1.2) 56(84) bytes of data. 64 bytes from 10.0.1.2: icmp_seq=1 ttl=63 time=21.4 ms --- 10.0.1.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 21.413/21.413/21.413/0.000 ms [root@ovn2 ~]#","title":"Setup"},{"location":"networking/ovn_standalone_on_fedora31/#adding-external-gateway-router-non-distributed","text":"","title":"Adding external gateway  router, non-distributed"},{"location":"networking/ovn_standalone_on_fedora31/#introduction_1","text":"http://www.openvswitch.org/support/dist-docs/ovn-architecture.7.html Gateway Routers A gateway router is a logical router that is bound to a physical loca\u2010 tion. This includes all of the logical patch ports of the logical router, as well as all of the peer logical patch ports on logical switches. In the OVN Southbound database, the Port_Binding entries for these logical patch ports use the type l3gateway rather than patch, in order to distinguish that these logical patch ports are bound to a chassis. When a hypervisor processes a packet on a logical datapath representing a logical switch, and the logical egress port is a l3gateway port rep\u2010 resenting connectivity to a gateway router, the packet will match a flow in table 32 that sends the packet on a tunnel port to the chassis where the gateway router resides. This processing in table 32 is done in the same manner as for VIFs. Gateway routers are typically used in between distributed logical routers and physical networks. The distributed logical router and the logical switches behind it, to which VMs and containers attach, effec\u2010 tively reside on each hypervisor. The distributed router and the gate\u2010 way router are connected by another logical switch, sometimes referred to as a join logical switch. On the other side, the gateway router con\u2010 nects to another logical switch that has a localnet port connecting to the physical network. When using gateway routers, DNAT and SNAT rules are associated with the gateway router, which provides a central location that can handle one- to-many SNAT (aka IP masquerading).","title":"Introduction"},{"location":"networking/ovn_standalone_on_fedora31/#setup_1","text":"Add provider bridge mapping on ovn1: # ovn1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=provider:br-provider ovs-vsctl --may-exist add-br br-provider ovs-vsctl --may-exist add-port br-provider eth1 Bind logical router rg (router gateway) to chassis ovn1 and assign an internal IP pointing towards the VMs: chassis=$(ovn-sbctl find chassis hostname=ovn1 | awk '/^name/ {print $NF}' | sed 's/\"//g') ovn-nbctl create Logical_Router name=rg options:chassis=$chassis ovn-nbctl lrp-add rg rgsj 00:00:00:01:00:01 10.1.0.1/30 Create switch sj (switch join) and add the router port of rg to it: ovn-nbctl ls-add sj ovn-nbctl lsp-add sj sjrg ovn-nbctl lsp-set-type sjrg router ovn-nbctl lsp-set-addresses sjrg 00:00:00:01:00:01 ovn-nbctl lsp-set-options sjrg router-port=rgsj Add r1 port to the join switch: ovn-nbctl lrp-add r1 r1sj 00:00:00:01:00:02 10.1.0.2/30 ovn-nbctl lsp-add sj sjr1 ovn-nbctl lsp-set-type sjr1 router ovn-nbctl lsp-set-addresses sjr1 00:00:00:01:00:02 ovn-nbctl lsp-set-options sjr1 router-port=r1sj Add routes - router gateway towards our 2 private subnets via r1 and r1 default route towards rg: ovn-nbctl lr-route-add rg \"10.0.0.0/24\" 10.1.0.2 ovn-nbctl lr-route-add rg \"10.0.1.0/24\" 10.1.0.2 ovn-nbctl lr-route-add r1 \"0.0.0.0/0\" 10.1.0.1 Verify routes: [root@ovn1 ~]# ovn-nbctl lr-route-list rg IPv4 Routes 10.0.0.0/24 10.1.0.2 dst-ip 10.0.1.0/24 10.1.0.2 dst-ip [root@ovn1 ~]# ovn-nbctl lr-route-list r1 IPv4 Routes 0.0.0.0/0 10.1.0.1 dst-ip [root@ovn1 ~]# ovn-nbctl list Logical_Router_Static_Route _uuid : 9ddd9b58-12c2-43c8-afe2-c20cb71aa824 external_ids : {} ip_prefix : \"0.0.0.0/0\" nexthop : \"10.1.0.1\" output_port : [] policy : [] _uuid : 5e915985-d98c-48bf-ba35-25a10c150bb1 external_ids : {} ip_prefix : \"10.0.0.0/24\" nexthop : \"10.1.0.2\" output_port : [] policy : [] _uuid : a73f670f-d435-471f-9273-58a6b8767f5f external_ids : {} ip_prefix : \"10.0.1.0/24\" nexthop : \"10.1.0.2\" output_port : [] policy : [] Add an external port to rg: ovn-nbctl lrp-add rg rgsp 00:00:00:02:00:ff 10.2.0.254/24 Add provider network switch (sp): ovn-nbctl ls-add sp ovn-nbctl lsp-add sp sprg ovn-nbctl lsp-set-type sprg router ovn-nbctl lsp-set-addresses sprg 00:00:00:02:00:ff ovn-nbctl lsp-set-options sprg router-port=rgsp Add localnet port (eth1) to provider switch (sp): ovn-nbctl lsp-add sp sp-localnet ovn-nbctl lsp-set-addresses sp-localnet unknown ovn-nbctl lsp-set-type sp-localnet localnet ovn-nbctl lsp-set-options sp-localnet network_name=provider Verify on ovn1: ovs-vsctl show ovn-nbctl show ovn-sbctl show [root@ovn1 ~]# ovs-vsctl show 64003a9a-1f2a-403c-8d21-cd187d2f717c Bridge br-int fail_mode: secure Port ovn-8b9ad2-0 Interface ovn-8b9ad2-0 type: geneve options: {csum=\"true\", key=flow, remote_ip=\"192.168.122.202\"} Port ovn-b5337e-0 Interface ovn-b5337e-0 type: geneve options: {csum=\"true\", key=flow, remote_ip=\"192.168.122.203\"} Port br-int Interface br-int type: internal Port patch-br-int-to-sp-localnet Interface patch-br-int-to-sp-localnet type: patch options: {peer=patch-sp-localnet-to-br-int} Bridge br-provider Port eth1 Interface eth1 Port patch-sp-localnet-to-br-int Interface patch-sp-localnet-to-br-int type: patch options: {peer=patch-br-int-to-sp-localnet} Port br-provider Interface br-provider type: internal ovs_version: \"2.13.0\" [root@ovn1 ~]# ovn-nbctl show switch c11aeac8-3469-444d-819e-cd0f50437175 (sp) port sp-localnet type: localnet addresses: [\"unknown\"] port sprg type: router addresses: [\"00:00:00:02:00:ff\"] router-port: rgsp switch 40f56f4d-db18-4d77-bb8b-c93e5179d346 (s0) port s0r1 type: router addresses: [\"00:00:00:00:00:ff\"] router-port: r1s0 port port02 addresses: [\"00:00:00:00:00:02\"] port port01 addresses: [\"00:00:00:00:00:01\"] switch fe514df1-5e66-4962-9962-0ce69436eaf7 (sj) port sjrg type: router addresses: [\"00:00:00:01:00:01\"] router-port: rgsj port sjr1 type: router addresses: [\"00:00:00:01:00:02\"] router-port: r1sj switch 722300a5-73e1-42de-ae19-b7997f1f9a86 (s1) port port12 addresses: [\"00:00:00:00:01:02\"] port s1r1 type: router addresses: [\"00:00:00:00:01:ff\"] router-port: r1s1 port port11 addresses: [\"00:00:00:00:01:01\"] router 3affb86c-5f1a-461d-bd07-b4a9580ed9bf (r1) port r1s0 mac: \"00:00:00:00:00:ff\" networks: [\"10.0.0.254/24\"] port r1sj mac: \"00:00:00:01:00:02\" networks: [\"10.1.0.2/30\"] port r1s1 mac: \"00:00:00:00:01:ff\" networks: [\"10.0.1.254/24\"] router d041e224-b40b-46fd-8888-df5d93362579 (rg) port rgsj mac: \"00:00:00:01:00:01\" networks: [\"10.1.0.1/30\"] port rgsp mac: \"00:00:00:02:00:ff\" networks: [\"10.2.0.254/24\"] [root@ovn1 ~]# ovn-sbctl show Chassis \"b5337e05-2495-4bf0-8728-25840472baa4\" hostname: ovn3 Encap geneve ip: \"192.168.122.203\" options: {csum=\"true\"} Port_Binding port12 Port_Binding port02 Chassis \"c3f90802-4fd5-44ad-a338-154a9150e46f\" hostname: ovn1 Encap geneve ip: \"192.168.122.201\" options: {csum=\"true\"} Port_Binding sjrg Port_Binding rgsp Port_Binding sprg Port_Binding rgsj Chassis \"8b9ad2e3-c1bc-4ea2-973e-a8bd1d38e502\" hostname: ovn2 Encap geneve ip: \"192.168.122.202\" options: {csum=\"true\"} Port_Binding port11 Port_Binding port01 Connect to ovn2 and simulate an external node: nmcli connection delete 76505bbf-7aef-32f0-bae6-9497dcf93d2e ip a a dev eth1 10.2.0.10/24 ip link set dev eth1 up ip route add 10.0.1.0/24 via 10.2.0.254 ip route add 10.1.0.0/24 via 10.2.0.254 ip route add 10.0.0.0/24 via 10.2.0.254 Verfify: [root@ovn2 ~]# ping 10.0.0.1 PING 10.0.0.1 (10.0.0.1) 56(84) bytes of data. 64 bytes from 10.0.0.1: icmp_seq=1 ttl=62 time=3.64 ms ^C --- 10.0.0.1 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 3.642/3.642/3.642/0.000 ms [root@ovn2 ~]# ping 10.0.0.1 -c1 -W1 PING 10.0.0.1 (10.0.0.1) 56(84) bytes of data. 64 bytes from 10.0.0.1: icmp_seq=1 ttl=62 time=1.73 ms --- 10.0.0.1 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 1.734/1.734/1.734/0.000 ms [root@ovn2 ~]# traceroute -n -I 10.0.0.1 traceroute to 10.0.0.1 (10.0.0.1), 30 hops max, 60 byte packets 1 * * * 2 * * * 3 10.0.0.1 10.736 ms 10.726 ms 10.716 ms [root@ovn2 ~]# ip netns exec ns0 ping 10.2.0.10 -c1 -W1 PING 10.2.0.10 (10.2.0.10) 56(84) bytes of data. 64 bytes from 10.2.0.10: icmp_seq=1 ttl=62 time=1.62 ms --- 10.2.0.10 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 1.615/1.615/1.615/0.000 ms [root@ovn2 ~]# ip netns exec ns0 traceroute -n -I 10.2.0.10 traceroute to 10.2.0.10 (10.2.0.10), 30 hops max, 60 byte packets 1 10.0.0.254 2.164 ms 1.773 ms 2.921 ms 2 * * * 3 10.2.0.10 5.078 ms * 4.308 ms [root@ovn2 ~]# ping 10.0.1.2 PING 10.0.1.2 (10.0.1.2) 56(84) bytes of data. 64 bytes from 10.0.1.2: icmp_seq=1 ttl=62 time=4.91 ms 64 bytes from 10.0.1.2: icmp_seq=2 ttl=62 time=1.82 ms ^C --- 10.0.1.2 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1001ms rtt min/avg/max/mdev = 1.815/3.360/4.906/1.545 ms [root@ovn2 ~]#","title":"Setup"},{"location":"networking/ovn_standalone_on_fedora31/#inspecting-tunnel-encapsulations","text":"","title":"Inspecting tunnel encapsulations"},{"location":"networking/ovn_standalone_on_fedora31/#introduction_2","text":"http://www.openvswitch.org/support/dist-docs/ovn-architecture.7.html DESIGN DECISIONS Tunnel Encapsulations OVN annotates logical network packets that it sends from one hypervisor to another with the following three pieces of metadata, which are en\u2010 coded in an encapsulation-specific fashion: \u2022 24-bit logical datapath identifier, from the tunnel_key column in the OVN Southbound Datapath_Binding table. \u2022 15-bit logical ingress port identifier. ID 0 is reserved for internal use within OVN. IDs 1 through 32767, inclu\u2010 sive, may be assigned to logical ports (see the tun\u2010 nel_key column in the OVN Southbound Port_Binding table). \u2022 16-bit logical egress port identifier. IDs 0 through 32767 have the same meaning as for logical ingress ports. IDs 32768 through 65535, inclusive, may be assigned to logical multicast groups (see the tunnel_key column in the OVN Southbound Multicast_Group table). For hypervisor-to-hypervisor traffic, OVN supports only Geneve and STT encapsulations, for the following reasons: \u2022 Only STT and Geneve support the large amounts of metadata (over 32 bits per packet) that OVN uses (as described above). \u2022 STT and Geneve use randomized UDP or TCP source ports that allows efficient distribution among multiple paths in environments that use ECMP in their underlay. \u2022 NICs are available to offload STT and Geneve encapsula\u2010 tion and decapsulation. Due to its flexibility, the preferred encapsulation between hypervisors is Geneve. For Geneve encapsulation, OVN transmits the logical datapath identifier in the Geneve VNI. OVN transmits the logical ingress and logical egress ports in a TLV with class 0x0102, type 0x80, and a 32-bit value encoded as follows, from MSB to LSB: 1 15 16 +---+------------+-----------+ |rsv|ingress port|egress port| +---+------------+-----------+ 0 Environments whose NICs lack Geneve offload may prefer STT encapsula\u2010 tion for performance reasons. For STT encapsulation, OVN encodes all three pieces of logical metadata in the STT 64-bit tunnel ID as fol\u2010 lows, from MSB to LSB: 9 15 16 24 +--------+------------+-----------+--------+ |reserved|ingress port|egress port|datapath| +--------+------------+-----------+--------+ 0 For connecting to gateways, in addition to Geneve and STT, OVN supports VXLAN, because only VXLAN support is common on top-of-rack (ToR) switches. Currently, gateways have a feature set that matches the capa\u2010 bilities as defined by the VTEP schema, so fewer bits of metadata are necessary. In the future, gateways that do not support encapsulations with large amounts of metadata may continue to have a reduced feature set.","title":"Introduction"},{"location":"networking/ovn_standalone_on_fedora31/#distributed-gateway-router","text":"https://developers.redhat.com/blog/2018/11/08/how-to-create-an-open-virtual-network-distributed-gateway-router/","title":"Distributed gateway router"},{"location":"networking/ovn_standalone_on_fedora31/#resources","text":"https://blog.scottlowe.org/2016/12/09/using-ovn-with-kvm-libvirt/ https://baturin.org/docs/iproute2/ http://dani.foroselectronica.es/multinode-ovn-setup-509/ https://hustcat.github.io/ovn-gateway-practice/ http://www.openvswitch.org/support/dist-docs/ovn-architecture.7.html https://www.redhat.com/en/blog/what-geneve https://tools.ietf.org/html/draft-davie-stt-08 https://tools.ietf.org/html/draft-ietf-nvo3-geneve-16 https://developers.redhat.com/blog/2018/11/08/how-to-create-an-open-virtual-network-distributed-gateway-router/ http://docs.openvswitch.org/en/latest/intro/install/bash-completion/ https://blog.russellbryant.net/2017/05/30/ovn-geneve-vs-vxlan-does-it-matter/","title":"Resources"},{"location":"networking/ovs-vxlan-tunnels-and-dscp/","text":"OVS VXLAN tunnels and DSCP Lab Fedora 32 yum install openvswitch -y systemctl enable --now openvswitch [root@ovs-tunnel-test-1 ~]# ovs-vsctl show | grep version ovs_version: \"2.13.0\" Setup node A ovs-vsctl --may-exist add-br br-int -- set Bridge br-int datapath_type=system -- br-set-external-id br-int bridge-id br-int ovs-vsctl add-port br-int vxlan0 -- set interface vxlan0 type=vxlan options:remote_ip=192.168.122.174 ip netns add private ip link add name veth-host type veth peer name veth-guest ovs-vsctl add-port br-int veth-host ip link set dev veth-guest netns private ip link set dev veth-host up ip -n private set dev veth-guest up ip -n private link set dev veth-guest up ip -n private link set dev lo up ip -n private a a dev veth-guest 192.168.123.1/24 node B ovs-vsctl --may-exist add-br br-int -- set Bridge br-int datapath_type=system -- br-set-external-id br-int bridge-id br-int ovs-vsctl add-port br-int vxlan0 -- set interface vxlan0 type=vxlan options:remote_ip=192.168.122.41 ip netns add private ip link add name veth-host type veth peer name veth-guest ovs-vsctl add-port br-int veth-host ip link set dev veth-guest netns private ip link set dev veth-host up ip -n private set dev veth-guest up ip -n private link set dev veth-guest up ip -n private link set dev lo up ip -n private a a dev veth-guest 192.168.123.2/24 Only setting the ECN bits to ECT(0) - ToS 0x2 [root@ovs-tunnel-test-1 ~]# ip netns exec private ping 192.168.123.2 -c1 -W1 -Q 0x2 PING 192.168.123.2 (192.168.123.2) 56(84) bytes of data. 64 bytes from 192.168.123.2: icmp_seq=1 ttl=64 time=2.51 ms --- 192.168.123.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 2.513/2.513/2.513/0.000 ms [root@ovs-tunnel-test-1 ~]# [root@ovs-tunnel-test-2 ~]# tshark -nn -i eth0 -O ip port vxlan Running as user \"root\" and group \"root\". This could be dangerous. Capturing on 'eth0' Frame 1: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface eth0, id 0 Ethernet II, Src: 52:54:00:e3:af:ab, Dst: 52:54:00:9e:bc:3a Internet Protocol Version 4, Src: 192.168.122.41, Dst: 192.168.122.174 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 134 Identification: 0x8486 (33926) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: UDP (17) Header checksum: 0x3fb6 [validation disabled] [Header checksum status: Unverified] Source: 192.168.122.41 Destination: 192.168.122.174 User Datagram Protocol, Src Port: 34788, Dst Port: 4789 Virtual eXtensible Local Area Network Ethernet II, Src: 16:28:3c:ba:5b:13, Dst: 56:13:a0:be:1f:f4 Internet Protocol Version 4, Src: 192.168.123.1, Dst: 192.168.123.2 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 84 Identification: 0xc12e (49454) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: ICMP (1) Header checksum: 0x0224 [validation disabled] [Header checksum status: Unverified] Source: 192.168.123.1 Destination: 192.168.123.2 Internet Control Message Protocol Frame 2: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface eth0, id 0 Ethernet II, Src: 52:54:00:9e:bc:3a, Dst: 52:54:00:e3:af:ab Internet Protocol Version 4, Src: 192.168.122.174, Dst: 192.168.122.41 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 134 Identification: 0x717a (29050) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: UDP (17) Header checksum: 0x52c2 [validation disabled] [Header checksum status: Unverified] Source: 192.168.122.174 Destination: 192.168.122.41 User Datagram Protocol, Src Port: 45375, Dst Port: 4789 Virtual eXtensible Local Area Network Ethernet II, Src: 56:13:a0:be:1f:f4, Dst: 16:28:3c:ba:5b:13 Internet Protocol Version 4, Src: 192.168.123.2, Dst: 192.168.123.1 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 84 Identification: 0x1916 (6422) Flags: 0x0000 0... .... .... .... = Reserved bit: Not set .0.. .... .... .... = Don't fragment: Not set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: ICMP (1) Header checksum: 0xea3c [validation disabled] [Header checksum status: Unverified] Source: 192.168.123.2 Destination: 192.168.123.1 Internet Control Message Protocol Expedited forwarding and ECT(0) - ToS 0xba [root@ovs-tunnel-test-1 ~]# ip netns exec private ping 192.168.123.2 -c1 -W1 -Q 0xba PING 192.168.123.2 (192.168.123.2) 56(84) bytes of data. 64 bytes from 192.168.123.2: icmp_seq=1 ttl=64 time=1.47 ms --- 192.168.123.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 1.471/1.471/1.471/0.000 ms [root@ovs-tunnel-test-1 ~]# ^C [root@ovs-tunnel-test-1 ~]# [root@ovs-tunnel-test-2 ~]# tshark -nn -i eth0 -O ip port vxlan Running as user \"root\" and group \"root\". This could be dangerous. Capturing on 'eth0' Frame 1: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface eth0, id 0 Ethernet II, Src: 52:54:00:e3:af:ab, Dst: 52:54:00:9e:bc:3a Internet Protocol Version 4, Src: 192.168.122.41, Dst: 192.168.122.174 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 134 Identification: 0xf90e (63758) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: UDP (17) Header checksum: 0xcb2d [validation disabled] [Header checksum status: Unverified] Source: 192.168.122.41 Destination: 192.168.122.174 User Datagram Protocol, Src Port: 34788, Dst Port: 4789 Virtual eXtensible Local Area Network Ethernet II, Src: 16:28:3c:ba:5b:13, Dst: 56:13:a0:be:1f:f4 Internet Protocol Version 4, Src: 192.168.123.1, Dst: 192.168.123.2 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0xba (DSCP: EF PHB, ECN: ECT(0)) 1011 10.. = Differentiated Services Codepoint: Expedited Forwarding (46) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 84 Identification: 0xf88b (63627) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: ICMP (1) Header checksum: 0xca0e [validation disabled] [Header checksum status: Unverified] Source: 192.168.123.1 Destination: 192.168.123.2 Internet Control Message Protocol Frame 2: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface eth0, id 0 Ethernet II, Src: 52:54:00:9e:bc:3a, Dst: 52:54:00:e3:af:ab Internet Protocol Version 4, Src: 192.168.122.174, Dst: 192.168.122.41 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 134 Identification: 0xf7ee (63470) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: UDP (17) Header checksum: 0xcc4d [validation disabled] [Header checksum status: Unverified] Source: 192.168.122.174 Destination: 192.168.122.41 User Datagram Protocol, Src Port: 45375, Dst Port: 4789 Virtual eXtensible Local Area Network Ethernet II, Src: 56:13:a0:be:1f:f4, Dst: 16:28:3c:ba:5b:13 Internet Protocol Version 4, Src: 192.168.123.2, Dst: 192.168.123.1 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0xba (DSCP: EF PHB, ECN: ECT(0)) 1011 10.. = Differentiated Services Codepoint: Expedited Forwarding (46) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 84 Identification: 0x67e2 (26594) Flags: 0x0000 0... .... .... .... = Reserved bit: Not set .0.. .... .... .... = Don't fragment: Not set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: ICMP (1) Header checksum: 0x9ab8 [validation disabled] [Header checksum status: Unverified] Source: 192.168.123.2 Destination: 192.168.123.1 Internet Control Message Protocol ^C2 packets captured ToS 0xba with tos = inherit 0xba = EF with ECN bits set to 10 tos=inherit will literally only inherit the ToS bits 4,3,2. See: https://en.wikipedia.org/wiki/Type_of_service Precedence and ToS Prior to its deprecation, the Type of Service field was defined as follows from RFC 791: 7 6 5 4 3 2 1 0 Precedence Type of Service Unused (0) http://www.openvswitch.org/support/dist-docs/ovs-vswitchd.conf.db.5.txt options : tos: optional string Optional. The value of the ToS bits to be set on the encapsulat\u2010 ing packet. ToS is interpreted as DSCP and ECN bits, ECN part must be zero. It may also be the word inherit, in which case the ToS will be copied from the inner packet if it is IPv4 or IPv6 (otherwise it will be 0). The ECN fields are always inherited. Default is 0. Run this on both hosts: ovs-vsctl set interface vxlan0 options:tos=inherit [root@ovs-tunnel-test-1 ~]# ip netns exec private ping 192.168.123.2 -c1 -W1 -Q 0xba PING 192.168.123.2 (192.168.123.2) 56(84) bytes of data. 64 bytes from 192.168.123.2: icmp_seq=1 ttl=64 time=2.49 ms --- 192.168.123.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 2.486/2.486/2.486/0.000 ms [root@ovs-tunnel-test-1 ~]# [root@ovs-tunnel-test-2 ~]# tshark -nn -i eth0 -O ip port vxlan Running as user \"root\" and group \"root\". This could be dangerous. Capturing on 'eth0' Frame 1: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface eth0, id 0 Ethernet II, Src: 52:54:00:e3:af:ab, Dst: 52:54:00:9e:bc:3a Internet Protocol Version 4, Src: 192.168.122.41, Dst: 192.168.122.174 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x1a (DSCP: Unknown, ECN: ECT(0)) 0001 10.. = Differentiated Services Codepoint: Unknown (6) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 134 Identification: 0xd0c2 (53442) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: UDP (17) Header checksum: 0xf361 [validation disabled] [Header checksum status: Unverified] Source: 192.168.122.41 Destination: 192.168.122.174 User Datagram Protocol, Src Port: 34788, Dst Port: 4789 Virtual eXtensible Local Area Network Ethernet II, Src: 16:28:3c:ba:5b:13, Dst: 56:13:a0:be:1f:f4 Internet Protocol Version 4, Src: 192.168.123.1, Dst: 192.168.123.2 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0xba (DSCP: EF PHB, ECN: ECT(0)) 1011 10.. = Differentiated Services Codepoint: Expedited Forwarding (46) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 84 Identification: 0xcf83 (53123) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: ICMP (1) Header checksum: 0xf316 [validation disabled] [Header checksum status: Unverified] Source: 192.168.123.1 Destination: 192.168.123.2 Internet Control Message Protocol Frame 2: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface eth0, id 0 Ethernet II, Src: 52:54:00:9e:bc:3a, Dst: 52:54:00:e3:af:ab Internet Protocol Version 4, Src: 192.168.122.174, Dst: 192.168.122.41 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x1a (DSCP: Unknown, ECN: ECT(0)) 0001 10.. = Differentiated Services Codepoint: Unknown (6) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 134 Identification: 0x3a20 (14880) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: UDP (17) Header checksum: 0x8a04 [validation disabled] [Header checksum status: Unverified] Source: 192.168.122.174 Destination: 192.168.122.41 User Datagram Protocol, Src Port: 45375, Dst Port: 4789 Virtual eXtensible Local Area Network Ethernet II, Src: 56:13:a0:be:1f:f4, Dst: 16:28:3c:ba:5b:13 Internet Protocol Version 4, Src: 192.168.123.2, Dst: 192.168.123.1 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0xba (DSCP: EF PHB, ECN: ECT(0)) 1011 10.. = Differentiated Services Codepoint: Expedited Forwarding (46) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 84 Identification: 0x4704 (18180) Flags: 0x0000 0... .... .... .... = Reserved bit: Not set .0.. .... .... .... = Don't fragment: Not set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: ICMP (1) Header checksum: 0xbb96 [validation disabled] [Header checksum status: Unverified] Source: 192.168.123.2 Destination: 192.168.123.1 Internet Control Message Protocol Force a specific ToS value to the outer header Valid values for the ToS only manipulate the legacy type of service field and leave the legacy IP precedence and ECN fields at 0: 000 111 00 - 0x1c 000 110 00 - 0x18 000 101 00 - 0x14 000 100 00 - 0x10 000 011 00 - 0xc 000 010 00 - 0x8 000 001 00 - 0x4 000 000 00 - 0x0 AF41 is ToS 0x22 - this is not in the aforementioned list and hence all bits will be set to 0. Run on both hosts: ovs-vsctl set interface vxlan0 options:tos=0x22 Verify - note that this marks down the class to 000000. 0x22 is an invalid value??? Perhaps??? [root@ovs-tunnel-test-1 ~]# ip netns exec private ping 192.168.123.2 -c1 -W1 -Q 0xba PING 192.168.123.2 (192.168.123.2) 56(84) bytes of data. 64 bytes from 192.168.123.2: icmp_seq=1 ttl=64 time=1.10 ms --- 192.168.123.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 1.103/1.103/1.103/0.000 ms [root@ovs-tunnel-test-1 ~]# [root@ovs-tunnel-test-2 ~]# tshark -nn -i eth0 -O ip port vxlan Running as user \"root\" and group \"root\". This could be dangerous. Capturing on 'eth0' Frame 1: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface eth0, id 0 Ethernet II, Src: 52:54:00:e3:af:ab, Dst: 52:54:00:9e:bc:3a Internet Protocol Version 4, Src: 192.168.122.41, Dst: 192.168.122.174 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 134 Identification: 0xa5b1 (42417) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: UDP (17) Header checksum: 0x1e8b [validation disabled] [Header checksum status: Unverified] Source: 192.168.122.41 Destination: 192.168.122.174 User Datagram Protocol, Src Port: 34788, Dst Port: 4789 Virtual eXtensible Local Area Network Ethernet II, Src: 16:28:3c:ba:5b:13, Dst: 56:13:a0:be:1f:f4 Internet Protocol Version 4, Src: 192.168.123.1, Dst: 192.168.123.2 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0xba (DSCP: EF PHB, ECN: ECT(0)) 1011 10.. = Differentiated Services Codepoint: Expedited Forwarding (46) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 84 Identification: 0xe36a (58218) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: ICMP (1) Header checksum: 0xdf2f [validation disabled] [Header checksum status: Unverified] Source: 192.168.123.1 Destination: 192.168.123.2 Internet Control Message Protocol Frame 2: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface eth0, id 0 Ethernet II, Src: 52:54:00:9e:bc:3a, Dst: 52:54:00:e3:af:ab Internet Protocol Version 4, Src: 192.168.122.174, Dst: 192.168.122.41 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 134 Identification: 0xb041 (45121) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: UDP (17) Header checksum: 0x13fb [validation disabled] [Header checksum status: Unverified] Source: 192.168.122.174 Destination: 192.168.122.41 User Datagram Protocol, Src Port: 45375, Dst Port: 4789 Virtual eXtensible Local Area Network Ethernet II, Src: 56:13:a0:be:1f:f4, Dst: 16:28:3c:ba:5b:13 Internet Protocol Version 4, Src: 192.168.123.2, Dst: 192.168.123.1 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0xba (DSCP: EF PHB, ECN: ECT(0)) 1011 10.. = Differentiated Services Codepoint: Expedited Forwarding (46) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 84 Identification: 0x75ca (30154) Flags: 0x0000 0... .... .... .... = Reserved bit: Not set .0.. .... .... .... = Don't fragment: Not set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: ICMP (1) Header checksum: 0x8cd0 [validation disabled] [Header checksum status: Unverified] Source: 192.168.123.2 Destination: 192.168.123.1 Internet Control Message Protocol Here is a valid test - on both nodes, set: ovs-vsctl set interface vxlan0 options:tos=0xc [root@ovs-tunnel-test-1 ~]# ip netns exec private ping 192.168.123.2 -c1 -W1 -Q 0xba PING 192.168.123.2 (192.168.123.2) 56(84) bytes of data. 64 bytes from 192.168.123.2: icmp_seq=1 ttl=64 time=2.77 ms --- 192.168.123.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 2.774/2.774/2.774/0.000 ms [root@ovs-tunnel-test-2 ~]# tshark -nn -i eth0 -O ip port vxlan Running as user \"root\" and group \"root\". This could be dangerous. Capturing on 'eth0' Frame 1: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface eth0, id 0 Ethernet II, Src: 52:54:00:e3:af:ab, Dst: 52:54:00:9e:bc:3a Internet Protocol Version 4, Src: 192.168.122.41, Dst: 192.168.122.174 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x0e (DSCP: Unknown, ECN: ECT(0)) 0000 11.. = Differentiated Services Codepoint: Unknown (3) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 134 Identification: 0xd7c2 (55234) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: UDP (17) Header checksum: 0xec6d [validation disabled] [Header checksum status: Unverified] Source: 192.168.122.41 Destination: 192.168.122.174 User Datagram Protocol, Src Port: 34788, Dst Port: 4789 Virtual eXtensible Local Area Network Ethernet II, Src: 16:28:3c:ba:5b:13, Dst: 56:13:a0:be:1f:f4 Internet Protocol Version 4, Src: 192.168.123.1, Dst: 192.168.123.2 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0xba (DSCP: EF PHB, ECN: ECT(0)) 1011 10.. = Differentiated Services Codepoint: Expedited Forwarding (46) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 84 Identification: 0xdce0 (56544) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: ICMP (1) Header checksum: 0xe5b9 [validation disabled] [Header checksum status: Unverified] Source: 192.168.123.1 Destination: 192.168.123.2 Internet Control Message Protocol Frame 2: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface eth0, id 0 Ethernet II, Src: 52:54:00:9e:bc:3a, Dst: 52:54:00:e3:af:ab Internet Protocol Version 4, Src: 192.168.122.174, Dst: 192.168.122.41 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x0e (DSCP: Unknown, ECN: ECT(0)) 0000 11.. = Differentiated Services Codepoint: Unknown (3) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 134 Identification: 0xdd9c (56732) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: UDP (17) Header checksum: 0xe693 [validation disabled] [Header checksum status: Unverified] Source: 192.168.122.174 Destination: 192.168.122.41 User Datagram Protocol, Src Port: 45375, Dst Port: 4789 Virtual eXtensible Local Area Network Ethernet II, Src: 56:13:a0:be:1f:f4, Dst: 16:28:3c:ba:5b:13 Internet Protocol Version 4, Src: 192.168.123.2, Dst: 192.168.123.1 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0xba (DSCP: EF PHB, ECN: ECT(0)) 1011 10.. = Differentiated Services Codepoint: Expedited Forwarding (46) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 84 Identification: 0xfab2 (64178) Flags: 0x0000 0... .... .... .... = Reserved bit: Not set .0.. .... .... .... = Don't fragment: Not set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: ICMP (1) Header checksum: 0x07e8 [validation disabled] [Header checksum status: Unverified] Source: 192.168.123.2 Destination: 192.168.123.1 Internet Control Message Protocol 2 packets captured Further troubleshooting [root@ovs-tunnel-test-1 ~]# ovs-vsctl set interface vxlan0 options:tos=0xb8 [root@ovs-tunnel-test-1 ~]# systemctl restart openvswitch [root@ovs-tunnel-test-1 ~]# ip netns exec private ping 192.168.123.2 -c1 -W1 -Q 0xba PING 192.168.123.2 (192.168.123.2) 56(84) bytes of data. 64 bytes from 192.168.123.2: icmp_seq=1 ttl=64 time=3.27 ms --- 192.168.123.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 3.272/3.272/3.272/0.000 ms (failed reverse-i-search)`dcp': ovs-dpctl ^Cmp-flows [root@ovs-tunnel-test-1 ~]# ovs-dpctl dump-flows recirc_id(0),tunnel(tun_id=0x0,src=192.168.122.174,dst=192.168.122.41,tos=0x18,flags(-df-csum+key)),in_port(2),eth(src=56:13:a0:be:1f:f4,dst=16:28:3c:ba:5b:13),eth_type(0x0806), packets:1, bytes:42, used:5.009s, actions:3 recirc_id(0),in_port(3),eth(src=16:28:3c:ba:5b:13,dst=56:13:a0:be:1f:f4),eth_type(0x0806), packets:1, bytes:42, used:5.009s, actions:set(tunnel(dst=192.168.122.174,tos=0xb8,ttl=64,tp_dst=4789,flags(df))),2 [root@ovs-tunnel-test-1 ~]# ~~~ ~~~ [root@ovs-tunnel-test-2 ~]# tshark -nn -i eth0 -O ip port vxlan | egrep 'Frame|Differentiated Services|Explicit Congestion Notification' Running as user \"root\" and group \"root\". This could be dangerous. Capturing on 'eth0' 2 Frame 1: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface eth0, id 0 6 Differentiated Services Field: 0x1a (DSCP: Unknown, ECN: ECT(0)) 0001 10.. = Differentiated Services Codepoint: Unknown (6) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Differentiated Services Field: 0xba (DSCP: EF PHB, ECN: ECT(0)) 1011 10.. = Differentiated Services Codepoint: Expedited Forwarding (46) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Frame 2: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface eth0, id 0 Differentiated Services Field: 0x1a (DSCP: Unknown, ECN: ECT(0)) 0001 10.. = Differentiated Services Codepoint: Unknown (6) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Differentiated Services Field: 0xba (DSCP: EF PHB, ECN: ECT(0)) 1011 10.. = Differentiated Services Codepoint: Expedited Forwarding (46) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Frame 3: 92 bytes on wire (736 bits), 92 bytes captured (736 bits) on interface eth0, id 0 Differentiated Services Field: 0x18 (DSCP: Unknown, ECN: Not-ECT) 0001 10.. = Differentiated Services Codepoint: Unknown (6) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Frame 4: 92 bytes on wire (736 bits), 92 bytes captured (736 bits) on interface eth0, id 0 Differentiated Services Field: 0x18 (DSCP: Unknown, ECN: Not-ECT) 0001 10.. = Differentiated Services Codepoint: Unknown (6) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Frame 5: 92 bytes on wire (736 bits), 92 bytes captured (736 bits) on interface eth0, id 0 Differentiated Services Field: 0x18 (DSCP: Unknown, ECN: Not-ECT) 0001 10.. = Differentiated Services Codepoint: Unknown (6) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Frame 6: 92 bytes on wire (736 bits), 92 bytes captured (736 bits) on interface eth0, id 0 Differentiated Services Field: 0x18 (DSCP: Unknown, ECN: Not-ECT) 0001 10.. = Differentiated Services Codepoint: Unknown (6) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) ^C^R ^C^C [root@ovs-tunnel-test-2 ~]# ovs-dpctl dump-flows recirc_id(0),tunnel(tun_id=0x0,src=192.168.122.41,dst=192.168.122.174,tos=0x18,flags(-df-csum+key)),in_port(2),eth(src=16:28:3c:ba:5b:13,dst=56:13:a0:be:1f:f4),eth_type(0x0806), packets:1, bytes:42, used:8.951s, actions:3 recirc_id(0),in_port(3),eth(src=56:13:a0:be:1f:f4,dst=16:28:3c:ba:5b:13),eth_type(0x0806), packets:1, bytes:42, used:8.951s, actions:set(tunnel(dst=192.168.122.41,tos=0xb8,ttl=64,tp_dst=4789,flags(df))),2 [root@ovs-tunnel-test-2 ~]# Test with RHEL 7.7 The restart is only to reset the dpctl flows. Not necessary to apply the change. In RHEL 7.7, the first 3 bits are set correctly. [root@ovs-tunnel-test-1 ~]# ovs-vsctl set interface vxlan0 options:tos=inherit [root@ovs-tunnel-test-1 ~]# systemctl restart openvswitch [root@ovs-tunnel-test-1 ~]# ip netns exec private ping 192.168.123.2 -c1 -W1 -Q 0xba PING 192.168.123.2 (192.168.123.2) 56(84) bytes of data. 64 bytes from 192.168.123.2: icmp_seq=1 ttl=64 time=0.971 ms --- 192.168.123.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.971/0.971/0.971/0.000 ms [root@ovs-tunnel-test-1 ~]# ovs-dpctl dump-flows 2020-07-30T13:49:12Z|00001|dpif_netlink|INFO|The kernel module does not support meters. recirc_id(0),in_port(3),eth(src=fe:61:62:0e:3b:65,dst=56:73:90:10:20:5f),eth_type(0x0800),ipv4(tos=0xba,frag=no), packets:0, bytes:0, used:never, actions:set(tunnel(dst=192.168.122.175,tos=0xba,ttl=64,tp_dst=4789,flags(df))),2 recirc_id(0),tunnel(tun_id=0x0,src=192.168.122.175,dst=192.168.122.42,flags(-df-csum+key)),in_port(2),eth(src=56:73:90:10:20:5f,dst=fe:61:62:0e:3b:65),eth_type(0x0806), packets:0, bytes:0, used:never, actions:3 recirc_id(0),tunnel(tun_id=0x0,src=192.168.122.175,dst=192.168.122.42,tos=0xba,flags(-df-csum+key)),in_port(2),eth(src=56:73:90:10:20:5f,dst=fe:61:62:0e:3b:65),eth_type(0x0800),ipv4(frag=no), packets:0, bytes:0, used:never, actions:3 recirc_id(0),in_port(3),eth(src=fe:61:62:0e:3b:65,dst=56:73:90:10:20:5f),eth_type(0x0806), packets:0, bytes:0, used:never, actions:set(tunnel(dst=192.168.122.175,ttl=64,tp_dst=4789,flags(df))),2 [root@ovs-tunnel-test-1 ~]# [root@ovs-tunnel-test-2 ~]# ovs-vsctl set interface vxlan0 options:tos=inherit [root@ovs-tunnel-test-2 ~]# systemctl restart openvswitch (reverse-i-search)`t': systemctl restart openvswi^Ch [root@ovs-tunnel-test-2 ~]# tshark -nn -i eth0 -O ip port 4789 | egrep 'Frame|Differentiated Services|Explicit Congestion Notification' Running as user \"root\" and group \"root\". This could be dangerous. Capturing on 'eth0' 4 Frame 1: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface 0 Differentiated Services Field: 0xba (DSCP 0x2e: Expedited Forwarding; ECN: 0x02: ECT(0) (ECN-Capable Transport)) 1011 10.. = Differentiated Services Codepoint: Expedited Forwarding (0x2e) .... ..10 = Explicit Congestion Notification: ECT(0) (ECN-Capable Transport) (0x02) Frame 2: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface 0 Differentiated Services Field: 0xba (DSCP 0x2e: Expedited Forwarding; ECN: 0x02: ECT(0) (ECN-Capable Transport)) 1011 10.. = Differentiated Services Codepoint: Expedited Forwarding (0x2e) .... ..10 = Explicit Congestion Notification: ECT(0) (ECN-Capable Transport) (0x02) Frame 3: 92 bytes on wire (736 bits), 92 bytes captured (736 bits) on interface 0 Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00: Not-ECT (Not ECN-Capable Transport)) 0000 00.. = Differentiated Services Codepoint: Default (0x00) .... ..00 = Explicit Congestion Notification: Not-ECT (Not ECN-Capable Transport) (0x00) Frame 4: 92 bytes on wire (736 bits), 92 bytes captured (736 bits) on interface 0 Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00: Not-ECT (Not ECN-Capable Transport)) 0000 00.. = Differentiated Services Codepoint: Default (0x00) ^C [root@ovs-tunnel-test-2 ~]# ovs-dpctl dump-flows 2020-07-30T13:49:15Z|00001|dpif_netlink|INFO|The kernel module does not support meters. recirc_id(0),in_port(3),eth(src=56:73:90:10:20:5f,dst=fe:61:62:0e:3b:65),eth_type(0x0806), packets:0, bytes:0, used:never, actions:set(tunnel(dst=192.168.122.42,ttl=64,tp_dst=4789,flags(df))),2 recirc_id(0),tunnel(tun_id=0x0,src=192.168.122.42,dst=192.168.122.175,tos=0xba,flags(-df-csum+key)),in_port(2),eth(src=fe:61:62:0e:3b:65,dst=56:73:90:10:20:5f),eth_type(0x0800),ipv4(frag=no), packets:0, bytes:0, used:never, actions:3 recirc_id(0),tunnel(tun_id=0x0,src=192.168.122.42,dst=192.168.122.175,flags(-df-csum+key)),in_port(2),eth(src=fe:61:62:0e:3b:65,dst=56:73:90:10:20:5f),eth_type(0x0806), packets:0, bytes:0, used:never, actions:3 recirc_id(0),in_port(3),eth(src=56:73:90:10:20:5f,dst=fe:61:62:0e:3b:65),eth_type(0x0800),ipv4(tos=0xba,frag=no), packets:0, bytes:0, used:never, actions:set(tunnel(dst=192.168.122.42,tos=0xba,ttl=64,tp_dst=4789,flags(df))),2 [root@ovs-tunnel-test-2 ~]# And explicitly setting this, e.g. to 1111 1100: [root@ovs-tunnel-test-1 ~]# systemctl restart openvswitch [root@ovs-tunnel-test-1 ~]# ovs-vsctl set interface vxlan0 options:tos=0xfc [root@ovs-tunnel-test-1 ~]# ip netns exec private ping 192.168.123.2 -c1 -W1 -Q 0xba PING 192.168.123.2 (192.168.123.2) 56(84) bytes of data. 64 bytes from 192.168.123.2: icmp_seq=1 ttl=64 time=2.33 ms --- 192.168.123.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 2.339/2.339/2.339/0.000 ms [root@ovs-tunnel-test-1 ~]# ovs-dpctl dump-flows 2020-07-30T13:55:10Z|00001|dpif_netlink|INFO|The kernel module does not support meters. recirc_id(0),in_port(3),eth(src=fe:61:62:0e:3b:65,dst=56:73:90:10:20:5f),eth_type(0x0800),ipv4(tos=0x2/0x3,frag=no), packets:0, bytes:0, used:never, actions:set(tunnel(dst=192.168.122.175,tos=0xfe,ttl=64,tp_dst=4789,flags(df))),2 recirc_id(0),tunnel(tun_id=0x0,src=192.168.122.175,dst=192.168.122.42,tos=0xfe,flags(-df-csum+key)),in_port(2),eth(src=56:73:90:10:20:5f,dst=fe:61:62:0e:3b:65),eth_type(0x0800),ipv4(frag=no), packets:0, bytes:0, used:never, actions:3 [root@ovs-tunnel-test-1 ~]# [root@ovs-tunnel-test-2 ~]# systemctl restart openvswitch [root@ovs-tunnel-test-2 ~]# ovs-vsctl set interface vxlan0 options:tos=0xfc [root@ovs-tunnel-test-2 ~]# tshark -nn -i eth0 -O ip port 4789 | egrep 'Frame|Differentiated Services|Explicit Congestion Notification' Running as user \"root\" and group \"root\". This could be dangerous. Capturing on 'eth0' 4 Frame 1: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface 0 Differentiated Services Field: 0xfe (DSCP 0x3f: Unknown DSCP; ECN: 0x02: ECT(0) (ECN-Capable Transport)) 1111 11.. = Differentiated Services Codepoint: Unknown (0x3f) .... ..10 = Explicit Congestion Notification: ECT(0) (ECN-Capable Transport) (0x02) Frame 2: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface 0 Differentiated Services Field: 0xfe (DSCP 0x3f: Unknown DSCP; ECN: 0x02: ECT(0) (ECN-Capable Transport)) 1111 11.. = Differentiated Services Codepoint: Unknown (0x3f) .... ..10 = Explicit Congestion Notification: ECT(0) (ECN-Capable Transport) (0x02) Frame 3: 92 bytes on wire (736 bits), 92 bytes captured (736 bits) on interface 0 Differentiated Services Field: 0xfc (DSCP 0x3f: Unknown DSCP; ECN: 0x00: Not-ECT (Not ECN-Capable Transport)) 1111 11.. = Differentiated Services Codepoint: Unknown (0x3f) .... ..00 = Explicit Congestion Notification: Not-ECT (Not ECN-Capable Transport) (0x00) Frame 4: 92 bytes on wire (736 bits), 92 bytes captured (736 bits) on interface 0 Differentiated Services Field: 0xfc (DSCP 0x3f: Unknown DSCP; ECN: 0x00: Not-ECT (Not ECN-Capable Transport)) 1111 11.. = Differentiated Services Codepoint: Unknown (0x3f) ^C [root@ovs-tunnel-test-2 ~]# ovs-dpctl dump-flows 2020-07-30T13:55:18Z|00001|dpif_netlink|INFO|The kernel module does not support meters. recirc_id(0),in_port(3),eth(src=56:73:90:10:20:5f,dst=fe:61:62:0e:3b:65),eth_type(0x0806), packets:0, bytes:0, used:never, actions:set(tunnel(dst=192.168.122.42,tos=0xfc,ttl=64,tp_dst=4789,flags(df))),2 recirc_id(0),tunnel(tun_id=0x0,src=192.168.122.42,dst=192.168.122.175,tos=0xfc,flags(-df-csum+key)),in_port(2),eth(src=fe:61:62:0e:3b:65,dst=56:73:90:10:20:5f),eth_type(0x0806), packets:0, bytes:0, used:never, actions:3 [root@ovs-tunnel-test-2 ~]# The outer header is correctly set to 1111 1110. BUG My tests were affected by: https://bugzilla.redhat.com/show_bug.cgi?id=1862166 ECN field Is always inherited: https://github.com/openvswitch/ovs/blob/252c24a61774cac1f593569dcce31e524725676c/ofproto/tunnel.c#L454 /* ECN fields are always inherited. */ if (is_ip_any(flow)) { wc->masks.nw_tos |= IP_ECN_MASK; if (IP_ECN_is_ce(flow->nw_tos)) { flow->tunnel.ip_tos |= IP_ECN_ECT_0; } else { flow->tunnel.ip_tos |= flow->nw_tos & IP_ECN_MASK; } } Resources https://www.tucny.com/Home/dscp-tos http://docs.openvswitch.org/en/latest/howto/userspace-tunneling/","title":"OVS VXLAN tunnels and DSCP"},{"location":"networking/ovs-vxlan-tunnels-and-dscp/#ovs-vxlan-tunnels-and-dscp","text":"","title":"OVS VXLAN tunnels and DSCP"},{"location":"networking/ovs-vxlan-tunnels-and-dscp/#lab","text":"Fedora 32 yum install openvswitch -y systemctl enable --now openvswitch [root@ovs-tunnel-test-1 ~]# ovs-vsctl show | grep version ovs_version: \"2.13.0\"","title":"Lab"},{"location":"networking/ovs-vxlan-tunnels-and-dscp/#setup","text":"node A ovs-vsctl --may-exist add-br br-int -- set Bridge br-int datapath_type=system -- br-set-external-id br-int bridge-id br-int ovs-vsctl add-port br-int vxlan0 -- set interface vxlan0 type=vxlan options:remote_ip=192.168.122.174 ip netns add private ip link add name veth-host type veth peer name veth-guest ovs-vsctl add-port br-int veth-host ip link set dev veth-guest netns private ip link set dev veth-host up ip -n private set dev veth-guest up ip -n private link set dev veth-guest up ip -n private link set dev lo up ip -n private a a dev veth-guest 192.168.123.1/24 node B ovs-vsctl --may-exist add-br br-int -- set Bridge br-int datapath_type=system -- br-set-external-id br-int bridge-id br-int ovs-vsctl add-port br-int vxlan0 -- set interface vxlan0 type=vxlan options:remote_ip=192.168.122.41 ip netns add private ip link add name veth-host type veth peer name veth-guest ovs-vsctl add-port br-int veth-host ip link set dev veth-guest netns private ip link set dev veth-host up ip -n private set dev veth-guest up ip -n private link set dev veth-guest up ip -n private link set dev lo up ip -n private a a dev veth-guest 192.168.123.2/24","title":"Setup"},{"location":"networking/ovs-vxlan-tunnels-and-dscp/#only-setting-the-ecn-bits-to-ect0-tos-0x2","text":"[root@ovs-tunnel-test-1 ~]# ip netns exec private ping 192.168.123.2 -c1 -W1 -Q 0x2 PING 192.168.123.2 (192.168.123.2) 56(84) bytes of data. 64 bytes from 192.168.123.2: icmp_seq=1 ttl=64 time=2.51 ms --- 192.168.123.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 2.513/2.513/2.513/0.000 ms [root@ovs-tunnel-test-1 ~]# [root@ovs-tunnel-test-2 ~]# tshark -nn -i eth0 -O ip port vxlan Running as user \"root\" and group \"root\". This could be dangerous. Capturing on 'eth0' Frame 1: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface eth0, id 0 Ethernet II, Src: 52:54:00:e3:af:ab, Dst: 52:54:00:9e:bc:3a Internet Protocol Version 4, Src: 192.168.122.41, Dst: 192.168.122.174 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 134 Identification: 0x8486 (33926) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: UDP (17) Header checksum: 0x3fb6 [validation disabled] [Header checksum status: Unverified] Source: 192.168.122.41 Destination: 192.168.122.174 User Datagram Protocol, Src Port: 34788, Dst Port: 4789 Virtual eXtensible Local Area Network Ethernet II, Src: 16:28:3c:ba:5b:13, Dst: 56:13:a0:be:1f:f4 Internet Protocol Version 4, Src: 192.168.123.1, Dst: 192.168.123.2 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 84 Identification: 0xc12e (49454) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: ICMP (1) Header checksum: 0x0224 [validation disabled] [Header checksum status: Unverified] Source: 192.168.123.1 Destination: 192.168.123.2 Internet Control Message Protocol Frame 2: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface eth0, id 0 Ethernet II, Src: 52:54:00:9e:bc:3a, Dst: 52:54:00:e3:af:ab Internet Protocol Version 4, Src: 192.168.122.174, Dst: 192.168.122.41 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 134 Identification: 0x717a (29050) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: UDP (17) Header checksum: 0x52c2 [validation disabled] [Header checksum status: Unverified] Source: 192.168.122.174 Destination: 192.168.122.41 User Datagram Protocol, Src Port: 45375, Dst Port: 4789 Virtual eXtensible Local Area Network Ethernet II, Src: 56:13:a0:be:1f:f4, Dst: 16:28:3c:ba:5b:13 Internet Protocol Version 4, Src: 192.168.123.2, Dst: 192.168.123.1 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 84 Identification: 0x1916 (6422) Flags: 0x0000 0... .... .... .... = Reserved bit: Not set .0.. .... .... .... = Don't fragment: Not set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: ICMP (1) Header checksum: 0xea3c [validation disabled] [Header checksum status: Unverified] Source: 192.168.123.2 Destination: 192.168.123.1 Internet Control Message Protocol","title":"Only setting the ECN bits to ECT(0) - ToS 0x2"},{"location":"networking/ovs-vxlan-tunnels-and-dscp/#expedited-forwarding-and-ect0-tos-0xba","text":"[root@ovs-tunnel-test-1 ~]# ip netns exec private ping 192.168.123.2 -c1 -W1 -Q 0xba PING 192.168.123.2 (192.168.123.2) 56(84) bytes of data. 64 bytes from 192.168.123.2: icmp_seq=1 ttl=64 time=1.47 ms --- 192.168.123.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 1.471/1.471/1.471/0.000 ms [root@ovs-tunnel-test-1 ~]# ^C [root@ovs-tunnel-test-1 ~]# [root@ovs-tunnel-test-2 ~]# tshark -nn -i eth0 -O ip port vxlan Running as user \"root\" and group \"root\". This could be dangerous. Capturing on 'eth0' Frame 1: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface eth0, id 0 Ethernet II, Src: 52:54:00:e3:af:ab, Dst: 52:54:00:9e:bc:3a Internet Protocol Version 4, Src: 192.168.122.41, Dst: 192.168.122.174 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 134 Identification: 0xf90e (63758) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: UDP (17) Header checksum: 0xcb2d [validation disabled] [Header checksum status: Unverified] Source: 192.168.122.41 Destination: 192.168.122.174 User Datagram Protocol, Src Port: 34788, Dst Port: 4789 Virtual eXtensible Local Area Network Ethernet II, Src: 16:28:3c:ba:5b:13, Dst: 56:13:a0:be:1f:f4 Internet Protocol Version 4, Src: 192.168.123.1, Dst: 192.168.123.2 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0xba (DSCP: EF PHB, ECN: ECT(0)) 1011 10.. = Differentiated Services Codepoint: Expedited Forwarding (46) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 84 Identification: 0xf88b (63627) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: ICMP (1) Header checksum: 0xca0e [validation disabled] [Header checksum status: Unverified] Source: 192.168.123.1 Destination: 192.168.123.2 Internet Control Message Protocol Frame 2: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface eth0, id 0 Ethernet II, Src: 52:54:00:9e:bc:3a, Dst: 52:54:00:e3:af:ab Internet Protocol Version 4, Src: 192.168.122.174, Dst: 192.168.122.41 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 134 Identification: 0xf7ee (63470) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: UDP (17) Header checksum: 0xcc4d [validation disabled] [Header checksum status: Unverified] Source: 192.168.122.174 Destination: 192.168.122.41 User Datagram Protocol, Src Port: 45375, Dst Port: 4789 Virtual eXtensible Local Area Network Ethernet II, Src: 56:13:a0:be:1f:f4, Dst: 16:28:3c:ba:5b:13 Internet Protocol Version 4, Src: 192.168.123.2, Dst: 192.168.123.1 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0xba (DSCP: EF PHB, ECN: ECT(0)) 1011 10.. = Differentiated Services Codepoint: Expedited Forwarding (46) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 84 Identification: 0x67e2 (26594) Flags: 0x0000 0... .... .... .... = Reserved bit: Not set .0.. .... .... .... = Don't fragment: Not set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: ICMP (1) Header checksum: 0x9ab8 [validation disabled] [Header checksum status: Unverified] Source: 192.168.123.2 Destination: 192.168.123.1 Internet Control Message Protocol ^C2 packets captured","title":"Expedited forwarding and ECT(0) - ToS 0xba"},{"location":"networking/ovs-vxlan-tunnels-and-dscp/#tos-0xba-with-tos-inherit","text":"0xba = EF with ECN bits set to 10 tos=inherit will literally only inherit the ToS bits 4,3,2. See: https://en.wikipedia.org/wiki/Type_of_service Precedence and ToS Prior to its deprecation, the Type of Service field was defined as follows from RFC 791: 7 6 5 4 3 2 1 0 Precedence Type of Service Unused (0) http://www.openvswitch.org/support/dist-docs/ovs-vswitchd.conf.db.5.txt options : tos: optional string Optional. The value of the ToS bits to be set on the encapsulat\u2010 ing packet. ToS is interpreted as DSCP and ECN bits, ECN part must be zero. It may also be the word inherit, in which case the ToS will be copied from the inner packet if it is IPv4 or IPv6 (otherwise it will be 0). The ECN fields are always inherited. Default is 0. Run this on both hosts: ovs-vsctl set interface vxlan0 options:tos=inherit [root@ovs-tunnel-test-1 ~]# ip netns exec private ping 192.168.123.2 -c1 -W1 -Q 0xba PING 192.168.123.2 (192.168.123.2) 56(84) bytes of data. 64 bytes from 192.168.123.2: icmp_seq=1 ttl=64 time=2.49 ms --- 192.168.123.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 2.486/2.486/2.486/0.000 ms [root@ovs-tunnel-test-1 ~]# [root@ovs-tunnel-test-2 ~]# tshark -nn -i eth0 -O ip port vxlan Running as user \"root\" and group \"root\". This could be dangerous. Capturing on 'eth0' Frame 1: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface eth0, id 0 Ethernet II, Src: 52:54:00:e3:af:ab, Dst: 52:54:00:9e:bc:3a Internet Protocol Version 4, Src: 192.168.122.41, Dst: 192.168.122.174 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x1a (DSCP: Unknown, ECN: ECT(0)) 0001 10.. = Differentiated Services Codepoint: Unknown (6) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 134 Identification: 0xd0c2 (53442) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: UDP (17) Header checksum: 0xf361 [validation disabled] [Header checksum status: Unverified] Source: 192.168.122.41 Destination: 192.168.122.174 User Datagram Protocol, Src Port: 34788, Dst Port: 4789 Virtual eXtensible Local Area Network Ethernet II, Src: 16:28:3c:ba:5b:13, Dst: 56:13:a0:be:1f:f4 Internet Protocol Version 4, Src: 192.168.123.1, Dst: 192.168.123.2 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0xba (DSCP: EF PHB, ECN: ECT(0)) 1011 10.. = Differentiated Services Codepoint: Expedited Forwarding (46) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 84 Identification: 0xcf83 (53123) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: ICMP (1) Header checksum: 0xf316 [validation disabled] [Header checksum status: Unverified] Source: 192.168.123.1 Destination: 192.168.123.2 Internet Control Message Protocol Frame 2: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface eth0, id 0 Ethernet II, Src: 52:54:00:9e:bc:3a, Dst: 52:54:00:e3:af:ab Internet Protocol Version 4, Src: 192.168.122.174, Dst: 192.168.122.41 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x1a (DSCP: Unknown, ECN: ECT(0)) 0001 10.. = Differentiated Services Codepoint: Unknown (6) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 134 Identification: 0x3a20 (14880) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: UDP (17) Header checksum: 0x8a04 [validation disabled] [Header checksum status: Unverified] Source: 192.168.122.174 Destination: 192.168.122.41 User Datagram Protocol, Src Port: 45375, Dst Port: 4789 Virtual eXtensible Local Area Network Ethernet II, Src: 56:13:a0:be:1f:f4, Dst: 16:28:3c:ba:5b:13 Internet Protocol Version 4, Src: 192.168.123.2, Dst: 192.168.123.1 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0xba (DSCP: EF PHB, ECN: ECT(0)) 1011 10.. = Differentiated Services Codepoint: Expedited Forwarding (46) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 84 Identification: 0x4704 (18180) Flags: 0x0000 0... .... .... .... = Reserved bit: Not set .0.. .... .... .... = Don't fragment: Not set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: ICMP (1) Header checksum: 0xbb96 [validation disabled] [Header checksum status: Unverified] Source: 192.168.123.2 Destination: 192.168.123.1 Internet Control Message Protocol","title":"ToS 0xba with tos = inherit"},{"location":"networking/ovs-vxlan-tunnels-and-dscp/#force-a-specific-tos-value-to-the-outer-header","text":"Valid values for the ToS only manipulate the legacy type of service field and leave the legacy IP precedence and ECN fields at 0: 000 111 00 - 0x1c 000 110 00 - 0x18 000 101 00 - 0x14 000 100 00 - 0x10 000 011 00 - 0xc 000 010 00 - 0x8 000 001 00 - 0x4 000 000 00 - 0x0 AF41 is ToS 0x22 - this is not in the aforementioned list and hence all bits will be set to 0. Run on both hosts: ovs-vsctl set interface vxlan0 options:tos=0x22 Verify - note that this marks down the class to 000000. 0x22 is an invalid value??? Perhaps??? [root@ovs-tunnel-test-1 ~]# ip netns exec private ping 192.168.123.2 -c1 -W1 -Q 0xba PING 192.168.123.2 (192.168.123.2) 56(84) bytes of data. 64 bytes from 192.168.123.2: icmp_seq=1 ttl=64 time=1.10 ms --- 192.168.123.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 1.103/1.103/1.103/0.000 ms [root@ovs-tunnel-test-1 ~]# [root@ovs-tunnel-test-2 ~]# tshark -nn -i eth0 -O ip port vxlan Running as user \"root\" and group \"root\". This could be dangerous. Capturing on 'eth0' Frame 1: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface eth0, id 0 Ethernet II, Src: 52:54:00:e3:af:ab, Dst: 52:54:00:9e:bc:3a Internet Protocol Version 4, Src: 192.168.122.41, Dst: 192.168.122.174 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 134 Identification: 0xa5b1 (42417) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: UDP (17) Header checksum: 0x1e8b [validation disabled] [Header checksum status: Unverified] Source: 192.168.122.41 Destination: 192.168.122.174 User Datagram Protocol, Src Port: 34788, Dst Port: 4789 Virtual eXtensible Local Area Network Ethernet II, Src: 16:28:3c:ba:5b:13, Dst: 56:13:a0:be:1f:f4 Internet Protocol Version 4, Src: 192.168.123.1, Dst: 192.168.123.2 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0xba (DSCP: EF PHB, ECN: ECT(0)) 1011 10.. = Differentiated Services Codepoint: Expedited Forwarding (46) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 84 Identification: 0xe36a (58218) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: ICMP (1) Header checksum: 0xdf2f [validation disabled] [Header checksum status: Unverified] Source: 192.168.123.1 Destination: 192.168.123.2 Internet Control Message Protocol Frame 2: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface eth0, id 0 Ethernet II, Src: 52:54:00:9e:bc:3a, Dst: 52:54:00:e3:af:ab Internet Protocol Version 4, Src: 192.168.122.174, Dst: 192.168.122.41 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 134 Identification: 0xb041 (45121) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: UDP (17) Header checksum: 0x13fb [validation disabled] [Header checksum status: Unverified] Source: 192.168.122.174 Destination: 192.168.122.41 User Datagram Protocol, Src Port: 45375, Dst Port: 4789 Virtual eXtensible Local Area Network Ethernet II, Src: 56:13:a0:be:1f:f4, Dst: 16:28:3c:ba:5b:13 Internet Protocol Version 4, Src: 192.168.123.2, Dst: 192.168.123.1 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0xba (DSCP: EF PHB, ECN: ECT(0)) 1011 10.. = Differentiated Services Codepoint: Expedited Forwarding (46) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 84 Identification: 0x75ca (30154) Flags: 0x0000 0... .... .... .... = Reserved bit: Not set .0.. .... .... .... = Don't fragment: Not set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: ICMP (1) Header checksum: 0x8cd0 [validation disabled] [Header checksum status: Unverified] Source: 192.168.123.2 Destination: 192.168.123.1 Internet Control Message Protocol Here is a valid test - on both nodes, set: ovs-vsctl set interface vxlan0 options:tos=0xc [root@ovs-tunnel-test-1 ~]# ip netns exec private ping 192.168.123.2 -c1 -W1 -Q 0xba PING 192.168.123.2 (192.168.123.2) 56(84) bytes of data. 64 bytes from 192.168.123.2: icmp_seq=1 ttl=64 time=2.77 ms --- 192.168.123.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 2.774/2.774/2.774/0.000 ms [root@ovs-tunnel-test-2 ~]# tshark -nn -i eth0 -O ip port vxlan Running as user \"root\" and group \"root\". This could be dangerous. Capturing on 'eth0' Frame 1: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface eth0, id 0 Ethernet II, Src: 52:54:00:e3:af:ab, Dst: 52:54:00:9e:bc:3a Internet Protocol Version 4, Src: 192.168.122.41, Dst: 192.168.122.174 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x0e (DSCP: Unknown, ECN: ECT(0)) 0000 11.. = Differentiated Services Codepoint: Unknown (3) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 134 Identification: 0xd7c2 (55234) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: UDP (17) Header checksum: 0xec6d [validation disabled] [Header checksum status: Unverified] Source: 192.168.122.41 Destination: 192.168.122.174 User Datagram Protocol, Src Port: 34788, Dst Port: 4789 Virtual eXtensible Local Area Network Ethernet II, Src: 16:28:3c:ba:5b:13, Dst: 56:13:a0:be:1f:f4 Internet Protocol Version 4, Src: 192.168.123.1, Dst: 192.168.123.2 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0xba (DSCP: EF PHB, ECN: ECT(0)) 1011 10.. = Differentiated Services Codepoint: Expedited Forwarding (46) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 84 Identification: 0xdce0 (56544) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: ICMP (1) Header checksum: 0xe5b9 [validation disabled] [Header checksum status: Unverified] Source: 192.168.123.1 Destination: 192.168.123.2 Internet Control Message Protocol Frame 2: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface eth0, id 0 Ethernet II, Src: 52:54:00:9e:bc:3a, Dst: 52:54:00:e3:af:ab Internet Protocol Version 4, Src: 192.168.122.174, Dst: 192.168.122.41 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x0e (DSCP: Unknown, ECN: ECT(0)) 0000 11.. = Differentiated Services Codepoint: Unknown (3) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 134 Identification: 0xdd9c (56732) Flags: 0x4000, Don't fragment 0... .... .... .... = Reserved bit: Not set .1.. .... .... .... = Don't fragment: Set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: UDP (17) Header checksum: 0xe693 [validation disabled] [Header checksum status: Unverified] Source: 192.168.122.174 Destination: 192.168.122.41 User Datagram Protocol, Src Port: 45375, Dst Port: 4789 Virtual eXtensible Local Area Network Ethernet II, Src: 56:13:a0:be:1f:f4, Dst: 16:28:3c:ba:5b:13 Internet Protocol Version 4, Src: 192.168.123.2, Dst: 192.168.123.1 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0xba (DSCP: EF PHB, ECN: ECT(0)) 1011 10.. = Differentiated Services Codepoint: Expedited Forwarding (46) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 84 Identification: 0xfab2 (64178) Flags: 0x0000 0... .... .... .... = Reserved bit: Not set .0.. .... .... .... = Don't fragment: Not set ..0. .... .... .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: ICMP (1) Header checksum: 0x07e8 [validation disabled] [Header checksum status: Unverified] Source: 192.168.123.2 Destination: 192.168.123.1 Internet Control Message Protocol 2 packets captured","title":"Force a specific ToS value to the outer header"},{"location":"networking/ovs-vxlan-tunnels-and-dscp/#further-troubleshooting","text":"[root@ovs-tunnel-test-1 ~]# ovs-vsctl set interface vxlan0 options:tos=0xb8 [root@ovs-tunnel-test-1 ~]# systemctl restart openvswitch [root@ovs-tunnel-test-1 ~]# ip netns exec private ping 192.168.123.2 -c1 -W1 -Q 0xba PING 192.168.123.2 (192.168.123.2) 56(84) bytes of data. 64 bytes from 192.168.123.2: icmp_seq=1 ttl=64 time=3.27 ms --- 192.168.123.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 3.272/3.272/3.272/0.000 ms (failed reverse-i-search)`dcp': ovs-dpctl ^Cmp-flows [root@ovs-tunnel-test-1 ~]# ovs-dpctl dump-flows recirc_id(0),tunnel(tun_id=0x0,src=192.168.122.174,dst=192.168.122.41,tos=0x18,flags(-df-csum+key)),in_port(2),eth(src=56:13:a0:be:1f:f4,dst=16:28:3c:ba:5b:13),eth_type(0x0806), packets:1, bytes:42, used:5.009s, actions:3 recirc_id(0),in_port(3),eth(src=16:28:3c:ba:5b:13,dst=56:13:a0:be:1f:f4),eth_type(0x0806), packets:1, bytes:42, used:5.009s, actions:set(tunnel(dst=192.168.122.174,tos=0xb8,ttl=64,tp_dst=4789,flags(df))),2 [root@ovs-tunnel-test-1 ~]# ~~~ ~~~ [root@ovs-tunnel-test-2 ~]# tshark -nn -i eth0 -O ip port vxlan | egrep 'Frame|Differentiated Services|Explicit Congestion Notification' Running as user \"root\" and group \"root\". This could be dangerous. Capturing on 'eth0' 2 Frame 1: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface eth0, id 0 6 Differentiated Services Field: 0x1a (DSCP: Unknown, ECN: ECT(0)) 0001 10.. = Differentiated Services Codepoint: Unknown (6) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Differentiated Services Field: 0xba (DSCP: EF PHB, ECN: ECT(0)) 1011 10.. = Differentiated Services Codepoint: Expedited Forwarding (46) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Frame 2: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface eth0, id 0 Differentiated Services Field: 0x1a (DSCP: Unknown, ECN: ECT(0)) 0001 10.. = Differentiated Services Codepoint: Unknown (6) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Differentiated Services Field: 0xba (DSCP: EF PHB, ECN: ECT(0)) 1011 10.. = Differentiated Services Codepoint: Expedited Forwarding (46) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Frame 3: 92 bytes on wire (736 bits), 92 bytes captured (736 bits) on interface eth0, id 0 Differentiated Services Field: 0x18 (DSCP: Unknown, ECN: Not-ECT) 0001 10.. = Differentiated Services Codepoint: Unknown (6) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Frame 4: 92 bytes on wire (736 bits), 92 bytes captured (736 bits) on interface eth0, id 0 Differentiated Services Field: 0x18 (DSCP: Unknown, ECN: Not-ECT) 0001 10.. = Differentiated Services Codepoint: Unknown (6) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Frame 5: 92 bytes on wire (736 bits), 92 bytes captured (736 bits) on interface eth0, id 0 Differentiated Services Field: 0x18 (DSCP: Unknown, ECN: Not-ECT) 0001 10.. = Differentiated Services Codepoint: Unknown (6) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Frame 6: 92 bytes on wire (736 bits), 92 bytes captured (736 bits) on interface eth0, id 0 Differentiated Services Field: 0x18 (DSCP: Unknown, ECN: Not-ECT) 0001 10.. = Differentiated Services Codepoint: Unknown (6) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) ^C^R ^C^C [root@ovs-tunnel-test-2 ~]# ovs-dpctl dump-flows recirc_id(0),tunnel(tun_id=0x0,src=192.168.122.41,dst=192.168.122.174,tos=0x18,flags(-df-csum+key)),in_port(2),eth(src=16:28:3c:ba:5b:13,dst=56:13:a0:be:1f:f4),eth_type(0x0806), packets:1, bytes:42, used:8.951s, actions:3 recirc_id(0),in_port(3),eth(src=56:13:a0:be:1f:f4,dst=16:28:3c:ba:5b:13),eth_type(0x0806), packets:1, bytes:42, used:8.951s, actions:set(tunnel(dst=192.168.122.41,tos=0xb8,ttl=64,tp_dst=4789,flags(df))),2 [root@ovs-tunnel-test-2 ~]#","title":"Further troubleshooting"},{"location":"networking/ovs-vxlan-tunnels-and-dscp/#test-with-rhel-77","text":"The restart is only to reset the dpctl flows. Not necessary to apply the change. In RHEL 7.7, the first 3 bits are set correctly. [root@ovs-tunnel-test-1 ~]# ovs-vsctl set interface vxlan0 options:tos=inherit [root@ovs-tunnel-test-1 ~]# systemctl restart openvswitch [root@ovs-tunnel-test-1 ~]# ip netns exec private ping 192.168.123.2 -c1 -W1 -Q 0xba PING 192.168.123.2 (192.168.123.2) 56(84) bytes of data. 64 bytes from 192.168.123.2: icmp_seq=1 ttl=64 time=0.971 ms --- 192.168.123.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.971/0.971/0.971/0.000 ms [root@ovs-tunnel-test-1 ~]# ovs-dpctl dump-flows 2020-07-30T13:49:12Z|00001|dpif_netlink|INFO|The kernel module does not support meters. recirc_id(0),in_port(3),eth(src=fe:61:62:0e:3b:65,dst=56:73:90:10:20:5f),eth_type(0x0800),ipv4(tos=0xba,frag=no), packets:0, bytes:0, used:never, actions:set(tunnel(dst=192.168.122.175,tos=0xba,ttl=64,tp_dst=4789,flags(df))),2 recirc_id(0),tunnel(tun_id=0x0,src=192.168.122.175,dst=192.168.122.42,flags(-df-csum+key)),in_port(2),eth(src=56:73:90:10:20:5f,dst=fe:61:62:0e:3b:65),eth_type(0x0806), packets:0, bytes:0, used:never, actions:3 recirc_id(0),tunnel(tun_id=0x0,src=192.168.122.175,dst=192.168.122.42,tos=0xba,flags(-df-csum+key)),in_port(2),eth(src=56:73:90:10:20:5f,dst=fe:61:62:0e:3b:65),eth_type(0x0800),ipv4(frag=no), packets:0, bytes:0, used:never, actions:3 recirc_id(0),in_port(3),eth(src=fe:61:62:0e:3b:65,dst=56:73:90:10:20:5f),eth_type(0x0806), packets:0, bytes:0, used:never, actions:set(tunnel(dst=192.168.122.175,ttl=64,tp_dst=4789,flags(df))),2 [root@ovs-tunnel-test-1 ~]# [root@ovs-tunnel-test-2 ~]# ovs-vsctl set interface vxlan0 options:tos=inherit [root@ovs-tunnel-test-2 ~]# systemctl restart openvswitch (reverse-i-search)`t': systemctl restart openvswi^Ch [root@ovs-tunnel-test-2 ~]# tshark -nn -i eth0 -O ip port 4789 | egrep 'Frame|Differentiated Services|Explicit Congestion Notification' Running as user \"root\" and group \"root\". This could be dangerous. Capturing on 'eth0' 4 Frame 1: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface 0 Differentiated Services Field: 0xba (DSCP 0x2e: Expedited Forwarding; ECN: 0x02: ECT(0) (ECN-Capable Transport)) 1011 10.. = Differentiated Services Codepoint: Expedited Forwarding (0x2e) .... ..10 = Explicit Congestion Notification: ECT(0) (ECN-Capable Transport) (0x02) Frame 2: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface 0 Differentiated Services Field: 0xba (DSCP 0x2e: Expedited Forwarding; ECN: 0x02: ECT(0) (ECN-Capable Transport)) 1011 10.. = Differentiated Services Codepoint: Expedited Forwarding (0x2e) .... ..10 = Explicit Congestion Notification: ECT(0) (ECN-Capable Transport) (0x02) Frame 3: 92 bytes on wire (736 bits), 92 bytes captured (736 bits) on interface 0 Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00: Not-ECT (Not ECN-Capable Transport)) 0000 00.. = Differentiated Services Codepoint: Default (0x00) .... ..00 = Explicit Congestion Notification: Not-ECT (Not ECN-Capable Transport) (0x00) Frame 4: 92 bytes on wire (736 bits), 92 bytes captured (736 bits) on interface 0 Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00: Not-ECT (Not ECN-Capable Transport)) 0000 00.. = Differentiated Services Codepoint: Default (0x00) ^C [root@ovs-tunnel-test-2 ~]# ovs-dpctl dump-flows 2020-07-30T13:49:15Z|00001|dpif_netlink|INFO|The kernel module does not support meters. recirc_id(0),in_port(3),eth(src=56:73:90:10:20:5f,dst=fe:61:62:0e:3b:65),eth_type(0x0806), packets:0, bytes:0, used:never, actions:set(tunnel(dst=192.168.122.42,ttl=64,tp_dst=4789,flags(df))),2 recirc_id(0),tunnel(tun_id=0x0,src=192.168.122.42,dst=192.168.122.175,tos=0xba,flags(-df-csum+key)),in_port(2),eth(src=fe:61:62:0e:3b:65,dst=56:73:90:10:20:5f),eth_type(0x0800),ipv4(frag=no), packets:0, bytes:0, used:never, actions:3 recirc_id(0),tunnel(tun_id=0x0,src=192.168.122.42,dst=192.168.122.175,flags(-df-csum+key)),in_port(2),eth(src=fe:61:62:0e:3b:65,dst=56:73:90:10:20:5f),eth_type(0x0806), packets:0, bytes:0, used:never, actions:3 recirc_id(0),in_port(3),eth(src=56:73:90:10:20:5f,dst=fe:61:62:0e:3b:65),eth_type(0x0800),ipv4(tos=0xba,frag=no), packets:0, bytes:0, used:never, actions:set(tunnel(dst=192.168.122.42,tos=0xba,ttl=64,tp_dst=4789,flags(df))),2 [root@ovs-tunnel-test-2 ~]# And explicitly setting this, e.g. to 1111 1100: [root@ovs-tunnel-test-1 ~]# systemctl restart openvswitch [root@ovs-tunnel-test-1 ~]# ovs-vsctl set interface vxlan0 options:tos=0xfc [root@ovs-tunnel-test-1 ~]# ip netns exec private ping 192.168.123.2 -c1 -W1 -Q 0xba PING 192.168.123.2 (192.168.123.2) 56(84) bytes of data. 64 bytes from 192.168.123.2: icmp_seq=1 ttl=64 time=2.33 ms --- 192.168.123.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 2.339/2.339/2.339/0.000 ms [root@ovs-tunnel-test-1 ~]# ovs-dpctl dump-flows 2020-07-30T13:55:10Z|00001|dpif_netlink|INFO|The kernel module does not support meters. recirc_id(0),in_port(3),eth(src=fe:61:62:0e:3b:65,dst=56:73:90:10:20:5f),eth_type(0x0800),ipv4(tos=0x2/0x3,frag=no), packets:0, bytes:0, used:never, actions:set(tunnel(dst=192.168.122.175,tos=0xfe,ttl=64,tp_dst=4789,flags(df))),2 recirc_id(0),tunnel(tun_id=0x0,src=192.168.122.175,dst=192.168.122.42,tos=0xfe,flags(-df-csum+key)),in_port(2),eth(src=56:73:90:10:20:5f,dst=fe:61:62:0e:3b:65),eth_type(0x0800),ipv4(frag=no), packets:0, bytes:0, used:never, actions:3 [root@ovs-tunnel-test-1 ~]# [root@ovs-tunnel-test-2 ~]# systemctl restart openvswitch [root@ovs-tunnel-test-2 ~]# ovs-vsctl set interface vxlan0 options:tos=0xfc [root@ovs-tunnel-test-2 ~]# tshark -nn -i eth0 -O ip port 4789 | egrep 'Frame|Differentiated Services|Explicit Congestion Notification' Running as user \"root\" and group \"root\". This could be dangerous. Capturing on 'eth0' 4 Frame 1: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface 0 Differentiated Services Field: 0xfe (DSCP 0x3f: Unknown DSCP; ECN: 0x02: ECT(0) (ECN-Capable Transport)) 1111 11.. = Differentiated Services Codepoint: Unknown (0x3f) .... ..10 = Explicit Congestion Notification: ECT(0) (ECN-Capable Transport) (0x02) Frame 2: 148 bytes on wire (1184 bits), 148 bytes captured (1184 bits) on interface 0 Differentiated Services Field: 0xfe (DSCP 0x3f: Unknown DSCP; ECN: 0x02: ECT(0) (ECN-Capable Transport)) 1111 11.. = Differentiated Services Codepoint: Unknown (0x3f) .... ..10 = Explicit Congestion Notification: ECT(0) (ECN-Capable Transport) (0x02) Frame 3: 92 bytes on wire (736 bits), 92 bytes captured (736 bits) on interface 0 Differentiated Services Field: 0xfc (DSCP 0x3f: Unknown DSCP; ECN: 0x00: Not-ECT (Not ECN-Capable Transport)) 1111 11.. = Differentiated Services Codepoint: Unknown (0x3f) .... ..00 = Explicit Congestion Notification: Not-ECT (Not ECN-Capable Transport) (0x00) Frame 4: 92 bytes on wire (736 bits), 92 bytes captured (736 bits) on interface 0 Differentiated Services Field: 0xfc (DSCP 0x3f: Unknown DSCP; ECN: 0x00: Not-ECT (Not ECN-Capable Transport)) 1111 11.. = Differentiated Services Codepoint: Unknown (0x3f) ^C [root@ovs-tunnel-test-2 ~]# ovs-dpctl dump-flows 2020-07-30T13:55:18Z|00001|dpif_netlink|INFO|The kernel module does not support meters. recirc_id(0),in_port(3),eth(src=56:73:90:10:20:5f,dst=fe:61:62:0e:3b:65),eth_type(0x0806), packets:0, bytes:0, used:never, actions:set(tunnel(dst=192.168.122.42,tos=0xfc,ttl=64,tp_dst=4789,flags(df))),2 recirc_id(0),tunnel(tun_id=0x0,src=192.168.122.42,dst=192.168.122.175,tos=0xfc,flags(-df-csum+key)),in_port(2),eth(src=fe:61:62:0e:3b:65,dst=56:73:90:10:20:5f),eth_type(0x0806), packets:0, bytes:0, used:never, actions:3 [root@ovs-tunnel-test-2 ~]# The outer header is correctly set to 1111 1110.","title":"Test with RHEL 7.7"},{"location":"networking/ovs-vxlan-tunnels-and-dscp/#bug","text":"My tests were affected by: https://bugzilla.redhat.com/show_bug.cgi?id=1862166","title":"BUG"},{"location":"networking/ovs-vxlan-tunnels-and-dscp/#ecn-field","text":"Is always inherited: https://github.com/openvswitch/ovs/blob/252c24a61774cac1f593569dcce31e524725676c/ofproto/tunnel.c#L454 /* ECN fields are always inherited. */ if (is_ip_any(flow)) { wc->masks.nw_tos |= IP_ECN_MASK; if (IP_ECN_is_ce(flow->nw_tos)) { flow->tunnel.ip_tos |= IP_ECN_ECT_0; } else { flow->tunnel.ip_tos |= flow->nw_tos & IP_ECN_MASK; } }","title":"ECN field"},{"location":"networking/ovs-vxlan-tunnels-and-dscp/#resources","text":"https://www.tucny.com/Home/dscp-tos http://docs.openvswitch.org/en/latest/howto/userspace-tunneling/","title":"Resources"},{"location":"networking/ovs_recirculation/","text":"OVS packet recirculation Packet recirculation means that a packet is sent back to the processing engine for another evaluation after it has already been processed. This can for example happen when an outer header is stripped off the packet such as VXLAN tunneling or MPLS. It is also important for connection tracking in OVS and is also used in OVS bonds when hashing packets to different ports. https://lwn.net/Articles/546476/ (...) Recirculation is a technique to allow a frame to re-enter frame processing. This is intended to be used after actions have been applied to the frame with modify the frame in some way that makes it possible for richer processing to occur. An example is and indeed targeted use case is MPLS. If an MPLS frame has an mpls_pop action applied with the IPv4 ethernet type then it becomes possible to decode the IPv4 portion of the frame. This may be used to construct a facet that modifies the IPv4 portion of the frame. This is not possible prior to the mpls_pop action as the contents of the frame after the MPLS stack is not known to be IPv4. (...) Also: https://lists.openwall.net/netdev/2013/04/16/16 http://www.openvswitch.org/support/dist-docs/ovs-vswitchd.8.txt (...) The sum of \"emc hits\", \"smc hits\", \"megaflow hits\" and \"miss\" is the number of packet lookups performed by the datapath. Beware that a recirculated packet experiences one additional lookup per recirculation, so there may be more lookups than forwarded pack\u2010 ets in the datapath. (...) https://mail.openvswitch.org/pipermail/ovs-dev/2017-March/330278.html https://patchwork.ozlabs.org/patch/709823/ https://lwn.net/Articles/679808/ (...) The IPv4 traffic coming from port 2 is first matched for the non-tracked state (-trk), which means that the packet has not been through a CT action yet. Such traffic is run trough the conntrack in zone 1 and all packets associated with a NATted connection are NATted also in the return direction. After the packet has been through conntrack it is recirculated back to OpenFlow table 0 (which is the default table, so all the rules above are in table 0). The CT action changes the 'trk' flag to being set, so the packets after recirculation no longer match the second rule. The third rule then matches the recirculated packets that were marked as established by conntrack (+est), and the packet is output on port 1. Matching on ct_zone is not strictly needed, but in this test case it verifies that the ct_zone key attribute is properly set by the conntrack action. (...) From openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h : openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * Freezing and recirculation openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * ========================== openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Freezing is a technique for halting and checkpointing packet translation in openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * a way that it can be restarted again later. This file has a couple of data openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * structures related to freezing in general; their names begin with \"frozen\". openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation is the use of freezing to allow a frame to re-enter the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * datapath packet processing path to achieve more flexible packet processing, openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * such as modifying header fields after MPLS POP action and selecting a slave openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * port for bond ports. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Data path and user space interface openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * ----------------------------------- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation uses two uint32_t fields, recirc_id and dp_hash, and a RECIRC openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * action. recirc_id is used to select the next packet processing steps among openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * multiple instances of recirculation. When a packet initially enters the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * datapath it is assigned with recirc_id 0, which indicates no recirculation. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirc_ids are managed by the user space, opaque to the datapath. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * On the other hand, dp_hash can only be computed by the datapath, opaque to openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * the user space, as the datapath is free to choose the hashing algorithm openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * without informing user space about it. The dp_hash value should be openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * wildcarded for newly received packets. HASH action specifies whether the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * hash is computed, and if computed, how many fields are to be included in the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * hash computation. The computed hash value is stored into the dp_hash field openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * prior to recirculation. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * The RECIRC action sets the recirc_id field and then reprocesses the packet openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * as if it was received again on the same input port. RECIRC action works openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * like a function call; actions listed after the RECIRC action will be openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * executed after recirculation. RECIRC action can be nested, but datapath openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * implementation limits the number of nested recirculations to prevent openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * unreasonable nesting depth or infinite loop. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * User space recirculation context openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * --------------------------------- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation is usually hidden from the OpenFlow controllers. Action openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * translation code deduces when recirculation is necessary and issues a openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * datapath recirculation action. All OpenFlow actions to be performed after openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * recirculation are derived from the OpenFlow pipeline and are stored with the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * recirculation ID. When the OpenFlow tables are changed in a way affecting openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * the recirculation flows, new recirculation ID with new metadata and actions openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * is allocated and the old one is timed out. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation ID pool openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * ---------------------- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation ID needs to be unique for all datapaths. Recirculation ID openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * pool keeps track of recirculation ids and stores OpenFlow pipeline openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * translation context so that flow processing may continue after openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * recirculation. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * A Recirculation ID can be any uint32_t value, except for that the value 0 is openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * reserved for 'no recirculation' case. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-#include \"ofproto-dpif-mirror.h\" openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-#include \"openvswitch/list.h\" openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-#include \"openvswitch/ofp-actions.h\" openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-#include \"ovs-thread.h\" openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-#include \"uuid.h\" openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-struct ofproto_dpif; openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-struct rule; openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-/* openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * Freezing and recirculation openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * ========================== openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Freezing is a technique for halting and checkpointing packet translation in openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * a way that it can be restarted again later. This file has a couple of data openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * structures related to freezing in general; their names begin with \"frozen\". openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation is the use of freezing to allow a frame to re-enter the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * datapath packet processing path to achieve more flexible packet processing, openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * such as modifying header fields after MPLS POP action and selecting a slave openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * port for bond ports. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Data path and user space interface openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * ----------------------------------- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation uses two uint32_t fields, recirc_id and dp_hash, and a RECIRC openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * action. recirc_id is used to select the next packet processing steps among openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * multiple instances of recirculation. When a packet initially enters the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * datapath it is assigned with recirc_id 0, which indicates no recirculation. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirc_ids are managed by the user space, opaque to the datapath. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * On the other hand, dp_hash can only be computed by the datapath, opaque to openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * the user space, as the datapath is free to choose the hashing algorithm openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * without informing user space about it. The dp_hash value should be openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * wildcarded for newly received packets. HASH action specifies whether the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * hash is computed, and if computed, how many fields are to be included in the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * hash computation. The computed hash value is stored into the dp_hash field openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * prior to recirculation. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * The RECIRC action sets the recirc_id field and then reprocesses the packet openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * as if it was received again on the same input port. RECIRC action works openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * like a function call; actions listed after the RECIRC action will be openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * executed after recirculation. RECIRC action can be nested, but datapath openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * implementation limits the number of nested recirculations to prevent openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * unreasonable nesting depth or infinite loop. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * User space recirculation context openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * --------------------------------- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation is usually hidden from the OpenFlow controllers. Action openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * translation code deduces when recirculation is necessary and issues a openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * datapath recirculation action. All OpenFlow actions to be performed after openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * recirculation are derived from the OpenFlow pipeline and are stored with the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * recirculation ID. When the OpenFlow tables are changed in a way affecting openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * the recirculation flows, new recirculation ID with new metadata and actions openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * is allocated and the old one is timed out. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation ID pool openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * ---------------------- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation ID needs to be unique for all datapaths. Recirculation ID openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * pool keeps track of recirculation ids and stores OpenFlow pipeline openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * translation context so that flow processing may continue after openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * recirculation. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * A Recirculation ID can be any uint32_t value, except for that the value 0 is openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * reserved for 'no recirculation' case. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Thread-safety openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * -------------- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * All APIs are thread safe. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- */ openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h:/* Metadata for restoring pipeline context after recirculation. Helpers openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * are inlined below to keep them together with the definition for easier openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * updates. */ openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-BUILD_ASSERT_DECL(FLOW_WC_SEQ == 40); openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-struct frozen_metadata { openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- /* Metadata in struct flow. */ openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- struct flow_tnl tunnel; /* Encapsulating tunnel parameters. */ openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- ovs_be64 metadata; /* OpenFlow Metadata. */ openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- uint64_t regs[FLOW_N_XREGS]; /* Registers. */ openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- ofp_port_t in_port; /* Incoming port. */ -- openvswitch-2.9.0/tests/ofproto-dpif.at:# This test verifies that the table ID is preserved across recirculation openvswitch-2.9.0/tests/ofproto-dpif.at-# when a resubmit action requires it (because the action is relative to openvswitch-2.9.0/tests/ofproto-dpif.at-# the current table rather than specifying a table). openvswitch-2.9.0/tests/ofproto-dpif.at:AT_SETUP([ofproto-dpif - resubmit with recirculation]) -- openvswitch-2.9.0/tests/ofproto-dpif.at-# This test verifies that tunnel metadata is preserved across openvswitch-2.9.0/tests/ofproto-dpif.at:# recirculation. At the time of recirculation, fields such as \"tun_id\" openvswitch-2.9.0/tests/ofproto-dpif.at-# may be set before the tunnel is \"valid\" (ie, has a destination openvswitch-2.9.0/tests/ofproto-dpif.at:# address), but the field should still be available after recirculation. openvswitch-2.9.0/tests/ofproto-dpif.at-AT_SETUP([ofproto-dpif - resubmit with tun_id]) -- openvswitch-2.9.0/tests/ofproto-dpif.at:# This test verifies that \"resubmit\", when it triggers recirculation openvswitch-2.9.0/tests/ofproto-dpif.at-# indirectly through the flow that it recursively invokes, is not openvswitch-2.9.0/tests/ofproto-dpif.at:# re-executed when execution continues later post-recirculation. openvswitch-2.9.0/tests/ofproto-dpif.at:AT_SETUP([ofproto-dpif - recirculation after resubmit]) -- openvswitch-2.9.0/tests/system-traffic.at-dnl Checks the implementation of conntrack with FTP ALGs in combination with openvswitch-2.9.0/tests/system-traffic.at-dnl NAT, with flow tables that implement the NATing after the first round openvswitch-2.9.0/tests/system-traffic.at:dnl of recirculation - that is, the first flow ct(table=foo) then a subsequent openvswitch-2.9.0/tests/system-traffic.at-dnl flow will implement the NATing with ct(nat..),output:foo. -- openvswitch-2.9.0/tests/system-traffic.at-dnl Checks the implementation of conntrack original direction tuple matching openvswitch-2.9.0/tests/system-traffic.at-dnl with FTP ALGs in combination with NAT, with flow tables that implement openvswitch-2.9.0/tests/system-traffic.at:dnl the NATing before the first round of recirculation - that is, the first openvswitch-2.9.0/tests/system-traffic.at-dnl flow ct(nat, table=foo) then a subsequent flow will implement the openvswitch-2.9.0/tests/system-traffic.at-dnl commiting of NATed and other connections with ct(nat..),output:foo. -- openvswitch-2.9.0/tests/packet-type-aware.at:# Goto_table after pop_mpls triggers recirculation. openvswitch-2.9.0/tests/packet-type-aware.at-AT_CHECK([ openvswitch-2.9.0/tests/packet-type-aware.at- ovs-ofctl del-flows br0 && openvswitch-2.9.0/tests/packet-type-aware.at- ovs-ofctl del-flows int-br && openvswitch-2.9.0/tests/packet-type-aware.at- ovs-ofctl add-flow br0 \"actions=normal\" openvswitch-2.9.0/tests/packet-type-aware.at- ovs-ofctl add-flow int-br \"table=0,in_port=tunnel,actions=pop_mpls:0x800,goto_table:20\" && openvswitch-2.9.0/tests/packet-type-aware.at- ovs-ofctl add-flow int-br \"table=20,actions=dec_ttl,output:LOCAL\" openvswitch-2.9.0/tests/packet-type-aware.at-], [0], [ignore]) openvswitch-2.9.0/tests/packet-type-aware.at- -- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-#include \"ofproto-dpif-mirror.h\" openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-#include \"openvswitch/list.h\" openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-#include \"openvswitch/ofp-actions.h\" openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-#include \"ovs-thread.h\" openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-#include \"uuid.h\" openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-struct ofproto_dpif; openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-struct rule; openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-/* openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * Freezing and recirculation openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * ========================== openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Freezing is a technique for halting and checkpointing packet translation in openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * a way that it can be restarted again later. This file has a couple of data openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * structures related to freezing in general; their names begin with \"frozen\". openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation is the use of freezing to allow a frame to re-enter the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * datapath packet processing path to achieve more flexible packet processing, openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * such as modifying header fields after MPLS POP action and selecting a slave openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * port for bond ports. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Data path and user space interface openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * ----------------------------------- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation uses two uint32_t fields, recirc_id and dp_hash, and a RECIRC openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * action. recirc_id is used to select the next packet processing steps among openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * multiple instances of recirculation. When a packet initially enters the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * datapath it is assigned with recirc_id 0, which indicates no recirculation. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirc_ids are managed by the user space, opaque to the datapath. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * On the other hand, dp_hash can only be computed by the datapath, opaque to openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * the user space, as the datapath is free to choose the hashing algorithm openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * without informing user space about it. The dp_hash value should be openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * wildcarded for newly received packets. HASH action specifies whether the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * hash is computed, and if computed, how many fields are to be included in the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * hash computation. The computed hash value is stored into the dp_hash field openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * prior to recirculation. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * The RECIRC action sets the recirc_id field and then reprocesses the packet openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * as if it was received again on the same input port. RECIRC action works openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * like a function call; actions listed after the RECIRC action will be openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * executed after recirculation. RECIRC action can be nested, but datapath openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * implementation limits the number of nested recirculations to prevent openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * unreasonable nesting depth or infinite loop. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * User space recirculation context openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * --------------------------------- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation is usually hidden from the OpenFlow controllers. Action openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * translation code deduces when recirculation is necessary and issues a openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * datapath recirculation action. All OpenFlow actions to be performed after openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * recirculation are derived from the OpenFlow pipeline and are stored with the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * recirculation ID. When the OpenFlow tables are changed in a way affecting openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * the recirculation flows, new recirculation ID with new metadata and actions openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * is allocated and the old one is timed out. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation ID pool openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * ---------------------- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation ID needs to be unique for all datapaths. Recirculation ID openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * pool keeps track of recirculation ids and stores OpenFlow pipeline openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * translation context so that flow processing may continue after openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * recirculation. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * A Recirculation ID can be any uint32_t value, except for that the value 0 is openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * reserved for 'no recirculation' case. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Thread-safety openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * -------------- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * All APIs are thread safe. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- */ openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h:/* Metadata for restoring pipeline context after recirculation. Helpers openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * are inlined below to keep them together with the definition for easier openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * updates. */ openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-BUILD_ASSERT_DECL(FLOW_WC_SEQ == 40); openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-struct frozen_metadata { openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- /* Metadata in struct flow. */ openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- struct flow_tnl tunnel; /* Encapsulating tunnel parameters. */ openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- ovs_be64 metadata; /* OpenFlow Metadata. */ openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- uint64_t regs[FLOW_N_XREGS]; /* Registers. */ openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- ofp_port_t in_port; /* Incoming port. */ -- openvswitch-2.9.0/tests/ofproto-dpif.at:# This test verifies that the table ID is preserved across recirculation openvswitch-2.9.0/tests/ofproto-dpif.at-# when a resubmit action requires it (because the action is relative to openvswitch-2.9.0/tests/ofproto-dpif.at-# the current table rather than specifying a table). openvswitch-2.9.0/tests/ofproto-dpif.at:AT_SETUP([ofproto-dpif - resubmit with recirculation]) -- openvswitch-2.9.0/tests/ofproto-dpif.at-# This test verifies that tunnel metadata is preserved across openvswitch-2.9.0/tests/ofproto-dpif.at:# recirculation. At the time of recirculation, fields such as \"tun_id\" openvswitch-2.9.0/tests/ofproto-dpif.at-# may be set before the tunnel is \"valid\" (ie, has a destination openvswitch-2.9.0/tests/ofproto-dpif.at:# address), but the field should still be available after recirculation. openvswitch-2.9.0/tests/ofproto-dpif.at-AT_SETUP([ofproto-dpif - resubmit with tun_id]) -- openvswitch-2.9.0/tests/ofproto-dpif.at:# This test verifies that \"resubmit\", when it triggers recirculation openvswitch-2.9.0/tests/ofproto-dpif.at-# indirectly through the flow that it recursively invokes, is not openvswitch-2.9.0/tests/ofproto-dpif.at:# re-executed when execution continues later post-recirculation. openvswitch-2.9.0/tests/ofproto-dpif.at:AT_SETUP([ofproto-dpif - recirculation after resubmit]) -- openvswitch-2.9.0/tests/system-traffic.at-dnl Checks the implementation of conntrack with FTP ALGs in combination with openvswitch-2.9.0/tests/system-traffic.at-dnl NAT, with flow tables that implement the NATing after the first round openvswitch-2.9.0/tests/system-traffic.at:dnl of recirculation - that is, the first flow ct(table=foo) then a subsequent openvswitch-2.9.0/tests/system-traffic.at-dnl flow will implement the NATing with ct(nat..),output:foo. -- openvswitch-2.9.0/tests/system-traffic.at-dnl Checks the implementation of conntrack original direction tuple matching openvswitch-2.9.0/tests/system-traffic.at-dnl with FTP ALGs in combination with NAT, with flow tables that implement openvswitch-2.9.0/tests/system-traffic.at:dnl the NATing before the first round of recirculation - that is, the first openvswitch-2.9.0/tests/system-traffic.at-dnl flow ct(nat, table=foo) then a subsequent flow will implement the openvswitch-2.9.0/tests/system-traffic.at-dnl commiting of NATed and other connections with ct(nat..),output:foo. -- openvswitch-2.9.0/tests/packet-type-aware.at:# Goto_table after pop_mpls triggers recirculation. openvswitch-2.9.0/tests/packet-type-aware.at-AT_CHECK([ openvswitch-2.9.0/tests/packet-type-aware.at- ovs-ofctl del-flows br0 && openvswitch-2.9.0/tests/packet-type-aware.at- ovs-ofctl del-flows int-br && openvswitch-2.9.0/tests/packet-type-aware.at- ovs-ofctl add-flow br0 \"actions=normal\" openvswitch-2.9.0/tests/packet-type-aware.at- ovs-ofctl add-flow int-br \"table=0,in_port=tunnel,actions=pop_mpls:0x800,goto_table:20\" && openvswitch-2.9.0/tests/packet-type-aware.at- ovs-ofctl add-flow int-br \"table=20,actions=dec_ttl,output:LOCAL\" openvswitch-2.9.0/tests/packet-type-aware.at-], [0], [ignore]) openvswitch-2.9.0/tests/packet-type-aware.at- -- openvswitch-2.9.0/include/openvswitch/meta-flow.h- /* \"recirc_id\". openvswitch-2.9.0/include/openvswitch/meta-flow.h- * openvswitch-2.9.0/include/openvswitch/meta-flow.h: * ID for recirculation. The value 0 is reserved for initially received openvswitch-2.9.0/include/openvswitch/meta-flow.h- * packets. Internal use only, not programmable from controller. openvswitch-2.9.0/include/openvswitch/meta-flow.h- * openvswitch-2.9.0/include/openvswitch/meta-flow.h- * Type: be32. openvswitch-2.9.0/include/openvswitch/meta-flow.h- * Maskable: no. openvswitch-2.9.0/include/openvswitch/meta-flow.h- * Formatting: decimal. openvswitch-2.9.0/include/openvswitch/meta-flow.h- * Prerequisites: none. openvswitch-2.9.0/include/openvswitch/meta-flow.h- * Access: read-only. openvswitch-2.9.0/include/openvswitch/meta-flow.h- * NXM: NXM_NX_RECIRC_ID(36) since v2.2. openvswitch-2.9.0/include/openvswitch/meta-flow.h- * OXM: none. openvswitch-2.9.0/include/openvswitch/meta-flow.h- */ -- openvswitch-2.9.0/lib/dpif-netdev.c- pmd_perf_update_counter(&pmd->perf_stats, PMD_STAT_MASKED_HIT, openvswitch-2.9.0/lib/dpif-netdev.c- cnt - upcall_ok_cnt - upcall_fail_cnt); openvswitch-2.9.0/lib/dpif-netdev.c- pmd_perf_update_counter(&pmd->perf_stats, PMD_STAT_MASKED_LOOKUP, openvswitch-2.9.0/lib/dpif-netdev.c- lookup_cnt); openvswitch-2.9.0/lib/dpif-netdev.c- pmd_perf_update_counter(&pmd->perf_stats, PMD_STAT_MISS, openvswitch-2.9.0/lib/dpif-netdev.c- upcall_ok_cnt); openvswitch-2.9.0/lib/dpif-netdev.c- pmd_perf_update_counter(&pmd->perf_stats, PMD_STAT_LOST, openvswitch-2.9.0/lib/dpif-netdev.c- upcall_fail_cnt); openvswitch-2.9.0/lib/dpif-netdev.c-} openvswitch-2.9.0/lib/dpif-netdev.c- openvswitch-2.9.0/lib/dpif-netdev.c:/* Packets enter the datapath from a port (or from recirculation) here. openvswitch-2.9.0/lib/dpif-netdev.c- * openvswitch-2.9.0/lib/dpif-netdev.c- * When 'md_is_valid' is true the metadata in 'packets' are already valid. openvswitch-2.9.0/lib/dpif-netdev.c- * When false the metadata in 'packets' need to be initialized. */ openvswitch-2.9.0/lib/dpif-netdev.c-static void openvswitch-2.9.0/lib/dpif-netdev.c-dp_netdev_input__(struct dp_netdev_pmd_thread *pmd, openvswitch-2.9.0/lib/dpif-netdev.c- struct dp_packet_batch *packets, openvswitch-2.9.0/lib/dpif-netdev.c- bool md_is_valid, odp_port_t port_no) openvswitch-2.9.0/lib/dpif-netdev.c-{ openvswitch-2.9.0/lib/dpif-netdev.c-#if !defined(__CHECKER__) && !defined(_WIN32) openvswitch-2.9.0/lib/dpif-netdev.c- const size_t PKT_ARRAY_SIZE = dp_packet_batch_size(packets); -- openvswitch-2.9.0/lib/dpif-netdev.c- md_is_valid, port_no); openvswitch-2.9.0/lib/dpif-netdev.c- if (!dp_packet_batch_is_empty(packets)) { openvswitch-2.9.0/lib/dpif-netdev.c- /* Get ingress port from first packet's metadata. */ openvswitch-2.9.0/lib/dpif-netdev.c- in_port = packets->packets[0]->md.in_port.odp_port; openvswitch-2.9.0/lib/dpif-netdev.c- fast_path_processing(pmd, packets, keys, openvswitch-2.9.0/lib/dpif-netdev.c- batches, &n_batches, in_port); openvswitch-2.9.0/lib/dpif-netdev.c- } openvswitch-2.9.0/lib/dpif-netdev.c- openvswitch-2.9.0/lib/dpif-netdev.c- /* All the flow batches need to be reset before any call to openvswitch-2.9.0/lib/dpif-netdev.c- * packet_batch_per_flow_execute() as it could potentially trigger openvswitch-2.9.0/lib/dpif-netdev.c: * recirculation. When a packet matching flow \u2018j\u2019 happens to be openvswitch-2.9.0/lib/dpif-netdev.c- * recirculated, the nested call to dp_netdev_input__() could potentially openvswitch-2.9.0/lib/dpif-netdev.c- * classify the packet as matching another flow - say 'k'. It could happen openvswitch-2.9.0/lib/dpif-netdev.c- * that in the previous call to dp_netdev_input__() that same flow 'k' had openvswitch-2.9.0/lib/dpif-netdev.c- * already its own batches[k] still waiting to be served. So if its openvswitch-2.9.0/lib/dpif-netdev.c- * \u2018batch\u2019 member is not reset, the recirculated packet would be wrongly openvswitch-2.9.0/lib/dpif-netdev.c- * appended to batches[k] of the 1st call to dp_netdev_input__(). */ openvswitch-2.9.0/lib/dpif-netdev.c- size_t i; openvswitch-2.9.0/lib/dpif-netdev.c- for (i = 0; i < n_batches; i++) { openvswitch-2.9.0/lib/dpif-netdev.c- batches[i].flow->batch = NULL; openvswitch-2.9.0/lib/dpif-netdev.c- } -- openvswitch-2.9.0/lib/dpif-netdev.c- packet->md.recirc_id = nl_attr_get_u32(a); openvswitch-2.9.0/lib/dpif-netdev.c- } openvswitch-2.9.0/lib/dpif-netdev.c- openvswitch-2.9.0/lib/dpif-netdev.c- (*depth)++; openvswitch-2.9.0/lib/dpif-netdev.c- dp_netdev_recirculate(pmd, packets_); openvswitch-2.9.0/lib/dpif-netdev.c- (*depth)--; openvswitch-2.9.0/lib/dpif-netdev.c- openvswitch-2.9.0/lib/dpif-netdev.c- return; openvswitch-2.9.0/lib/dpif-netdev.c- } openvswitch-2.9.0/lib/dpif-netdev.c- openvswitch-2.9.0/lib/dpif-netdev.c: VLOG_WARN(\"Packet dropped. Max recirculation depth exceeded.\"); openvswitch-2.9.0/lib/dpif-netdev.c- break; openvswitch-2.9.0/lib/dpif-netdev.c- openvswitch-2.9.0/lib/dpif-netdev.c- case OVS_ACTION_ATTR_CT: { openvswitch-2.9.0/lib/dpif-netdev.c- const struct nlattr *b; openvswitch-2.9.0/lib/dpif-netdev.c- bool force = false; openvswitch-2.9.0/lib/dpif-netdev.c- bool commit = false; openvswitch-2.9.0/lib/dpif-netdev.c- unsigned int left; openvswitch-2.9.0/lib/dpif-netdev.c- uint16_t zone = 0; openvswitch-2.9.0/lib/dpif-netdev.c- const char *helper = NULL; openvswitch-2.9.0/lib/dpif-netdev.c- const uint32_t *setmark = NULL; -- openvswitch-2.9.0/lib/odp-util.c- break; openvswitch-2.9.0/lib/odp-util.c- default: openvswitch-2.9.0/lib/odp-util.c- /* Only the above protocols are supported for encap. openvswitch-2.9.0/lib/odp-util.c- * The check is done at action translation. */ openvswitch-2.9.0/lib/odp-util.c- OVS_NOT_REACHED(); openvswitch-2.9.0/lib/odp-util.c- } openvswitch-2.9.0/lib/odp-util.c- } else { openvswitch-2.9.0/lib/odp-util.c- /* This is an explicit or implicit decap case. */ openvswitch-2.9.0/lib/odp-util.c- if (pt_ns(flow->packet_type) == OFPHTN_ETHERTYPE && openvswitch-2.9.0/lib/odp-util.c- base_flow->packet_type == htonl(PT_ETH)) { openvswitch-2.9.0/lib/odp-util.c: /* Generate pop_eth and continue without recirculation. */ openvswitch-2.9.0/lib/odp-util.c- odp_put_pop_eth_action(odp_actions); openvswitch-2.9.0/lib/odp-util.c- base_flow->packet_type = flow->packet_type; openvswitch-2.9.0/lib/odp-util.c- base_flow->dl_src = eth_addr_zero; openvswitch-2.9.0/lib/odp-util.c- base_flow->dl_dst = eth_addr_zero; openvswitch-2.9.0/lib/odp-util.c- } else { openvswitch-2.9.0/lib/odp-util.c: /* All other decap cases require recirculation. openvswitch-2.9.0/lib/odp-util.c- * No need to update the base flow here. */ openvswitch-2.9.0/lib/odp-util.c- switch (ntohl(base_flow->packet_type)) { openvswitch-2.9.0/lib/odp-util.c- case PT_NSH: openvswitch-2.9.0/lib/odp-util.c- /* pop_nsh. */ openvswitch-2.9.0/lib/odp-util.c- odp_put_pop_nsh_action(odp_actions); openvswitch-2.9.0/lib/odp-util.c- break; openvswitch-2.9.0/lib/odp-util.c- default: openvswitch-2.9.0/lib/odp-util.c- /* Checks are done during translation. */ openvswitch-2.9.0/lib/odp-util.c- OVS_NOT_REACHED(); openvswitch-2.9.0/lib/odp-util.c- } -- openvswitch-2.9.0/ovn/lib/actions.c- nat->flags |= NX_NAT_F_DST; openvswitch-2.9.0/ovn/lib/actions.c- } openvswitch-2.9.0/ovn/lib/actions.c- } openvswitch-2.9.0/ovn/lib/actions.c- openvswitch-2.9.0/ovn/lib/actions.c- ofpacts->header = ofpbuf_push_uninit(ofpacts, nat_offset); openvswitch-2.9.0/ovn/lib/actions.c- ct = ofpacts->header; openvswitch-2.9.0/ovn/lib/actions.c- if (cn->ip) { openvswitch-2.9.0/ovn/lib/actions.c- ct->flags |= NX_CT_F_COMMIT; openvswitch-2.9.0/ovn/lib/actions.c- } else if (snat && ep->is_gateway_router) { openvswitch-2.9.0/ovn/lib/actions.c- /* For performance reasons, we try to prevent additional openvswitch-2.9.0/ovn/lib/actions.c: * recirculations. ct_snat which is used in a gateway router openvswitch-2.9.0/ovn/lib/actions.c: * does not need a recirculation. ct_snat(IP) does need a openvswitch-2.9.0/ovn/lib/actions.c: * recirculation. ct_snat in a distributed router needs openvswitch-2.9.0/ovn/lib/actions.c: * recirculation regardless of whether an IP address is openvswitch-2.9.0/ovn/lib/actions.c- * specified. openvswitch-2.9.0/ovn/lib/actions.c- * XXX Should we consider a method to let the actions specify openvswitch-2.9.0/ovn/lib/actions.c: * whether an action needs recirculation if there are more use openvswitch-2.9.0/ovn/lib/actions.c- * cases?. */ openvswitch-2.9.0/ovn/lib/actions.c- ct->recirc_table = NX_CT_RECIRC_NONE; openvswitch-2.9.0/ovn/lib/actions.c- } openvswitch-2.9.0/ovn/lib/actions.c- ofpact_finish(ofpacts, &ct->ofpact); openvswitch-2.9.0/ovn/lib/actions.c- ofpbuf_push_uninit(ofpacts, ct_offset); openvswitch-2.9.0/ovn/lib/actions.c-} openvswitch-2.9.0/ovn/lib/actions.c- openvswitch-2.9.0/ovn/lib/actions.c-static void openvswitch-2.9.0/ovn/lib/actions.c-encode_CT_DNAT(const struct ovnact_ct_nat *cn, openvswitch-2.9.0/ovn/lib/actions.c- const struct ovnact_encode_params *ep, -- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * later. We call the checkpointing process \"freezing\" and the restarting openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * process \"thawing\". openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * The use cases for freezing are: openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * - \"Recirculation\", where the translation process discovers that it openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * doesn't have enough information to complete translation without openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * actually executing the actions that have already been translated, openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * which provides the additionally needed information. In these openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * situations, translation freezes translation and assigns the frozen openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: * data a unique \"recirculation ID\", which it associates with the data openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * in a table in userspace (see ofproto-dpif-rid.h). It also adds a openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * OVS_ACTION_ATTR_RECIRC action specifying that ID to the datapath openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * actions. When a packet hits that action, the datapath looks its openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * flow up again using the ID. If there's a miss, it comes back to openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: * userspace, which find the recirculation table entry for the ID, openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * thaws the associated frozen data, and continues translation from openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * that point given the additional information that is now known. openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * The archetypal example is MPLS. As MPLS is implemented in openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * OpenFlow, the protocol that follows the last MPLS label becomes openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * known only when that label is popped by an OpenFlow action. That openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * means that Open vSwitch can't extract the headers beyond the MPLS openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * labels until the pop action is executed. Thus, at that point openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: * translation uses the recirculation process to extract the headers openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * beyond the MPLS labels. openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * (OVS also uses OVS_ACTION_ATTR_RECIRC to implement hashing for openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * output to bonds. OVS pre-populates all the datapath flows for bond openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * output in the datapath, though, which means that the elaborate openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * process of coming back to userspace for a second round of openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * translation isn't needed, and so bonds don't follow the above openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * process.) openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * - \"Continuation\". A continuation is a way for an OpenFlow controller -- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * translation process: openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * 1. Sets 'freezing' to true. openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * 2. Sets 'exit' to true to tell later steps that we're exiting from the openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * translation process. openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * 3. Adds an OFPACT_UNROLL_XLATE action to 'frozen_actions', and points openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * frozen_actions.header to the action to make it easy to find it later. openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * This action holds the current table ID and cookie so that they can be openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: * restored during a post-recirculation upcall translation. openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: * 4. Adds the action that prompted recirculation and any actions following openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * it within the same flow to 'frozen_actions', so that they can be openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: * executed during a post-recirculation upcall translation. openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * 5. Returns. openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: * 6. The action that prompted recirculation might be nested in a stack of openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * nested \"resubmit\"s that have actions remaining. Each of these notices openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * that we're exiting and freezing and responds by adding more openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * OFPACT_UNROLL_XLATE actions to 'frozen_actions', as necessary, openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * followed by any actions that were yet unprocessed. openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: * If we're freezing because of recirculation, the caller generates a openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: * recirculation ID and associates all the state produced by this process openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: * with it. For post-recirculation upcall translation, the caller passes it openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * back in for the new translation to execute. The process yielded a set of openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * ofpacts that can be translated directly, so it is not much of a special openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * case at that point. openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- */ openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- bool freezing; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: bool recirc_update_dp_hash; /* Generated recirculation will be preceded openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * by datapath HASH action to get an updated openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: * dp_hash after recirculation. */ openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- uint32_t dp_hash_alg; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- uint32_t dp_hash_basis; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- struct ofpbuf frozen_actions; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- const struct ofpact_controller *pause; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- /* True if a packet was but is no longer MPLS (due to an MPLS pop action). openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: * This is a trigger for recirculation in cases where translating an action openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * or looking up a flow requires access to the fields of the packet after openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * the MPLS label stack that was originally present. */ openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- bool was_mpls; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- /* True if conntrack has been performed on this packet during processing openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * on the current bridge. This is used to determine whether conntrack openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * state from the datapath should be honored after thawing. */ openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- bool conntracked; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- /* Pointer to an embedded NAT action in a conntrack action, or NULL. */ -- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c-static void xvlan_put(struct flow *, const struct xvlan *); openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c-static void xvlan_input_translate(const struct xbundle *, openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- const struct xvlan *in, openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- struct xvlan *xvlan); openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c-static void xvlan_output_translate(const struct xbundle *, openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- const struct xvlan *xvlan, openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- struct xvlan *out); openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c-static void output_normal(struct xlate_ctx *, const struct xbundle *, openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- const struct xvlan *); openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c:/* Optional bond recirculation parameter to compose_output_action(). */ openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c-struct xlate_bond_recirc { openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: uint32_t recirc_id; /* !0 Use recirculation instead of output. */ openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- uint8_t hash_alg; /* !0 Compute hash for recirc before. */ openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- uint32_t hash_basis; /* Compute hash for recirc before. */ openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c-}; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c-static void compose_output_action(struct xlate_ctx *, ofp_port_t ofp_port, openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- const struct xlate_bond_recirc *xr, openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- bool is_last_action, bool truncate); openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c-static struct xbridge *xbridge_lookup(struct xlate_cfg *, openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- const struct ofproto_dpif *); -- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- /* If packet is recirculated, xport can be retrieved from frozen state. */ openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- if (flow->recirc_id) { openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- const struct recirc_id_node *recirc_id_node; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- recirc_id_node = recirc_id_node_find(flow->recirc_id); openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- if (OVS_UNLIKELY(!recirc_id_node)) { openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- return NULL; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- } openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: /* If recirculation was initiated due to bond (in_port = OFPP_NONE) openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * then frozen state is static and xport_uuid is not defined, so xport openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * cannot be restored from frozen state. */ openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- if (recirc_id_node->state.metadata.in_port != OFPP_NONE) { openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- struct uuid xport_uuid = recirc_id_node->state.xport_uuid; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- xport = xport_lookup_by_uuid(xcfg, &xport_uuid); openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- if (xport && xport->xbridge && xport->xbridge->ofproto) { openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- goto out; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- } openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- } openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- } -- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- netdev_init_tnl_build_header_params(&tnl_params, flow, &s_ip6, dmac, smac); openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- err = tnl_port_build_header(xport->ofport, &tnl_push_data, &tnl_params); openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- if (err) { openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- return err; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- } openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- tnl_push_data.tnl_port = tunnel_odp_port; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- tnl_push_data.out_port = out_dev->odp_port; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- /* After tunnel header has been added, MAC and IP data of flow and openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: * base_flow need to be set properly, since there is not recirculation openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * any more when sending packet to tunnel. */ openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- propagate_tunnel_data_to_flow(ctx, dmac, smac, s_ip6, openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- s_ip, tnl_params.is_ipv6, openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- tnl_push_data.tnl_type); openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- size_t clone_ofs = 0; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- size_t push_action_size; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- clone_ofs = nl_msg_start_nested(ctx->odp_actions, OVS_ACTION_ATTR_CLONE); -- penvswitch-2.9.0/ofproto/bond.h-void *bond_choose_output_slave(struct bond *, const struct flow *, openvswitch-2.9.0/ofproto/bond.h- struct flow_wildcards *, uint16_t vlan); openvswitch-2.9.0/ofproto/bond.h- openvswitch-2.9.0/ofproto/bond.h-/* Rebalancing. */ openvswitch-2.9.0/ofproto/bond.h-void bond_account(struct bond *, const struct flow *, uint16_t vlan, openvswitch-2.9.0/ofproto/bond.h- uint64_t n_bytes); openvswitch-2.9.0/ofproto/bond.h-void bond_rebalance(struct bond *); openvswitch-2.9.0/ofproto/bond.h- openvswitch-2.9.0/ofproto/bond.h-/* Recirculation openvswitch-2.9.0/ofproto/bond.h- * openvswitch-2.9.0/ofproto/bond.h: * Only balance_tcp mode uses recirculation. openvswitch-2.9.0/ofproto/bond.h- * openvswitch-2.9.0/ofproto/bond.h: * When recirculation is used, each bond port is assigned with a unique openvswitch-2.9.0/ofproto/bond.h- * recirc_id. The output action to the bond port will be replaced by openvswitch-2.9.0/ofproto/bond.h- * a Hash action, followed by a RECIRC action. openvswitch-2.9.0/ofproto/bond.h- * openvswitch-2.9.0/ofproto/bond.h- * ... actions= ... HASH(hash(L4)), RECIRC(recirc_id) .... openvswitch-2.9.0/ofproto/bond.h- * openvswitch-2.9.0/ofproto/bond.h: * On handling first output packet, 256 post recirculation flows are installed: openvswitch-2.9.0/ofproto/bond.h- * openvswitch-2.9.0/ofproto/bond.h- * recirc_id=<bond_recirc_id>, dp_hash=<[0..255]>/0xff, actions: output<slave> openvswitch-2.9.0/ofproto/bond.h- * openvswitch-2.9.0/ofproto/bond.h: * Bond module pulls stats from those post recirculation rules. If rebalancing openvswitch-2.9.0/ofproto/bond.h- * is needed, those rules are updated with new output actions. openvswitch-2.9.0/ofproto/bond.h-*/ openvswitch-2.9.0/ofproto/bond.h-void bond_update_post_recirc_rules(struct bond *, uint32_t *recirc_id, openvswitch-2.9.0/ofproto/bond.h- uint32_t *hash_basis); openvswitch-2.9.0/ofproto/bond.h-#endif /* bond.h */","title":"OVS packet recirculation"},{"location":"networking/ovs_recirculation/#ovs-packet-recirculation","text":"Packet recirculation means that a packet is sent back to the processing engine for another evaluation after it has already been processed. This can for example happen when an outer header is stripped off the packet such as VXLAN tunneling or MPLS. It is also important for connection tracking in OVS and is also used in OVS bonds when hashing packets to different ports. https://lwn.net/Articles/546476/ (...) Recirculation is a technique to allow a frame to re-enter frame processing. This is intended to be used after actions have been applied to the frame with modify the frame in some way that makes it possible for richer processing to occur. An example is and indeed targeted use case is MPLS. If an MPLS frame has an mpls_pop action applied with the IPv4 ethernet type then it becomes possible to decode the IPv4 portion of the frame. This may be used to construct a facet that modifies the IPv4 portion of the frame. This is not possible prior to the mpls_pop action as the contents of the frame after the MPLS stack is not known to be IPv4. (...) Also: https://lists.openwall.net/netdev/2013/04/16/16 http://www.openvswitch.org/support/dist-docs/ovs-vswitchd.8.txt (...) The sum of \"emc hits\", \"smc hits\", \"megaflow hits\" and \"miss\" is the number of packet lookups performed by the datapath. Beware that a recirculated packet experiences one additional lookup per recirculation, so there may be more lookups than forwarded pack\u2010 ets in the datapath. (...) https://mail.openvswitch.org/pipermail/ovs-dev/2017-March/330278.html https://patchwork.ozlabs.org/patch/709823/ https://lwn.net/Articles/679808/ (...) The IPv4 traffic coming from port 2 is first matched for the non-tracked state (-trk), which means that the packet has not been through a CT action yet. Such traffic is run trough the conntrack in zone 1 and all packets associated with a NATted connection are NATted also in the return direction. After the packet has been through conntrack it is recirculated back to OpenFlow table 0 (which is the default table, so all the rules above are in table 0). The CT action changes the 'trk' flag to being set, so the packets after recirculation no longer match the second rule. The third rule then matches the recirculated packets that were marked as established by conntrack (+est), and the packet is output on port 1. Matching on ct_zone is not strictly needed, but in this test case it verifies that the ct_zone key attribute is properly set by the conntrack action. (...) From openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h : openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * Freezing and recirculation openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * ========================== openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Freezing is a technique for halting and checkpointing packet translation in openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * a way that it can be restarted again later. This file has a couple of data openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * structures related to freezing in general; their names begin with \"frozen\". openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation is the use of freezing to allow a frame to re-enter the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * datapath packet processing path to achieve more flexible packet processing, openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * such as modifying header fields after MPLS POP action and selecting a slave openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * port for bond ports. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Data path and user space interface openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * ----------------------------------- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation uses two uint32_t fields, recirc_id and dp_hash, and a RECIRC openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * action. recirc_id is used to select the next packet processing steps among openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * multiple instances of recirculation. When a packet initially enters the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * datapath it is assigned with recirc_id 0, which indicates no recirculation. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirc_ids are managed by the user space, opaque to the datapath. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * On the other hand, dp_hash can only be computed by the datapath, opaque to openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * the user space, as the datapath is free to choose the hashing algorithm openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * without informing user space about it. The dp_hash value should be openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * wildcarded for newly received packets. HASH action specifies whether the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * hash is computed, and if computed, how many fields are to be included in the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * hash computation. The computed hash value is stored into the dp_hash field openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * prior to recirculation. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * The RECIRC action sets the recirc_id field and then reprocesses the packet openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * as if it was received again on the same input port. RECIRC action works openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * like a function call; actions listed after the RECIRC action will be openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * executed after recirculation. RECIRC action can be nested, but datapath openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * implementation limits the number of nested recirculations to prevent openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * unreasonable nesting depth or infinite loop. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * User space recirculation context openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * --------------------------------- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation is usually hidden from the OpenFlow controllers. Action openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * translation code deduces when recirculation is necessary and issues a openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * datapath recirculation action. All OpenFlow actions to be performed after openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * recirculation are derived from the OpenFlow pipeline and are stored with the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * recirculation ID. When the OpenFlow tables are changed in a way affecting openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * the recirculation flows, new recirculation ID with new metadata and actions openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * is allocated and the old one is timed out. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation ID pool openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * ---------------------- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation ID needs to be unique for all datapaths. Recirculation ID openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * pool keeps track of recirculation ids and stores OpenFlow pipeline openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * translation context so that flow processing may continue after openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * recirculation. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * A Recirculation ID can be any uint32_t value, except for that the value 0 is openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * reserved for 'no recirculation' case. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-#include \"ofproto-dpif-mirror.h\" openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-#include \"openvswitch/list.h\" openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-#include \"openvswitch/ofp-actions.h\" openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-#include \"ovs-thread.h\" openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-#include \"uuid.h\" openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-struct ofproto_dpif; openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-struct rule; openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-/* openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * Freezing and recirculation openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * ========================== openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Freezing is a technique for halting and checkpointing packet translation in openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * a way that it can be restarted again later. This file has a couple of data openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * structures related to freezing in general; their names begin with \"frozen\". openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation is the use of freezing to allow a frame to re-enter the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * datapath packet processing path to achieve more flexible packet processing, openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * such as modifying header fields after MPLS POP action and selecting a slave openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * port for bond ports. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Data path and user space interface openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * ----------------------------------- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation uses two uint32_t fields, recirc_id and dp_hash, and a RECIRC openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * action. recirc_id is used to select the next packet processing steps among openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * multiple instances of recirculation. When a packet initially enters the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * datapath it is assigned with recirc_id 0, which indicates no recirculation. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirc_ids are managed by the user space, opaque to the datapath. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * On the other hand, dp_hash can only be computed by the datapath, opaque to openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * the user space, as the datapath is free to choose the hashing algorithm openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * without informing user space about it. The dp_hash value should be openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * wildcarded for newly received packets. HASH action specifies whether the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * hash is computed, and if computed, how many fields are to be included in the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * hash computation. The computed hash value is stored into the dp_hash field openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * prior to recirculation. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * The RECIRC action sets the recirc_id field and then reprocesses the packet openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * as if it was received again on the same input port. RECIRC action works openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * like a function call; actions listed after the RECIRC action will be openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * executed after recirculation. RECIRC action can be nested, but datapath openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * implementation limits the number of nested recirculations to prevent openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * unreasonable nesting depth or infinite loop. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * User space recirculation context openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * --------------------------------- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation is usually hidden from the OpenFlow controllers. Action openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * translation code deduces when recirculation is necessary and issues a openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * datapath recirculation action. All OpenFlow actions to be performed after openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * recirculation are derived from the OpenFlow pipeline and are stored with the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * recirculation ID. When the OpenFlow tables are changed in a way affecting openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * the recirculation flows, new recirculation ID with new metadata and actions openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * is allocated and the old one is timed out. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation ID pool openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * ---------------------- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation ID needs to be unique for all datapaths. Recirculation ID openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * pool keeps track of recirculation ids and stores OpenFlow pipeline openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * translation context so that flow processing may continue after openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * recirculation. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * A Recirculation ID can be any uint32_t value, except for that the value 0 is openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * reserved for 'no recirculation' case. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Thread-safety openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * -------------- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * All APIs are thread safe. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- */ openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h:/* Metadata for restoring pipeline context after recirculation. Helpers openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * are inlined below to keep them together with the definition for easier openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * updates. */ openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-BUILD_ASSERT_DECL(FLOW_WC_SEQ == 40); openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-struct frozen_metadata { openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- /* Metadata in struct flow. */ openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- struct flow_tnl tunnel; /* Encapsulating tunnel parameters. */ openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- ovs_be64 metadata; /* OpenFlow Metadata. */ openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- uint64_t regs[FLOW_N_XREGS]; /* Registers. */ openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- ofp_port_t in_port; /* Incoming port. */ -- openvswitch-2.9.0/tests/ofproto-dpif.at:# This test verifies that the table ID is preserved across recirculation openvswitch-2.9.0/tests/ofproto-dpif.at-# when a resubmit action requires it (because the action is relative to openvswitch-2.9.0/tests/ofproto-dpif.at-# the current table rather than specifying a table). openvswitch-2.9.0/tests/ofproto-dpif.at:AT_SETUP([ofproto-dpif - resubmit with recirculation]) -- openvswitch-2.9.0/tests/ofproto-dpif.at-# This test verifies that tunnel metadata is preserved across openvswitch-2.9.0/tests/ofproto-dpif.at:# recirculation. At the time of recirculation, fields such as \"tun_id\" openvswitch-2.9.0/tests/ofproto-dpif.at-# may be set before the tunnel is \"valid\" (ie, has a destination openvswitch-2.9.0/tests/ofproto-dpif.at:# address), but the field should still be available after recirculation. openvswitch-2.9.0/tests/ofproto-dpif.at-AT_SETUP([ofproto-dpif - resubmit with tun_id]) -- openvswitch-2.9.0/tests/ofproto-dpif.at:# This test verifies that \"resubmit\", when it triggers recirculation openvswitch-2.9.0/tests/ofproto-dpif.at-# indirectly through the flow that it recursively invokes, is not openvswitch-2.9.0/tests/ofproto-dpif.at:# re-executed when execution continues later post-recirculation. openvswitch-2.9.0/tests/ofproto-dpif.at:AT_SETUP([ofproto-dpif - recirculation after resubmit]) -- openvswitch-2.9.0/tests/system-traffic.at-dnl Checks the implementation of conntrack with FTP ALGs in combination with openvswitch-2.9.0/tests/system-traffic.at-dnl NAT, with flow tables that implement the NATing after the first round openvswitch-2.9.0/tests/system-traffic.at:dnl of recirculation - that is, the first flow ct(table=foo) then a subsequent openvswitch-2.9.0/tests/system-traffic.at-dnl flow will implement the NATing with ct(nat..),output:foo. -- openvswitch-2.9.0/tests/system-traffic.at-dnl Checks the implementation of conntrack original direction tuple matching openvswitch-2.9.0/tests/system-traffic.at-dnl with FTP ALGs in combination with NAT, with flow tables that implement openvswitch-2.9.0/tests/system-traffic.at:dnl the NATing before the first round of recirculation - that is, the first openvswitch-2.9.0/tests/system-traffic.at-dnl flow ct(nat, table=foo) then a subsequent flow will implement the openvswitch-2.9.0/tests/system-traffic.at-dnl commiting of NATed and other connections with ct(nat..),output:foo. -- openvswitch-2.9.0/tests/packet-type-aware.at:# Goto_table after pop_mpls triggers recirculation. openvswitch-2.9.0/tests/packet-type-aware.at-AT_CHECK([ openvswitch-2.9.0/tests/packet-type-aware.at- ovs-ofctl del-flows br0 && openvswitch-2.9.0/tests/packet-type-aware.at- ovs-ofctl del-flows int-br && openvswitch-2.9.0/tests/packet-type-aware.at- ovs-ofctl add-flow br0 \"actions=normal\" openvswitch-2.9.0/tests/packet-type-aware.at- ovs-ofctl add-flow int-br \"table=0,in_port=tunnel,actions=pop_mpls:0x800,goto_table:20\" && openvswitch-2.9.0/tests/packet-type-aware.at- ovs-ofctl add-flow int-br \"table=20,actions=dec_ttl,output:LOCAL\" openvswitch-2.9.0/tests/packet-type-aware.at-], [0], [ignore]) openvswitch-2.9.0/tests/packet-type-aware.at- -- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-#include \"ofproto-dpif-mirror.h\" openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-#include \"openvswitch/list.h\" openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-#include \"openvswitch/ofp-actions.h\" openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-#include \"ovs-thread.h\" openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-#include \"uuid.h\" openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-struct ofproto_dpif; openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-struct rule; openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-/* openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * Freezing and recirculation openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * ========================== openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Freezing is a technique for halting and checkpointing packet translation in openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * a way that it can be restarted again later. This file has a couple of data openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * structures related to freezing in general; their names begin with \"frozen\". openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation is the use of freezing to allow a frame to re-enter the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * datapath packet processing path to achieve more flexible packet processing, openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * such as modifying header fields after MPLS POP action and selecting a slave openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * port for bond ports. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Data path and user space interface openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * ----------------------------------- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation uses two uint32_t fields, recirc_id and dp_hash, and a RECIRC openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * action. recirc_id is used to select the next packet processing steps among openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * multiple instances of recirculation. When a packet initially enters the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * datapath it is assigned with recirc_id 0, which indicates no recirculation. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirc_ids are managed by the user space, opaque to the datapath. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * On the other hand, dp_hash can only be computed by the datapath, opaque to openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * the user space, as the datapath is free to choose the hashing algorithm openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * without informing user space about it. The dp_hash value should be openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * wildcarded for newly received packets. HASH action specifies whether the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * hash is computed, and if computed, how many fields are to be included in the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * hash computation. The computed hash value is stored into the dp_hash field openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * prior to recirculation. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * The RECIRC action sets the recirc_id field and then reprocesses the packet openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * as if it was received again on the same input port. RECIRC action works openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * like a function call; actions listed after the RECIRC action will be openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * executed after recirculation. RECIRC action can be nested, but datapath openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * implementation limits the number of nested recirculations to prevent openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * unreasonable nesting depth or infinite loop. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * User space recirculation context openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * --------------------------------- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation is usually hidden from the OpenFlow controllers. Action openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * translation code deduces when recirculation is necessary and issues a openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * datapath recirculation action. All OpenFlow actions to be performed after openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * recirculation are derived from the OpenFlow pipeline and are stored with the openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * recirculation ID. When the OpenFlow tables are changed in a way affecting openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * the recirculation flows, new recirculation ID with new metadata and actions openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * is allocated and the old one is timed out. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation ID pool openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * ---------------------- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Recirculation ID needs to be unique for all datapaths. Recirculation ID openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * pool keeps track of recirculation ids and stores OpenFlow pipeline openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * translation context so that flow processing may continue after openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * recirculation. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * A Recirculation ID can be any uint32_t value, except for that the value 0 is openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h: * reserved for 'no recirculation' case. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * Thread-safety openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * -------------- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * All APIs are thread safe. openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- */ openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h:/* Metadata for restoring pipeline context after recirculation. Helpers openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * are inlined below to keep them together with the definition for easier openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- * updates. */ openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-BUILD_ASSERT_DECL(FLOW_WC_SEQ == 40); openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h-struct frozen_metadata { openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- /* Metadata in struct flow. */ openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- struct flow_tnl tunnel; /* Encapsulating tunnel parameters. */ openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- ovs_be64 metadata; /* OpenFlow Metadata. */ openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- uint64_t regs[FLOW_N_XREGS]; /* Registers. */ openvswitch-2.9.0/ofproto/ofproto-dpif-rid.h- ofp_port_t in_port; /* Incoming port. */ -- openvswitch-2.9.0/tests/ofproto-dpif.at:# This test verifies that the table ID is preserved across recirculation openvswitch-2.9.0/tests/ofproto-dpif.at-# when a resubmit action requires it (because the action is relative to openvswitch-2.9.0/tests/ofproto-dpif.at-# the current table rather than specifying a table). openvswitch-2.9.0/tests/ofproto-dpif.at:AT_SETUP([ofproto-dpif - resubmit with recirculation]) -- openvswitch-2.9.0/tests/ofproto-dpif.at-# This test verifies that tunnel metadata is preserved across openvswitch-2.9.0/tests/ofproto-dpif.at:# recirculation. At the time of recirculation, fields such as \"tun_id\" openvswitch-2.9.0/tests/ofproto-dpif.at-# may be set before the tunnel is \"valid\" (ie, has a destination openvswitch-2.9.0/tests/ofproto-dpif.at:# address), but the field should still be available after recirculation. openvswitch-2.9.0/tests/ofproto-dpif.at-AT_SETUP([ofproto-dpif - resubmit with tun_id]) -- openvswitch-2.9.0/tests/ofproto-dpif.at:# This test verifies that \"resubmit\", when it triggers recirculation openvswitch-2.9.0/tests/ofproto-dpif.at-# indirectly through the flow that it recursively invokes, is not openvswitch-2.9.0/tests/ofproto-dpif.at:# re-executed when execution continues later post-recirculation. openvswitch-2.9.0/tests/ofproto-dpif.at:AT_SETUP([ofproto-dpif - recirculation after resubmit]) -- openvswitch-2.9.0/tests/system-traffic.at-dnl Checks the implementation of conntrack with FTP ALGs in combination with openvswitch-2.9.0/tests/system-traffic.at-dnl NAT, with flow tables that implement the NATing after the first round openvswitch-2.9.0/tests/system-traffic.at:dnl of recirculation - that is, the first flow ct(table=foo) then a subsequent openvswitch-2.9.0/tests/system-traffic.at-dnl flow will implement the NATing with ct(nat..),output:foo. -- openvswitch-2.9.0/tests/system-traffic.at-dnl Checks the implementation of conntrack original direction tuple matching openvswitch-2.9.0/tests/system-traffic.at-dnl with FTP ALGs in combination with NAT, with flow tables that implement openvswitch-2.9.0/tests/system-traffic.at:dnl the NATing before the first round of recirculation - that is, the first openvswitch-2.9.0/tests/system-traffic.at-dnl flow ct(nat, table=foo) then a subsequent flow will implement the openvswitch-2.9.0/tests/system-traffic.at-dnl commiting of NATed and other connections with ct(nat..),output:foo. -- openvswitch-2.9.0/tests/packet-type-aware.at:# Goto_table after pop_mpls triggers recirculation. openvswitch-2.9.0/tests/packet-type-aware.at-AT_CHECK([ openvswitch-2.9.0/tests/packet-type-aware.at- ovs-ofctl del-flows br0 && openvswitch-2.9.0/tests/packet-type-aware.at- ovs-ofctl del-flows int-br && openvswitch-2.9.0/tests/packet-type-aware.at- ovs-ofctl add-flow br0 \"actions=normal\" openvswitch-2.9.0/tests/packet-type-aware.at- ovs-ofctl add-flow int-br \"table=0,in_port=tunnel,actions=pop_mpls:0x800,goto_table:20\" && openvswitch-2.9.0/tests/packet-type-aware.at- ovs-ofctl add-flow int-br \"table=20,actions=dec_ttl,output:LOCAL\" openvswitch-2.9.0/tests/packet-type-aware.at-], [0], [ignore]) openvswitch-2.9.0/tests/packet-type-aware.at- -- openvswitch-2.9.0/include/openvswitch/meta-flow.h- /* \"recirc_id\". openvswitch-2.9.0/include/openvswitch/meta-flow.h- * openvswitch-2.9.0/include/openvswitch/meta-flow.h: * ID for recirculation. The value 0 is reserved for initially received openvswitch-2.9.0/include/openvswitch/meta-flow.h- * packets. Internal use only, not programmable from controller. openvswitch-2.9.0/include/openvswitch/meta-flow.h- * openvswitch-2.9.0/include/openvswitch/meta-flow.h- * Type: be32. openvswitch-2.9.0/include/openvswitch/meta-flow.h- * Maskable: no. openvswitch-2.9.0/include/openvswitch/meta-flow.h- * Formatting: decimal. openvswitch-2.9.0/include/openvswitch/meta-flow.h- * Prerequisites: none. openvswitch-2.9.0/include/openvswitch/meta-flow.h- * Access: read-only. openvswitch-2.9.0/include/openvswitch/meta-flow.h- * NXM: NXM_NX_RECIRC_ID(36) since v2.2. openvswitch-2.9.0/include/openvswitch/meta-flow.h- * OXM: none. openvswitch-2.9.0/include/openvswitch/meta-flow.h- */ -- openvswitch-2.9.0/lib/dpif-netdev.c- pmd_perf_update_counter(&pmd->perf_stats, PMD_STAT_MASKED_HIT, openvswitch-2.9.0/lib/dpif-netdev.c- cnt - upcall_ok_cnt - upcall_fail_cnt); openvswitch-2.9.0/lib/dpif-netdev.c- pmd_perf_update_counter(&pmd->perf_stats, PMD_STAT_MASKED_LOOKUP, openvswitch-2.9.0/lib/dpif-netdev.c- lookup_cnt); openvswitch-2.9.0/lib/dpif-netdev.c- pmd_perf_update_counter(&pmd->perf_stats, PMD_STAT_MISS, openvswitch-2.9.0/lib/dpif-netdev.c- upcall_ok_cnt); openvswitch-2.9.0/lib/dpif-netdev.c- pmd_perf_update_counter(&pmd->perf_stats, PMD_STAT_LOST, openvswitch-2.9.0/lib/dpif-netdev.c- upcall_fail_cnt); openvswitch-2.9.0/lib/dpif-netdev.c-} openvswitch-2.9.0/lib/dpif-netdev.c- openvswitch-2.9.0/lib/dpif-netdev.c:/* Packets enter the datapath from a port (or from recirculation) here. openvswitch-2.9.0/lib/dpif-netdev.c- * openvswitch-2.9.0/lib/dpif-netdev.c- * When 'md_is_valid' is true the metadata in 'packets' are already valid. openvswitch-2.9.0/lib/dpif-netdev.c- * When false the metadata in 'packets' need to be initialized. */ openvswitch-2.9.0/lib/dpif-netdev.c-static void openvswitch-2.9.0/lib/dpif-netdev.c-dp_netdev_input__(struct dp_netdev_pmd_thread *pmd, openvswitch-2.9.0/lib/dpif-netdev.c- struct dp_packet_batch *packets, openvswitch-2.9.0/lib/dpif-netdev.c- bool md_is_valid, odp_port_t port_no) openvswitch-2.9.0/lib/dpif-netdev.c-{ openvswitch-2.9.0/lib/dpif-netdev.c-#if !defined(__CHECKER__) && !defined(_WIN32) openvswitch-2.9.0/lib/dpif-netdev.c- const size_t PKT_ARRAY_SIZE = dp_packet_batch_size(packets); -- openvswitch-2.9.0/lib/dpif-netdev.c- md_is_valid, port_no); openvswitch-2.9.0/lib/dpif-netdev.c- if (!dp_packet_batch_is_empty(packets)) { openvswitch-2.9.0/lib/dpif-netdev.c- /* Get ingress port from first packet's metadata. */ openvswitch-2.9.0/lib/dpif-netdev.c- in_port = packets->packets[0]->md.in_port.odp_port; openvswitch-2.9.0/lib/dpif-netdev.c- fast_path_processing(pmd, packets, keys, openvswitch-2.9.0/lib/dpif-netdev.c- batches, &n_batches, in_port); openvswitch-2.9.0/lib/dpif-netdev.c- } openvswitch-2.9.0/lib/dpif-netdev.c- openvswitch-2.9.0/lib/dpif-netdev.c- /* All the flow batches need to be reset before any call to openvswitch-2.9.0/lib/dpif-netdev.c- * packet_batch_per_flow_execute() as it could potentially trigger openvswitch-2.9.0/lib/dpif-netdev.c: * recirculation. When a packet matching flow \u2018j\u2019 happens to be openvswitch-2.9.0/lib/dpif-netdev.c- * recirculated, the nested call to dp_netdev_input__() could potentially openvswitch-2.9.0/lib/dpif-netdev.c- * classify the packet as matching another flow - say 'k'. It could happen openvswitch-2.9.0/lib/dpif-netdev.c- * that in the previous call to dp_netdev_input__() that same flow 'k' had openvswitch-2.9.0/lib/dpif-netdev.c- * already its own batches[k] still waiting to be served. So if its openvswitch-2.9.0/lib/dpif-netdev.c- * \u2018batch\u2019 member is not reset, the recirculated packet would be wrongly openvswitch-2.9.0/lib/dpif-netdev.c- * appended to batches[k] of the 1st call to dp_netdev_input__(). */ openvswitch-2.9.0/lib/dpif-netdev.c- size_t i; openvswitch-2.9.0/lib/dpif-netdev.c- for (i = 0; i < n_batches; i++) { openvswitch-2.9.0/lib/dpif-netdev.c- batches[i].flow->batch = NULL; openvswitch-2.9.0/lib/dpif-netdev.c- } -- openvswitch-2.9.0/lib/dpif-netdev.c- packet->md.recirc_id = nl_attr_get_u32(a); openvswitch-2.9.0/lib/dpif-netdev.c- } openvswitch-2.9.0/lib/dpif-netdev.c- openvswitch-2.9.0/lib/dpif-netdev.c- (*depth)++; openvswitch-2.9.0/lib/dpif-netdev.c- dp_netdev_recirculate(pmd, packets_); openvswitch-2.9.0/lib/dpif-netdev.c- (*depth)--; openvswitch-2.9.0/lib/dpif-netdev.c- openvswitch-2.9.0/lib/dpif-netdev.c- return; openvswitch-2.9.0/lib/dpif-netdev.c- } openvswitch-2.9.0/lib/dpif-netdev.c- openvswitch-2.9.0/lib/dpif-netdev.c: VLOG_WARN(\"Packet dropped. Max recirculation depth exceeded.\"); openvswitch-2.9.0/lib/dpif-netdev.c- break; openvswitch-2.9.0/lib/dpif-netdev.c- openvswitch-2.9.0/lib/dpif-netdev.c- case OVS_ACTION_ATTR_CT: { openvswitch-2.9.0/lib/dpif-netdev.c- const struct nlattr *b; openvswitch-2.9.0/lib/dpif-netdev.c- bool force = false; openvswitch-2.9.0/lib/dpif-netdev.c- bool commit = false; openvswitch-2.9.0/lib/dpif-netdev.c- unsigned int left; openvswitch-2.9.0/lib/dpif-netdev.c- uint16_t zone = 0; openvswitch-2.9.0/lib/dpif-netdev.c- const char *helper = NULL; openvswitch-2.9.0/lib/dpif-netdev.c- const uint32_t *setmark = NULL; -- openvswitch-2.9.0/lib/odp-util.c- break; openvswitch-2.9.0/lib/odp-util.c- default: openvswitch-2.9.0/lib/odp-util.c- /* Only the above protocols are supported for encap. openvswitch-2.9.0/lib/odp-util.c- * The check is done at action translation. */ openvswitch-2.9.0/lib/odp-util.c- OVS_NOT_REACHED(); openvswitch-2.9.0/lib/odp-util.c- } openvswitch-2.9.0/lib/odp-util.c- } else { openvswitch-2.9.0/lib/odp-util.c- /* This is an explicit or implicit decap case. */ openvswitch-2.9.0/lib/odp-util.c- if (pt_ns(flow->packet_type) == OFPHTN_ETHERTYPE && openvswitch-2.9.0/lib/odp-util.c- base_flow->packet_type == htonl(PT_ETH)) { openvswitch-2.9.0/lib/odp-util.c: /* Generate pop_eth and continue without recirculation. */ openvswitch-2.9.0/lib/odp-util.c- odp_put_pop_eth_action(odp_actions); openvswitch-2.9.0/lib/odp-util.c- base_flow->packet_type = flow->packet_type; openvswitch-2.9.0/lib/odp-util.c- base_flow->dl_src = eth_addr_zero; openvswitch-2.9.0/lib/odp-util.c- base_flow->dl_dst = eth_addr_zero; openvswitch-2.9.0/lib/odp-util.c- } else { openvswitch-2.9.0/lib/odp-util.c: /* All other decap cases require recirculation. openvswitch-2.9.0/lib/odp-util.c- * No need to update the base flow here. */ openvswitch-2.9.0/lib/odp-util.c- switch (ntohl(base_flow->packet_type)) { openvswitch-2.9.0/lib/odp-util.c- case PT_NSH: openvswitch-2.9.0/lib/odp-util.c- /* pop_nsh. */ openvswitch-2.9.0/lib/odp-util.c- odp_put_pop_nsh_action(odp_actions); openvswitch-2.9.0/lib/odp-util.c- break; openvswitch-2.9.0/lib/odp-util.c- default: openvswitch-2.9.0/lib/odp-util.c- /* Checks are done during translation. */ openvswitch-2.9.0/lib/odp-util.c- OVS_NOT_REACHED(); openvswitch-2.9.0/lib/odp-util.c- } -- openvswitch-2.9.0/ovn/lib/actions.c- nat->flags |= NX_NAT_F_DST; openvswitch-2.9.0/ovn/lib/actions.c- } openvswitch-2.9.0/ovn/lib/actions.c- } openvswitch-2.9.0/ovn/lib/actions.c- openvswitch-2.9.0/ovn/lib/actions.c- ofpacts->header = ofpbuf_push_uninit(ofpacts, nat_offset); openvswitch-2.9.0/ovn/lib/actions.c- ct = ofpacts->header; openvswitch-2.9.0/ovn/lib/actions.c- if (cn->ip) { openvswitch-2.9.0/ovn/lib/actions.c- ct->flags |= NX_CT_F_COMMIT; openvswitch-2.9.0/ovn/lib/actions.c- } else if (snat && ep->is_gateway_router) { openvswitch-2.9.0/ovn/lib/actions.c- /* For performance reasons, we try to prevent additional openvswitch-2.9.0/ovn/lib/actions.c: * recirculations. ct_snat which is used in a gateway router openvswitch-2.9.0/ovn/lib/actions.c: * does not need a recirculation. ct_snat(IP) does need a openvswitch-2.9.0/ovn/lib/actions.c: * recirculation. ct_snat in a distributed router needs openvswitch-2.9.0/ovn/lib/actions.c: * recirculation regardless of whether an IP address is openvswitch-2.9.0/ovn/lib/actions.c- * specified. openvswitch-2.9.0/ovn/lib/actions.c- * XXX Should we consider a method to let the actions specify openvswitch-2.9.0/ovn/lib/actions.c: * whether an action needs recirculation if there are more use openvswitch-2.9.0/ovn/lib/actions.c- * cases?. */ openvswitch-2.9.0/ovn/lib/actions.c- ct->recirc_table = NX_CT_RECIRC_NONE; openvswitch-2.9.0/ovn/lib/actions.c- } openvswitch-2.9.0/ovn/lib/actions.c- ofpact_finish(ofpacts, &ct->ofpact); openvswitch-2.9.0/ovn/lib/actions.c- ofpbuf_push_uninit(ofpacts, ct_offset); openvswitch-2.9.0/ovn/lib/actions.c-} openvswitch-2.9.0/ovn/lib/actions.c- openvswitch-2.9.0/ovn/lib/actions.c-static void openvswitch-2.9.0/ovn/lib/actions.c-encode_CT_DNAT(const struct ovnact_ct_nat *cn, openvswitch-2.9.0/ovn/lib/actions.c- const struct ovnact_encode_params *ep, -- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * later. We call the checkpointing process \"freezing\" and the restarting openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * process \"thawing\". openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * The use cases for freezing are: openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * - \"Recirculation\", where the translation process discovers that it openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * doesn't have enough information to complete translation without openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * actually executing the actions that have already been translated, openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * which provides the additionally needed information. In these openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * situations, translation freezes translation and assigns the frozen openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: * data a unique \"recirculation ID\", which it associates with the data openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * in a table in userspace (see ofproto-dpif-rid.h). It also adds a openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * OVS_ACTION_ATTR_RECIRC action specifying that ID to the datapath openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * actions. When a packet hits that action, the datapath looks its openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * flow up again using the ID. If there's a miss, it comes back to openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: * userspace, which find the recirculation table entry for the ID, openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * thaws the associated frozen data, and continues translation from openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * that point given the additional information that is now known. openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * The archetypal example is MPLS. As MPLS is implemented in openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * OpenFlow, the protocol that follows the last MPLS label becomes openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * known only when that label is popped by an OpenFlow action. That openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * means that Open vSwitch can't extract the headers beyond the MPLS openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * labels until the pop action is executed. Thus, at that point openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: * translation uses the recirculation process to extract the headers openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * beyond the MPLS labels. openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * (OVS also uses OVS_ACTION_ATTR_RECIRC to implement hashing for openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * output to bonds. OVS pre-populates all the datapath flows for bond openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * output in the datapath, though, which means that the elaborate openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * process of coming back to userspace for a second round of openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * translation isn't needed, and so bonds don't follow the above openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * process.) openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * - \"Continuation\". A continuation is a way for an OpenFlow controller -- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * translation process: openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * 1. Sets 'freezing' to true. openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * 2. Sets 'exit' to true to tell later steps that we're exiting from the openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * translation process. openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * 3. Adds an OFPACT_UNROLL_XLATE action to 'frozen_actions', and points openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * frozen_actions.header to the action to make it easy to find it later. openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * This action holds the current table ID and cookie so that they can be openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: * restored during a post-recirculation upcall translation. openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: * 4. Adds the action that prompted recirculation and any actions following openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * it within the same flow to 'frozen_actions', so that they can be openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: * executed during a post-recirculation upcall translation. openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * 5. Returns. openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: * 6. The action that prompted recirculation might be nested in a stack of openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * nested \"resubmit\"s that have actions remaining. Each of these notices openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * that we're exiting and freezing and responds by adding more openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * OFPACT_UNROLL_XLATE actions to 'frozen_actions', as necessary, openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * followed by any actions that were yet unprocessed. openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: * If we're freezing because of recirculation, the caller generates a openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: * recirculation ID and associates all the state produced by this process openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: * with it. For post-recirculation upcall translation, the caller passes it openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * back in for the new translation to execute. The process yielded a set of openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * ofpacts that can be translated directly, so it is not much of a special openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * case at that point. openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- */ openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- bool freezing; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: bool recirc_update_dp_hash; /* Generated recirculation will be preceded openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * by datapath HASH action to get an updated openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: * dp_hash after recirculation. */ openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- uint32_t dp_hash_alg; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- uint32_t dp_hash_basis; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- struct ofpbuf frozen_actions; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- const struct ofpact_controller *pause; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- /* True if a packet was but is no longer MPLS (due to an MPLS pop action). openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: * This is a trigger for recirculation in cases where translating an action openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * or looking up a flow requires access to the fields of the packet after openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * the MPLS label stack that was originally present. */ openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- bool was_mpls; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- /* True if conntrack has been performed on this packet during processing openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * on the current bridge. This is used to determine whether conntrack openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * state from the datapath should be honored after thawing. */ openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- bool conntracked; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- /* Pointer to an embedded NAT action in a conntrack action, or NULL. */ -- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c-static void xvlan_put(struct flow *, const struct xvlan *); openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c-static void xvlan_input_translate(const struct xbundle *, openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- const struct xvlan *in, openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- struct xvlan *xvlan); openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c-static void xvlan_output_translate(const struct xbundle *, openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- const struct xvlan *xvlan, openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- struct xvlan *out); openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c-static void output_normal(struct xlate_ctx *, const struct xbundle *, openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- const struct xvlan *); openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c:/* Optional bond recirculation parameter to compose_output_action(). */ openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c-struct xlate_bond_recirc { openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: uint32_t recirc_id; /* !0 Use recirculation instead of output. */ openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- uint8_t hash_alg; /* !0 Compute hash for recirc before. */ openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- uint32_t hash_basis; /* Compute hash for recirc before. */ openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c-}; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c-static void compose_output_action(struct xlate_ctx *, ofp_port_t ofp_port, openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- const struct xlate_bond_recirc *xr, openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- bool is_last_action, bool truncate); openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c-static struct xbridge *xbridge_lookup(struct xlate_cfg *, openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- const struct ofproto_dpif *); -- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- /* If packet is recirculated, xport can be retrieved from frozen state. */ openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- if (flow->recirc_id) { openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- const struct recirc_id_node *recirc_id_node; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- recirc_id_node = recirc_id_node_find(flow->recirc_id); openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- if (OVS_UNLIKELY(!recirc_id_node)) { openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- return NULL; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- } openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: /* If recirculation was initiated due to bond (in_port = OFPP_NONE) openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * then frozen state is static and xport_uuid is not defined, so xport openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * cannot be restored from frozen state. */ openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- if (recirc_id_node->state.metadata.in_port != OFPP_NONE) { openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- struct uuid xport_uuid = recirc_id_node->state.xport_uuid; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- xport = xport_lookup_by_uuid(xcfg, &xport_uuid); openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- if (xport && xport->xbridge && xport->xbridge->ofproto) { openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- goto out; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- } openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- } openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- } -- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- netdev_init_tnl_build_header_params(&tnl_params, flow, &s_ip6, dmac, smac); openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- err = tnl_port_build_header(xport->ofport, &tnl_push_data, &tnl_params); openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- if (err) { openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- return err; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- } openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- tnl_push_data.tnl_port = tunnel_odp_port; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- tnl_push_data.out_port = out_dev->odp_port; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- /* After tunnel header has been added, MAC and IP data of flow and openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c: * base_flow need to be set properly, since there is not recirculation openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- * any more when sending packet to tunnel. */ openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- propagate_tunnel_data_to_flow(ctx, dmac, smac, s_ip6, openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- s_ip, tnl_params.is_ipv6, openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- tnl_push_data.tnl_type); openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- size_t clone_ofs = 0; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- size_t push_action_size; openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- openvswitch-2.9.0/ofproto/ofproto-dpif-xlate.c- clone_ofs = nl_msg_start_nested(ctx->odp_actions, OVS_ACTION_ATTR_CLONE); -- penvswitch-2.9.0/ofproto/bond.h-void *bond_choose_output_slave(struct bond *, const struct flow *, openvswitch-2.9.0/ofproto/bond.h- struct flow_wildcards *, uint16_t vlan); openvswitch-2.9.0/ofproto/bond.h- openvswitch-2.9.0/ofproto/bond.h-/* Rebalancing. */ openvswitch-2.9.0/ofproto/bond.h-void bond_account(struct bond *, const struct flow *, uint16_t vlan, openvswitch-2.9.0/ofproto/bond.h- uint64_t n_bytes); openvswitch-2.9.0/ofproto/bond.h-void bond_rebalance(struct bond *); openvswitch-2.9.0/ofproto/bond.h- openvswitch-2.9.0/ofproto/bond.h-/* Recirculation openvswitch-2.9.0/ofproto/bond.h- * openvswitch-2.9.0/ofproto/bond.h: * Only balance_tcp mode uses recirculation. openvswitch-2.9.0/ofproto/bond.h- * openvswitch-2.9.0/ofproto/bond.h: * When recirculation is used, each bond port is assigned with a unique openvswitch-2.9.0/ofproto/bond.h- * recirc_id. The output action to the bond port will be replaced by openvswitch-2.9.0/ofproto/bond.h- * a Hash action, followed by a RECIRC action. openvswitch-2.9.0/ofproto/bond.h- * openvswitch-2.9.0/ofproto/bond.h- * ... actions= ... HASH(hash(L4)), RECIRC(recirc_id) .... openvswitch-2.9.0/ofproto/bond.h- * openvswitch-2.9.0/ofproto/bond.h: * On handling first output packet, 256 post recirculation flows are installed: openvswitch-2.9.0/ofproto/bond.h- * openvswitch-2.9.0/ofproto/bond.h- * recirc_id=<bond_recirc_id>, dp_hash=<[0..255]>/0xff, actions: output<slave> openvswitch-2.9.0/ofproto/bond.h- * openvswitch-2.9.0/ofproto/bond.h: * Bond module pulls stats from those post recirculation rules. If rebalancing openvswitch-2.9.0/ofproto/bond.h- * is needed, those rules are updated with new output actions. openvswitch-2.9.0/ofproto/bond.h-*/ openvswitch-2.9.0/ofproto/bond.h-void bond_update_post_recirc_rules(struct bond *, uint32_t *recirc_id, openvswitch-2.9.0/ofproto/bond.h- uint32_t *hash_basis); openvswitch-2.9.0/ofproto/bond.h-#endif /* bond.h */","title":"OVS packet recirculation"},{"location":"networking/ovs_with_gdb/","text":"Inspecting OVS with GDB Introduction Most of this article is based on http://www.openvswitch.org/support/ovscon2018/6/1345-chaudron.pdf This is mainly a walkthrough of how to get there on RHEL 7. Generate a core dump of OVS Enable core dump collection with abrt: yum install abrt abrt-addon-ccpp abrt-tui abrt-install-ccpp-hook install abrt-install-ccpp-hook is-installed; echo $?; service abrtd start service abrt-ccpp start abrt-auto-reporting enabled abrt-cli list Kill OVS: kill -11 $(pidof ovs-vswitchd) List core dumps: [root@overcloud-computedpdk-0 ~]# abrt-cli list id 60b0650c5db6225ab1debfba0f80bbdee60cef43 reason: ovs-vswitchd killed by SIGSEGV time: Mon 31 Dec 2018 05:09:12 PM UTC cmdline: ovs-vswitchd unix:/var/run/openvswitch/db.sock -vconsole:emer -vsyslog:err -vfile:info --mlockall --user openvswitch:hugetlbfs --no-chdir --log-file=/var/log/openvswitch/ovs-vswitchd.log --pidfile=/var/run/openvswitch/ovs-vswitchd.pid --detach package: openvswitch-2.9.0-56.el7fdp uid: 993 (openvswitch) count: 1 Directory: /var/spool/abrt/ccpp-2018-12-31-17:09:12-671118 [root@overcloud-computedpdk-0 ~]# file !$ file /var/spool/abrt/ccpp-2018-12-31-17:09:12-671118/coredump /var/spool/abrt/ccpp-2018-12-31-17:09:12-671118/coredump: ELF 64-bit LSB core file x86-64, version 1 (SYSV), SVR4-style, from 'ovs-vswitchd unix:/var/run/openvswitch/db.sock -vconsole:emer -vsyslog:err -vfi', real uid: 0, effective uid: 0, real gid: 0, effective gid: 0, execfn: '/sbin/ovs-vswitchd', platform: 'x86_64' Install debuginfo for OVS It's important that the right debuginfo be installed. Otherwise, you will get a message like this from the script later down the road: (gdb) ovs_dump_bridge ports Can't find all_bridges global variable, are you sure your debugging OVS? debuginfo-install $(rpm -qa | egrep '^openvswitch-[0-9]') Download the script It should be better to get the script from a source RPM. However, my version of OVS doesn't have the script yet. Hence I'm directly downloading it: curl -o ovs_gdb.py \\ https://raw.githubusercontent.com/openvswitch/ovs/cd5b89a5a99c3ead973b168326eaef47d4e4c077/utilities/gdb/ovs_gdb.py Open core dump with GDB gdb $(which ovs-vswitchd) /var/spool/abrt/ccpp-2018-12-31-17:09:12-671118/coredump Once at the prompt: source ovs_gdb.py Using the debug script (gdb) ovs_dump_bridge (struct bridge *) 0x5563a14fd2b0: name = br-link0, type = netdev (struct bridge *) 0x5563a14fd8c0: name = br-int, type = netdev (gdb) ovs_dump_bridge ports (struct bridge *) 0x5563a14fd2b0: name = br-link0, type = netdev (struct port *) 0x5563a1552800: name = br-link0, brige = (struct bridge *) 0x5563a14fd2b0 (struct iface *) 0x5563a157b090: name = 0x5563a1502070 \"br-link0\", ofp_port = 65534, netdev = (struct netdev *) 0x5563a157a7d0 (struct port *) 0x5563a157d480: name = dpdk0, brige = (struct bridge *) 0x5563a14fd2b0 (struct iface *) 0x5563a157d720: name = 0x5563a157bc40 \"dpdk0\", ofp_port = 1, netdev = (struct netdev *) 0x7f223fc6b6c0 (struct port *) 0x5563a15809b0: name = phy-br-link0, brige = (struct bridge *) 0x5563a14fd2b0 (struct iface *) 0x5563a15806a0: name = 0x5563a1580720 \"phy-br-link0\", ofp_port = 2, netdev = (struct netdev *) 0x5563a1582d20 (struct bridge *) 0x5563a14fd8c0: name = br-int, type = netdev (struct port *) 0x5563a1583220: name = int-br-link0, brige = (struct bridge *) 0x5563a14fd8c0 (struct iface *) 0x5563a15839c0: name = 0x5563a1580790 \"int-br-link0\", ofp_port = 1, netdev = (struct netdev *) 0x5563a1583620 (struct port *) 0x5563a1584560: name = br-int, brige = (struct bridge *) 0x5563a14fd8c0 (struct iface *) 0x5563a1584e70: name = 0x5563a1584ef0 \"br-int\", ofp_port = 65534, netdev = (struct netdev *) 0x5563a1584150","title":"OVS with GDB"},{"location":"networking/ovs_with_gdb/#inspecting-ovs-with-gdb","text":"","title":"Inspecting OVS with GDB"},{"location":"networking/ovs_with_gdb/#introduction","text":"Most of this article is based on http://www.openvswitch.org/support/ovscon2018/6/1345-chaudron.pdf This is mainly a walkthrough of how to get there on RHEL 7.","title":"Introduction"},{"location":"networking/ovs_with_gdb/#generate-a-core-dump-of-ovs","text":"Enable core dump collection with abrt: yum install abrt abrt-addon-ccpp abrt-tui abrt-install-ccpp-hook install abrt-install-ccpp-hook is-installed; echo $?; service abrtd start service abrt-ccpp start abrt-auto-reporting enabled abrt-cli list Kill OVS: kill -11 $(pidof ovs-vswitchd) List core dumps: [root@overcloud-computedpdk-0 ~]# abrt-cli list id 60b0650c5db6225ab1debfba0f80bbdee60cef43 reason: ovs-vswitchd killed by SIGSEGV time: Mon 31 Dec 2018 05:09:12 PM UTC cmdline: ovs-vswitchd unix:/var/run/openvswitch/db.sock -vconsole:emer -vsyslog:err -vfile:info --mlockall --user openvswitch:hugetlbfs --no-chdir --log-file=/var/log/openvswitch/ovs-vswitchd.log --pidfile=/var/run/openvswitch/ovs-vswitchd.pid --detach package: openvswitch-2.9.0-56.el7fdp uid: 993 (openvswitch) count: 1 Directory: /var/spool/abrt/ccpp-2018-12-31-17:09:12-671118 [root@overcloud-computedpdk-0 ~]# file !$ file /var/spool/abrt/ccpp-2018-12-31-17:09:12-671118/coredump /var/spool/abrt/ccpp-2018-12-31-17:09:12-671118/coredump: ELF 64-bit LSB core file x86-64, version 1 (SYSV), SVR4-style, from 'ovs-vswitchd unix:/var/run/openvswitch/db.sock -vconsole:emer -vsyslog:err -vfi', real uid: 0, effective uid: 0, real gid: 0, effective gid: 0, execfn: '/sbin/ovs-vswitchd', platform: 'x86_64'","title":"Generate a core dump of OVS"},{"location":"networking/ovs_with_gdb/#install-debuginfo-for-ovs","text":"It's important that the right debuginfo be installed. Otherwise, you will get a message like this from the script later down the road: (gdb) ovs_dump_bridge ports Can't find all_bridges global variable, are you sure your debugging OVS? debuginfo-install $(rpm -qa | egrep '^openvswitch-[0-9]')","title":"Install debuginfo for OVS"},{"location":"networking/ovs_with_gdb/#download-the-script","text":"It should be better to get the script from a source RPM. However, my version of OVS doesn't have the script yet. Hence I'm directly downloading it: curl -o ovs_gdb.py \\ https://raw.githubusercontent.com/openvswitch/ovs/cd5b89a5a99c3ead973b168326eaef47d4e4c077/utilities/gdb/ovs_gdb.py","title":"Download the script"},{"location":"networking/ovs_with_gdb/#open-core-dump-with-gdb","text":"gdb $(which ovs-vswitchd) /var/spool/abrt/ccpp-2018-12-31-17:09:12-671118/coredump Once at the prompt: source ovs_gdb.py","title":"Open core dump with GDB"},{"location":"networking/ovs_with_gdb/#using-the-debug-script","text":"(gdb) ovs_dump_bridge (struct bridge *) 0x5563a14fd2b0: name = br-link0, type = netdev (struct bridge *) 0x5563a14fd8c0: name = br-int, type = netdev (gdb) ovs_dump_bridge ports (struct bridge *) 0x5563a14fd2b0: name = br-link0, type = netdev (struct port *) 0x5563a1552800: name = br-link0, brige = (struct bridge *) 0x5563a14fd2b0 (struct iface *) 0x5563a157b090: name = 0x5563a1502070 \"br-link0\", ofp_port = 65534, netdev = (struct netdev *) 0x5563a157a7d0 (struct port *) 0x5563a157d480: name = dpdk0, brige = (struct bridge *) 0x5563a14fd2b0 (struct iface *) 0x5563a157d720: name = 0x5563a157bc40 \"dpdk0\", ofp_port = 1, netdev = (struct netdev *) 0x7f223fc6b6c0 (struct port *) 0x5563a15809b0: name = phy-br-link0, brige = (struct bridge *) 0x5563a14fd2b0 (struct iface *) 0x5563a15806a0: name = 0x5563a1580720 \"phy-br-link0\", ofp_port = 2, netdev = (struct netdev *) 0x5563a1582d20 (struct bridge *) 0x5563a14fd8c0: name = br-int, type = netdev (struct port *) 0x5563a1583220: name = int-br-link0, brige = (struct bridge *) 0x5563a14fd8c0 (struct iface *) 0x5563a15839c0: name = 0x5563a1580790 \"int-br-link0\", ofp_port = 1, netdev = (struct netdev *) 0x5563a1583620 (struct port *) 0x5563a1584560: name = br-int, brige = (struct bridge *) 0x5563a14fd8c0 (struct iface *) 0x5563a1584e70: name = 0x5563a1584ef0 \"br-int\", ofp_port = 65534, netdev = (struct netdev *) 0x5563a1584150","title":"Using the debug script"},{"location":"networking/sctp/","text":"Analyzing SCTP in OpenShift This short blod post shows how to deploy SCTP capability and pods in OpenShift and has a brief look at an SCTP packet capture. For further info and more details, see the Resources section. Deploying in Openshift load-sctp-module.yaml : apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: labels: machineconfiguration.openshift.io/role: worker name: load-sctp-module spec: config: ignition: version: 2.2.0 storage: files: - contents: source: data:, verification: {} filesystem: root mode: 420 path: /etc/modprobe.d/sctp-blacklist.conf - contents: source: data:text/plain;charset=utf-8,sctp filesystem: root mode: 420 path: /etc/modules-load.d/sctp-load.conf oc apply -f load-sctp-module.yaml sctp-pods.yaml : #!/bin/bash echo \"Installing SCTP on nodes\" echo \"https://docs.openshift.com/container-platform/4.4/networking/using-sctp.html\" oc new-project sctp oc project sctp cat <<'EOF' | oc apply -f - apiVersion: v1 kind: Service metadata: name: sctpservice labels: app: sctpserver spec: type: NodePort selector: app: sctpserver ports: - name: sctpserver protocol: SCTP port: 30102 targetPort: 30102 EOF cat <<'EOF' | oc apply -f - apiVersion: v1 kind: Pod metadata: name: sctpserver labels: app: sctpserver spec: containers: - name: sctpserver image: fedora command: [\"/bin/sh\", \"-c\"] args: [\"dnf install -y nc iperf3 procps-ng iproute && iperf3 -s\"] ports: - containerPort: 30102 name: sctpserver protocol: SCTP - name: tshark image: danielguerra/alpine-tshark command: - \"tshark\" - \"-i\" - \"eth0\" - \"-V\" - \"sctp\" EOF cat <<'EOF' | oc apply -f - apiVersion: v1 kind: Pod metadata: name: sctpclient labels: app: sctpclient spec: containers: - name: sctpclient image: fedora command: [\"/bin/sh\", \"-c\"] args: [\"dnf install -y nc tcpdump iperf3 procps-ng iproute && echo 'iperf3 -c <IP> -t 3600 --sctp' > /etc/motd && sleep inf\"] - name: tshark image: danielguerra/alpine-tshark command: - \"tshark\" - \"-i\" - \"eth0\" - \"-V\" - \"sctp\" EOF oc get pods -o wide sctpserverip=$(oc get pods -o wide | grep sctpserver | awk '{print $6}') echo \"Run:\" echo \"oc rsh sctpclient\" echo \"iperf3 -c $sctpserverip -t 60 --sctp\" echo \"You can access the packet capture with:\" echo \"oc logs -f -c tcpdump sctpserver\" oc apply -f sctp-pods.yaml Analysis Server: sh-5.0# ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 3: eth0@if17: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1400 qdisc noqueue state UP group default link/ether d6:96:23:1b:02:05 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.27.2.4/23 brd 172.27.3.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::7066:5ff:fe64:1990/64 scope link valid_lft forever preferred_lft forever sh-5.0# nc -l -p 30102 --sctp -k hello world! Client: sh-5.0# ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 3: eth0@if18: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1400 qdisc noqueue state UP group default link/ether d6:96:23:18:02:06 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.24.2.5/23 brd 172.24.3.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::a856:85ff:feb0:f262/64 scope link valid_lft forever preferred_lft forever sh-5.0# nc --sctp 172.27.2.4 30102 hello world! [root@openshift-jumpserver-0 ~]# oc logs -f sctpserver tshark --tail=0 ### connection setup ### Verification tag: 0x2222afc5 [Association index: 0] Checksum: 0x00000000 [unverified] [Checksum Status: Unverified] SHUTDOWN_COMPLETE chunk Chunk type: SHUTDOWN_COMPLETE (14) 0... .... = Bit: Stop processing of the packet .0.. .... = Bit: Do not report Chunk flags: 0x00 .... ...0 = T-Bit: Tag not reflected Chunk length: 4 Frame 30: 82 bytes on wire (656 bits), 82 bytes captured (656 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 Encapsulation type: Ethernet (1) Arrival Time: Jul 23, 2020 15:41:58.370315008 UTC [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1595518918.370315008 seconds [Time delta from previous captured frame: 36.580134390 seconds] [Time delta from previous displayed frame: 36.580134390 seconds] [Time since reference or first frame: 206.820352621 seconds] Frame Number: 30 Frame Length: 82 bytes (656 bits) Capture Length: 82 bytes (656 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:sctp] Ethernet II, Src: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01), Dst: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Destination: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Address: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Address: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 172.24.2.5, Dst: 172.27.2.4 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 68 Identification: 0x0000 (0) Flags: 0x02 (Don't Fragment) 0... .... = Reserved bit: Not set .1.. .... = Don't fragment: Set ..0. .... = More fragments: Not set Fragment offset: 0 Time to live: 63 Protocol: SCTP (132) Header checksum: 0xdef7 [validation disabled] [Header checksum status: Unverified] Source: 172.24.2.5 Destination: 172.27.2.4 Stream Control Transmission Protocol, Src Port: 57630 (57630), Dst Port: 30102 (30102) Source port: 57630 Destination port: 30102 Verification tag: 0x00000000 [Association index: 1] Checksum: 0x87034a86 [unverified] [Checksum Status: Unverified] INIT chunk (Outbound streams: 10, inbound streams: 65535) Chunk type: INIT (1) 0... .... = Bit: Stop processing of the packet .0.. .... = Bit: Do not report Chunk flags: 0x00 Chunk length: 36 Initiate tag: 0xf3dc3087 Advertised receiver window credit (a_rwnd): 106496 Number of outbound streams: 10 Number of inbound streams: 65535 Initial TSN: 3207583439 Supported address types parameter (Supported types: IPv4) Parameter type: Supported address types (0x000c) 0... .... .... .... = Bit: Stop processing of chunk .0.. .... .... .... = Bit: Do not report Parameter length: 6 Supported address type: IPv4 address (5) Parameter padding: 0000 ECN parameter Parameter type: ECN (0x8000) 1... .... .... .... = Bit: Skip parameter and continue processing of the chunk .0.. .... .... .... = Bit: Do not report Parameter length: 4 Forward TSN supported parameter Parameter type: Forward TSN supported (0xc000) 1... .... .... .... = Bit: Skip parameter and continue processing of the chunk .1.. .... .... .... = Bit: Do report Parameter length: 4 Frame 31: 306 bytes on wire (2448 bits), 306 bytes captured (2448 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 Encapsulation type: Ethernet (1) Arrival Time: Jul 23, 2020 15:41:58.370379346 UTC [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1595518918.370379346 seconds [Time delta from previous captured frame: 0.000064338 seconds] [Time delta from previous displayed frame: 0.000064338 seconds] [Time since reference or first frame: 206.820416959 seconds] Frame Number: 31 Frame Length: 306 bytes (2448 bits) Capture Length: 306 bytes (2448 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:sctp] Ethernet II, Src: d6:96:23:1b:02:05 (d6:96:23:1b:02:05), Dst: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Destination: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Address: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Address: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 172.27.2.4, Dst: 172.24.2.5 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 292 Identification: 0x0000 (0) Flags: 0x02 (Don't Fragment) 0... .... = Reserved bit: Not set .1.. .... = Don't fragment: Set ..0. .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: SCTP (132) Header checksum: 0xdd17 [validation disabled] [Header checksum status: Unverified] Source: 172.27.2.4 Destination: 172.24.2.5 Stream Control Transmission Protocol, Src Port: 30102 (30102), Dst Port: 57630 (57630) Source port: 30102 Destination port: 57630 Verification tag: 0xf3dc3087 [Association index: 1] Checksum: 0x00000000 [unverified] [Checksum Status: Unverified] INIT_ACK chunk (Outbound streams: 10, inbound streams: 10) Chunk type: INIT_ACK (2) 0... .... = Bit: Stop processing of the packet .0.. .... = Bit: Do not report Chunk flags: 0x00 Chunk length: 260 Initiate tag: 0xaeacd509 Advertised receiver window credit (a_rwnd): 106496 Number of outbound streams: 10 Number of inbound streams: 10 Initial TSN: 2976219343 State cookie parameter (Cookie length: 228 bytes) Parameter type: State cookie (0x0007) 0... .... .... .... = Bit: Stop processing of chunk .0.. .... .... .... = Bit: Do not report Parameter length: 232 State cookie: 7754958a8d7e18094f520b506e872d0c68bfe8f200000000... ECN parameter Parameter type: ECN (0x8000) 1... .... .... .... = Bit: Skip parameter and continue processing of the chunk .0.. .... .... .... = Bit: Do not report Parameter length: 4 Forward TSN supported parameter Parameter type: Forward TSN supported (0xc000) 1... .... .... .... = Bit: Skip parameter and continue processing of the chunk .1.. .... .... .... = Bit: Do report Parameter length: 4 Frame 32: 278 bytes on wire (2224 bits), 278 bytes captured (2224 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 Encapsulation type: Ethernet (1) Arrival Time: Jul 23, 2020 15:41:58.372081995 UTC [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1595518918.372081995 seconds [Time delta from previous captured frame: 0.001702649 seconds] [Time delta from previous displayed frame: 0.001702649 seconds] [Time since reference or first frame: 206.822119608 seconds] Frame Number: 32 Frame Length: 278 bytes (2224 bits) Capture Length: 278 bytes (2224 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:sctp] Ethernet II, Src: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01), Dst: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Destination: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Address: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Address: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 172.24.2.5, Dst: 172.27.2.4 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 264 Identification: 0x0000 (0) Flags: 0x02 (Don't Fragment) 0... .... = Reserved bit: Not set .1.. .... = Don't fragment: Set ..0. .... = More fragments: Not set Fragment offset: 0 Time to live: 63 Protocol: SCTP (132) Header checksum: 0xde33 [validation disabled] [Header checksum status: Unverified] Source: 172.24.2.5 Destination: 172.27.2.4 Stream Control Transmission Protocol, Src Port: 57630 (57630), Dst Port: 30102 (30102) Source port: 57630 Destination port: 30102 Verification tag: 0xaeacd509 [Association index: 1] Checksum: 0xfd98e801 [unverified] [Checksum Status: Unverified] COOKIE_ECHO chunk (Cookie length: 228 bytes) Chunk type: COOKIE_ECHO (10) 0... .... = Bit: Stop processing of the packet .0.. .... = Bit: Do not report Chunk flags: 0x00 Chunk length: 232 Cookie: 7754958a8d7e18094f520b506e872d0c68bfe8f200000000... Frame 33: 50 bytes on wire (400 bits), 50 bytes captured (400 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 Encapsulation type: Ethernet (1) Arrival Time: Jul 23, 2020 15:41:58.372120464 UTC [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1595518918.372120464 seconds [Time delta from previous captured frame: 0.000038469 seconds] [Time delta from previous displayed frame: 0.000038469 seconds] [Time since reference or first frame: 206.822158077 seconds] Frame Number: 33 Frame Length: 50 bytes (400 bits) Capture Length: 50 bytes (400 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:sctp] Ethernet II, Src: d6:96:23:1b:02:05 (d6:96:23:1b:02:05), Dst: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Destination: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Address: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Address: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 172.27.2.4, Dst: 172.24.2.5 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 36 Identification: 0x0000 (0) Flags: 0x02 (Don't Fragment) 0... .... = Reserved bit: Not set .1.. .... = Don't fragment: Set ..0. .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: SCTP (132) Header checksum: 0xde17 [validation disabled] [Header checksum status: Unverified] Source: 172.27.2.4 Destination: 172.24.2.5 Stream Control Transmission Protocol, Src Port: 30102 (30102), Dst Port: 57630 (57630) Source port: 30102 Destination port: 57630 Verification tag: 0xf3dc3087 [Association index: 1] Checksum: 0x00000000 [unverified] [Checksum Status: Unverified] COOKIE_ACK chunk Chunk type: COOKIE_ACK (11) 0... .... = Bit: Stop processing of the packet .0.. .... = Bit: Do not report Chunk flags: 0x00 Chunk length: 4 ### Sending \"hello world!\" ### Frame 34: 78 bytes on wire (624 bits), 78 bytes captured (624 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 Encapsulation type: Ethernet (1) Arrival Time: Jul 23, 2020 15:42:23.462046650 UTC [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1595518943.462046650 seconds [Time delta from previous captured frame: 25.089926186 seconds] [Time delta from previous displayed frame: 25.089926186 seconds] [Time since reference or first frame: 231.912084263 seconds] Frame Number: 34 Frame Length: 78 bytes (624 bits) Capture Length: 78 bytes (624 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:sctp:data] Ethernet II, Src: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01), Dst: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Destination: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Address: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Address: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 172.24.2.5, Dst: 172.27.2.4 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 64 Identification: 0x0001 (1) Flags: 0x02 (Don't Fragment) 0... .... = Reserved bit: Not set .1.. .... = Don't fragment: Set ..0. .... = More fragments: Not set Fragment offset: 0 Time to live: 63 Protocol: SCTP (132) Header checksum: 0xdefa [validation disabled] [Header checksum status: Unverified] Source: 172.24.2.5 Destination: 172.27.2.4 Stream Control Transmission Protocol, Src Port: 57630 (57630), Dst Port: 30102 (30102) Source port: 57630 Destination port: 30102 Verification tag: 0xaeacd509 [Association index: 1] Checksum: 0x47de77f1 [unverified] [Checksum Status: Unverified] DATA chunk(ordered, complete segment, TSN: 3207583439, SID: 0, SSN: 0, PPID: 0, payload length: 13 bytes) Chunk type: DATA (0) 0... .... = Bit: Stop processing of the packet .0.. .... = Bit: Do not report Chunk flags: 0x03 .... ...1 = E-Bit: Last segment .... ..1. = B-Bit: First segment .... .0.. = U-Bit: Ordered delivery .... 0... = I-Bit: Possibly delay SACK Chunk length: 29 Transmission sequence number: 3207583439 Stream identifier: 0x0000 Stream sequence number: 0 Payload protocol identifier: not specified (0) Chunk padding: 000000 Data (13 bytes) 0000 68 65 6c 6c 6f 20 77 6f 72 6c 64 21 0a hello world!. Data: 68656c6c6f20776f726c64210a [Length: 13] Frame 35: 62 bytes on wire (496 bits), 62 bytes captured (496 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 Encapsulation type: Ethernet (1) Arrival Time: Jul 23, 2020 15:42:23.462088655 UTC [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1595518943.462088655 seconds [Time delta from previous captured frame: 0.000042005 seconds] [Time delta from previous displayed frame: 0.000042005 seconds] [Time since reference or first frame: 231.912126268 seconds] Frame Number: 35 Frame Length: 62 bytes (496 bits) Capture Length: 62 bytes (496 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:sctp] Ethernet II, Src: d6:96:23:1b:02:05 (d6:96:23:1b:02:05), Dst: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Destination: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Address: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Address: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 172.27.2.4, Dst: 172.24.2.5 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 48 Identification: 0x6d25 (27941) Flags: 0x02 (Don't Fragment) 0... .... = Reserved bit: Not set .1.. .... = Don't fragment: Set ..0. .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: SCTP (132) Header checksum: 0x70e6 [validation disabled] [Header checksum status: Unverified] Source: 172.27.2.4 Destination: 172.24.2.5 Stream Control Transmission Protocol, Src Port: 30102 (30102), Dst Port: 57630 (57630) Source port: 30102 Destination port: 57630 Verification tag: 0xf3dc3087 [Association index: 1] Checksum: 0x00000000 [unverified] [Checksum Status: Unverified] SACK chunk (Cumulative TSN: 3207583439, a_rwnd: 106483, gaps: 0, duplicate TSNs: 0) Chunk type: SACK (3) 0... .... = Bit: Stop processing of the packet .0.. .... = Bit: Do not report Chunk flags: 0x00 .... ...0 = Nounce sum: 0 Chunk length: 16 Cumulative TSN ACK: 3207583439 [Acknowledges TSN: 3207583439] [Acknowledges TSN in frame: 34] [The RTT since DATA was: 0.000042005 seconds] Advertised receiver window credit (a_rwnd): 106483 Number of gap acknowledgement blocks: 0 Number of duplicated TSNs: 0 ### Connection teardown Frame 36: 98 bytes on wire (784 bits), 98 bytes captured (784 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 Encapsulation type: Ethernet (1) Arrival Time: Jul 23, 2020 15:42:33.743280804 UTC [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1595518953.743280804 seconds [Time delta from previous captured frame: 10.281192149 seconds] [Time delta from previous displayed frame: 10.281192149 seconds] [Time since reference or first frame: 242.193318417 seconds] Frame Number: 36 Frame Length: 98 bytes (784 bits) Capture Length: 98 bytes (784 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:sctp] Ethernet II, Src: d6:96:23:1b:02:05 (d6:96:23:1b:02:05), Dst: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Destination: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Address: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Address: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 172.27.2.4, Dst: 172.24.2.5 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 84 Identification: 0x6d26 (27942) Flags: 0x02 (Don't Fragment) 0... .... = Reserved bit: Not set .1.. .... = Don't fragment: Set ..0. .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: SCTP (132) Header checksum: 0x70c1 [validation disabled] [Header checksum status: Unverified] Source: 172.27.2.4 Destination: 172.24.2.5 Stream Control Transmission Protocol, Src Port: 30102 (30102), Dst Port: 57630 (57630) Source port: 30102 Destination port: 57630 Verification tag: 0xf3dc3087 [Association index: 1] Checksum: 0x00000000 [unverified] [Checksum Status: Unverified] HEARTBEAT chunk (Information: 48 bytes) Chunk type: HEARTBEAT (4) 0... .... = Bit: Stop processing of the packet .0.. .... = Bit: Do not report Chunk flags: 0x00 Chunk length: 52 Heartbeat info parameter (Information: 44 bytes) Parameter type: Heartbeat info (0x0001) 0... .... .... .... = Bit: Stop processing of chunk .0.. .... .... .... = Bit: Do not report Parameter length: 48 Heartbeat information: 0200e11eac18020500000000000000000000000000000000... Frame 37: 98 bytes on wire (784 bits), 98 bytes captured (784 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 Encapsulation type: Ethernet (1) Arrival Time: Jul 23, 2020 15:42:33.745135942 UTC [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1595518953.745135942 seconds [Time delta from previous captured frame: 0.001855138 seconds] [Time delta from previous displayed frame: 0.001855138 seconds] [Time since reference or first frame: 242.195173555 seconds] Frame Number: 37 Frame Length: 98 bytes (784 bits) Capture Length: 98 bytes (784 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:sctp] Ethernet II, Src: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01), Dst: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Destination: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Address: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Address: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 172.24.2.5, Dst: 172.27.2.4 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 84 Identification: 0x0002 (2) Flags: 0x02 (Don't Fragment) 0... .... = Reserved bit: Not set .1.. .... = Don't fragment: Set ..0. .... = More fragments: Not set Fragment offset: 0 Time to live: 63 Protocol: SCTP (132) Header checksum: 0xdee5 [validation disabled] [Header checksum status: Unverified] Source: 172.24.2.5 Destination: 172.27.2.4 Stream Control Transmission Protocol, Src Port: 57630 (57630), Dst Port: 30102 (30102) Source port: 57630 Destination port: 30102 Verification tag: 0xaeacd509 [Association index: 1] Checksum: 0x0818f127 [unverified] [Checksum Status: Unverified] HEARTBEAT_ACK chunk (Information: 48 bytes) Chunk type: HEARTBEAT_ACK (5) 0... .... = Bit: Stop processing of the packet .0.. .... = Bit: Do not report Chunk flags: 0x00 Chunk length: 52 Heartbeat info parameter (Information: 44 bytes) Parameter type: Heartbeat info (0x0001) 0... .... .... .... = Bit: Stop processing of chunk .0.. .... .... .... = Bit: Do not report Parameter length: 48 Heartbeat information: 0200e11eac18020500000000000000000000000000000000... Frame 38: 54 bytes on wire (432 bits), 54 bytes captured (432 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 Encapsulation type: Ethernet (1) Arrival Time: Jul 23, 2020 15:42:34.594482028 UTC [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1595518954.594482028 seconds [Time delta from previous captured frame: 0.849346086 seconds] [Time delta from previous displayed frame: 0.849346086 seconds] [Time since reference or first frame: 243.044519641 seconds] Frame Number: 38 Frame Length: 54 bytes (432 bits) Capture Length: 54 bytes (432 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:sctp] Ethernet II, Src: d6:96:23:1b:02:05 (d6:96:23:1b:02:05), Dst: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Destination: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Address: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Address: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 172.27.2.4, Dst: 172.24.2.5 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 40 Identification: 0x6d27 (27943) Flags: 0x02 (Don't Fragment) 0... .... = Reserved bit: Not set .1.. .... = Don't fragment: Set ..0. .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: SCTP (132) Header checksum: 0x70ec [validation disabled] [Header checksum status: Unverified] Source: 172.27.2.4 Destination: 172.24.2.5 Stream Control Transmission Protocol, Src Port: 30102 (30102), Dst Port: 57630 (57630) Source port: 30102 Destination port: 57630 Verification tag: 0xf3dc3087 [Association index: 1] Checksum: 0x00000000 [unverified] [Checksum Status: Unverified] SHUTDOWN chunk (Cumulative TSN ack: 3207583439) Chunk type: SHUTDOWN (7) 0... .... = Bit: Stop processing of the packet .0.. .... = Bit: Do not report Chunk flags: 0x00 Chunk length: 8 Cumulative TSN Ack: 3207583439 Frame 39: 50 bytes on wire (400 bits), 50 bytes captured (400 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 Encapsulation type: Ethernet (1) Arrival Time: Jul 23, 2020 15:42:34.594825489 UTC [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1595518954.594825489 seconds [Time delta from previous captured frame: 0.000343461 seconds] [Time delta from previous displayed frame: 0.000343461 seconds] [Time since reference or first frame: 243.044863102 seconds] Frame Number: 39 Frame Length: 50 bytes (400 bits) Capture Length: 50 bytes (400 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:sctp] Ethernet II, Src: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01), Dst: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Destination: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Address: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Address: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 172.24.2.5, Dst: 172.27.2.4 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 36 Identification: 0x0003 (3) Flags: 0x02 (Don't Fragment) 0... .... = Reserved bit: Not set .1.. .... = Don't fragment: Set ..0. .... = More fragments: Not set Fragment offset: 0 Time to live: 63 Protocol: SCTP (132) Header checksum: 0xdf14 [validation disabled] [Header checksum status: Unverified] Source: 172.24.2.5 Destination: 172.27.2.4 Stream Control Transmission Protocol, Src Port: 57630 (57630), Dst Port: 30102 (30102) Source port: 57630 Destination port: 30102 Verification tag: 0xaeacd509 [Association index: 1] Checksum: 0xd7e61e5f [unverified] [Checksum Status: Unverified] SHUTDOWN_ACK chunk Chunk type: SHUTDOWN_ACK (8) 0... .... = Bit: Stop processing of the packet .0.. .... = Bit: Do not report Chunk flags: 0x00 Chunk length: 4 Frame 40: 50 bytes on wire (400 bits), 50 bytes captured (400 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 Encapsulation type: Ethernet (1) Arrival Time: Jul 23, 2020 15:42:34.594846074 UTC [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1595518954.594846074 seconds [Time delta from previous captured frame: 0.000020585 seconds] [Time delta from previous displayed frame: 0.000020585 seconds] [Time since reference or first frame: 243.044883687 seconds] Frame Number: 40 Frame Length: 50 bytes (400 bits) Capture Length: 50 bytes (400 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:sctp] Ethernet II, Src: d6:96:23:1b:02:05 (d6:96:23:1b:02:05), Dst: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Destination: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Address: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Address: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 172.27.2.4, Dst: 172.24.2.5 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 36 Identification: 0x6d28 (27944) Flags: 0x02 (Don't Fragment) 0... .... = Reserved bit: Not set .1.. .... = Don't fragment: Set ..0. .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: SCTP (132) Header checksum: 0x70ef [validation disabled] [Header checksum status: Unverified] Source: 172.27.2.4 Destination: 172.24.2.5 Stream Control Transmission Protocol, Src Port: 30102 (30102), Dst Port: 57630 (57630) Source port: 30102 Destination port: 57630 Verification tag: 0xf3dc3087 [Association index: 1] Checksum: 0x00000000 [unverified] [Checksum Status: Unverified] SHUTDOWN_COMPLETE chunk Chunk type: SHUTDOWN_COMPLETE (14) 0... .... = Bit: Stop processing of the packet .0.. .... = Bit: Do not report Chunk flags: 0x00 ^C [root@openshift-jumpserver-0 ~]# Resources https://docs.openshift.com/container-platform/4.4/networking/using-sctp.html https://en.wikipedia.org/wiki/Stream_Control_Transmission_Protocol https://tools.ietf.org/html/rfc4960","title":"SCTP"},{"location":"networking/sctp/#analyzing-sctp-in-openshift","text":"This short blod post shows how to deploy SCTP capability and pods in OpenShift and has a brief look at an SCTP packet capture. For further info and more details, see the Resources section.","title":"Analyzing SCTP in OpenShift"},{"location":"networking/sctp/#deploying-in-openshift","text":"load-sctp-module.yaml : apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: labels: machineconfiguration.openshift.io/role: worker name: load-sctp-module spec: config: ignition: version: 2.2.0 storage: files: - contents: source: data:, verification: {} filesystem: root mode: 420 path: /etc/modprobe.d/sctp-blacklist.conf - contents: source: data:text/plain;charset=utf-8,sctp filesystem: root mode: 420 path: /etc/modules-load.d/sctp-load.conf oc apply -f load-sctp-module.yaml sctp-pods.yaml : #!/bin/bash echo \"Installing SCTP on nodes\" echo \"https://docs.openshift.com/container-platform/4.4/networking/using-sctp.html\" oc new-project sctp oc project sctp cat <<'EOF' | oc apply -f - apiVersion: v1 kind: Service metadata: name: sctpservice labels: app: sctpserver spec: type: NodePort selector: app: sctpserver ports: - name: sctpserver protocol: SCTP port: 30102 targetPort: 30102 EOF cat <<'EOF' | oc apply -f - apiVersion: v1 kind: Pod metadata: name: sctpserver labels: app: sctpserver spec: containers: - name: sctpserver image: fedora command: [\"/bin/sh\", \"-c\"] args: [\"dnf install -y nc iperf3 procps-ng iproute && iperf3 -s\"] ports: - containerPort: 30102 name: sctpserver protocol: SCTP - name: tshark image: danielguerra/alpine-tshark command: - \"tshark\" - \"-i\" - \"eth0\" - \"-V\" - \"sctp\" EOF cat <<'EOF' | oc apply -f - apiVersion: v1 kind: Pod metadata: name: sctpclient labels: app: sctpclient spec: containers: - name: sctpclient image: fedora command: [\"/bin/sh\", \"-c\"] args: [\"dnf install -y nc tcpdump iperf3 procps-ng iproute && echo 'iperf3 -c <IP> -t 3600 --sctp' > /etc/motd && sleep inf\"] - name: tshark image: danielguerra/alpine-tshark command: - \"tshark\" - \"-i\" - \"eth0\" - \"-V\" - \"sctp\" EOF oc get pods -o wide sctpserverip=$(oc get pods -o wide | grep sctpserver | awk '{print $6}') echo \"Run:\" echo \"oc rsh sctpclient\" echo \"iperf3 -c $sctpserverip -t 60 --sctp\" echo \"You can access the packet capture with:\" echo \"oc logs -f -c tcpdump sctpserver\" oc apply -f sctp-pods.yaml","title":"Deploying in Openshift"},{"location":"networking/sctp/#analysis","text":"Server: sh-5.0# ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 3: eth0@if17: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1400 qdisc noqueue state UP group default link/ether d6:96:23:1b:02:05 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.27.2.4/23 brd 172.27.3.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::7066:5ff:fe64:1990/64 scope link valid_lft forever preferred_lft forever sh-5.0# nc -l -p 30102 --sctp -k hello world! Client: sh-5.0# ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 3: eth0@if18: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1400 qdisc noqueue state UP group default link/ether d6:96:23:18:02:06 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.24.2.5/23 brd 172.24.3.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::a856:85ff:feb0:f262/64 scope link valid_lft forever preferred_lft forever sh-5.0# nc --sctp 172.27.2.4 30102 hello world! [root@openshift-jumpserver-0 ~]# oc logs -f sctpserver tshark --tail=0 ### connection setup ### Verification tag: 0x2222afc5 [Association index: 0] Checksum: 0x00000000 [unverified] [Checksum Status: Unverified] SHUTDOWN_COMPLETE chunk Chunk type: SHUTDOWN_COMPLETE (14) 0... .... = Bit: Stop processing of the packet .0.. .... = Bit: Do not report Chunk flags: 0x00 .... ...0 = T-Bit: Tag not reflected Chunk length: 4 Frame 30: 82 bytes on wire (656 bits), 82 bytes captured (656 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 Encapsulation type: Ethernet (1) Arrival Time: Jul 23, 2020 15:41:58.370315008 UTC [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1595518918.370315008 seconds [Time delta from previous captured frame: 36.580134390 seconds] [Time delta from previous displayed frame: 36.580134390 seconds] [Time since reference or first frame: 206.820352621 seconds] Frame Number: 30 Frame Length: 82 bytes (656 bits) Capture Length: 82 bytes (656 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:sctp] Ethernet II, Src: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01), Dst: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Destination: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Address: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Address: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 172.24.2.5, Dst: 172.27.2.4 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 68 Identification: 0x0000 (0) Flags: 0x02 (Don't Fragment) 0... .... = Reserved bit: Not set .1.. .... = Don't fragment: Set ..0. .... = More fragments: Not set Fragment offset: 0 Time to live: 63 Protocol: SCTP (132) Header checksum: 0xdef7 [validation disabled] [Header checksum status: Unverified] Source: 172.24.2.5 Destination: 172.27.2.4 Stream Control Transmission Protocol, Src Port: 57630 (57630), Dst Port: 30102 (30102) Source port: 57630 Destination port: 30102 Verification tag: 0x00000000 [Association index: 1] Checksum: 0x87034a86 [unverified] [Checksum Status: Unverified] INIT chunk (Outbound streams: 10, inbound streams: 65535) Chunk type: INIT (1) 0... .... = Bit: Stop processing of the packet .0.. .... = Bit: Do not report Chunk flags: 0x00 Chunk length: 36 Initiate tag: 0xf3dc3087 Advertised receiver window credit (a_rwnd): 106496 Number of outbound streams: 10 Number of inbound streams: 65535 Initial TSN: 3207583439 Supported address types parameter (Supported types: IPv4) Parameter type: Supported address types (0x000c) 0... .... .... .... = Bit: Stop processing of chunk .0.. .... .... .... = Bit: Do not report Parameter length: 6 Supported address type: IPv4 address (5) Parameter padding: 0000 ECN parameter Parameter type: ECN (0x8000) 1... .... .... .... = Bit: Skip parameter and continue processing of the chunk .0.. .... .... .... = Bit: Do not report Parameter length: 4 Forward TSN supported parameter Parameter type: Forward TSN supported (0xc000) 1... .... .... .... = Bit: Skip parameter and continue processing of the chunk .1.. .... .... .... = Bit: Do report Parameter length: 4 Frame 31: 306 bytes on wire (2448 bits), 306 bytes captured (2448 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 Encapsulation type: Ethernet (1) Arrival Time: Jul 23, 2020 15:41:58.370379346 UTC [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1595518918.370379346 seconds [Time delta from previous captured frame: 0.000064338 seconds] [Time delta from previous displayed frame: 0.000064338 seconds] [Time since reference or first frame: 206.820416959 seconds] Frame Number: 31 Frame Length: 306 bytes (2448 bits) Capture Length: 306 bytes (2448 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:sctp] Ethernet II, Src: d6:96:23:1b:02:05 (d6:96:23:1b:02:05), Dst: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Destination: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Address: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Address: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 172.27.2.4, Dst: 172.24.2.5 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 292 Identification: 0x0000 (0) Flags: 0x02 (Don't Fragment) 0... .... = Reserved bit: Not set .1.. .... = Don't fragment: Set ..0. .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: SCTP (132) Header checksum: 0xdd17 [validation disabled] [Header checksum status: Unverified] Source: 172.27.2.4 Destination: 172.24.2.5 Stream Control Transmission Protocol, Src Port: 30102 (30102), Dst Port: 57630 (57630) Source port: 30102 Destination port: 57630 Verification tag: 0xf3dc3087 [Association index: 1] Checksum: 0x00000000 [unverified] [Checksum Status: Unverified] INIT_ACK chunk (Outbound streams: 10, inbound streams: 10) Chunk type: INIT_ACK (2) 0... .... = Bit: Stop processing of the packet .0.. .... = Bit: Do not report Chunk flags: 0x00 Chunk length: 260 Initiate tag: 0xaeacd509 Advertised receiver window credit (a_rwnd): 106496 Number of outbound streams: 10 Number of inbound streams: 10 Initial TSN: 2976219343 State cookie parameter (Cookie length: 228 bytes) Parameter type: State cookie (0x0007) 0... .... .... .... = Bit: Stop processing of chunk .0.. .... .... .... = Bit: Do not report Parameter length: 232 State cookie: 7754958a8d7e18094f520b506e872d0c68bfe8f200000000... ECN parameter Parameter type: ECN (0x8000) 1... .... .... .... = Bit: Skip parameter and continue processing of the chunk .0.. .... .... .... = Bit: Do not report Parameter length: 4 Forward TSN supported parameter Parameter type: Forward TSN supported (0xc000) 1... .... .... .... = Bit: Skip parameter and continue processing of the chunk .1.. .... .... .... = Bit: Do report Parameter length: 4 Frame 32: 278 bytes on wire (2224 bits), 278 bytes captured (2224 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 Encapsulation type: Ethernet (1) Arrival Time: Jul 23, 2020 15:41:58.372081995 UTC [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1595518918.372081995 seconds [Time delta from previous captured frame: 0.001702649 seconds] [Time delta from previous displayed frame: 0.001702649 seconds] [Time since reference or first frame: 206.822119608 seconds] Frame Number: 32 Frame Length: 278 bytes (2224 bits) Capture Length: 278 bytes (2224 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:sctp] Ethernet II, Src: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01), Dst: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Destination: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Address: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Address: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 172.24.2.5, Dst: 172.27.2.4 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 264 Identification: 0x0000 (0) Flags: 0x02 (Don't Fragment) 0... .... = Reserved bit: Not set .1.. .... = Don't fragment: Set ..0. .... = More fragments: Not set Fragment offset: 0 Time to live: 63 Protocol: SCTP (132) Header checksum: 0xde33 [validation disabled] [Header checksum status: Unverified] Source: 172.24.2.5 Destination: 172.27.2.4 Stream Control Transmission Protocol, Src Port: 57630 (57630), Dst Port: 30102 (30102) Source port: 57630 Destination port: 30102 Verification tag: 0xaeacd509 [Association index: 1] Checksum: 0xfd98e801 [unverified] [Checksum Status: Unverified] COOKIE_ECHO chunk (Cookie length: 228 bytes) Chunk type: COOKIE_ECHO (10) 0... .... = Bit: Stop processing of the packet .0.. .... = Bit: Do not report Chunk flags: 0x00 Chunk length: 232 Cookie: 7754958a8d7e18094f520b506e872d0c68bfe8f200000000... Frame 33: 50 bytes on wire (400 bits), 50 bytes captured (400 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 Encapsulation type: Ethernet (1) Arrival Time: Jul 23, 2020 15:41:58.372120464 UTC [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1595518918.372120464 seconds [Time delta from previous captured frame: 0.000038469 seconds] [Time delta from previous displayed frame: 0.000038469 seconds] [Time since reference or first frame: 206.822158077 seconds] Frame Number: 33 Frame Length: 50 bytes (400 bits) Capture Length: 50 bytes (400 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:sctp] Ethernet II, Src: d6:96:23:1b:02:05 (d6:96:23:1b:02:05), Dst: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Destination: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Address: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Address: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 172.27.2.4, Dst: 172.24.2.5 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 36 Identification: 0x0000 (0) Flags: 0x02 (Don't Fragment) 0... .... = Reserved bit: Not set .1.. .... = Don't fragment: Set ..0. .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: SCTP (132) Header checksum: 0xde17 [validation disabled] [Header checksum status: Unverified] Source: 172.27.2.4 Destination: 172.24.2.5 Stream Control Transmission Protocol, Src Port: 30102 (30102), Dst Port: 57630 (57630) Source port: 30102 Destination port: 57630 Verification tag: 0xf3dc3087 [Association index: 1] Checksum: 0x00000000 [unverified] [Checksum Status: Unverified] COOKIE_ACK chunk Chunk type: COOKIE_ACK (11) 0... .... = Bit: Stop processing of the packet .0.. .... = Bit: Do not report Chunk flags: 0x00 Chunk length: 4 ### Sending \"hello world!\" ### Frame 34: 78 bytes on wire (624 bits), 78 bytes captured (624 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 Encapsulation type: Ethernet (1) Arrival Time: Jul 23, 2020 15:42:23.462046650 UTC [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1595518943.462046650 seconds [Time delta from previous captured frame: 25.089926186 seconds] [Time delta from previous displayed frame: 25.089926186 seconds] [Time since reference or first frame: 231.912084263 seconds] Frame Number: 34 Frame Length: 78 bytes (624 bits) Capture Length: 78 bytes (624 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:sctp:data] Ethernet II, Src: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01), Dst: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Destination: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Address: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Address: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 172.24.2.5, Dst: 172.27.2.4 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 64 Identification: 0x0001 (1) Flags: 0x02 (Don't Fragment) 0... .... = Reserved bit: Not set .1.. .... = Don't fragment: Set ..0. .... = More fragments: Not set Fragment offset: 0 Time to live: 63 Protocol: SCTP (132) Header checksum: 0xdefa [validation disabled] [Header checksum status: Unverified] Source: 172.24.2.5 Destination: 172.27.2.4 Stream Control Transmission Protocol, Src Port: 57630 (57630), Dst Port: 30102 (30102) Source port: 57630 Destination port: 30102 Verification tag: 0xaeacd509 [Association index: 1] Checksum: 0x47de77f1 [unverified] [Checksum Status: Unverified] DATA chunk(ordered, complete segment, TSN: 3207583439, SID: 0, SSN: 0, PPID: 0, payload length: 13 bytes) Chunk type: DATA (0) 0... .... = Bit: Stop processing of the packet .0.. .... = Bit: Do not report Chunk flags: 0x03 .... ...1 = E-Bit: Last segment .... ..1. = B-Bit: First segment .... .0.. = U-Bit: Ordered delivery .... 0... = I-Bit: Possibly delay SACK Chunk length: 29 Transmission sequence number: 3207583439 Stream identifier: 0x0000 Stream sequence number: 0 Payload protocol identifier: not specified (0) Chunk padding: 000000 Data (13 bytes) 0000 68 65 6c 6c 6f 20 77 6f 72 6c 64 21 0a hello world!. Data: 68656c6c6f20776f726c64210a [Length: 13] Frame 35: 62 bytes on wire (496 bits), 62 bytes captured (496 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 Encapsulation type: Ethernet (1) Arrival Time: Jul 23, 2020 15:42:23.462088655 UTC [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1595518943.462088655 seconds [Time delta from previous captured frame: 0.000042005 seconds] [Time delta from previous displayed frame: 0.000042005 seconds] [Time since reference or first frame: 231.912126268 seconds] Frame Number: 35 Frame Length: 62 bytes (496 bits) Capture Length: 62 bytes (496 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:sctp] Ethernet II, Src: d6:96:23:1b:02:05 (d6:96:23:1b:02:05), Dst: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Destination: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Address: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Address: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 172.27.2.4, Dst: 172.24.2.5 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 48 Identification: 0x6d25 (27941) Flags: 0x02 (Don't Fragment) 0... .... = Reserved bit: Not set .1.. .... = Don't fragment: Set ..0. .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: SCTP (132) Header checksum: 0x70e6 [validation disabled] [Header checksum status: Unverified] Source: 172.27.2.4 Destination: 172.24.2.5 Stream Control Transmission Protocol, Src Port: 30102 (30102), Dst Port: 57630 (57630) Source port: 30102 Destination port: 57630 Verification tag: 0xf3dc3087 [Association index: 1] Checksum: 0x00000000 [unverified] [Checksum Status: Unverified] SACK chunk (Cumulative TSN: 3207583439, a_rwnd: 106483, gaps: 0, duplicate TSNs: 0) Chunk type: SACK (3) 0... .... = Bit: Stop processing of the packet .0.. .... = Bit: Do not report Chunk flags: 0x00 .... ...0 = Nounce sum: 0 Chunk length: 16 Cumulative TSN ACK: 3207583439 [Acknowledges TSN: 3207583439] [Acknowledges TSN in frame: 34] [The RTT since DATA was: 0.000042005 seconds] Advertised receiver window credit (a_rwnd): 106483 Number of gap acknowledgement blocks: 0 Number of duplicated TSNs: 0 ### Connection teardown Frame 36: 98 bytes on wire (784 bits), 98 bytes captured (784 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 Encapsulation type: Ethernet (1) Arrival Time: Jul 23, 2020 15:42:33.743280804 UTC [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1595518953.743280804 seconds [Time delta from previous captured frame: 10.281192149 seconds] [Time delta from previous displayed frame: 10.281192149 seconds] [Time since reference or first frame: 242.193318417 seconds] Frame Number: 36 Frame Length: 98 bytes (784 bits) Capture Length: 98 bytes (784 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:sctp] Ethernet II, Src: d6:96:23:1b:02:05 (d6:96:23:1b:02:05), Dst: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Destination: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Address: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Address: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 172.27.2.4, Dst: 172.24.2.5 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 84 Identification: 0x6d26 (27942) Flags: 0x02 (Don't Fragment) 0... .... = Reserved bit: Not set .1.. .... = Don't fragment: Set ..0. .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: SCTP (132) Header checksum: 0x70c1 [validation disabled] [Header checksum status: Unverified] Source: 172.27.2.4 Destination: 172.24.2.5 Stream Control Transmission Protocol, Src Port: 30102 (30102), Dst Port: 57630 (57630) Source port: 30102 Destination port: 57630 Verification tag: 0xf3dc3087 [Association index: 1] Checksum: 0x00000000 [unverified] [Checksum Status: Unverified] HEARTBEAT chunk (Information: 48 bytes) Chunk type: HEARTBEAT (4) 0... .... = Bit: Stop processing of the packet .0.. .... = Bit: Do not report Chunk flags: 0x00 Chunk length: 52 Heartbeat info parameter (Information: 44 bytes) Parameter type: Heartbeat info (0x0001) 0... .... .... .... = Bit: Stop processing of chunk .0.. .... .... .... = Bit: Do not report Parameter length: 48 Heartbeat information: 0200e11eac18020500000000000000000000000000000000... Frame 37: 98 bytes on wire (784 bits), 98 bytes captured (784 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 Encapsulation type: Ethernet (1) Arrival Time: Jul 23, 2020 15:42:33.745135942 UTC [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1595518953.745135942 seconds [Time delta from previous captured frame: 0.001855138 seconds] [Time delta from previous displayed frame: 0.001855138 seconds] [Time since reference or first frame: 242.195173555 seconds] Frame Number: 37 Frame Length: 98 bytes (784 bits) Capture Length: 98 bytes (784 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:sctp] Ethernet II, Src: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01), Dst: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Destination: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Address: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Address: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 172.24.2.5, Dst: 172.27.2.4 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 84 Identification: 0x0002 (2) Flags: 0x02 (Don't Fragment) 0... .... = Reserved bit: Not set .1.. .... = Don't fragment: Set ..0. .... = More fragments: Not set Fragment offset: 0 Time to live: 63 Protocol: SCTP (132) Header checksum: 0xdee5 [validation disabled] [Header checksum status: Unverified] Source: 172.24.2.5 Destination: 172.27.2.4 Stream Control Transmission Protocol, Src Port: 57630 (57630), Dst Port: 30102 (30102) Source port: 57630 Destination port: 30102 Verification tag: 0xaeacd509 [Association index: 1] Checksum: 0x0818f127 [unverified] [Checksum Status: Unverified] HEARTBEAT_ACK chunk (Information: 48 bytes) Chunk type: HEARTBEAT_ACK (5) 0... .... = Bit: Stop processing of the packet .0.. .... = Bit: Do not report Chunk flags: 0x00 Chunk length: 52 Heartbeat info parameter (Information: 44 bytes) Parameter type: Heartbeat info (0x0001) 0... .... .... .... = Bit: Stop processing of chunk .0.. .... .... .... = Bit: Do not report Parameter length: 48 Heartbeat information: 0200e11eac18020500000000000000000000000000000000... Frame 38: 54 bytes on wire (432 bits), 54 bytes captured (432 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 Encapsulation type: Ethernet (1) Arrival Time: Jul 23, 2020 15:42:34.594482028 UTC [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1595518954.594482028 seconds [Time delta from previous captured frame: 0.849346086 seconds] [Time delta from previous displayed frame: 0.849346086 seconds] [Time since reference or first frame: 243.044519641 seconds] Frame Number: 38 Frame Length: 54 bytes (432 bits) Capture Length: 54 bytes (432 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:sctp] Ethernet II, Src: d6:96:23:1b:02:05 (d6:96:23:1b:02:05), Dst: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Destination: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Address: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Address: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 172.27.2.4, Dst: 172.24.2.5 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 40 Identification: 0x6d27 (27943) Flags: 0x02 (Don't Fragment) 0... .... = Reserved bit: Not set .1.. .... = Don't fragment: Set ..0. .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: SCTP (132) Header checksum: 0x70ec [validation disabled] [Header checksum status: Unverified] Source: 172.27.2.4 Destination: 172.24.2.5 Stream Control Transmission Protocol, Src Port: 30102 (30102), Dst Port: 57630 (57630) Source port: 30102 Destination port: 57630 Verification tag: 0xf3dc3087 [Association index: 1] Checksum: 0x00000000 [unverified] [Checksum Status: Unverified] SHUTDOWN chunk (Cumulative TSN ack: 3207583439) Chunk type: SHUTDOWN (7) 0... .... = Bit: Stop processing of the packet .0.. .... = Bit: Do not report Chunk flags: 0x00 Chunk length: 8 Cumulative TSN Ack: 3207583439 Frame 39: 50 bytes on wire (400 bits), 50 bytes captured (400 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 Encapsulation type: Ethernet (1) Arrival Time: Jul 23, 2020 15:42:34.594825489 UTC [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1595518954.594825489 seconds [Time delta from previous captured frame: 0.000343461 seconds] [Time delta from previous displayed frame: 0.000343461 seconds] [Time since reference or first frame: 243.044863102 seconds] Frame Number: 39 Frame Length: 50 bytes (400 bits) Capture Length: 50 bytes (400 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:sctp] Ethernet II, Src: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01), Dst: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Destination: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Address: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Address: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 172.24.2.5, Dst: 172.27.2.4 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 36 Identification: 0x0003 (3) Flags: 0x02 (Don't Fragment) 0... .... = Reserved bit: Not set .1.. .... = Don't fragment: Set ..0. .... = More fragments: Not set Fragment offset: 0 Time to live: 63 Protocol: SCTP (132) Header checksum: 0xdf14 [validation disabled] [Header checksum status: Unverified] Source: 172.24.2.5 Destination: 172.27.2.4 Stream Control Transmission Protocol, Src Port: 57630 (57630), Dst Port: 30102 (30102) Source port: 57630 Destination port: 30102 Verification tag: 0xaeacd509 [Association index: 1] Checksum: 0xd7e61e5f [unverified] [Checksum Status: Unverified] SHUTDOWN_ACK chunk Chunk type: SHUTDOWN_ACK (8) 0... .... = Bit: Stop processing of the packet .0.. .... = Bit: Do not report Chunk flags: 0x00 Chunk length: 4 Frame 40: 50 bytes on wire (400 bits), 50 bytes captured (400 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 Encapsulation type: Ethernet (1) Arrival Time: Jul 23, 2020 15:42:34.594846074 UTC [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1595518954.594846074 seconds [Time delta from previous captured frame: 0.000020585 seconds] [Time delta from previous displayed frame: 0.000020585 seconds] [Time since reference or first frame: 243.044883687 seconds] Frame Number: 40 Frame Length: 50 bytes (400 bits) Capture Length: 50 bytes (400 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:sctp] Ethernet II, Src: d6:96:23:1b:02:05 (d6:96:23:1b:02:05), Dst: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Destination: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) Address: 0a:58:ac:1b:02:01 (0a:58:ac:1b:02:01) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) Address: d6:96:23:1b:02:05 (d6:96:23:1b:02:05) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 172.27.2.4, Dst: 172.24.2.5 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x02 (DSCP: CS0, ECN: ECT(0)) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..10 = Explicit Congestion Notification: ECN-Capable Transport codepoint '10' (2) Total Length: 36 Identification: 0x6d28 (27944) Flags: 0x02 (Don't Fragment) 0... .... = Reserved bit: Not set .1.. .... = Don't fragment: Set ..0. .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: SCTP (132) Header checksum: 0x70ef [validation disabled] [Header checksum status: Unverified] Source: 172.27.2.4 Destination: 172.24.2.5 Stream Control Transmission Protocol, Src Port: 30102 (30102), Dst Port: 57630 (57630) Source port: 30102 Destination port: 57630 Verification tag: 0xf3dc3087 [Association index: 1] Checksum: 0x00000000 [unverified] [Checksum Status: Unverified] SHUTDOWN_COMPLETE chunk Chunk type: SHUTDOWN_COMPLETE (14) 0... .... = Bit: Stop processing of the packet .0.. .... = Bit: Do not report Chunk flags: 0x00 ^C [root@openshift-jumpserver-0 ~]#","title":"Analysis"},{"location":"networking/sctp/#resources","text":"https://docs.openshift.com/container-platform/4.4/networking/using-sctp.html https://en.wikipedia.org/wiki/Stream_Control_Transmission_Protocol https://tools.ietf.org/html/rfc4960","title":"Resources"},{"location":"networking/wireguard/","text":"Wireguard demo on Fedora 32 Prerequisites 2 Fedora 32 nodes with kernel: [root@wireguard01 ~]# uname -a Linux wireguard01 5.6.6-300.fc32.x86_64 #1 SMP Tue Apr 21 13:44:19 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux [root@wireguard01 ~]# ip a | grep 192.168.122 inet 192.168.122.250/24 brd 192.168.122.255 scope global dynamic noprefixroute eth0 [root@wireguard02 wireguard]# uname -a Linux wireguard02 5.6.6-300.fc32.x86_64 #1 SMP Tue Apr 21 13:44:19 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux [root@wireguard02 wireguard]# ip a | grep 192.168.122 inet 192.168.122.81/24 brd 192.168.122.255 scope global dynamic noprefixroute eth0 Install wireguard-tools, wireshark and tcpdump (the latter tools for monitoring): yum install -y wireguard-tools wireshark tcpdump Create keys wireguard01: wg genkey > /etc/wireguard/private chmod 600 /etc/wireguard/private wg pubkey < /etc/wireguard/private > /etc/wireguard/publickey [root@wireguard01 ~]# cat /etc/wireguard/private 0JYD7daZL+Bh7u77vFnwXHs5Rdi7iFfpk8NC1pg542Q= [root@wireguard01 ~]# cat /etc/wireguard/publickey pkyJK9ePlOTW9+GK73UnCZ4/b7/3xOthdfdbF7OQUzw= wireguard02: wg genkey > /etc/wireguard/private chmod 600 /etc/wireguard/private wg pubkey < /etc/wireguard/private > /etc/wireguard/publickey [root@wireguard02 wireguard]# cat /etc/wireguard/private wHpqAYwqSp6F4lO09uFj8BQGVJ6OjZIkvVWUQdy+GGk= [root@wireguard02 wireguard]# cat /etc/wireguard/publickey uD48Km4aRYXD7OvUtiatwqvvBG35lAad2j4hCpgNYEc= Write down IP addresses and keys for later wireguard01: /etc/wireguard/private: 0JYD7daZL+Bh7u77vFnwXHs5Rdi7iFfpk8NC1pg542Q= /etc/wireguard/publickey: pkyJK9ePlOTW9+GK73UnCZ4/b7/3xOthdfdbF7OQUzw= Outer IP: 192.168.122.250/24 Inner IP: 192.168.123.1/24 wireguard02: /etc/wireguard/private: wHpqAYwqSp6F4lO09uFj8BQGVJ6OjZIkvVWUQdy+GGk= /etc/wireguard/publickey: uD48Km4aRYXD7OvUtiatwqvvBG35lAad2j4hCpgNYEc= Outer IP: 192.168.122.81/24 Inner IP: 192.168.123.2/32 Manual tunnel setup Follow these steps to create tunnels manually. wireguard01: ip link add wg0 type wireguard ip a a 192.168.123.1/24 dev wg0 wg set wg0 private-key /etc/wireguard/private wg set wg0 listen-port 51820 wg set wg0 peer uD48Km4aRYXD7OvUtiatwqvvBG35lAad2j4hCpgNYEc= allowed-ips 192.168.123.2/32 endpoint 192.168.122.81:51820 persistent-keepalive 30 ip link set dev wg0 up wireguard02: ip link add wg0 type wireguard ip a a 192.168.123.2/24 dev wg0 wg set wg0 private-key /etc/wireguard/private wg set wg0 listen-port 51820 wg set wg0 peer pkyJK9ePlOTW9+GK73UnCZ4/b7/3xOthdfdbF7OQUzw= allowed-ips 192.168.123.1/32 endpoint 192.168.122.250:51820 persistent-keepalive 30 ip link set dev wg0 up Verify wireguard: [root@wireguard01 ~]# wg interface: wg0 public key: pkyJK9ePlOTW9+GK73UnCZ4/b7/3xOthdfdbF7OQUzw= private key: (hidden) listening port: 51820 peer: uD48Km4aRYXD7OvUtiatwqvvBG35lAad2j4hCpgNYEc= endpoint: 192.168.122.81:51820 allowed ips: 192.168.123.2/32 latest handshake: 20 seconds ago transfer: 180 B received, 568 B sent persistent keepalive: every 30 seconds [root@wireguard02 wireguard]# wg interface: wg0 public key: uD48Km4aRYXD7OvUtiatwqvvBG35lAad2j4hCpgNYEc= private key: (hidden) listening port: 51820 peer: pkyJK9ePlOTW9+GK73UnCZ4/b7/3xOthdfdbF7OQUzw= endpoint: 192.168.122.250:51820 allowed ips: 192.168.123.1/32 latest handshake: 34 seconds ago transfer: 156 B received, 180 B sent persistent keepalive: every 30 seconds [root@wireguard01 ~]# ping -c1 -W1 192.168.123.2 PING 192.168.123.2 (192.168.123.2) 56(84) bytes of data. 64 bytes from 192.168.123.2: icmp_seq=1 ttl=64 time=0.383 ms --- 192.168.123.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.383/0.383/0.383/0.000 ms Delete interfaces again, run on both hosts: ip link del dev wg0 Tunnel setup with config files Follow these steps to set up tunnels with systemd: [root@wireguard01 ~]# cat <<'EOF' > /etc/wireguard/wg0.conf [Interface] Address = 192.168.123.1/24 PrivateKey = 0JYD7daZL+Bh7u77vFnwXHs5Rdi7iFfpk8NC1pg542Q= ListenPort = 51820 [Peer] PublicKey = uD48Km4aRYXD7OvUtiatwqvvBG35lAad2j4hCpgNYEc= AllowedIPs = 192.168.123.2/32 Endpoint = 192.168.122.81:51820 PersistentKeepalive = 30 EOF [root@wireguard02 wireguard]# cat <<'EOF' > /etc/wireguard/wg0.conf [Interface] Address = 192.168.123.2/32 PrivateKey = wHpqAYwqSp6F4lO09uFj8BQGVJ6OjZIkvVWUQdy+GGk= [Peer] PublicKey = pkyJK9ePlOTW9+GK73UnCZ4/b7/3xOthdfdbF7OQUzw= AllowedIPs = 192.168.123.1/32 Endpoint = 192.168.122.250:51820 PersistentKeepalive = 30 EOF [root@wireguard01 ~]# journalctl -u wg-quick@wg0 -f -n0 & [1] 11296 [root@wireguard01 ~]# -- Logs begin at Wed 2020-08-05 15:44:13 UTC. -- [root@wireguard01 ~]# systemctl start wg-quick@wg0 [root@wireguard01 ~]# Aug 05 20:06:57 wireguard01 systemd[1]: Starting WireGuard via wg-quick(8) for wg0... Aug 05 20:06:57 wireguard01 wg-quick[11299]: [#] ip link add wg0 type wireguard Aug 05 20:06:57 wireguard01 wg-quick[11299]: [#] wg setconf wg0 /dev/fd/63 Aug 05 20:06:57 wireguard01 wg-quick[11299]: [#] ip -4 address add 192.168.123.1/24 dev wg0 Aug 05 20:06:57 wireguard01 wg-quick[11299]: [#] ip link set mtu 1420 up dev wg0 Aug 05 20:06:57 wireguard01 systemd[1]: Finished WireGuard via wg-quick(8) for wg0. [root@wireguard01 ~]# fg journalctl -u wg-quick@wg0 -f -n0 ^C [root@wireguard01 ~]# wg interface: wg0 public key: pkyJK9ePlOTW9+GK73UnCZ4/b7/3xOthdfdbF7OQUzw= private key: (hidden) listening port: 51820 peer: uD48Km4aRYXD7OvUtiatwqvvBG35lAad2j4hCpgNYEc= endpoint: 192.168.122.81:34730 allowed ips: 192.168.123.2/32 latest handshake: 1 minute, 7 seconds ago transfer: 180 B received, 484 B sent persistent keepalive: every 30 seconds [root@wireguard02 wireguard]# -- Logs begin at Wed 2020-08-05 15:44:47 UTC. -- [root@wireguard02 wireguard]# systemctl start wg-quick@wg0 [root@wireguard02 wireguard]# Aug 05 20:07:04 wireguard02 systemd[1]: Starting WireGuard via wg-quick(8) for wg0... Aug 05 20:07:04 wireguard02 wg-quick[10492]: [#] ip link add wg0 type wireguard Aug 05 20:07:04 wireguard02 wg-quick[10492]: [#] wg setconf wg0 /dev/fd/63 Aug 05 20:07:04 wireguard02 wg-quick[10492]: [#] ip -4 address add 192.168.123.2/32 dev wg0 Aug 05 20:07:04 wireguard02 wg-quick[10492]: [#] ip link set mtu 1420 up dev wg0 Aug 05 20:07:04 wireguard02 wg-quick[10492]: [#] ip -4 route add 192.168.123.1/32 dev wg0 Aug 05 20:07:04 wireguard02 systemd[1]: Finished WireGuard via wg-quick(8) for wg0. ^C [root@wireguard02 wireguard]# fg journalctl -u wg-quick@wg0 -f -n0 ^C [root@wireguard02 wireguard]# wg interface: wg0 public key: uD48Km4aRYXD7OvUtiatwqvvBG35lAad2j4hCpgNYEc= private key: (hidden) listening port: 34730 peer: pkyJK9ePlOTW9+GK73UnCZ4/b7/3xOthdfdbF7OQUzw= endpoint: 192.168.122.250:51820 allowed ips: 192.168.123.1/32 latest handshake: 1 minute, 8 seconds ago transfer: 188 B received, 180 B sent persistent keepalive: every 30 seconds And test ping: [root@wireguard01 ~]# ping -c1 -W1 192.168.123.2 PING 192.168.123.2 (192.168.123.2) 56(84) bytes of data. 64 bytes from 192.168.123.2: icmp_seq=1 ttl=64 time=0.356 ms --- 192.168.123.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.356/0.356/0.356/0.000 ms [root@wireguard01 ~]# Starting tunnels on system boot Simply run: systemctl enable wg-quick@wg0 Resources https://www.wireguard.com/quickstart/ https://fedoramagazine.org/build-a-virtual-private-network-with-wireguard/ https://lore.kernel.org/lkml/CAHk-=wi9ZT7Stg-uSpX0UWQzam6OP9Jzz6Xu1CkYu1cicpD5OA@mail.gmail.com/ https://arstechnica.com/gadgets/2020/03/wireguard-vpn-makes-it-to-1-0-0-and-into-the-next-linux-kernel/ https://arstechnica.com/gadgets/2020/03/wireguard-vpn-makes-it-to-1-0-0-and-into-the-next-linux-kernel/","title":"Wireguard"},{"location":"networking/wireguard/#wireguard-demo-on-fedora-32","text":"","title":"Wireguard demo on Fedora 32"},{"location":"networking/wireguard/#prerequisites","text":"2 Fedora 32 nodes with kernel: [root@wireguard01 ~]# uname -a Linux wireguard01 5.6.6-300.fc32.x86_64 #1 SMP Tue Apr 21 13:44:19 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux [root@wireguard01 ~]# ip a | grep 192.168.122 inet 192.168.122.250/24 brd 192.168.122.255 scope global dynamic noprefixroute eth0 [root@wireguard02 wireguard]# uname -a Linux wireguard02 5.6.6-300.fc32.x86_64 #1 SMP Tue Apr 21 13:44:19 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux [root@wireguard02 wireguard]# ip a | grep 192.168.122 inet 192.168.122.81/24 brd 192.168.122.255 scope global dynamic noprefixroute eth0 Install wireguard-tools, wireshark and tcpdump (the latter tools for monitoring): yum install -y wireguard-tools wireshark tcpdump","title":"Prerequisites"},{"location":"networking/wireguard/#create-keys","text":"wireguard01: wg genkey > /etc/wireguard/private chmod 600 /etc/wireguard/private wg pubkey < /etc/wireguard/private > /etc/wireguard/publickey [root@wireguard01 ~]# cat /etc/wireguard/private 0JYD7daZL+Bh7u77vFnwXHs5Rdi7iFfpk8NC1pg542Q= [root@wireguard01 ~]# cat /etc/wireguard/publickey pkyJK9ePlOTW9+GK73UnCZ4/b7/3xOthdfdbF7OQUzw= wireguard02: wg genkey > /etc/wireguard/private chmod 600 /etc/wireguard/private wg pubkey < /etc/wireguard/private > /etc/wireguard/publickey [root@wireguard02 wireguard]# cat /etc/wireguard/private wHpqAYwqSp6F4lO09uFj8BQGVJ6OjZIkvVWUQdy+GGk= [root@wireguard02 wireguard]# cat /etc/wireguard/publickey uD48Km4aRYXD7OvUtiatwqvvBG35lAad2j4hCpgNYEc=","title":"Create keys"},{"location":"networking/wireguard/#write-down-ip-addresses-and-keys-for-later","text":"wireguard01: /etc/wireguard/private: 0JYD7daZL+Bh7u77vFnwXHs5Rdi7iFfpk8NC1pg542Q= /etc/wireguard/publickey: pkyJK9ePlOTW9+GK73UnCZ4/b7/3xOthdfdbF7OQUzw= Outer IP: 192.168.122.250/24 Inner IP: 192.168.123.1/24 wireguard02: /etc/wireguard/private: wHpqAYwqSp6F4lO09uFj8BQGVJ6OjZIkvVWUQdy+GGk= /etc/wireguard/publickey: uD48Km4aRYXD7OvUtiatwqvvBG35lAad2j4hCpgNYEc= Outer IP: 192.168.122.81/24 Inner IP: 192.168.123.2/32","title":"Write down IP addresses and keys for later"},{"location":"networking/wireguard/#manual-tunnel-setup","text":"Follow these steps to create tunnels manually. wireguard01: ip link add wg0 type wireguard ip a a 192.168.123.1/24 dev wg0 wg set wg0 private-key /etc/wireguard/private wg set wg0 listen-port 51820 wg set wg0 peer uD48Km4aRYXD7OvUtiatwqvvBG35lAad2j4hCpgNYEc= allowed-ips 192.168.123.2/32 endpoint 192.168.122.81:51820 persistent-keepalive 30 ip link set dev wg0 up wireguard02: ip link add wg0 type wireguard ip a a 192.168.123.2/24 dev wg0 wg set wg0 private-key /etc/wireguard/private wg set wg0 listen-port 51820 wg set wg0 peer pkyJK9ePlOTW9+GK73UnCZ4/b7/3xOthdfdbF7OQUzw= allowed-ips 192.168.123.1/32 endpoint 192.168.122.250:51820 persistent-keepalive 30 ip link set dev wg0 up Verify wireguard: [root@wireguard01 ~]# wg interface: wg0 public key: pkyJK9ePlOTW9+GK73UnCZ4/b7/3xOthdfdbF7OQUzw= private key: (hidden) listening port: 51820 peer: uD48Km4aRYXD7OvUtiatwqvvBG35lAad2j4hCpgNYEc= endpoint: 192.168.122.81:51820 allowed ips: 192.168.123.2/32 latest handshake: 20 seconds ago transfer: 180 B received, 568 B sent persistent keepalive: every 30 seconds [root@wireguard02 wireguard]# wg interface: wg0 public key: uD48Km4aRYXD7OvUtiatwqvvBG35lAad2j4hCpgNYEc= private key: (hidden) listening port: 51820 peer: pkyJK9ePlOTW9+GK73UnCZ4/b7/3xOthdfdbF7OQUzw= endpoint: 192.168.122.250:51820 allowed ips: 192.168.123.1/32 latest handshake: 34 seconds ago transfer: 156 B received, 180 B sent persistent keepalive: every 30 seconds [root@wireguard01 ~]# ping -c1 -W1 192.168.123.2 PING 192.168.123.2 (192.168.123.2) 56(84) bytes of data. 64 bytes from 192.168.123.2: icmp_seq=1 ttl=64 time=0.383 ms --- 192.168.123.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.383/0.383/0.383/0.000 ms Delete interfaces again, run on both hosts: ip link del dev wg0","title":"Manual tunnel setup"},{"location":"networking/wireguard/#tunnel-setup-with-config-files","text":"Follow these steps to set up tunnels with systemd: [root@wireguard01 ~]# cat <<'EOF' > /etc/wireguard/wg0.conf [Interface] Address = 192.168.123.1/24 PrivateKey = 0JYD7daZL+Bh7u77vFnwXHs5Rdi7iFfpk8NC1pg542Q= ListenPort = 51820 [Peer] PublicKey = uD48Km4aRYXD7OvUtiatwqvvBG35lAad2j4hCpgNYEc= AllowedIPs = 192.168.123.2/32 Endpoint = 192.168.122.81:51820 PersistentKeepalive = 30 EOF [root@wireguard02 wireguard]# cat <<'EOF' > /etc/wireguard/wg0.conf [Interface] Address = 192.168.123.2/32 PrivateKey = wHpqAYwqSp6F4lO09uFj8BQGVJ6OjZIkvVWUQdy+GGk= [Peer] PublicKey = pkyJK9ePlOTW9+GK73UnCZ4/b7/3xOthdfdbF7OQUzw= AllowedIPs = 192.168.123.1/32 Endpoint = 192.168.122.250:51820 PersistentKeepalive = 30 EOF [root@wireguard01 ~]# journalctl -u wg-quick@wg0 -f -n0 & [1] 11296 [root@wireguard01 ~]# -- Logs begin at Wed 2020-08-05 15:44:13 UTC. -- [root@wireguard01 ~]# systemctl start wg-quick@wg0 [root@wireguard01 ~]# Aug 05 20:06:57 wireguard01 systemd[1]: Starting WireGuard via wg-quick(8) for wg0... Aug 05 20:06:57 wireguard01 wg-quick[11299]: [#] ip link add wg0 type wireguard Aug 05 20:06:57 wireguard01 wg-quick[11299]: [#] wg setconf wg0 /dev/fd/63 Aug 05 20:06:57 wireguard01 wg-quick[11299]: [#] ip -4 address add 192.168.123.1/24 dev wg0 Aug 05 20:06:57 wireguard01 wg-quick[11299]: [#] ip link set mtu 1420 up dev wg0 Aug 05 20:06:57 wireguard01 systemd[1]: Finished WireGuard via wg-quick(8) for wg0. [root@wireguard01 ~]# fg journalctl -u wg-quick@wg0 -f -n0 ^C [root@wireguard01 ~]# wg interface: wg0 public key: pkyJK9ePlOTW9+GK73UnCZ4/b7/3xOthdfdbF7OQUzw= private key: (hidden) listening port: 51820 peer: uD48Km4aRYXD7OvUtiatwqvvBG35lAad2j4hCpgNYEc= endpoint: 192.168.122.81:34730 allowed ips: 192.168.123.2/32 latest handshake: 1 minute, 7 seconds ago transfer: 180 B received, 484 B sent persistent keepalive: every 30 seconds [root@wireguard02 wireguard]# -- Logs begin at Wed 2020-08-05 15:44:47 UTC. -- [root@wireguard02 wireguard]# systemctl start wg-quick@wg0 [root@wireguard02 wireguard]# Aug 05 20:07:04 wireguard02 systemd[1]: Starting WireGuard via wg-quick(8) for wg0... Aug 05 20:07:04 wireguard02 wg-quick[10492]: [#] ip link add wg0 type wireguard Aug 05 20:07:04 wireguard02 wg-quick[10492]: [#] wg setconf wg0 /dev/fd/63 Aug 05 20:07:04 wireguard02 wg-quick[10492]: [#] ip -4 address add 192.168.123.2/32 dev wg0 Aug 05 20:07:04 wireguard02 wg-quick[10492]: [#] ip link set mtu 1420 up dev wg0 Aug 05 20:07:04 wireguard02 wg-quick[10492]: [#] ip -4 route add 192.168.123.1/32 dev wg0 Aug 05 20:07:04 wireguard02 systemd[1]: Finished WireGuard via wg-quick(8) for wg0. ^C [root@wireguard02 wireguard]# fg journalctl -u wg-quick@wg0 -f -n0 ^C [root@wireguard02 wireguard]# wg interface: wg0 public key: uD48Km4aRYXD7OvUtiatwqvvBG35lAad2j4hCpgNYEc= private key: (hidden) listening port: 34730 peer: pkyJK9ePlOTW9+GK73UnCZ4/b7/3xOthdfdbF7OQUzw= endpoint: 192.168.122.250:51820 allowed ips: 192.168.123.1/32 latest handshake: 1 minute, 8 seconds ago transfer: 188 B received, 180 B sent persistent keepalive: every 30 seconds And test ping: [root@wireguard01 ~]# ping -c1 -W1 192.168.123.2 PING 192.168.123.2 (192.168.123.2) 56(84) bytes of data. 64 bytes from 192.168.123.2: icmp_seq=1 ttl=64 time=0.356 ms --- 192.168.123.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.356/0.356/0.356/0.000 ms [root@wireguard01 ~]#","title":"Tunnel setup with config files"},{"location":"networking/wireguard/#starting-tunnels-on-system-boot","text":"Simply run: systemctl enable wg-quick@wg0","title":"Starting tunnels on system boot"},{"location":"networking/wireguard/#resources","text":"https://www.wireguard.com/quickstart/ https://fedoramagazine.org/build-a-virtual-private-network-with-wireguard/ https://lore.kernel.org/lkml/CAHk-=wi9ZT7Stg-uSpX0UWQzam6OP9Jzz6Xu1CkYu1cicpD5OA@mail.gmail.com/ https://arstechnica.com/gadgets/2020/03/wireguard-vpn-makes-it-to-1-0-0-and-into-the-next-linux-kernel/ https://arstechnica.com/gadgets/2020/03/wireguard-vpn-makes-it-to-1-0-0-and-into-the-next-linux-kernel/","title":"Resources"},{"location":"openshift/alertmanager/","text":"Alertmanager AlertManager Resources https://coreos.com/tectonic/docs/latest/tectonic-prometheus-operator/user-guides/configuring-prometheus-alertmanager.html https://medium.com/@abhishekbhardwaj510/alertmanager-integration-in-prometheus-197e03bfabdf Configuring Alertmanager with webhooks and httpbin container with tshark sidecar as a consumer Summary The following describe a setup on OCP 3.11 with: * a container running httpbin and a sidecar running tshark and filtering for incoming http requests and logging them * configuration of Alertmanager so that it sends alerts via webhook to httpbin * loading cluster with high number of pods * analyzing generated alarms Prerequisites Make sure that ocntainers can run as any uid: # oc adm policy add-scc-to-user anyuid -z default scc \"anyuid\" added to: [\"system:serviceaccount:default:default\"] OpenShift httpbin with tshark sidecar The following allows us to see any incoming requests to httpbin but to filter out httpbin's answers. Prerequisites: # oc adm policy add-scc-to-user anyuid -z default scc \"anyuid\" added to: [\"system:serviceaccount:default:default\"] Create file httpbin.yaml : apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app: httpbin-deploymentconfig name: httpbin-service spec: host: httpbin.apps.akaris2.lab.pnq2.cee.redhat.com port: targetPort: 80 to: kind: Service name: httpbin-service weight: 100 wildcardPolicy: None --- apiVersion: v1 kind: Service metadata: name: httpbin-service labels: app: httpbin-deploymentconfig spec: selector: app: httpbin-pod ports: - protocol: TCP port: 80 targetPort: 80 --- apiVersion: v1 kind: DeploymentConfig metadata: name: httpbin-deploymentconfig labels: app: httpbin-deploymentconfig spec: replicas: 1 selector: app: httpbin-pod template: metadata: labels: app: httpbin-pod spec: containers: - name: tshark image: danielguerra/alpine-tshark command: - \"tshark\" - \"-i\" - \"eth0\" - \"-Y\" - \"http\" - \"-V\" - \"dst\" - \"port\" - \"80\" - name: httpbin image: kennethreitz/httpbin imagePullPolicy: Always command: - \"gunicorn\" - \"-b\" - \"0.0.0.0:80\" - \"httpbin:app\" - \"-k\" - \"gevent\" - \"--capture-output\" - \"--error-logfile\" - \"-\" - \"--access-logfile\" - \"-\" - \"--access-logformat\" - \"'%(h)s %(t)s %(r)s %(s)s Host: %({Host}i)s} Header-i: %({Header}i)s Header-o: %({Header}o)s'\" Apply config: oc apply -f httpbin.yaml Get the pod name and loolk at the pod's logs for container tshark : [root@master-0 ~]# oc get pods -l app=httpbin-pod NAME READY STATUS RESTARTS AGE httpbin-deploymentconfig-8-tgmvn 2/2 Running 0 3m [root@master-0 ~]# oc logs httpbin-deploymentconfig-8-tgmvn -c tshark -f Capturing on 'eth0' Frame 4: 535 bytes on wire (4280 bits), 535 bytes captured (4280 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 Encapsulation type: Ethernet (1) Arrival Time: Mar 11, 2020 12:17:13.290037158 UTC [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1583929033.290037158 seconds [Time delta from previous captured frame: 0.000002253 seconds] [Time delta from previous displayed frame: 0.000000000 seconds] [Time since reference or first frame: 36.739477011 seconds] Frame Number: 4 Frame Length: 535 bytes (4280 bits) Capture Length: 535 bytes (4280 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:tcp:http:urlencoded-form] Ethernet II, Src: 7a:9c:fa:d2:07:d8 (7a:9c:fa:d2:07:d8), Dst: 0a:58:0a:80:00:0c (0a:58:0a:80:00:0c) Destination: 0a:58:0a:80:00:0c (0a:58:0a:80:00:0c) Address: 0a:58:0a:80:00:0c (0a:58:0a:80:00:0c) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: 7a:9c:fa:d2:07:d8 (7a:9c:fa:d2:07:d8) Address: 7a:9c:fa:d2:07:d8 (7a:9c:fa:d2:07:d8) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 10.130.0.1, Dst: 10.128.0.12 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x00 (DSCP: CS0, ECN: Not-ECT) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Total Length: 521 Identification: 0xdfdf (57311) Flags: 0x02 (Don't Fragment) 0... .... = Reserved bit: Not set .1.. .... = Don't fragment: Set ..0. .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: TCP (6) Header checksum: 0x4401 [validation disabled] [Header checksum status: Unverified] Source: 10.130.0.1 Destination: 10.128.0.12 Transmission Control Protocol, Src Port: 38288, Dst Port: 80, Seq: 1, Ack: 1, Len: 469 Source Port: 38288 Destination Port: 80 [Stream index: 1] [TCP Segment Len: 469] Sequence number: 1 (relative sequence number) [Next sequence number: 470 (relative sequence number)] Acknowledgment number: 1 (relative ack number) 1000 .... = Header Length: 32 bytes (8) Flags: 0x018 (PSH, ACK) 000. .... .... = Reserved: Not set ...0 .... .... = Nonce: Not set .... 0... .... = Congestion Window Reduced (CWR): Not set .... .0.. .... = ECN-Echo: Not set .... ..0. .... = Urgent: Not set .... ...1 .... = Acknowledgment: Set .... .... 1... = Push: Set .... .... .0.. = Reset: Not set .... .... ..0. = Syn: Not set .... .... ...0 = Fin: Not set [TCP Flags: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7AP\u00b7\u00b7\u00b7] Window size value: 221 [Calculated window size: 28288] [Window size scaling factor: 128] Checksum: 0xd9c6 [unverified] [Checksum Status: Unverified] Urgent pointer: 0 Options: (12 bytes), No-Operation (NOP), No-Operation (NOP), Timestamps TCP Option - No-Operation (NOP) Kind: No-Operation (1) TCP Option - No-Operation (NOP) Kind: No-Operation (1) TCP Option - Timestamps: TSval 44637623, TSecr 44644920 Kind: Time Stamp Option (8) Length: 10 Timestamp value: 44637623 Timestamp echo reply: 44644920 [SEQ/ACK analysis] [iRTT: 0.001410475 seconds] [Bytes in flight: 470] [Bytes sent since last PSH flag: 469] TCP payload (469 bytes) Hypertext Transfer Protocol POST /post HTTP/1.1\\r\\n [Expert Info (Chat/Sequence): POST /post HTTP/1.1\\r\\n] [POST /post HTTP/1.1\\r\\n] [Severity level: Chat] [Group: Sequence] Request Method: POST Request URI: /post Request Version: HTTP/1.1 Configuring Alertmanager to send webhooks to httpbin pod Prerequisites: * https://docs.openshift.com/container-platform/3.11/install_config/prometheus_cluster_monitoring.html In the following, replace myuser with the user who shall log into alertmanager: $ oc adm policy add-cluster-role-to-user cluster-monitoring-view myuser cluster role \"cluster-monitoring-view\" added: \"myuser\" We can use the above to tell alertmanager to use httpbin as its web hook: $ oc get routes -n openshift-monitoring NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD alertmanager-main alertmanager-main-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com alertmanager-main web reencrypt None grafana grafana-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com grafana https reencrypt None prometheus-k8s prometheus-k8s-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com prometheus-k8s web reencrypt None Now, access: https://alertmanager-main-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com The status page will show the current alertmanager configuration. The following Red Hat knowledge base solution shows how to update the alertmanager config: https://access.redhat.com/solutions/3804781 Create file: ~/group_vars/OSEv3.yml : openshift_cluster_monitoring_operator_alertmanager_config: |+ global: resolve_timeout: 2m route: group_wait: 5s group_interval: 10s repeat_interval: 20s receiver: default routes: - match: alertname: DeadMansSwitch repeat_interval: 30s receiver: deadmansswitch - match: alertname: DeadMansSwitch repeat_interval: 30s receiver: wh - match: alertname: '*' repeat_interval: 2m receiver: wh - match: severity: critical receiver: wh - match: severity: warning receiver: wh - match: alertname: KubeAPILatencyHigh receiver: wh receivers: - name: default - name: deadmansswitch - name: wh webhook_configs: - url: \"http://httpbin.apps.akaris2.lab.pnq2.cee.redhat.com/anything\" And run: ansible-playbook -i hosts openshift-ansible/playbooks/openshift-monitoring/config.yml -e=\"openshift_cluster_monitoring_operator_install=true\" Verify: $ oc get secret -n openshift-monitoring alertmanager-main -o yaml | awk '/alertmanager.yaml:/ {print $NF}' | base64 -d global: resolve_timeout: 2m route: group_wait: 5s group_interval: 10s repeat_interval: 20s receiver: default routes: - match: alertname: DeadMansSwitch repeat_interval: 30s receiver: deadmansswitch - match: alertname: DeadMansSwitch repeat_interval: 30s receiver: wh - match: alertname: '*' repeat_interval: 2m receiver: wh - match: severity: critical receiver: wh - match: severity: warning receiver: wh - match: alertname: KubeAPILatencyHigh receiver: wh receivers: - name: default - name: deadmansswitch - name: wh webhook_configs: - url: \"http://httpbin.apps.akaris2.lab.pnq2.cee.redhat.com/anything\" Restart pods: $ oc delete pods --selector=app=alertmanager -n openshift-monitoring pod \"alertmanager-main-0\" deleted pod \"alertmanager-main-1\" deleted pod \"alertmanager-main-2\" deleted And check in the web interface of alertmanager to make sure that the new configuration shows up. Loading the cluster An easy way to generate an alert in a small lab is to trigger alert KubeletTooManyPods . Go to prometheus and check its configuration: alert: KubeletTooManyPods expr: kubelet_running_pod_count{job=\"kubelet\"} > 250 * 0.9 for: 15m labels: severity: warning annotations: message: Kubelet {{ $labels.instance }} is running {{ $value }} Pods, close to the limit of 250. Then, create the following busybox deployment with a number of pods that exceeds this number, e.g.: busybox.yaml : apiVersion: apps/v1 kind: Deployment metadata: name: busybox-deployment labels: app: busybox-deployment spec: replicas: 500 selector: matchLabels: app: busybox-pod template: metadata: labels: app: busybox-pod spec: containers: - name: busybox image: busybox command: - sleep - infinity imagePullPolicy: IfNotPresent oc apply -f busybox.yaml The cluster will need some time to create those pods and it'll take 15 minutes for the alarm to fire. So take a coffee and come back later. Once the alarm fires in prometheus, go to alertmanager and make sure that it shows there, too. Among others, Alertmanager should show: alertname=\"KubeletTooManyPods\" 16:06:32, 2020-03-11 message: Kubelet 10.74.176.204:10250 is running 250 Pods, close to the limit of 250. severity=\"warning\" service=\"kubelet\"prometheus=\"openshift-monitoring/k8s\"namespace=\"kube-system\"job=\"kubelet\"instance=\"10.74.176.204:10250\"endpoint=\"https-metrics\" Now, it's time to go back to the httpbin pod. Monitoring incoming webhook reuests Get the pod name: # oc get pods | grep httpbin httpbin-deploymentconfig-8-8crvh 2/2 Running 0 1h And check the logs of the tshark container which will show a verbose packet capture of HTTP with a destination port of 80 (so we are not capturing the response): # oc logs httpbin-deploymentconfig-8-8crvh -c tshark | tail -n 400 (...) Frame 1708: 5220 bytes on wire (41760 bits), 5220 bytes captured (41760 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 (...) Ethernet II, Src: ... (...), Dst: ... (...) (...) Internet Protocol Version 4, Src: ..., Dst: ... (...) Transmission Control Protocol, Src Port: 41606, Dst Port: 80, Seq: 1, Ack: 1, Len: 5154 (...) Hypertext Transfer Protocol POST /anything HTTP/1.1\\r\\n [Expert Info (Chat/Sequence): POST /anything HTTP/1.1\\r\\n] [POST /anything HTTP/1.1\\r\\n] [Severity level: Chat] [Group: Sequence] Request Method: POST Request URI: /anything Request Version: HTTP/1.1 User-Agent: Alertmanager/0.15.2\\r\\n Content-Length: 4743\\r\\n [Content length: 4743] Content-Type: application/json\\r\\n (...) JavaScript Object Notation: application/json Object Member Key: receiver String value: wh Key: receiver Member Key: status String value: firing Key: status Member Key: alerts Array Object Member Key: status String value: firing Key: status Member Key: labels Object Member Key: alertname String value: KubeDaemonSetRolloutStuck Key: alertname Member Key: cluster String value: openshift.akaris2.lab.pnq2.cee.redhat.com Key: cluster Member Key: daemonset String value: node-exporter Key: daemonset Member Key: endpoint String value: https-main Key: endpoint Member Key: instance String value: ...:8443 Key: instance Member Key: job String value: kube-state-metrics Key: job Member Key: namespace String value: openshift-monitoring Key: namespace Member Key: pod String value: kube-state-metrics-6f4c658bcc-v57b6 Key: pod Member Key: prometheus String value: openshift-monitoring/k8s Key: prometheus Member Key: service String value: kube-state-metrics Key: service Member Key: severity String value: critical Key: severity Key: labels Member Key: annotations Object Member Key: message String value: Only 66.66666666666666% of desired pods scheduled and ready for daemon set openshift-monitoring/node-exporter Key: message Key: annotations Member Key: startsAt String value: 2020-03-11T16:07:40.59085788Z Key: startsAt Member Key: endsAt String value: 0001-01-01T00:00:00Z Key: endsAt Member Key: generatorURL String value [truncated]: https://prometheus-k8s-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com/graph?g0.expr=kube_daemonset_status_number_ready%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22%28openshift-.%2A%7Ckube-.%2A%7C Key: generatorURL Object Member Key: status String value: firing Key: status Member Key: labels Object Member Key: alertname String value: KubeDaemonSetRolloutStuck Key: alertname Member Key: cluster String value: openshift.akaris2.lab.pnq2.cee.redhat.com Key: cluster Member Key: daemonset String value: ovs Key: daemonset Member Key: endpoint String value: https-main Key: endpoint Member Key: instance String value: ...:8443 Key: instance Member Key: job String value: kube-state-metrics Key: job Member Key: namespace String value: openshift-sdn Key: namespace Member Key: pod String value: kube-state-metrics-6f4c658bcc-v57b6 Key: pod Member Key: prometheus String value: openshift-monitoring/k8s Key: prometheus Member Key: service String value: kube-state-metrics Key: service Member Key: severity String value: critical Key: severity Key: labels Member Key: annotations Object Member Key: message String value: Only 66.66666666666666% of desired pods scheduled and ready for daemon set openshift-sdn/ovs Key: message Key: annotations Member Key: startsAt String value: 2020-03-11T16:07:40.59085788Z Key: startsAt Member Key: endsAt String value: 0001-01-01T00:00:00Z Key: endsAt Member Key: generatorURL String value [truncated]: https://prometheus-k8s-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com/graph?g0.expr=kube_daemonset_status_number_ready%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22%28openshift-.%2A%7Ckube-.%2A%7C Key: generatorURL Object Member Key: status String value: firing Key: status Member Key: labels Object Member Key: alertname String value: KubeDaemonSetRolloutStuck Key: alertname Member Key: cluster String value: openshift.akaris2.lab.pnq2.cee.redhat.com Key: cluster Member Key: daemonset String value: sdn Key: daemonset Member Key: endpoint String value: https-main Key: endpoint Member Key: instance String value: ...:8443 Key: instance Member Key: job String value: kube-state-metrics Key: job Member Key: namespace String value: openshift-sdn Key: namespace Member Key: pod String value: kube-state-metrics-6f4c658bcc-v57b6 Key: pod Member Key: prometheus String value: openshift-monitoring/k8s Key: prometheus Member Key: service String value: kube-state-metrics Key: service Member Key: severity String value: critical Key: severity Key: labels Member Key: annotations Object Member Key: message String value: Only 66.66666666666666% of desired pods scheduled and ready for daemon set openshift-sdn/sdn Key: message Key: annotations Member Key: startsAt String value: 2020-03-11T16:07:40.59085788Z Key: startsAt Member Key: endsAt String value: 0001-01-01T00:00:00Z Key: endsAt Member Key: generatorURL String value [truncated]: https://prometheus-k8s-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com/graph?g0.expr=kube_daemonset_status_number_ready%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22%28openshift-.%2A%7Ckube-.%2A%7C Key: generatorURL Object Member Key: status String value: firing Key: status Member Key: labels Object Member Key: alertname String value: KubeDaemonSetRolloutStuck Key: alertname Member Key: cluster String value: openshift.akaris2.lab.pnq2.cee.redhat.com Key: cluster Member Key: daemonset String value: sync Key: daemonset Member Key: endpoint String value: https-main Key: endpoint Member Key: instance String value: ...:8443 Key: instance Member Key: job String value: kube-state-metrics Key: job Member Key: namespace String value: openshift-node Key: namespace Member Key: pod String value: kube-state-metrics-6f4c658bcc-v57b6 Key: pod Member Key: prometheus String value: openshift-monitoring/k8s Key: prometheus Member Key: service String value: kube-state-metrics Key: service Member Key: severity String value: critical Key: severity Key: labels Member Key: annotations Object Member Key: message String value: Only 66.66666666666666% of desired pods scheduled and ready for daemon set openshift-node/sync Key: message Key: annotations Member Key: startsAt String value: 2020-03-11T16:07:40.59085788Z Key: startsAt Member Key: endsAt String value: 0001-01-01T00:00:00Z Key: endsAt Member Key: generatorURL String value [truncated]: https://prometheus-k8s-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com/graph?g0.expr=kube_daemonset_status_number_ready%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22%28openshift-.%2A%7Ckube-.%2A%7C Key: generatorURL Key: alerts Member Key: groupLabels Object Key: groupLabels Member Key: commonLabels Object Member Key: alertname String value: KubeDaemonSetRolloutStuck Key: alertname Member Key: cluster String value: openshift.akaris2.lab.pnq2.cee.redhat.com Key: cluster Member Key: endpoint String value: https-main Key: endpoint Member Key: instance String value: ...:8443 Key: instance Member Key: job String value: kube-state-metrics Key: job Member Key: pod String value: kube-state-metrics-6f4c658bcc-v57b6 Key: pod Member Key: prometheus String value: openshift-monitoring/k8s Key: prometheus Member Key: service String value: kube-state-metrics Key: service Member Key: severity String value: critical Key: severity Key: commonLabels","title":"AlertManager"},{"location":"openshift/alertmanager/#alertmanager","text":"","title":"Alertmanager"},{"location":"openshift/alertmanager/#alertmanager-resources","text":"https://coreos.com/tectonic/docs/latest/tectonic-prometheus-operator/user-guides/configuring-prometheus-alertmanager.html https://medium.com/@abhishekbhardwaj510/alertmanager-integration-in-prometheus-197e03bfabdf","title":"AlertManager Resources"},{"location":"openshift/alertmanager/#configuring-alertmanager-with-webhooks-and-httpbin-container-with-tshark-sidecar-as-a-consumer","text":"","title":"Configuring Alertmanager with webhooks and httpbin container with tshark sidecar as a consumer"},{"location":"openshift/alertmanager/#summary","text":"The following describe a setup on OCP 3.11 with: * a container running httpbin and a sidecar running tshark and filtering for incoming http requests and logging them * configuration of Alertmanager so that it sends alerts via webhook to httpbin * loading cluster with high number of pods * analyzing generated alarms","title":"Summary"},{"location":"openshift/alertmanager/#prerequisites","text":"Make sure that ocntainers can run as any uid: # oc adm policy add-scc-to-user anyuid -z default scc \"anyuid\" added to: [\"system:serviceaccount:default:default\"]","title":"Prerequisites"},{"location":"openshift/alertmanager/#openshift-httpbin-with-tshark-sidecar","text":"The following allows us to see any incoming requests to httpbin but to filter out httpbin's answers. Prerequisites: # oc adm policy add-scc-to-user anyuid -z default scc \"anyuid\" added to: [\"system:serviceaccount:default:default\"] Create file httpbin.yaml : apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app: httpbin-deploymentconfig name: httpbin-service spec: host: httpbin.apps.akaris2.lab.pnq2.cee.redhat.com port: targetPort: 80 to: kind: Service name: httpbin-service weight: 100 wildcardPolicy: None --- apiVersion: v1 kind: Service metadata: name: httpbin-service labels: app: httpbin-deploymentconfig spec: selector: app: httpbin-pod ports: - protocol: TCP port: 80 targetPort: 80 --- apiVersion: v1 kind: DeploymentConfig metadata: name: httpbin-deploymentconfig labels: app: httpbin-deploymentconfig spec: replicas: 1 selector: app: httpbin-pod template: metadata: labels: app: httpbin-pod spec: containers: - name: tshark image: danielguerra/alpine-tshark command: - \"tshark\" - \"-i\" - \"eth0\" - \"-Y\" - \"http\" - \"-V\" - \"dst\" - \"port\" - \"80\" - name: httpbin image: kennethreitz/httpbin imagePullPolicy: Always command: - \"gunicorn\" - \"-b\" - \"0.0.0.0:80\" - \"httpbin:app\" - \"-k\" - \"gevent\" - \"--capture-output\" - \"--error-logfile\" - \"-\" - \"--access-logfile\" - \"-\" - \"--access-logformat\" - \"'%(h)s %(t)s %(r)s %(s)s Host: %({Host}i)s} Header-i: %({Header}i)s Header-o: %({Header}o)s'\" Apply config: oc apply -f httpbin.yaml Get the pod name and loolk at the pod's logs for container tshark : [root@master-0 ~]# oc get pods -l app=httpbin-pod NAME READY STATUS RESTARTS AGE httpbin-deploymentconfig-8-tgmvn 2/2 Running 0 3m [root@master-0 ~]# oc logs httpbin-deploymentconfig-8-tgmvn -c tshark -f Capturing on 'eth0' Frame 4: 535 bytes on wire (4280 bits), 535 bytes captured (4280 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 Encapsulation type: Ethernet (1) Arrival Time: Mar 11, 2020 12:17:13.290037158 UTC [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1583929033.290037158 seconds [Time delta from previous captured frame: 0.000002253 seconds] [Time delta from previous displayed frame: 0.000000000 seconds] [Time since reference or first frame: 36.739477011 seconds] Frame Number: 4 Frame Length: 535 bytes (4280 bits) Capture Length: 535 bytes (4280 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:tcp:http:urlencoded-form] Ethernet II, Src: 7a:9c:fa:d2:07:d8 (7a:9c:fa:d2:07:d8), Dst: 0a:58:0a:80:00:0c (0a:58:0a:80:00:0c) Destination: 0a:58:0a:80:00:0c (0a:58:0a:80:00:0c) Address: 0a:58:0a:80:00:0c (0a:58:0a:80:00:0c) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: 7a:9c:fa:d2:07:d8 (7a:9c:fa:d2:07:d8) Address: 7a:9c:fa:d2:07:d8 (7a:9c:fa:d2:07:d8) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 10.130.0.1, Dst: 10.128.0.12 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x00 (DSCP: CS0, ECN: Not-ECT) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Total Length: 521 Identification: 0xdfdf (57311) Flags: 0x02 (Don't Fragment) 0... .... = Reserved bit: Not set .1.. .... = Don't fragment: Set ..0. .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: TCP (6) Header checksum: 0x4401 [validation disabled] [Header checksum status: Unverified] Source: 10.130.0.1 Destination: 10.128.0.12 Transmission Control Protocol, Src Port: 38288, Dst Port: 80, Seq: 1, Ack: 1, Len: 469 Source Port: 38288 Destination Port: 80 [Stream index: 1] [TCP Segment Len: 469] Sequence number: 1 (relative sequence number) [Next sequence number: 470 (relative sequence number)] Acknowledgment number: 1 (relative ack number) 1000 .... = Header Length: 32 bytes (8) Flags: 0x018 (PSH, ACK) 000. .... .... = Reserved: Not set ...0 .... .... = Nonce: Not set .... 0... .... = Congestion Window Reduced (CWR): Not set .... .0.. .... = ECN-Echo: Not set .... ..0. .... = Urgent: Not set .... ...1 .... = Acknowledgment: Set .... .... 1... = Push: Set .... .... .0.. = Reset: Not set .... .... ..0. = Syn: Not set .... .... ...0 = Fin: Not set [TCP Flags: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7AP\u00b7\u00b7\u00b7] Window size value: 221 [Calculated window size: 28288] [Window size scaling factor: 128] Checksum: 0xd9c6 [unverified] [Checksum Status: Unverified] Urgent pointer: 0 Options: (12 bytes), No-Operation (NOP), No-Operation (NOP), Timestamps TCP Option - No-Operation (NOP) Kind: No-Operation (1) TCP Option - No-Operation (NOP) Kind: No-Operation (1) TCP Option - Timestamps: TSval 44637623, TSecr 44644920 Kind: Time Stamp Option (8) Length: 10 Timestamp value: 44637623 Timestamp echo reply: 44644920 [SEQ/ACK analysis] [iRTT: 0.001410475 seconds] [Bytes in flight: 470] [Bytes sent since last PSH flag: 469] TCP payload (469 bytes) Hypertext Transfer Protocol POST /post HTTP/1.1\\r\\n [Expert Info (Chat/Sequence): POST /post HTTP/1.1\\r\\n] [POST /post HTTP/1.1\\r\\n] [Severity level: Chat] [Group: Sequence] Request Method: POST Request URI: /post Request Version: HTTP/1.1","title":"OpenShift httpbin with tshark sidecar"},{"location":"openshift/alertmanager/#configuring-alertmanager-to-send-webhooks-to-httpbin-pod","text":"Prerequisites: * https://docs.openshift.com/container-platform/3.11/install_config/prometheus_cluster_monitoring.html In the following, replace myuser with the user who shall log into alertmanager: $ oc adm policy add-cluster-role-to-user cluster-monitoring-view myuser cluster role \"cluster-monitoring-view\" added: \"myuser\" We can use the above to tell alertmanager to use httpbin as its web hook: $ oc get routes -n openshift-monitoring NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD alertmanager-main alertmanager-main-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com alertmanager-main web reencrypt None grafana grafana-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com grafana https reencrypt None prometheus-k8s prometheus-k8s-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com prometheus-k8s web reencrypt None Now, access: https://alertmanager-main-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com The status page will show the current alertmanager configuration. The following Red Hat knowledge base solution shows how to update the alertmanager config: https://access.redhat.com/solutions/3804781 Create file: ~/group_vars/OSEv3.yml : openshift_cluster_monitoring_operator_alertmanager_config: |+ global: resolve_timeout: 2m route: group_wait: 5s group_interval: 10s repeat_interval: 20s receiver: default routes: - match: alertname: DeadMansSwitch repeat_interval: 30s receiver: deadmansswitch - match: alertname: DeadMansSwitch repeat_interval: 30s receiver: wh - match: alertname: '*' repeat_interval: 2m receiver: wh - match: severity: critical receiver: wh - match: severity: warning receiver: wh - match: alertname: KubeAPILatencyHigh receiver: wh receivers: - name: default - name: deadmansswitch - name: wh webhook_configs: - url: \"http://httpbin.apps.akaris2.lab.pnq2.cee.redhat.com/anything\" And run: ansible-playbook -i hosts openshift-ansible/playbooks/openshift-monitoring/config.yml -e=\"openshift_cluster_monitoring_operator_install=true\" Verify: $ oc get secret -n openshift-monitoring alertmanager-main -o yaml | awk '/alertmanager.yaml:/ {print $NF}' | base64 -d global: resolve_timeout: 2m route: group_wait: 5s group_interval: 10s repeat_interval: 20s receiver: default routes: - match: alertname: DeadMansSwitch repeat_interval: 30s receiver: deadmansswitch - match: alertname: DeadMansSwitch repeat_interval: 30s receiver: wh - match: alertname: '*' repeat_interval: 2m receiver: wh - match: severity: critical receiver: wh - match: severity: warning receiver: wh - match: alertname: KubeAPILatencyHigh receiver: wh receivers: - name: default - name: deadmansswitch - name: wh webhook_configs: - url: \"http://httpbin.apps.akaris2.lab.pnq2.cee.redhat.com/anything\" Restart pods: $ oc delete pods --selector=app=alertmanager -n openshift-monitoring pod \"alertmanager-main-0\" deleted pod \"alertmanager-main-1\" deleted pod \"alertmanager-main-2\" deleted And check in the web interface of alertmanager to make sure that the new configuration shows up.","title":"Configuring Alertmanager to send webhooks to httpbin pod"},{"location":"openshift/alertmanager/#loading-the-cluster","text":"An easy way to generate an alert in a small lab is to trigger alert KubeletTooManyPods . Go to prometheus and check its configuration: alert: KubeletTooManyPods expr: kubelet_running_pod_count{job=\"kubelet\"} > 250 * 0.9 for: 15m labels: severity: warning annotations: message: Kubelet {{ $labels.instance }} is running {{ $value }} Pods, close to the limit of 250. Then, create the following busybox deployment with a number of pods that exceeds this number, e.g.: busybox.yaml : apiVersion: apps/v1 kind: Deployment metadata: name: busybox-deployment labels: app: busybox-deployment spec: replicas: 500 selector: matchLabels: app: busybox-pod template: metadata: labels: app: busybox-pod spec: containers: - name: busybox image: busybox command: - sleep - infinity imagePullPolicy: IfNotPresent oc apply -f busybox.yaml The cluster will need some time to create those pods and it'll take 15 minutes for the alarm to fire. So take a coffee and come back later. Once the alarm fires in prometheus, go to alertmanager and make sure that it shows there, too. Among others, Alertmanager should show: alertname=\"KubeletTooManyPods\" 16:06:32, 2020-03-11 message: Kubelet 10.74.176.204:10250 is running 250 Pods, close to the limit of 250. severity=\"warning\" service=\"kubelet\"prometheus=\"openshift-monitoring/k8s\"namespace=\"kube-system\"job=\"kubelet\"instance=\"10.74.176.204:10250\"endpoint=\"https-metrics\" Now, it's time to go back to the httpbin pod.","title":"Loading the cluster"},{"location":"openshift/alertmanager/#monitoring-incoming-webhook-reuests","text":"Get the pod name: # oc get pods | grep httpbin httpbin-deploymentconfig-8-8crvh 2/2 Running 0 1h And check the logs of the tshark container which will show a verbose packet capture of HTTP with a destination port of 80 (so we are not capturing the response): # oc logs httpbin-deploymentconfig-8-8crvh -c tshark | tail -n 400 (...) Frame 1708: 5220 bytes on wire (41760 bits), 5220 bytes captured (41760 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 (...) Ethernet II, Src: ... (...), Dst: ... (...) (...) Internet Protocol Version 4, Src: ..., Dst: ... (...) Transmission Control Protocol, Src Port: 41606, Dst Port: 80, Seq: 1, Ack: 1, Len: 5154 (...) Hypertext Transfer Protocol POST /anything HTTP/1.1\\r\\n [Expert Info (Chat/Sequence): POST /anything HTTP/1.1\\r\\n] [POST /anything HTTP/1.1\\r\\n] [Severity level: Chat] [Group: Sequence] Request Method: POST Request URI: /anything Request Version: HTTP/1.1 User-Agent: Alertmanager/0.15.2\\r\\n Content-Length: 4743\\r\\n [Content length: 4743] Content-Type: application/json\\r\\n (...) JavaScript Object Notation: application/json Object Member Key: receiver String value: wh Key: receiver Member Key: status String value: firing Key: status Member Key: alerts Array Object Member Key: status String value: firing Key: status Member Key: labels Object Member Key: alertname String value: KubeDaemonSetRolloutStuck Key: alertname Member Key: cluster String value: openshift.akaris2.lab.pnq2.cee.redhat.com Key: cluster Member Key: daemonset String value: node-exporter Key: daemonset Member Key: endpoint String value: https-main Key: endpoint Member Key: instance String value: ...:8443 Key: instance Member Key: job String value: kube-state-metrics Key: job Member Key: namespace String value: openshift-monitoring Key: namespace Member Key: pod String value: kube-state-metrics-6f4c658bcc-v57b6 Key: pod Member Key: prometheus String value: openshift-monitoring/k8s Key: prometheus Member Key: service String value: kube-state-metrics Key: service Member Key: severity String value: critical Key: severity Key: labels Member Key: annotations Object Member Key: message String value: Only 66.66666666666666% of desired pods scheduled and ready for daemon set openshift-monitoring/node-exporter Key: message Key: annotations Member Key: startsAt String value: 2020-03-11T16:07:40.59085788Z Key: startsAt Member Key: endsAt String value: 0001-01-01T00:00:00Z Key: endsAt Member Key: generatorURL String value [truncated]: https://prometheus-k8s-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com/graph?g0.expr=kube_daemonset_status_number_ready%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22%28openshift-.%2A%7Ckube-.%2A%7C Key: generatorURL Object Member Key: status String value: firing Key: status Member Key: labels Object Member Key: alertname String value: KubeDaemonSetRolloutStuck Key: alertname Member Key: cluster String value: openshift.akaris2.lab.pnq2.cee.redhat.com Key: cluster Member Key: daemonset String value: ovs Key: daemonset Member Key: endpoint String value: https-main Key: endpoint Member Key: instance String value: ...:8443 Key: instance Member Key: job String value: kube-state-metrics Key: job Member Key: namespace String value: openshift-sdn Key: namespace Member Key: pod String value: kube-state-metrics-6f4c658bcc-v57b6 Key: pod Member Key: prometheus String value: openshift-monitoring/k8s Key: prometheus Member Key: service String value: kube-state-metrics Key: service Member Key: severity String value: critical Key: severity Key: labels Member Key: annotations Object Member Key: message String value: Only 66.66666666666666% of desired pods scheduled and ready for daemon set openshift-sdn/ovs Key: message Key: annotations Member Key: startsAt String value: 2020-03-11T16:07:40.59085788Z Key: startsAt Member Key: endsAt String value: 0001-01-01T00:00:00Z Key: endsAt Member Key: generatorURL String value [truncated]: https://prometheus-k8s-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com/graph?g0.expr=kube_daemonset_status_number_ready%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22%28openshift-.%2A%7Ckube-.%2A%7C Key: generatorURL Object Member Key: status String value: firing Key: status Member Key: labels Object Member Key: alertname String value: KubeDaemonSetRolloutStuck Key: alertname Member Key: cluster String value: openshift.akaris2.lab.pnq2.cee.redhat.com Key: cluster Member Key: daemonset String value: sdn Key: daemonset Member Key: endpoint String value: https-main Key: endpoint Member Key: instance String value: ...:8443 Key: instance Member Key: job String value: kube-state-metrics Key: job Member Key: namespace String value: openshift-sdn Key: namespace Member Key: pod String value: kube-state-metrics-6f4c658bcc-v57b6 Key: pod Member Key: prometheus String value: openshift-monitoring/k8s Key: prometheus Member Key: service String value: kube-state-metrics Key: service Member Key: severity String value: critical Key: severity Key: labels Member Key: annotations Object Member Key: message String value: Only 66.66666666666666% of desired pods scheduled and ready for daemon set openshift-sdn/sdn Key: message Key: annotations Member Key: startsAt String value: 2020-03-11T16:07:40.59085788Z Key: startsAt Member Key: endsAt String value: 0001-01-01T00:00:00Z Key: endsAt Member Key: generatorURL String value [truncated]: https://prometheus-k8s-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com/graph?g0.expr=kube_daemonset_status_number_ready%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22%28openshift-.%2A%7Ckube-.%2A%7C Key: generatorURL Object Member Key: status String value: firing Key: status Member Key: labels Object Member Key: alertname String value: KubeDaemonSetRolloutStuck Key: alertname Member Key: cluster String value: openshift.akaris2.lab.pnq2.cee.redhat.com Key: cluster Member Key: daemonset String value: sync Key: daemonset Member Key: endpoint String value: https-main Key: endpoint Member Key: instance String value: ...:8443 Key: instance Member Key: job String value: kube-state-metrics Key: job Member Key: namespace String value: openshift-node Key: namespace Member Key: pod String value: kube-state-metrics-6f4c658bcc-v57b6 Key: pod Member Key: prometheus String value: openshift-monitoring/k8s Key: prometheus Member Key: service String value: kube-state-metrics Key: service Member Key: severity String value: critical Key: severity Key: labels Member Key: annotations Object Member Key: message String value: Only 66.66666666666666% of desired pods scheduled and ready for daemon set openshift-node/sync Key: message Key: annotations Member Key: startsAt String value: 2020-03-11T16:07:40.59085788Z Key: startsAt Member Key: endsAt String value: 0001-01-01T00:00:00Z Key: endsAt Member Key: generatorURL String value [truncated]: https://prometheus-k8s-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com/graph?g0.expr=kube_daemonset_status_number_ready%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22%28openshift-.%2A%7Ckube-.%2A%7C Key: generatorURL Key: alerts Member Key: groupLabels Object Key: groupLabels Member Key: commonLabels Object Member Key: alertname String value: KubeDaemonSetRolloutStuck Key: alertname Member Key: cluster String value: openshift.akaris2.lab.pnq2.cee.redhat.com Key: cluster Member Key: endpoint String value: https-main Key: endpoint Member Key: instance String value: ...:8443 Key: instance Member Key: job String value: kube-state-metrics Key: job Member Key: pod String value: kube-state-metrics-6f4c658bcc-v57b6 Key: pod Member Key: prometheus String value: openshift-monitoring/k8s Key: prometheus Member Key: service String value: kube-state-metrics Key: service Member Key: severity String value: critical Key: severity Key: commonLabels","title":"Monitoring incoming webhook reuests"},{"location":"openshift/cpu-manager-with-custom-machine-config-pool/","text":"CPU manager with custom MachineConfigPool in OCP 4.x How to apply the CPU manager to only a subset of worker nodes Create a custom MachineConfigPool named worker-cpu-manager . worker-cpu-manager.yaml : apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfigPool metadata: name: worker-cpu-manager labels: custom-kubelet: cpumanager-enabled spec: machineConfigSelector: matchExpressions: - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,worker-cpu-manager]} nodeSelector: matchLabels: node-role.kubernetes.io/worker-cpu-manager: \"\" paused: false Apply the pool: oc apply -f worker-cpu-manager.yaml Create the cpumanager-kubelet.yaml : apiVersion: machineconfiguration.openshift.io/v1 kind: KubeletConfig metadata: name: cpumanager-enabled spec: machineConfigPoolSelector: matchLabels: custom-kubelet: cpumanager-enabled kubeletConfig: cpuManagerPolicy: static cpuManagerReconcilePeriod: 5s oc apply -f cpumanager-kubelet.yaml Change worker node to worker-cpu-manager role: oc label node <node> node-role.kubernetes.io/worker-cpu-manager= Verify: # oc get nodes NAME STATUS ROLES AGE VERSION openshift-master-0.example.com Ready master 4h3m v1.17.1+1aa1c48 openshift-master-1.example.com Ready master 4h2m v1.17.1+1aa1c48 openshift-master-2.example.com Ready master 3h56m v1.17.1+1aa1c48 openshift-worker-0.example.com Ready worker 18m v1.17.1+1aa1c48 openshift-worker-1.example.com Ready worker,worker-cpu-manager 18m v1.17.1+1aa1c48 oc get machineconfig (...) rendered-worker-cpu-manager-bc48d7bf24df726f468b357482032845 8af4f709c4ba9c0afff3408ecc99c8fce61dd314 2.2.0 87s rendered-worker-cpu-manager-ca0a09ddea41402490e1c39a138cd44e 8af4f709c4ba9c0afff3408ecc99c8fce61dd314 2.2.0 18m [root@openshift-jumpserver-0 cpuman]# diff <(oc get machineconfig rendered-worker-cpu-manager-bc48d7bf24df726f468b357482032845 -o yaml) <(oc get machineconfig rendered-worker-cpu-manager-ca0a09ddea41402490e1c39a138cd44e -o yaml) 6c6 < creationTimestamp: \"2020-06-26T16:40:44Z\" --- > creationTimestamp: \"2020-06-26T16:24:01Z\" 8c8 < name: rendered-worker-cpu-manager-bc48d7bf24df726f468b357482032845 --- > name: rendered-worker-cpu-manager-ca0a09ddea41402490e1c39a138cd44e 16,18c16,18 < resourceVersion: \"451269\" < selfLink: /apis/machineconfiguration.openshift.io/v1/machineconfigs/rendered-worker-cpu-manager-bc48d7bf24df726f468b357482032845 < uid: bf950ef8-7270-4699-a113-dfe6c6c1d4fb --- > resourceVersion: \"445466\" > selfLink: /apis/machineconfiguration.openshift.io/v1/machineconfigs/rendered-worker-cpu-manager-ca0a09ddea41402490e1c39a138cd44e > uid: 74acdcd1-d195-48a5-8689-cecca137605c 163,168d162 < verification: {} < filesystem: root < mode: 420 < path: /etc/kubernetes/kubelet.conf < - contents: < source: data:text/plain,%7B%odcy%2(...) Compare unmodified node to node with CPU manager: [root@openshift-jumpserver-0 cpuman]# oc debug node/openshift-worker-0.example.com Starting pod/openshift-worker-0examplecom-debug ... To use host binaries, run `chroot /host` Pod IP: 192.168.123.215 If you don't see a command prompt, try pressing enter. sh-4.2# cat /host/etc/kubernetes/kubelet.conf | grep cpuManager sh-4.2# exit exit Removing debug pod ... [root@openshift-jumpserver-0 cpuman]# oc debug node/openshift-worker-1.example.com Starting pod/openshift-worker-1examplecom-debug ... To use host binaries, run `chroot /host` Pod IP: 192.168.123.204 If you don't see a command prompt, try pressing enter. sh-4.2# cat /host/etc/kubernetes/kubelet.conf | grep cpuManager {\"kind\":\"KubeletConfiguration\",\"apiVersion\":\"kubelet.config.k8s.io/v1beta1\",\"staticPodPath\":\"/etc/kubernetes/manifests\",\"syncFrequency\":\"0s\",\"fileCheckFrequency\":\"0s\",\"httpCheckFrequency\":\"0s\",\"rotateCertificates\":true,\"serverTLSBootstrap\":true,\"authentication\":{\"x509\":{\"clientCAFile\":\"/etc/kubernetes/kubelet-ca.crt\"},\"webhook\":{\"cacheTTL\":\"0s\"},\"anonymous\":{\"enabled\":false}},\"authorization\":{\"webhook\":{\"cacheAuthorizedTTL\":\"0s\",\"cacheUnauthorizedTTL\":\"0s\"}},\"clusterDomain\":\"cluster.local\",\"clusterDNS\":[\"172.30.0.10\"],\"streamingConnectionIdleTimeout\":\"0s\",\"nodeStatusUpdateFrequency\":\"0s\",\"nodeStatusReportFrequency\":\"0s\",\"imageMinimumGCAge\":\"0s\",\"volumeStatsAggPeriod\":\"0s\",\"systemCgroups\":\"/system.slice\",\"cgroupRoot\":\"/\",\"cgroupDriver\":\"systemd\",\"cpuManagerPolicy\":\"static\",\"cpuManagerReconcilePeriod\":\"5s\",\"runtimeRequestTimeout\":\"0s\",\"maxPods\":250,\"kubeAPIQPS\":50,\"kubeAPIBurst\":100,\"serializeImagePulls\":false,\"evictionPressureTransitionPeriod\":\"0s\",\"featureGates\":{\"LegacyNodeRoleBehavior\":false,\"NodeDisruptionExclusion\":true,\"RotateKubeletServerCertificate\":true,\"SCTPSupport\":true,\"ServiceNodeExclusion\":true,\"SupportPodPidsLimit\":true},\"containerLogMaxSize\":\"50Mi\",\"systemReserved\":{\"cpu\":\"500m\",\"ephemeral-storage\":\"1Gi\",\"memory\":\"1Gi\"}} sh-4.2# exit exit Removing debug pod ... Spawn 2 pods on the same worker, one with and one without CPU manager enabled: [root@openshift-jumpserver-0 cpuman]# cat cpumanager-pod.yaml apiVersion: v1 kind: Pod metadata: name: cpumanager spec: containers: - name: cpumanager image: gcr.io/google_containers/pause-amd64:3.0 resources: requests: cpu: 1 memory: \"1G\" limits: cpu: 1 memory: \"1G\" nodeSelector: cpumanager: \"true\" [root@openshift-jumpserver-0 cpuman]# cat non-cpumanager-pod.yaml apiVersion: v1 kind: Pod metadata: name: noncpumanager spec: containers: - name: cpumanager image: gcr.io/google_containers/pause-amd64:3.0 [root@openshift-jumpserver-0 cpuman]# oc get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES cpumanager 1/1 Running 0 11m 172.24.2.4 openshift-worker-1.example.com <none> <none> noncpumanager 1/1 Running 0 8m10s 172.24.2.5 openshift-worker-1.example.com <none> <none> Verify CPU pinning for the pods' CPUs: [root@openshift-jumpserver-0 cpuman]# oc debug node/openshift-worker-1.example.com Starting pod/openshift-worker-1examplecom-debug ... To use host binaries, run `chroot /host` systemctlPod IP: 192.168.123.204 If you don't see a command prompt, try pressing enter. sh-4.2# chroot /host sh-4.4# ps aux | grep pause root 39072 0.0 0.0 1028 4 ? Ss 16:55 0:00 /pause root 47750 0.0 0.0 1028 4 ? Ss 16:58 0:00 /pause root 64183 0.0 0.0 9180 960 ? S+ 17:03 0:00 grep pause sh-4.4# cat /proc/39072/status | grep -i cpu Cpus_allowed: 00,00100000 Cpus_allowed_list: 20 sh-4.4# cat /proc/47750/status | grep -i cpu Cpus_allowed: ff,ffefffff Cpus_allowed_list: 0-19,21-39 Alternatively: sh-4.4# systemctl status 39072 \u25cf crio-ff43bfb551a274eb9e9040510753db6271ef27c13c203fe69e87aad5f7d49f17.scope - libcontainer container ff43bfb551a274eb9e9040510753db6271ef27c13c203fe69e87aad5f7d49f17 Loaded: loaded (/run/systemd/transient/crio-ff43bfb551a274eb9e9040510753db6271ef27c13c203fe69e87aad5f7d49f17.scope; transient) Transient: yes Active: active (running) since Fri 2020-06-26 16:55:34 UTC; 15min ago Tasks: 1 (limit: 1024) Memory: 1.4M (limit: 953.6M) CPU: 33ms CGroup: /kubepods.slice/kubepods-podc405f7cf_b2d1_49c4_bd4c_09f01a0b8e2d.slice/crio-ff43bfb551a274eb9e9040510753db6271ef27c13c203fe69e87aad5f7d49f17.scope \u2514\u250039072 /pause Jun 26 16:55:34 openshift-worker-1.example.com systemd[1]: Started libcontainer container ff43bfb551a274eb9e9040510753db6271ef27c13c203fe69e87aad5f7d49f17. sh-4.4# systemctl status 47750 \u25cf crio-473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260.scope - libcontainer container 473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260 Loaded: loaded (/run/systemd/transient/crio-473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260.scope; transient) Transient: yes Active: active (running) since Fri 2020-06-26 16:58:32 UTC; 13min ago Tasks: 1 (limit: 1024) Memory: 1.0M CPU: 35ms CGroup: /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod3b4059d0_8030_461a_b192_b5820f6c1119.slice/crio-473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260.scope \u2514\u250047750 /pause Jun 26 16:58:32 openshift-worker-1.example.com systemd[1]: Started libcontainer container 473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260. sh-4.4# cat /sys/fs/cgroup/ blkio/ cpu,cpuacct/ cpuset/ freezer/ memory/ net_cls,net_prio/ perf_event/ rdma/ cpu/ cpuacct/ devices/ hugetlb/ net_cls/ net_prio/ pids/ systemd/ sh-4.4# cat /sys/fs/cgroup/cpuset//kubepods.slice/kubepods-podc405f7cf_b2d1_49c4_bd4c_09f01a0b8e2d.slice/crio-ff43bfb551a274eb9e9040510753db6271ef27c13c203fe69e87aad5f7d49f17.scope/cpuset.cpus 20 sh-4.4# cat /sys/fs/cgroup/cpuset//kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod3b4059d0_8030_461a_b192_b5820f6c1119.slice/crio-473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260.scope/cpuset.pus 0-19,21-39 Exploring resource limits Let's spawn more pods than the hypervisor has CPUs: [root@openshift-jumpserver-0 cpuman]# cat cpumanager-pod-generated-name.yaml apiVersion: v1 kind: Pod metadata: generateName: cpumanager- spec: containers: - name: cpumanager image: gcr.io/google_containers/pause-amd64:3.0 resources: requests: cpu: 1 memory: \"1G\" limits: cpu: 1 memory: \"1G\" nodeSelector: cpumanager: \"true\" for i in {0..45}; do oc create -f cpumanager-pod-generated-name.yaml ; done [root@openshift-jumpserver-0 cpuman]# oc get pods | grep Pending cpumanager-8rtpt 0/1 Pending 0 4m23s cpumanager-bw848 0/1 Pending 0 4m24s cpumanager-cm2st 0/1 Pending 0 4m24s cpumanager-cwj82 0/1 Pending 0 4m23s cpumanager-r5krw 0/1 Pending 0 4m24s [root@openshift-jumpserver-0 cpuman]# oc describe node openshift-worker-1.example.com Name: openshift-worker-1.example.com Roles: worker-cpu-manager Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux cpumanager=true kubernetes.io/arch=amd64 kubernetes.io/hostname=openshift-worker-1.example.com kubernetes.io/os=linux node-role.kubernetes.io/worker-cpu-manager= node.openshift.io/os_id=rhcos Annotations: k8s.ovn.org/l3-gateway-config: {\"default\":{\"mode\":\"local\",\"interface-id\":\"br-local_openshift-worker-1.example.com\",\"mac-address\":\"8e:c7:71:4d:0f:49\",\"ip-addresses\":[\"169... k8s.ovn.org/node-chassis-id: 5a8880a6-5b50-4be5-9d84-f195bbc306a2 k8s.ovn.org/node-join-subnets: {\"default\":\"100.64.3.0/29\"} k8s.ovn.org/node-mgmt-port-mac-address: 9a:b7:cd:ab:bf:b9 k8s.ovn.org/node-subnets: {\"default\":\"172.24.2.0/23\"} machineconfiguration.openshift.io/currentConfig: rendered-worker-cpu-manager-bc48d7bf24df726f468b357482032845 machineconfiguration.openshift.io/desiredConfig: rendered-worker-cpu-manager-bc48d7bf24df726f468b357482032845 machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state: Done volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Thu, 25 Jun 2020 12:03:55 -0400 Taints: <none> Unschedulable: false Lease: HolderIdentity: openshift-worker-1.example.com AcquireTime: <unset> RenewTime: Fri, 26 Jun 2020 14:14:05 -0400 Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Fri, 26 Jun 2020 14:10:56 -0400 Fri, 26 Jun 2020 12:43:22 -0400 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Fri, 26 Jun 2020 14:10:56 -0400 Fri, 26 Jun 2020 12:43:22 -0400 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Fri, 26 Jun 2020 14:10:56 -0400 Fri, 26 Jun 2020 12:43:22 -0400 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Fri, 26 Jun 2020 14:10:56 -0400 Fri, 26 Jun 2020 12:43:33 -0400 KubeletReady kubelet is posting ready status Addresses: InternalIP: 192.168.123.204 Hostname: openshift-worker-1.example.com Capacity: cpu: 40 ephemeral-storage: 584946668Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 131924236Ki pods: 250 Allocatable: cpu: 39500m ephemeral-storage: 538013106513 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 130773260Ki pods: 250 System Info: Machine ID: 21668e85e1264ac78ea115b2fe79408e System UUID: 4c4c4544-004b-5a10-8050-cac04f484832 Boot ID: f20ee17e-226f-4f34-b9ce-965043215c2d Kernel Version: 4.18.0-147.8.1.el8_1.x86_64 OS Image: Red Hat Enterprise Linux CoreOS 44.81.202005250830-0 (Ootpa) Operating System: linux Architecture: amd64 Container Runtime Version: cri-o://1.17.4-12.dev.rhaos4.4.git2be4d9c.el8 Kubelet Version: v1.17.1 Kube-Proxy Version: v1.17.1 Non-terminated Pods: (49 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits AGE --------- ---- ------------ ---------- --------------- ------------- --- default cpumanager 1 (2%) 1 (2%) 1G (0%) 1G (0%) 78m default cpumanager-2548b 1 (2%) 1 (2%) 1G (0%) 1G (0%) 7m41s default cpumanager-44nxg 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m29s default cpumanager-5zx5v 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m30s default cpumanager-72spc 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m28s default cpumanager-9754w 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m29s default cpumanager-cff7t 1 (2%) 1 (2%) 1G (0%) 1G (0%) 8m6s default cpumanager-djhz9 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m27s default cpumanager-dl7lf 1 (2%) 1 (2%) 1G (0%) 1G (0%) 7m42s default cpumanager-dnz7k 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m28s default cpumanager-drhf8 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m27s default cpumanager-ff7h5 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m30s default cpumanager-fh77s 1 (2%) 1 (2%) 1G (0%) 1G (0%) 7m40s default cpumanager-fsptg 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m30s default cpumanager-h74fq 1 (2%) 1 (2%) 1G (0%) 1G (0%) 7m38s default cpumanager-hb72w 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m28s default cpumanager-hbhz2 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m29s default cpumanager-j7smp 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m28s default cpumanager-jlwwn 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m30s default cpumanager-jmdfk 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m30s default cpumanager-kvfkp 1 (2%) 1 (2%) 1G (0%) 1G (0%) 7m45s default cpumanager-kx4ft 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m28s default cpumanager-kz6zg 1 (2%) 1 (2%) 1G (0%) 1G (0%) 7m43s default cpumanager-m4qrn 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m31s default cpumanager-mzqqs 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m30s default cpumanager-n2tt5 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m28s default cpumanager-n79js 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m31s default cpumanager-qwjkf 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m27s default cpumanager-sn658 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m29s default cpumanager-tc7q6 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m29s default cpumanager-vss6b 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m29s default cpumanager-w846n 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m31s default cpumanager-wchfq 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m30s default cpumanager-whwxr 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m28s default cpumanager-wkhw8 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m31s default cpumanager-wtr5z 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m31s default cpumanager-z7zgl 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m28s default cpumanager-zckx8 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m30s default cpumanager-zsnwv 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m27s default noncpumanager 0 (0%) 0 (0%) 0 (0%) 0 (0%) 75m openshift-cluster-node-tuning-operator tuned-tkhrp 10m (0%) 0 (0%) 50Mi (0%) 0 (0%) 26h openshift-dns dns-default-q8hww 110m (0%) 0 (0%) 70Mi (0%) 512Mi (0%) 26h openshift-image-registry node-ca-sfjgq 10m (0%) 0 (0%) 10Mi (0%) 0 (0%) 26h openshift-machine-config-operator machine-config-daemon-8nw2s 40m (0%) 0 (0%) 100Mi (0%) 0 (0%) 26h openshift-marketplace certified-operators-74d989c4dd-w6l7f 10m (0%) 0 (0%) 100Mi (0%) 0 (0%) 50m openshift-monitoring node-exporter-nlgr5 9m (0%) 0 (0%) 210Mi (0%) 0 (0%) 26h openshift-multus multus-w2sgc 10m (0%) 0 (0%) 150Mi (0%) 0 (0%) 26h openshift-ovn-kubernetes ovnkube-node-pg57x 200m (0%) 0 (0%) 600Mi (0%) 0 (0%) 26h openshift-ovn-kubernetes ovs-node-ktvtm 100m (0%) 0 (0%) 300Mi (0%) 0 (0%) 26h Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 39499m (99%) 39 (98%) memory 40667235840 (30%) 39536870912 (29%) ephemeral-storage 0 (0%) 0 (0%) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal NodeNotSchedulable 110m kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeNotSchedulable Normal Starting 106m kubelet, openshift-worker-1.example.com Starting kubelet. Normal NodeHasSufficientMemory 106m (x2 over 106m) kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeHasSufficientMemory Normal NodeHasNoDiskPressure 106m (x2 over 106m) kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeHasNoDiskPressure Normal NodeHasSufficientPID 106m (x2 over 106m) kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeHasSufficientPID Warning Rebooted 106m kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com has been rebooted, boot id: 7df19e71-4ebc-4a66-94e8-1e475d11e095 Normal NodeNotReady 106m kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeNotReady Normal NodeNotSchedulable 106m kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeNotSchedulable Normal NodeAllocatableEnforced 106m kubelet, openshift-worker-1.example.com Updated Node Allocatable limit across pods Normal NodeReady 106m kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeReady Normal NodeSchedulable 100m kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeSchedulable Normal Starting 90m kubelet, openshift-worker-1.example.com Starting kubelet. Normal NodeHasSufficientMemory 90m (x2 over 90m) kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeHasSufficientMemory Normal NodeHasNoDiskPressure 90m (x2 over 90m) kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeHasNoDiskPressure Normal NodeHasSufficientPID 90m (x2 over 90m) kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeHasSufficientPID Warning Rebooted 90m kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com has been rebooted, boot id: f20ee17e-226f-4f34-b9ce-965043215c2d Normal NodeNotReady 90m kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeNotReady Normal NodeNotSchedulable 90m kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeNotSchedulable Normal NodeAllocatableEnforced 90m kubelet, openshift-worker-1.example.com Updated Node Allocatable limit across pods Normal NodeReady 90m kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeReady Normal NodeSchedulable 85m kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeSchedulable Verify processes and cgroup limits on the worker node: [root@openshift-jumpserver-0 cpuman]# oc debug node/openshift-worker-1.example.com Starting pod/openshift-worker-1examplecom-debug ... To use host binaries, run `chroot /host` Pod IP: 192.168.123.204 If you don't see a command prompt, try pressing enter. sh-4.2# chroot /host sh-4.4# ps aux | grep pause root 39072 0.0 0.0 1028 4 ? Ss 16:55 0:00 /pause root 47750 0.0 0.0 1028 4 ? Ss 16:58 0:00 /pause root 274649 0.0 0.0 1028 4 ? Ss 18:06 0:00 /pause root 276349 0.0 0.0 1028 4 ? Ss 18:06 0:00 /pause root 276526 0.0 0.0 1028 4 ? Ss 18:06 0:00 /pause root 276685 0.0 0.0 1028 4 ? Ss 18:06 0:00 /pause root 276824 0.0 0.0 1028 4 ? Ss 18:06 0:00 /pause root 276988 0.0 0.0 1028 4 ? Ss 18:06 0:00 /pause root 277279 0.0 0.0 1028 4 ? Ss 18:06 0:00 /pause root 291552 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 291674 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 291783 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 291900 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 292072 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 292145 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 292191 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 292638 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 292639 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 292693 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 292941 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 292949 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 292971 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 292986 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 293300 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 293422 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 293528 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 293615 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 293678 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 293823 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 293909 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 293951 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 293965 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 293983 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 294087 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 294164 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 294184 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 294239 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 294330 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 294352 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 294426 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 301832 0.0 0.0 9180 964 ? R+ 18:11 0:00 grep pause sh-4.4# systemctl status 47750 \u25cf crio-473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260.scope - libcontainer container 473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260 Loaded: loaded (/run/systemd/transient/crio-473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260.scope; transient) Transient: yes Active: active (running) since Fri 2020-06-26 16:58:32 UTC; 1h 12min ago Tasks: 1 (limit: 1024) Memory: 1.0M CPU: 35ms CGroup: /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod3b4059d0_8030_461a_b192_b5820f6c1119.slice/crio-473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260.scope \u2514\u250047750 /pause Jun 26 16:58:32 openshift-worker-1.example.com systemd[1]: Started libcontainer container 473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260. sh-4.4# cat /sys/fs/cgroup/cpuset/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod3b4059d0_8030_461a_b192_b5820f6c1119.slice/crio-473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260.scope/cpuset.cpus 0 sh-4.4# exit exit sh-4.2# exit exit Removing debug pod ... Default host limits: sh-4.4# cat /etc/kubernetes/kubelet.conf | jq '.systemReserved' { \"cpu\": \"500m\", \"ephemeral-storage\": \"1Gi\", \"memory\": \"1Gi\" } Applying custom limits Let's now reserve 10 CPUs for the host. cpumanager-kubeletconfig.yaml : apiVersion: machineconfiguration.openshift.io/v1 kind: KubeletConfig metadata: name: cpumanager-enabled spec: machineConfigPoolSelector: matchLabels: custom-kubelet: cpumanager-enabled kubeletConfig: cpuManagerPolicy: static cpuManagerReconcilePeriod: 5s systemReserved: cpu: \"10\" memory: \"1Gi\" ephemeral-storage: \"10Gi\" [root@openshift-jumpserver-0 cpuman]# oc apply -f cpumanager-kubeletconfig.yaml kubeletconfig.machineconfiguration.openshift.io/cpumanager-enabled configured [root@openshift-jumpserver-0 cpuman]# oc get -o yaml -f cpumanager-kubeletconfig.yaml apiVersion: machineconfiguration.openshift.io/v1 kind: KubeletConfig metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"machineconfiguration.openshift.io/v1\",\"kind\":\"KubeletConfig\",\"metadata\":{\"annotations\":{},\"name\":\"cpumanager-enabled\"},\"spec\":{\"kubeletConfig\":{\"cpuManagerPolicy\":\"static\",\"cpuManagerReconcilePeriod\":\"5s\",\"systemReserved\":{\"cpu\":\"10\",\"ephemeral-storage\":\"10Gi\",\"memory\":\"1Gi\"}},\"machineConfigPoolSelector\":{\"matchLabels\":{\"custom-kubelet\":\"cpumanager-enabled\"}}}} creationTimestamp: \"2020-06-26T18:29:07Z\" finalizers: - b5ac419d-573d-428c-adc1-f7b5bedb27e3 - 21fa7ce4-49fa-470d-b4ed-f0972dbdd040 generation: 2 name: cpumanager-enabled resourceVersion: \"484854\" selfLink: /apis/machineconfiguration.openshift.io/v1/kubeletconfigs/cpumanager-enabled uid: 75d54d68-7b96-4b9e-8c9a-db3820b2629b spec: kubeletConfig: cpuManagerPolicy: static cpuManagerReconcilePeriod: 5s systemReserved: cpu: \"10\" ephemeral-storage: 10Gi memory: 1Gi machineConfigPoolSelector: matchLabels: custom-kubelet: cpumanager-enabled status: conditions: - lastTransitionTime: \"2020-06-26T18:29:07Z\" message: Success status: \"True\" type: Success - lastTransitionTime: \"2020-06-26T18:33:14Z\" message: Success status: \"True\" type: Success [root@openshift-jumpserver-0 cpuman]# oc get nodes NAME STATUS ROLES AGE VERSION openshift-master-0.example.com Ready master 27h v1.17.1 openshift-master-1.example.com Ready master 27h v1.17.1 openshift-master-2.example.com Ready master 27h v1.17.1 openshift-worker-0.example.com Ready worker 26h v1.17.1 openshift-worker-1.example.com Ready,SchedulingDisabled worker-cpu-manager 26h v1.17.1 [root@openshift-jumpserver-0 cpuman]# oc get machineconfig | grep worker-cpu-manager 99-worker-cpu-manager-ab32b145-ada3-4f96-adf9-1fb8388ba183-kubelet 8af4f709c4ba9c0afff3408ecc99c8fce61dd314 2.2.0 4m42s rendered-worker-cpu-manager-171f3675f101028b058ffe27d6344bb2 8af4f709c4ba9c0afff3408ecc99c8fce61dd314 2.2.0 30s rendered-worker-cpu-manager-bc48d7bf24df726f468b357482032845 8af4f709c4ba9c0afff3408ecc99c8fce61dd314 2.2.0 113m rendered-worker-cpu-manager-ca0a09ddea41402490e1c39a138cd44e 8af4f709c4ba9c0afff3408ecc99c8fce61dd314 2.2.0 129m [root@openshift-jumpserver-0 cpuman]# for i in {0..35}; do oc create -f cpumanager-pod-generated-name.yaml ; done pod/cpumanager-msf8c created pod/cpumanager-2kwws created pod/cpumanager-7mx5k created pod/cpumanager-bq7sf created pod/cpumanager-86kkl created pod/cpumanager-z7h5f created pod/cpumanager-c5pd4 created pod/cpumanager-sr6jt created pod/cpumanager-dp7ck created pod/cpumanager-n4ffv created pod/cpumanager-5q77h created pod/cpumanager-zxbj2 created pod/cpumanager-7f8fw created pod/cpumanager-j5kdw created pod/cpumanager-fhfmw created pod/cpumanager-hz5r8 created pod/cpumanager-44xst created pod/cpumanager-r6h98 created pod/cpumanager-mjsfq created pod/cpumanager-dh897 created pod/cpumanager-s445q created pod/cpumanager-brwcz created pod/cpumanager-mnbrg created pod/cpumanager-56vmv created pod/cpumanager-kt5fg created pod/cpumanager-fscpd created pod/cpumanager-hsjzn created pod/cpumanager-cvsth created pod/cpumanager-j7wlv created pod/cpumanager-z87tp created pod/cpumanager-xggjm created pod/cpumanager-jg646 created pod/cpumanager-ml7kg created pod/cpumanager-qwhjm created pod/cpumanager-m6jw8 created pod/cpumanager-f8qsh created [root@openshift-jumpserver-0 cpuman]# oc get pods | grep Runn cpumanager-2kwws 1/1 Running 0 15s cpumanager-44xst 1/1 Running 0 13s cpumanager-56vmv 1/1 Running 0 12s cpumanager-5q77h 1/1 Running 0 14s cpumanager-7f8fw 1/1 Running 0 14s cpumanager-7mx5k 1/1 Running 0 15s cpumanager-86kkl 1/1 Running 0 15s cpumanager-8rtpt 1/1 Running 0 32m cpumanager-bq7sf 1/1 Running 0 15s cpumanager-brwcz 1/1 Running 0 13s cpumanager-bw848 1/1 Running 0 32m cpumanager-c5pd4 1/1 Running 0 14s cpumanager-cm2st 1/1 Running 0 32m cpumanager-cwj82 1/1 Running 0 32m cpumanager-dh897 1/1 Running 0 13s cpumanager-dp7ck 1/1 Running 0 14s cpumanager-fhfmw 1/1 Running 0 13s cpumanager-hz5r8 1/1 Running 0 13s cpumanager-j5kdw 1/1 Running 0 14s cpumanager-mjsfq 1/1 Running 0 13s cpumanager-mnbrg 1/1 Running 0 12s cpumanager-msf8c 1/1 Running 0 15s cpumanager-n4ffv 1/1 Running 0 14s cpumanager-r5krw 1/1 Running 0 32m cpumanager-r6h98 1/1 Running 0 13s cpumanager-s445q 1/1 Running 0 13s cpumanager-sr6jt 1/1 Running 0 14s cpumanager-z7h5f 1/1 Running 0 15s cpumanager-zxbj2 1/1 Running 0 14s [root@openshift-jumpserver-0 cpuman]# oc get pods | grep Runn | wc -l 29 Verify host limits: sh-4.4# cat /etc/kubernetes/kubelet.conf | jq '.systemReserved' { \"cpu\": \"10\", \"ephemeral-storage\": \"10Gi\", \"memory\": \"1Gi\" } Reserving 0 CPUs for the host This will not work - cpumanager-kubeletconfig.yaml : apiVersion: machineconfiguration.openshift.io/v1 kind: KubeletConfig metadata: name: cpumanager-enabled spec: machineConfigPoolSelector: matchLabels: custom-kubelet: cpumanager-enabled kubeletConfig: cpuManagerPolicy: static cpuManagerReconcilePeriod: 5s systemReserved: cpu: \"0\" memory: \"1Gi\" ephemeral-storage: \"10Gi\" Upon worker restart, the kubelet will not be able to start, reporting: [root@openshift-worker-1 ~]# Jun 26 19:05:10 openshift-worker-1.example.com systemd[1]: Stopped Kubernetes Kubelet. Jun 26 19:05:10 openshift-worker-1.example.com systemd[1]: kubelet.service: Consumed 115ms CPU time Jun 26 19:05:10 openshift-worker-1.example.com systemd[1]: Starting Kubernetes Kubelet... Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: Flag --minimum-container-ttl-duration has been deprecated, Use --eviction-hard or --eviction-soft instead. Will be removed in a future version. Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421694 5492 flags.go:33] FLAG: --add-dir-header=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421726 5492 flags.go:33] FLAG: --address=\"0.0.0.0\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421741 5492 flags.go:33] FLAG: --allowed-unsafe-sysctls=\"[]\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421748 5492 flags.go:33] FLAG: --alsologtostderr=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421752 5492 flags.go:33] FLAG: --anonymous-auth=\"true\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421756 5492 flags.go:33] FLAG: --application-metrics-count-limit=\"100\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421759 5492 flags.go:33] FLAG: --authentication-token-webhook=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421763 5492 flags.go:33] FLAG: --authentication-token-webhook-cache-ttl=\"2m0s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421767 5492 flags.go:33] FLAG: --authorization-mode=\"AlwaysAllow\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421771 5492 flags.go:33] FLAG: --authorization-webhook-cache-authorized-ttl=\"5m0s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421775 5492 flags.go:33] FLAG: --authorization-webhook-cache-unauthorized-ttl=\"30s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421778 5492 flags.go:33] FLAG: --azure-container-registry-config=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421781 5492 flags.go:33] FLAG: --boot-id-file=\"/proc/sys/kernel/random/boot_id\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421787 5492 flags.go:33] FLAG: --bootstrap-checkpoint-path=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421790 5492 flags.go:33] FLAG: --bootstrap-kubeconfig=\"/etc/kubernetes/kubeconfig\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421793 5492 flags.go:33] FLAG: --cert-dir=\"/var/lib/kubelet/pki\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421797 5492 flags.go:33] FLAG: --cgroup-driver=\"cgroupfs\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421800 5492 flags.go:33] FLAG: --cgroup-root=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421803 5492 flags.go:33] FLAG: --cgroups-per-qos=\"true\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421806 5492 flags.go:33] FLAG: --chaos-chance=\"0\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421811 5492 flags.go:33] FLAG: --client-ca-file=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421814 5492 flags.go:33] FLAG: --cloud-config=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421817 5492 flags.go:33] FLAG: --cloud-provider=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421820 5492 flags.go:33] FLAG: --cluster-dns=\"[]\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421824 5492 flags.go:33] FLAG: --cluster-domain=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421828 5492 flags.go:33] FLAG: --cni-bin-dir=\"/opt/cni/bin\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421831 5492 flags.go:33] FLAG: --cni-cache-dir=\"/var/lib/cni/cache\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421834 5492 flags.go:33] FLAG: --cni-conf-dir=\"/etc/cni/net.d\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421838 5492 flags.go:33] FLAG: --config=\"/etc/kubernetes/kubelet.conf\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421842 5492 flags.go:33] FLAG: --container-hints=\"/etc/cadvisor/container_hints.json\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421845 5492 flags.go:33] FLAG: --container-log-max-files=\"5\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421849 5492 flags.go:33] FLAG: --container-log-max-size=\"10Mi\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421852 5492 flags.go:33] FLAG: --container-runtime=\"remote\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421865 5492 flags.go:33] FLAG: --container-runtime-endpoint=\"/var/run/crio/crio.sock\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421868 5492 flags.go:33] FLAG: --containerd=\"/run/containerd/containerd.sock\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421872 5492 flags.go:33] FLAG: --contention-profiling=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421876 5492 flags.go:33] FLAG: --cpu-cfs-quota=\"true\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421879 5492 flags.go:33] FLAG: --cpu-cfs-quota-period=\"100ms\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421883 5492 flags.go:33] FLAG: --cpu-manager-policy=\"none\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421886 5492 flags.go:33] FLAG: --cpu-manager-reconcile-period=\"10s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421889 5492 flags.go:33] FLAG: --docker=\"unix:///var/run/docker.sock\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421893 5492 flags.go:33] FLAG: --docker-endpoint=\"unix:///var/run/docker.sock\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421896 5492 flags.go:33] FLAG: --docker-env-metadata-whitelist=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421899 5492 flags.go:33] FLAG: --docker-only=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421902 5492 flags.go:33] FLAG: --docker-root=\"/var/lib/docker\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421905 5492 flags.go:33] FLAG: --docker-tls=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421908 5492 flags.go:33] FLAG: --docker-tls-ca=\"ca.pem\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421911 5492 flags.go:33] FLAG: --docker-tls-cert=\"cert.pem\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421914 5492 flags.go:33] FLAG: --docker-tls-key=\"key.pem\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421917 5492 flags.go:33] FLAG: --dynamic-config-dir=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421921 5492 flags.go:33] FLAG: --enable-cadvisor-json-endpoints=\"true\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421924 5492 flags.go:33] FLAG: --enable-controller-attach-detach=\"true\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421929 5492 flags.go:33] FLAG: --enable-debugging-handlers=\"true\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421932 5492 flags.go:33] FLAG: --enable-load-reader=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421951 5492 flags.go:33] FLAG: --enable-server=\"true\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421954 5492 flags.go:33] FLAG: --enforce-node-allocatable=\"[pods]\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421958 5492 flags.go:33] FLAG: --event-burst=\"10\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421961 5492 flags.go:33] FLAG: --event-qps=\"5\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421963 5492 flags.go:33] FLAG: --event-storage-age-limit=\"default=0\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421966 5492 flags.go:33] FLAG: --event-storage-event-limit=\"default=0\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421969 5492 flags.go:33] FLAG: --eviction-hard=\"imagefs.available<15%,memory.available<100Mi,nodefs.available<10%,nodefs.inodesFree<5%\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421979 5492 flags.go:33] FLAG: --eviction-max-pod-grace-period=\"0\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421982 5492 flags.go:33] FLAG: --eviction-minimum-reclaim=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421986 5492 flags.go:33] FLAG: --eviction-pressure-transition-period=\"5m0s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421990 5492 flags.go:33] FLAG: --eviction-soft=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422000 5492 flags.go:33] FLAG: --eviction-soft-grace-period=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422003 5492 flags.go:33] FLAG: --exit-on-lock-contention=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422006 5492 flags.go:33] FLAG: --experimental-allocatable-ignore-eviction=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422009 5492 flags.go:33] FLAG: --experimental-bootstrap-kubeconfig=\"/etc/kubernetes/kubeconfig\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422012 5492 flags.go:33] FLAG: --experimental-check-node-capabilities-before-mount=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422015 5492 flags.go:33] FLAG: --experimental-dockershim=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422018 5492 flags.go:33] FLAG: --experimental-dockershim-root-directory=\"/var/lib/dockershim\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422021 5492 flags.go:33] FLAG: --experimental-kernel-memcg-notification=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422024 5492 flags.go:33] FLAG: --experimental-mounter-path=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422027 5492 flags.go:33] FLAG: --fail-swap-on=\"true\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422030 5492 flags.go:33] FLAG: --feature-gates=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422034 5492 flags.go:33] FLAG: --file-check-frequency=\"20s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422037 5492 flags.go:33] FLAG: --global-housekeeping-interval=\"1m0s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422040 5492 flags.go:33] FLAG: --hairpin-mode=\"promiscuous-bridge\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422043 5492 flags.go:33] FLAG: --healthz-bind-address=\"127.0.0.1\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422046 5492 flags.go:33] FLAG: --healthz-port=\"10248\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422049 5492 flags.go:33] FLAG: --help=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422052 5492 flags.go:33] FLAG: --hostname-override=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422055 5492 flags.go:33] FLAG: --housekeeping-interval=\"10s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422058 5492 flags.go:33] FLAG: --http-check-frequency=\"20s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422060 5492 flags.go:33] FLAG: --image-gc-high-threshold=\"85\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422063 5492 flags.go:33] FLAG: --image-gc-low-threshold=\"80\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422066 5492 flags.go:33] FLAG: --image-pull-progress-deadline=\"1m0s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422069 5492 flags.go:33] FLAG: --image-service-endpoint=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422072 5492 flags.go:33] FLAG: --iptables-drop-bit=\"15\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422075 5492 flags.go:33] FLAG: --iptables-masquerade-bit=\"14\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422078 5492 flags.go:33] FLAG: --keep-terminated-pod-volumes=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422081 5492 flags.go:33] FLAG: --kube-api-burst=\"10\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422084 5492 flags.go:33] FLAG: --kube-api-content-type=\"application/vnd.kubernetes.protobuf\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422087 5492 flags.go:33] FLAG: --kube-api-qps=\"5\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422090 5492 flags.go:33] FLAG: --kube-reserved=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422093 5492 flags.go:33] FLAG: --kube-reserved-cgroup=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422103 5492 flags.go:33] FLAG: --kubeconfig=\"/var/lib/kubelet/kubeconfig\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422106 5492 flags.go:33] FLAG: --kubelet-cgroups=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422109 5492 flags.go:33] FLAG: --lock-file=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422112 5492 flags.go:33] FLAG: --log-backtrace-at=\":0\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422115 5492 flags.go:33] FLAG: --log-cadvisor-usage=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422118 5492 flags.go:33] FLAG: --log-dir=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422121 5492 flags.go:33] FLAG: --log-file=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422124 5492 flags.go:33] FLAG: --log-file-max-size=\"1800\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422127 5492 flags.go:33] FLAG: --log-flush-frequency=\"5s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422130 5492 flags.go:33] FLAG: --logtostderr=\"true\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422133 5492 flags.go:33] FLAG: --machine-id-file=\"/etc/machine-id,/var/lib/dbus/machine-id\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422136 5492 flags.go:33] FLAG: --make-iptables-util-chains=\"true\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422139 5492 flags.go:33] FLAG: --manifest-url=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422142 5492 flags.go:33] FLAG: --manifest-url-header=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422146 5492 flags.go:33] FLAG: --master-service-namespace=\"default\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422149 5492 flags.go:33] FLAG: --max-open-files=\"1000000\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422153 5492 flags.go:33] FLAG: --max-pods=\"110\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422156 5492 flags.go:33] FLAG: --maximum-dead-containers=\"-1\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422159 5492 flags.go:33] FLAG: --maximum-dead-containers-per-container=\"1\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422162 5492 flags.go:33] FLAG: --minimum-container-ttl-duration=\"6m0s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422165 5492 flags.go:33] FLAG: --minimum-image-ttl-duration=\"2m0s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422168 5492 flags.go:33] FLAG: --network-plugin=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422170 5492 flags.go:33] FLAG: --network-plugin-mtu=\"0\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422173 5492 flags.go:33] FLAG: --node-ip=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422176 5492 flags.go:33] FLAG: --node-labels=\"node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422182 5492 flags.go:33] FLAG: --node-status-max-images=\"50\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422185 5492 flags.go:33] FLAG: --node-status-update-frequency=\"10s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422188 5492 flags.go:33] FLAG: --non-masquerade-cidr=\"10.0.0.0/8\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422191 5492 flags.go:33] FLAG: --oom-score-adj=\"-999\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422194 5492 flags.go:33] FLAG: --pod-cidr=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422196 5492 flags.go:33] FLAG: --pod-infra-container-image=\"k8s.gcr.io/pause:3.1\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422200 5492 flags.go:33] FLAG: --pod-manifest-path=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422211 5492 flags.go:33] FLAG: --pod-max-pids=\"-1\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422214 5492 flags.go:33] FLAG: --pods-per-core=\"0\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422217 5492 flags.go:33] FLAG: --port=\"10250\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422219 5492 flags.go:33] FLAG: --protect-kernel-defaults=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422222 5492 flags.go:33] FLAG: --provider-id=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422225 5492 flags.go:33] FLAG: --qos-reserved=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422228 5492 flags.go:33] FLAG: --read-only-port=\"10255\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422231 5492 flags.go:33] FLAG: --really-crash-for-testing=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422234 5492 flags.go:33] FLAG: --redirect-container-streaming=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422237 5492 flags.go:33] FLAG: --register-node=\"true\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422240 5492 flags.go:33] FLAG: --register-schedulable=\"true\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422243 5492 flags.go:33] FLAG: --register-with-taints=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422246 5492 flags.go:33] FLAG: --registry-burst=\"10\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422249 5492 flags.go:33] FLAG: --registry-qps=\"5\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422252 5492 flags.go:33] FLAG: --reserved-cpus=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422255 5492 flags.go:33] FLAG: --resolv-conf=\"/etc/resolv.conf\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422258 5492 flags.go:33] FLAG: --root-dir=\"/var/lib/kubelet\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422261 5492 flags.go:33] FLAG: --rotate-certificates=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422264 5492 flags.go:33] FLAG: --rotate-server-certificates=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422266 5492 flags.go:33] FLAG: --runonce=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422269 5492 flags.go:33] FLAG: --runtime-cgroups=\"/system.slice/crio.service\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422272 5492 flags.go:33] FLAG: --runtime-request-timeout=\"2m0s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422275 5492 flags.go:33] FLAG: --seccomp-profile-root=\"/var/lib/kubelet/seccomp\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422278 5492 flags.go:33] FLAG: --serialize-image-pulls=\"true\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422281 5492 flags.go:33] FLAG: --skip-headers=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422284 5492 flags.go:33] FLAG: --skip-log-headers=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422287 5492 flags.go:33] FLAG: --stderrthreshold=\"2\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422290 5492 flags.go:33] FLAG: --storage-driver-buffer-duration=\"1m0s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422293 5492 flags.go:33] FLAG: --storage-driver-db=\"cadvisor\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422296 5492 flags.go:33] FLAG: --storage-driver-host=\"localhost:8086\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422299 5492 flags.go:33] FLAG: --storage-driver-password=\"root\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422301 5492 flags.go:33] FLAG: --storage-driver-secure=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422312 5492 flags.go:33] FLAG: --storage-driver-table=\"stats\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422315 5492 flags.go:33] FLAG: --storage-driver-user=\"root\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422318 5492 flags.go:33] FLAG: --streaming-connection-idle-timeout=\"4h0m0s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422321 5492 flags.go:33] FLAG: --sync-frequency=\"1m0s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422324 5492 flags.go:33] FLAG: --system-cgroups=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422326 5492 flags.go:33] FLAG: --system-reserved=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422329 5492 flags.go:33] FLAG: --system-reserved-cgroup=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422333 5492 flags.go:33] FLAG: --tls-cert-file=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422336 5492 flags.go:33] FLAG: --tls-cipher-suites=\"[]\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422340 5492 flags.go:33] FLAG: --tls-min-version=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422343 5492 flags.go:33] FLAG: --tls-private-key-file=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422345 5492 flags.go:33] FLAG: --topology-manager-policy=\"none\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422349 5492 flags.go:33] FLAG: --v=\"3\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422352 5492 flags.go:33] FLAG: --version=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422361 5492 flags.go:33] FLAG: --vmodule=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422365 5492 flags.go:33] FLAG: --volume-plugin-dir=\"/etc/kubernetes/kubelet-plugins/volume/exec\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422369 5492 flags.go:33] FLAG: --volume-stats-agg-period=\"1m0s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422394 5492 feature_gate.go:244] feature gates: &{map[]} Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: Flag --minimum-container-ttl-duration has been deprecated, Use --eviction-hard or --eviction-soft instead. Will be removed in a future version. Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.423859 5492 feature_gate.go:244] feature gates: &{map[LegacyNodeRoleBehavior:false NodeDisruptionExclusion:true RotateKubeletServerCertificate:true SCTPSupport:true ServiceNodeExclusion:true SupportPodPidsLimit:true]} Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.423940 5492 feature_gate.go:244] feature gates: &{map[LegacyNodeRoleBehavior:false NodeDisruptionExclusion:true RotateKubeletServerCertificate:true SCTPSupport:true ServiceNodeExclusion:true SupportPodPidsLimit:true]} Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.433150 5492 mount_linux.go:168] Detected OS with systemd Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.433271 5492 server.go:424] Version: v1.17.1 Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.433323 5492 feature_gate.go:244] feature gates: &{map[LegacyNodeRoleBehavior:false NodeDisruptionExclusion:true RotateKubeletServerCertificate:true SCTPSupport:true ServiceNodeExclusion:true SupportPodPidsLimit:true]} Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.433385 5492 feature_gate.go:244] feature gates: &{map[LegacyNodeRoleBehavior:false NodeDisruptionExclusion:true RotateKubeletServerCertificate:true SCTPSupport:true ServiceNodeExclusion:true SupportPodPidsLimit:true]} Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.433491 5492 plugins.go:100] No cloud provider specified. Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.433510 5492 server.go:540] No cloud provider specified: \"\" from the config file: \"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.433519 5492 server.go:830] Client rotation is on, will bootstrap in background Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.445886 5492 bootstrap.go:84] Current kubeconfig file contents are still valid, no bootstrap necessary Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.445957 5492 certificate_store.go:129] Loading cert/key pair from \"/var/lib/kubelet/pki/kubelet-client-current.pem\". Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.446355 5492 server.go:857] Starting client certificate rotation. Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.446367 5492 certificate_manager.go:285] Certificate rotation is enabled. Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.446467 5492 certificate_manager.go:556] Certificate expiration is 2020-07-26 09:59:31 +0000 UTC, rotation deadline is 2020-07-20 19:25:44.002114354 +0000 UTC Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.446497 5492 certificate_manager.go:291] Waiting 576h20m33.555623075s for next certificate rotation Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.447060 5492 dynamic_cafile_content.go:129] Loaded a new CA Bundle and Verifier for \"client-ca-bundle::/etc/kubernetes/kubelet-ca.crt\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.447171 5492 dynamic_cafile_content.go:167] Starting client-ca-bundle::/etc/kubernetes/kubelet-ca.crt Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.447357 5492 manager.go:146] cAdvisor running in container: \"/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.448446 5492 fs.go:125] Filesystem UUIDs: map[00000000-0000-4000-a000-000000000002:/dev/sda4 1ccc3ac6-b59d-46bb-9850-229cd8b4e007:/dev/dm-0 676b3306-2850-478c-a817-3f723d49377d:/dev/sda1 D802-CD71:/dev/sda2] Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.448470 5492 fs.go:126] Filesystem partitions: map[/dev/mapper/coreos-luks-root-nocrypt:{mountpoint:/var major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:ext4 blockSize:0} /dev/shm:{mountpoint:/dev/shm major:0 minor:22 fsType:tmpfs blockSize:0} /run:{mountpoint:/run major:0 minor:24 fsType:tmpfs blockSize:0} /run/user/1000:{mountpoint:/run/user/1000 major:0 minor:44 fsType:tmpfs blockSize:0} /sys/fs/cgroup:{mountpoint:/sys/fs/cgroup major:0 minor:25 fsType:tmpfs blockSize:0}] Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.453331 5492 manager.go:193] Machine: {NumCores:40 CpuFrequency:3100000 MemoryCapacity:135090417664 HugePages:[{PageSize:1048576 NumPages:0} {PageSize:2048 NumPages:0}] MachineID:21668e85e1264ac78ea115b2fe79408e SystemUUID:4c4c4544-004b-5a10-8050-cac04f484832 BootID:16f2699c-8a76-444a-8f56-e32057152db4 Filesystems:[{Device:/run/user/1000 DeviceMajor:0 DeviceMinor:44 Capacity:13509038080 Type:vfs Inodes:16490529 HasInodes:true} {Device:/dev/shm DeviceMajor:0 DeviceMinor:22 Capacity:67545206784 Type:vfs Inodes:16490529 HasInodes:true} {Device:/run DeviceMajor:0 DeviceMinor:24 Capacity:67545206784 Type:vfs Inodes:16490529 HasInodes:true} {Device:/sys/fs/cgroup DeviceMajor:0 DeviceMinor:25 Capacity:67545206784 Type:vfs Inodes:16490529 HasInodes:true} {Device:/dev/mapper/coreos-luks-root-nocrypt DeviceMajor:253 DeviceMinor:0 Capacity:598985388032 Type:vfs Inodes:292478400 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:381549568 Type:vfs Inodes:98304 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:598995877376 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:599550590976 Scheduler:mq-deadline}] NetworkDevices:[{Name:eno1 MacAddress:18:66:da:9f:b1:0a Speed:1000 Mtu:1500} {Name:eno2 MacAddress:18:66:da:9f:b1:0b Speed:-1 Mtu:1500} {Name:eno3 MacAddress:18:66:da:9f:b1:0c Speed:-1 Mtu:1500} {Name:eno4 MacAddress:18:66:da:9f:b1:0d Speed:1000 Mtu:1500} {Name:enp4s0f0 MacAddress:a0:36:9f:e5:e9:fc Speed:10000 Mtu:1500} {Name:enp4s0f1 MacAddress:a0:36:9f:e5:e9:fe Speed:10000 Mtu:1500} {Name:enp5s0f0 MacAddress:a0:36:9f:e5:e2:a8 Speed:10000 Mtu:1500} {Name:enp5s0f1 MacAddress:a0:36:9f:e5:e2:aa Speed:10000 Mtu:1500}] Topology:[{Id:0 Memory:67476070400 HugePages:[{PageSize:1048576 NumPages:0} {PageSize:2048 NumPages:0}] Cores:[{Id:0 Threads:[0 20] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:1 Threads:[2 22] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:2 Threads:[4 24] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:3 Threads:[6 26] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:4 Threads:[8 28] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:8 Threads:[10 30] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:9 Threads:[12 32] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:10 Threads:[14 34] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:11 Threads:[16 36] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:12 Threads:[18 38] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:26214400 Type:Unified Level:3}]} {Id:1 Memory:67614347264 HugePages:[{PageSize:1048576 NumPages:0} {PageSize:2048 NumPages:0}] Cores:[{Id:0 Threads:[1 21] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:1 Threads:[3 23] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:2 Threads:[5 25] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:3 Threads:[7 27] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:4 Threads:[9 29] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:8 Threads:[11 31] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:9 Threads:[13 33] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:10 Threads:[15 35] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:11 Threads:[17 37] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:12 Threads:[19 39] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:26214400 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None} Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.453643 5492 manager.go:199] Version: {KernelVersion:4.18.0-147.8.1.el8_1.x86_64 ContainerOsVersion:Red Hat Enterprise Linux CoreOS 44.81.202005250830-0 (Ootpa) DockerVersion:Unknown DockerAPIVersion:Unknown CadvisorVersion: CadvisorRevision:} Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.454069 5492 container_manager_linux.go:265] container manager verified user specified cgroup-root exists: [] Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.454102 5492 container_manager_linux.go:270] Creating Container Manager object based on Node Config: {RuntimeCgroupsName:/system.slice/crio.service SystemCgroupsName:/system.slice KubeletCgroupsName: ContainerRuntime:remote CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:systemd KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[cpu:{i:{value:0 scale:0} d:{Dec:<nil>} s:0 Format:DecimalSI} ephemeral-storage:{i:{value:10737418240 scale:0} d:{Dec:<nil>} s:10Gi Format:BinarySI} memory:{i:{value:1073741824 scale:0} d:{Dec:<nil>} s:1Gi Format:BinarySI}] HardEvictionThresholds:[{Signal:memory.available Operator:LessThan Value:{Quantity:100Mi Percentage:0} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.1} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.inodesFree Operator:LessThan Value:{Quantity:<nil> Percentage:0.05} GracePeriod:0s MinReclaim:<nil>} {Signal:imagefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.15} GracePeriod:0s MinReclaim:<nil>}]} QOSReserved:map[] ExperimentalCPUManagerPolicy:static ExperimentalCPUManagerReconcilePeriod:5s ExperimentalPodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms ExperimentalTopologyManagerPolicy:none} Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.454183 5492 fake_topology_manager.go:29] [fake topologymanager] NewFakeManager Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.454189 5492 container_manager_linux.go:305] Creating device plugin manager: true Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.454201 5492 manager.go:126] Creating Device Plugin manager at /var/lib/kubelet/device-plugins/kubelet.sock Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.454215 5492 fake_topology_manager.go:39] [fake topologymanager] AddHintProvider HintProvider: &{kubelet.sock /var/lib/kubelet/device-plugins/ map[] {0 0} <nil> {{} [0 0 0]} 0x1b64c30 0x72dc9c8 0x1b65500 map[] map[] map[] map[] map[] 0xc000853ce0 [0 1] 0x72dc9c8} Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.454252 5492 cpu_manager.go:131] [cpumanager] detected CPU topology: &{40 20 2 map[0:{0 0 0} 1:{1 1 1} 2:{0 0 2} 3:{1 1 3} 4:{0 0 4} 5:{1 1 5} 6:{0 0 6} 7:{1 1 7} 8:{0 0 8} 9:{1 1 9} 10:{0 0 10} 11:{1 1 11} 12:{0 0 12} 13:{1 1 13} 14:{0 0 14} 15:{1 1 15} 16:{0 0 16} 17:{1 1 17} 18:{0 0 18} 19:{1 1 19} 20:{0 0 0} 21:{1 1 1} 22:{0 0 2} 23:{1 1 3} 24:{0 0 4} 25:{1 1 5} 26:{0 0 6} 27:{1 1 7} 28:{0 0 8} 29:{1 1 9} 30:{0 0 10} 31:{1 1 11} 32:{0 0 12} 33:{1 1 13} 34:{0 0 14} 35:{1 1 15} 36:{0 0 16} 37:{1 1 17} 38:{0 0 18} 39:{1 1 19}]} Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: E0626 19:05:10.454300 5492 container_manager_linux.go:329] failed to initialize cpu manager: [cpumanager] unable to determine reserved CPU resources for static policy Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: F0626 19:05:10.454309 5492 server.go:273] failed to run Kubelet: [cpumanager] unable to determine reserved CPU resources for static policy Jun 26 19:05:10 openshift-worker-1.example.com systemd[1]: kubelet.service: Main process exited, code=exited, status=255/n/a Jun 26 19:05:10 openshift-worker-1.example.com systemd[1]: kubelet.service: Failed with result 'exit-code'. Jun 26 19:05:10 openshift-worker-1.example.com systemd[1]: Failed to start Kubernetes Kubelet. Jun 26 19:05:10 openshift-worker-1.example.com systemd[1]: kubelet.service: Consumed 117ms CPU time And the node will never become Ready in the node list. References https://www.redhat.com/en/blog/openshift-container-platform-4-how-does-machine-config-pool-work https://docs.openshift.com/container-platform/4.4/scalability_and_performance/using-cpu-manager.html https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/","title":"CPU manager with custom MachineConfigPool"},{"location":"openshift/cpu-manager-with-custom-machine-config-pool/#cpu-manager-with-custom-machineconfigpool-in-ocp-4x","text":"","title":"CPU manager with custom MachineConfigPool in OCP 4.x"},{"location":"openshift/cpu-manager-with-custom-machine-config-pool/#how-to-apply-the-cpu-manager-to-only-a-subset-of-worker-nodes","text":"Create a custom MachineConfigPool named worker-cpu-manager . worker-cpu-manager.yaml : apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfigPool metadata: name: worker-cpu-manager labels: custom-kubelet: cpumanager-enabled spec: machineConfigSelector: matchExpressions: - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,worker-cpu-manager]} nodeSelector: matchLabels: node-role.kubernetes.io/worker-cpu-manager: \"\" paused: false Apply the pool: oc apply -f worker-cpu-manager.yaml Create the cpumanager-kubelet.yaml : apiVersion: machineconfiguration.openshift.io/v1 kind: KubeletConfig metadata: name: cpumanager-enabled spec: machineConfigPoolSelector: matchLabels: custom-kubelet: cpumanager-enabled kubeletConfig: cpuManagerPolicy: static cpuManagerReconcilePeriod: 5s oc apply -f cpumanager-kubelet.yaml Change worker node to worker-cpu-manager role: oc label node <node> node-role.kubernetes.io/worker-cpu-manager= Verify: # oc get nodes NAME STATUS ROLES AGE VERSION openshift-master-0.example.com Ready master 4h3m v1.17.1+1aa1c48 openshift-master-1.example.com Ready master 4h2m v1.17.1+1aa1c48 openshift-master-2.example.com Ready master 3h56m v1.17.1+1aa1c48 openshift-worker-0.example.com Ready worker 18m v1.17.1+1aa1c48 openshift-worker-1.example.com Ready worker,worker-cpu-manager 18m v1.17.1+1aa1c48 oc get machineconfig (...) rendered-worker-cpu-manager-bc48d7bf24df726f468b357482032845 8af4f709c4ba9c0afff3408ecc99c8fce61dd314 2.2.0 87s rendered-worker-cpu-manager-ca0a09ddea41402490e1c39a138cd44e 8af4f709c4ba9c0afff3408ecc99c8fce61dd314 2.2.0 18m [root@openshift-jumpserver-0 cpuman]# diff <(oc get machineconfig rendered-worker-cpu-manager-bc48d7bf24df726f468b357482032845 -o yaml) <(oc get machineconfig rendered-worker-cpu-manager-ca0a09ddea41402490e1c39a138cd44e -o yaml) 6c6 < creationTimestamp: \"2020-06-26T16:40:44Z\" --- > creationTimestamp: \"2020-06-26T16:24:01Z\" 8c8 < name: rendered-worker-cpu-manager-bc48d7bf24df726f468b357482032845 --- > name: rendered-worker-cpu-manager-ca0a09ddea41402490e1c39a138cd44e 16,18c16,18 < resourceVersion: \"451269\" < selfLink: /apis/machineconfiguration.openshift.io/v1/machineconfigs/rendered-worker-cpu-manager-bc48d7bf24df726f468b357482032845 < uid: bf950ef8-7270-4699-a113-dfe6c6c1d4fb --- > resourceVersion: \"445466\" > selfLink: /apis/machineconfiguration.openshift.io/v1/machineconfigs/rendered-worker-cpu-manager-ca0a09ddea41402490e1c39a138cd44e > uid: 74acdcd1-d195-48a5-8689-cecca137605c 163,168d162 < verification: {} < filesystem: root < mode: 420 < path: /etc/kubernetes/kubelet.conf < - contents: < source: data:text/plain,%7B%odcy%2(...) Compare unmodified node to node with CPU manager: [root@openshift-jumpserver-0 cpuman]# oc debug node/openshift-worker-0.example.com Starting pod/openshift-worker-0examplecom-debug ... To use host binaries, run `chroot /host` Pod IP: 192.168.123.215 If you don't see a command prompt, try pressing enter. sh-4.2# cat /host/etc/kubernetes/kubelet.conf | grep cpuManager sh-4.2# exit exit Removing debug pod ... [root@openshift-jumpserver-0 cpuman]# oc debug node/openshift-worker-1.example.com Starting pod/openshift-worker-1examplecom-debug ... To use host binaries, run `chroot /host` Pod IP: 192.168.123.204 If you don't see a command prompt, try pressing enter. sh-4.2# cat /host/etc/kubernetes/kubelet.conf | grep cpuManager {\"kind\":\"KubeletConfiguration\",\"apiVersion\":\"kubelet.config.k8s.io/v1beta1\",\"staticPodPath\":\"/etc/kubernetes/manifests\",\"syncFrequency\":\"0s\",\"fileCheckFrequency\":\"0s\",\"httpCheckFrequency\":\"0s\",\"rotateCertificates\":true,\"serverTLSBootstrap\":true,\"authentication\":{\"x509\":{\"clientCAFile\":\"/etc/kubernetes/kubelet-ca.crt\"},\"webhook\":{\"cacheTTL\":\"0s\"},\"anonymous\":{\"enabled\":false}},\"authorization\":{\"webhook\":{\"cacheAuthorizedTTL\":\"0s\",\"cacheUnauthorizedTTL\":\"0s\"}},\"clusterDomain\":\"cluster.local\",\"clusterDNS\":[\"172.30.0.10\"],\"streamingConnectionIdleTimeout\":\"0s\",\"nodeStatusUpdateFrequency\":\"0s\",\"nodeStatusReportFrequency\":\"0s\",\"imageMinimumGCAge\":\"0s\",\"volumeStatsAggPeriod\":\"0s\",\"systemCgroups\":\"/system.slice\",\"cgroupRoot\":\"/\",\"cgroupDriver\":\"systemd\",\"cpuManagerPolicy\":\"static\",\"cpuManagerReconcilePeriod\":\"5s\",\"runtimeRequestTimeout\":\"0s\",\"maxPods\":250,\"kubeAPIQPS\":50,\"kubeAPIBurst\":100,\"serializeImagePulls\":false,\"evictionPressureTransitionPeriod\":\"0s\",\"featureGates\":{\"LegacyNodeRoleBehavior\":false,\"NodeDisruptionExclusion\":true,\"RotateKubeletServerCertificate\":true,\"SCTPSupport\":true,\"ServiceNodeExclusion\":true,\"SupportPodPidsLimit\":true},\"containerLogMaxSize\":\"50Mi\",\"systemReserved\":{\"cpu\":\"500m\",\"ephemeral-storage\":\"1Gi\",\"memory\":\"1Gi\"}} sh-4.2# exit exit Removing debug pod ... Spawn 2 pods on the same worker, one with and one without CPU manager enabled: [root@openshift-jumpserver-0 cpuman]# cat cpumanager-pod.yaml apiVersion: v1 kind: Pod metadata: name: cpumanager spec: containers: - name: cpumanager image: gcr.io/google_containers/pause-amd64:3.0 resources: requests: cpu: 1 memory: \"1G\" limits: cpu: 1 memory: \"1G\" nodeSelector: cpumanager: \"true\" [root@openshift-jumpserver-0 cpuman]# cat non-cpumanager-pod.yaml apiVersion: v1 kind: Pod metadata: name: noncpumanager spec: containers: - name: cpumanager image: gcr.io/google_containers/pause-amd64:3.0 [root@openshift-jumpserver-0 cpuman]# oc get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES cpumanager 1/1 Running 0 11m 172.24.2.4 openshift-worker-1.example.com <none> <none> noncpumanager 1/1 Running 0 8m10s 172.24.2.5 openshift-worker-1.example.com <none> <none> Verify CPU pinning for the pods' CPUs: [root@openshift-jumpserver-0 cpuman]# oc debug node/openshift-worker-1.example.com Starting pod/openshift-worker-1examplecom-debug ... To use host binaries, run `chroot /host` systemctlPod IP: 192.168.123.204 If you don't see a command prompt, try pressing enter. sh-4.2# chroot /host sh-4.4# ps aux | grep pause root 39072 0.0 0.0 1028 4 ? Ss 16:55 0:00 /pause root 47750 0.0 0.0 1028 4 ? Ss 16:58 0:00 /pause root 64183 0.0 0.0 9180 960 ? S+ 17:03 0:00 grep pause sh-4.4# cat /proc/39072/status | grep -i cpu Cpus_allowed: 00,00100000 Cpus_allowed_list: 20 sh-4.4# cat /proc/47750/status | grep -i cpu Cpus_allowed: ff,ffefffff Cpus_allowed_list: 0-19,21-39 Alternatively: sh-4.4# systemctl status 39072 \u25cf crio-ff43bfb551a274eb9e9040510753db6271ef27c13c203fe69e87aad5f7d49f17.scope - libcontainer container ff43bfb551a274eb9e9040510753db6271ef27c13c203fe69e87aad5f7d49f17 Loaded: loaded (/run/systemd/transient/crio-ff43bfb551a274eb9e9040510753db6271ef27c13c203fe69e87aad5f7d49f17.scope; transient) Transient: yes Active: active (running) since Fri 2020-06-26 16:55:34 UTC; 15min ago Tasks: 1 (limit: 1024) Memory: 1.4M (limit: 953.6M) CPU: 33ms CGroup: /kubepods.slice/kubepods-podc405f7cf_b2d1_49c4_bd4c_09f01a0b8e2d.slice/crio-ff43bfb551a274eb9e9040510753db6271ef27c13c203fe69e87aad5f7d49f17.scope \u2514\u250039072 /pause Jun 26 16:55:34 openshift-worker-1.example.com systemd[1]: Started libcontainer container ff43bfb551a274eb9e9040510753db6271ef27c13c203fe69e87aad5f7d49f17. sh-4.4# systemctl status 47750 \u25cf crio-473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260.scope - libcontainer container 473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260 Loaded: loaded (/run/systemd/transient/crio-473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260.scope; transient) Transient: yes Active: active (running) since Fri 2020-06-26 16:58:32 UTC; 13min ago Tasks: 1 (limit: 1024) Memory: 1.0M CPU: 35ms CGroup: /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod3b4059d0_8030_461a_b192_b5820f6c1119.slice/crio-473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260.scope \u2514\u250047750 /pause Jun 26 16:58:32 openshift-worker-1.example.com systemd[1]: Started libcontainer container 473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260. sh-4.4# cat /sys/fs/cgroup/ blkio/ cpu,cpuacct/ cpuset/ freezer/ memory/ net_cls,net_prio/ perf_event/ rdma/ cpu/ cpuacct/ devices/ hugetlb/ net_cls/ net_prio/ pids/ systemd/ sh-4.4# cat /sys/fs/cgroup/cpuset//kubepods.slice/kubepods-podc405f7cf_b2d1_49c4_bd4c_09f01a0b8e2d.slice/crio-ff43bfb551a274eb9e9040510753db6271ef27c13c203fe69e87aad5f7d49f17.scope/cpuset.cpus 20 sh-4.4# cat /sys/fs/cgroup/cpuset//kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod3b4059d0_8030_461a_b192_b5820f6c1119.slice/crio-473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260.scope/cpuset.pus 0-19,21-39","title":"How to apply the CPU manager to only a subset of worker nodes"},{"location":"openshift/cpu-manager-with-custom-machine-config-pool/#exploring-resource-limits","text":"Let's spawn more pods than the hypervisor has CPUs: [root@openshift-jumpserver-0 cpuman]# cat cpumanager-pod-generated-name.yaml apiVersion: v1 kind: Pod metadata: generateName: cpumanager- spec: containers: - name: cpumanager image: gcr.io/google_containers/pause-amd64:3.0 resources: requests: cpu: 1 memory: \"1G\" limits: cpu: 1 memory: \"1G\" nodeSelector: cpumanager: \"true\" for i in {0..45}; do oc create -f cpumanager-pod-generated-name.yaml ; done [root@openshift-jumpserver-0 cpuman]# oc get pods | grep Pending cpumanager-8rtpt 0/1 Pending 0 4m23s cpumanager-bw848 0/1 Pending 0 4m24s cpumanager-cm2st 0/1 Pending 0 4m24s cpumanager-cwj82 0/1 Pending 0 4m23s cpumanager-r5krw 0/1 Pending 0 4m24s [root@openshift-jumpserver-0 cpuman]# oc describe node openshift-worker-1.example.com Name: openshift-worker-1.example.com Roles: worker-cpu-manager Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux cpumanager=true kubernetes.io/arch=amd64 kubernetes.io/hostname=openshift-worker-1.example.com kubernetes.io/os=linux node-role.kubernetes.io/worker-cpu-manager= node.openshift.io/os_id=rhcos Annotations: k8s.ovn.org/l3-gateway-config: {\"default\":{\"mode\":\"local\",\"interface-id\":\"br-local_openshift-worker-1.example.com\",\"mac-address\":\"8e:c7:71:4d:0f:49\",\"ip-addresses\":[\"169... k8s.ovn.org/node-chassis-id: 5a8880a6-5b50-4be5-9d84-f195bbc306a2 k8s.ovn.org/node-join-subnets: {\"default\":\"100.64.3.0/29\"} k8s.ovn.org/node-mgmt-port-mac-address: 9a:b7:cd:ab:bf:b9 k8s.ovn.org/node-subnets: {\"default\":\"172.24.2.0/23\"} machineconfiguration.openshift.io/currentConfig: rendered-worker-cpu-manager-bc48d7bf24df726f468b357482032845 machineconfiguration.openshift.io/desiredConfig: rendered-worker-cpu-manager-bc48d7bf24df726f468b357482032845 machineconfiguration.openshift.io/reason: machineconfiguration.openshift.io/state: Done volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Thu, 25 Jun 2020 12:03:55 -0400 Taints: <none> Unschedulable: false Lease: HolderIdentity: openshift-worker-1.example.com AcquireTime: <unset> RenewTime: Fri, 26 Jun 2020 14:14:05 -0400 Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Fri, 26 Jun 2020 14:10:56 -0400 Fri, 26 Jun 2020 12:43:22 -0400 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Fri, 26 Jun 2020 14:10:56 -0400 Fri, 26 Jun 2020 12:43:22 -0400 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Fri, 26 Jun 2020 14:10:56 -0400 Fri, 26 Jun 2020 12:43:22 -0400 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Fri, 26 Jun 2020 14:10:56 -0400 Fri, 26 Jun 2020 12:43:33 -0400 KubeletReady kubelet is posting ready status Addresses: InternalIP: 192.168.123.204 Hostname: openshift-worker-1.example.com Capacity: cpu: 40 ephemeral-storage: 584946668Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 131924236Ki pods: 250 Allocatable: cpu: 39500m ephemeral-storage: 538013106513 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 130773260Ki pods: 250 System Info: Machine ID: 21668e85e1264ac78ea115b2fe79408e System UUID: 4c4c4544-004b-5a10-8050-cac04f484832 Boot ID: f20ee17e-226f-4f34-b9ce-965043215c2d Kernel Version: 4.18.0-147.8.1.el8_1.x86_64 OS Image: Red Hat Enterprise Linux CoreOS 44.81.202005250830-0 (Ootpa) Operating System: linux Architecture: amd64 Container Runtime Version: cri-o://1.17.4-12.dev.rhaos4.4.git2be4d9c.el8 Kubelet Version: v1.17.1 Kube-Proxy Version: v1.17.1 Non-terminated Pods: (49 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits AGE --------- ---- ------------ ---------- --------------- ------------- --- default cpumanager 1 (2%) 1 (2%) 1G (0%) 1G (0%) 78m default cpumanager-2548b 1 (2%) 1 (2%) 1G (0%) 1G (0%) 7m41s default cpumanager-44nxg 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m29s default cpumanager-5zx5v 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m30s default cpumanager-72spc 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m28s default cpumanager-9754w 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m29s default cpumanager-cff7t 1 (2%) 1 (2%) 1G (0%) 1G (0%) 8m6s default cpumanager-djhz9 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m27s default cpumanager-dl7lf 1 (2%) 1 (2%) 1G (0%) 1G (0%) 7m42s default cpumanager-dnz7k 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m28s default cpumanager-drhf8 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m27s default cpumanager-ff7h5 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m30s default cpumanager-fh77s 1 (2%) 1 (2%) 1G (0%) 1G (0%) 7m40s default cpumanager-fsptg 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m30s default cpumanager-h74fq 1 (2%) 1 (2%) 1G (0%) 1G (0%) 7m38s default cpumanager-hb72w 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m28s default cpumanager-hbhz2 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m29s default cpumanager-j7smp 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m28s default cpumanager-jlwwn 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m30s default cpumanager-jmdfk 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m30s default cpumanager-kvfkp 1 (2%) 1 (2%) 1G (0%) 1G (0%) 7m45s default cpumanager-kx4ft 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m28s default cpumanager-kz6zg 1 (2%) 1 (2%) 1G (0%) 1G (0%) 7m43s default cpumanager-m4qrn 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m31s default cpumanager-mzqqs 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m30s default cpumanager-n2tt5 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m28s default cpumanager-n79js 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m31s default cpumanager-qwjkf 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m27s default cpumanager-sn658 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m29s default cpumanager-tc7q6 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m29s default cpumanager-vss6b 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m29s default cpumanager-w846n 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m31s default cpumanager-wchfq 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m30s default cpumanager-whwxr 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m28s default cpumanager-wkhw8 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m31s default cpumanager-wtr5z 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m31s default cpumanager-z7zgl 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m28s default cpumanager-zckx8 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m30s default cpumanager-zsnwv 1 (2%) 1 (2%) 1G (0%) 1G (0%) 4m27s default noncpumanager 0 (0%) 0 (0%) 0 (0%) 0 (0%) 75m openshift-cluster-node-tuning-operator tuned-tkhrp 10m (0%) 0 (0%) 50Mi (0%) 0 (0%) 26h openshift-dns dns-default-q8hww 110m (0%) 0 (0%) 70Mi (0%) 512Mi (0%) 26h openshift-image-registry node-ca-sfjgq 10m (0%) 0 (0%) 10Mi (0%) 0 (0%) 26h openshift-machine-config-operator machine-config-daemon-8nw2s 40m (0%) 0 (0%) 100Mi (0%) 0 (0%) 26h openshift-marketplace certified-operators-74d989c4dd-w6l7f 10m (0%) 0 (0%) 100Mi (0%) 0 (0%) 50m openshift-monitoring node-exporter-nlgr5 9m (0%) 0 (0%) 210Mi (0%) 0 (0%) 26h openshift-multus multus-w2sgc 10m (0%) 0 (0%) 150Mi (0%) 0 (0%) 26h openshift-ovn-kubernetes ovnkube-node-pg57x 200m (0%) 0 (0%) 600Mi (0%) 0 (0%) 26h openshift-ovn-kubernetes ovs-node-ktvtm 100m (0%) 0 (0%) 300Mi (0%) 0 (0%) 26h Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 39499m (99%) 39 (98%) memory 40667235840 (30%) 39536870912 (29%) ephemeral-storage 0 (0%) 0 (0%) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal NodeNotSchedulable 110m kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeNotSchedulable Normal Starting 106m kubelet, openshift-worker-1.example.com Starting kubelet. Normal NodeHasSufficientMemory 106m (x2 over 106m) kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeHasSufficientMemory Normal NodeHasNoDiskPressure 106m (x2 over 106m) kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeHasNoDiskPressure Normal NodeHasSufficientPID 106m (x2 over 106m) kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeHasSufficientPID Warning Rebooted 106m kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com has been rebooted, boot id: 7df19e71-4ebc-4a66-94e8-1e475d11e095 Normal NodeNotReady 106m kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeNotReady Normal NodeNotSchedulable 106m kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeNotSchedulable Normal NodeAllocatableEnforced 106m kubelet, openshift-worker-1.example.com Updated Node Allocatable limit across pods Normal NodeReady 106m kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeReady Normal NodeSchedulable 100m kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeSchedulable Normal Starting 90m kubelet, openshift-worker-1.example.com Starting kubelet. Normal NodeHasSufficientMemory 90m (x2 over 90m) kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeHasSufficientMemory Normal NodeHasNoDiskPressure 90m (x2 over 90m) kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeHasNoDiskPressure Normal NodeHasSufficientPID 90m (x2 over 90m) kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeHasSufficientPID Warning Rebooted 90m kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com has been rebooted, boot id: f20ee17e-226f-4f34-b9ce-965043215c2d Normal NodeNotReady 90m kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeNotReady Normal NodeNotSchedulable 90m kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeNotSchedulable Normal NodeAllocatableEnforced 90m kubelet, openshift-worker-1.example.com Updated Node Allocatable limit across pods Normal NodeReady 90m kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeReady Normal NodeSchedulable 85m kubelet, openshift-worker-1.example.com Node openshift-worker-1.example.com status is now: NodeSchedulable Verify processes and cgroup limits on the worker node: [root@openshift-jumpserver-0 cpuman]# oc debug node/openshift-worker-1.example.com Starting pod/openshift-worker-1examplecom-debug ... To use host binaries, run `chroot /host` Pod IP: 192.168.123.204 If you don't see a command prompt, try pressing enter. sh-4.2# chroot /host sh-4.4# ps aux | grep pause root 39072 0.0 0.0 1028 4 ? Ss 16:55 0:00 /pause root 47750 0.0 0.0 1028 4 ? Ss 16:58 0:00 /pause root 274649 0.0 0.0 1028 4 ? Ss 18:06 0:00 /pause root 276349 0.0 0.0 1028 4 ? Ss 18:06 0:00 /pause root 276526 0.0 0.0 1028 4 ? Ss 18:06 0:00 /pause root 276685 0.0 0.0 1028 4 ? Ss 18:06 0:00 /pause root 276824 0.0 0.0 1028 4 ? Ss 18:06 0:00 /pause root 276988 0.0 0.0 1028 4 ? Ss 18:06 0:00 /pause root 277279 0.0 0.0 1028 4 ? Ss 18:06 0:00 /pause root 291552 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 291674 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 291783 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 291900 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 292072 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 292145 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 292191 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 292638 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 292639 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 292693 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 292941 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 292949 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 292971 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 292986 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 293300 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 293422 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 293528 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 293615 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 293678 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 293823 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 293909 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 293951 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 293965 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 293983 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 294087 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 294164 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 294184 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 294239 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 294330 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 294352 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 294426 0.0 0.0 1028 4 ? Ss 18:09 0:00 /pause root 301832 0.0 0.0 9180 964 ? R+ 18:11 0:00 grep pause sh-4.4# systemctl status 47750 \u25cf crio-473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260.scope - libcontainer container 473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260 Loaded: loaded (/run/systemd/transient/crio-473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260.scope; transient) Transient: yes Active: active (running) since Fri 2020-06-26 16:58:32 UTC; 1h 12min ago Tasks: 1 (limit: 1024) Memory: 1.0M CPU: 35ms CGroup: /kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod3b4059d0_8030_461a_b192_b5820f6c1119.slice/crio-473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260.scope \u2514\u250047750 /pause Jun 26 16:58:32 openshift-worker-1.example.com systemd[1]: Started libcontainer container 473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260. sh-4.4# cat /sys/fs/cgroup/cpuset/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod3b4059d0_8030_461a_b192_b5820f6c1119.slice/crio-473fd6bdac78596e0dbcf0d33dc11e11285725b4cf4e55410ce70dbdf088d260.scope/cpuset.cpus 0 sh-4.4# exit exit sh-4.2# exit exit Removing debug pod ... Default host limits: sh-4.4# cat /etc/kubernetes/kubelet.conf | jq '.systemReserved' { \"cpu\": \"500m\", \"ephemeral-storage\": \"1Gi\", \"memory\": \"1Gi\" }","title":"Exploring resource limits"},{"location":"openshift/cpu-manager-with-custom-machine-config-pool/#applying-custom-limits","text":"Let's now reserve 10 CPUs for the host. cpumanager-kubeletconfig.yaml : apiVersion: machineconfiguration.openshift.io/v1 kind: KubeletConfig metadata: name: cpumanager-enabled spec: machineConfigPoolSelector: matchLabels: custom-kubelet: cpumanager-enabled kubeletConfig: cpuManagerPolicy: static cpuManagerReconcilePeriod: 5s systemReserved: cpu: \"10\" memory: \"1Gi\" ephemeral-storage: \"10Gi\" [root@openshift-jumpserver-0 cpuman]# oc apply -f cpumanager-kubeletconfig.yaml kubeletconfig.machineconfiguration.openshift.io/cpumanager-enabled configured [root@openshift-jumpserver-0 cpuman]# oc get -o yaml -f cpumanager-kubeletconfig.yaml apiVersion: machineconfiguration.openshift.io/v1 kind: KubeletConfig metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"machineconfiguration.openshift.io/v1\",\"kind\":\"KubeletConfig\",\"metadata\":{\"annotations\":{},\"name\":\"cpumanager-enabled\"},\"spec\":{\"kubeletConfig\":{\"cpuManagerPolicy\":\"static\",\"cpuManagerReconcilePeriod\":\"5s\",\"systemReserved\":{\"cpu\":\"10\",\"ephemeral-storage\":\"10Gi\",\"memory\":\"1Gi\"}},\"machineConfigPoolSelector\":{\"matchLabels\":{\"custom-kubelet\":\"cpumanager-enabled\"}}}} creationTimestamp: \"2020-06-26T18:29:07Z\" finalizers: - b5ac419d-573d-428c-adc1-f7b5bedb27e3 - 21fa7ce4-49fa-470d-b4ed-f0972dbdd040 generation: 2 name: cpumanager-enabled resourceVersion: \"484854\" selfLink: /apis/machineconfiguration.openshift.io/v1/kubeletconfigs/cpumanager-enabled uid: 75d54d68-7b96-4b9e-8c9a-db3820b2629b spec: kubeletConfig: cpuManagerPolicy: static cpuManagerReconcilePeriod: 5s systemReserved: cpu: \"10\" ephemeral-storage: 10Gi memory: 1Gi machineConfigPoolSelector: matchLabels: custom-kubelet: cpumanager-enabled status: conditions: - lastTransitionTime: \"2020-06-26T18:29:07Z\" message: Success status: \"True\" type: Success - lastTransitionTime: \"2020-06-26T18:33:14Z\" message: Success status: \"True\" type: Success [root@openshift-jumpserver-0 cpuman]# oc get nodes NAME STATUS ROLES AGE VERSION openshift-master-0.example.com Ready master 27h v1.17.1 openshift-master-1.example.com Ready master 27h v1.17.1 openshift-master-2.example.com Ready master 27h v1.17.1 openshift-worker-0.example.com Ready worker 26h v1.17.1 openshift-worker-1.example.com Ready,SchedulingDisabled worker-cpu-manager 26h v1.17.1 [root@openshift-jumpserver-0 cpuman]# oc get machineconfig | grep worker-cpu-manager 99-worker-cpu-manager-ab32b145-ada3-4f96-adf9-1fb8388ba183-kubelet 8af4f709c4ba9c0afff3408ecc99c8fce61dd314 2.2.0 4m42s rendered-worker-cpu-manager-171f3675f101028b058ffe27d6344bb2 8af4f709c4ba9c0afff3408ecc99c8fce61dd314 2.2.0 30s rendered-worker-cpu-manager-bc48d7bf24df726f468b357482032845 8af4f709c4ba9c0afff3408ecc99c8fce61dd314 2.2.0 113m rendered-worker-cpu-manager-ca0a09ddea41402490e1c39a138cd44e 8af4f709c4ba9c0afff3408ecc99c8fce61dd314 2.2.0 129m [root@openshift-jumpserver-0 cpuman]# for i in {0..35}; do oc create -f cpumanager-pod-generated-name.yaml ; done pod/cpumanager-msf8c created pod/cpumanager-2kwws created pod/cpumanager-7mx5k created pod/cpumanager-bq7sf created pod/cpumanager-86kkl created pod/cpumanager-z7h5f created pod/cpumanager-c5pd4 created pod/cpumanager-sr6jt created pod/cpumanager-dp7ck created pod/cpumanager-n4ffv created pod/cpumanager-5q77h created pod/cpumanager-zxbj2 created pod/cpumanager-7f8fw created pod/cpumanager-j5kdw created pod/cpumanager-fhfmw created pod/cpumanager-hz5r8 created pod/cpumanager-44xst created pod/cpumanager-r6h98 created pod/cpumanager-mjsfq created pod/cpumanager-dh897 created pod/cpumanager-s445q created pod/cpumanager-brwcz created pod/cpumanager-mnbrg created pod/cpumanager-56vmv created pod/cpumanager-kt5fg created pod/cpumanager-fscpd created pod/cpumanager-hsjzn created pod/cpumanager-cvsth created pod/cpumanager-j7wlv created pod/cpumanager-z87tp created pod/cpumanager-xggjm created pod/cpumanager-jg646 created pod/cpumanager-ml7kg created pod/cpumanager-qwhjm created pod/cpumanager-m6jw8 created pod/cpumanager-f8qsh created [root@openshift-jumpserver-0 cpuman]# oc get pods | grep Runn cpumanager-2kwws 1/1 Running 0 15s cpumanager-44xst 1/1 Running 0 13s cpumanager-56vmv 1/1 Running 0 12s cpumanager-5q77h 1/1 Running 0 14s cpumanager-7f8fw 1/1 Running 0 14s cpumanager-7mx5k 1/1 Running 0 15s cpumanager-86kkl 1/1 Running 0 15s cpumanager-8rtpt 1/1 Running 0 32m cpumanager-bq7sf 1/1 Running 0 15s cpumanager-brwcz 1/1 Running 0 13s cpumanager-bw848 1/1 Running 0 32m cpumanager-c5pd4 1/1 Running 0 14s cpumanager-cm2st 1/1 Running 0 32m cpumanager-cwj82 1/1 Running 0 32m cpumanager-dh897 1/1 Running 0 13s cpumanager-dp7ck 1/1 Running 0 14s cpumanager-fhfmw 1/1 Running 0 13s cpumanager-hz5r8 1/1 Running 0 13s cpumanager-j5kdw 1/1 Running 0 14s cpumanager-mjsfq 1/1 Running 0 13s cpumanager-mnbrg 1/1 Running 0 12s cpumanager-msf8c 1/1 Running 0 15s cpumanager-n4ffv 1/1 Running 0 14s cpumanager-r5krw 1/1 Running 0 32m cpumanager-r6h98 1/1 Running 0 13s cpumanager-s445q 1/1 Running 0 13s cpumanager-sr6jt 1/1 Running 0 14s cpumanager-z7h5f 1/1 Running 0 15s cpumanager-zxbj2 1/1 Running 0 14s [root@openshift-jumpserver-0 cpuman]# oc get pods | grep Runn | wc -l 29 Verify host limits: sh-4.4# cat /etc/kubernetes/kubelet.conf | jq '.systemReserved' { \"cpu\": \"10\", \"ephemeral-storage\": \"10Gi\", \"memory\": \"1Gi\" }","title":"Applying custom limits"},{"location":"openshift/cpu-manager-with-custom-machine-config-pool/#reserving-0-cpus-for-the-host","text":"This will not work - cpumanager-kubeletconfig.yaml : apiVersion: machineconfiguration.openshift.io/v1 kind: KubeletConfig metadata: name: cpumanager-enabled spec: machineConfigPoolSelector: matchLabels: custom-kubelet: cpumanager-enabled kubeletConfig: cpuManagerPolicy: static cpuManagerReconcilePeriod: 5s systemReserved: cpu: \"0\" memory: \"1Gi\" ephemeral-storage: \"10Gi\" Upon worker restart, the kubelet will not be able to start, reporting: [root@openshift-worker-1 ~]# Jun 26 19:05:10 openshift-worker-1.example.com systemd[1]: Stopped Kubernetes Kubelet. Jun 26 19:05:10 openshift-worker-1.example.com systemd[1]: kubelet.service: Consumed 115ms CPU time Jun 26 19:05:10 openshift-worker-1.example.com systemd[1]: Starting Kubernetes Kubelet... Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: Flag --minimum-container-ttl-duration has been deprecated, Use --eviction-hard or --eviction-soft instead. Will be removed in a future version. Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421694 5492 flags.go:33] FLAG: --add-dir-header=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421726 5492 flags.go:33] FLAG: --address=\"0.0.0.0\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421741 5492 flags.go:33] FLAG: --allowed-unsafe-sysctls=\"[]\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421748 5492 flags.go:33] FLAG: --alsologtostderr=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421752 5492 flags.go:33] FLAG: --anonymous-auth=\"true\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421756 5492 flags.go:33] FLAG: --application-metrics-count-limit=\"100\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421759 5492 flags.go:33] FLAG: --authentication-token-webhook=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421763 5492 flags.go:33] FLAG: --authentication-token-webhook-cache-ttl=\"2m0s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421767 5492 flags.go:33] FLAG: --authorization-mode=\"AlwaysAllow\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421771 5492 flags.go:33] FLAG: --authorization-webhook-cache-authorized-ttl=\"5m0s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421775 5492 flags.go:33] FLAG: --authorization-webhook-cache-unauthorized-ttl=\"30s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421778 5492 flags.go:33] FLAG: --azure-container-registry-config=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421781 5492 flags.go:33] FLAG: --boot-id-file=\"/proc/sys/kernel/random/boot_id\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421787 5492 flags.go:33] FLAG: --bootstrap-checkpoint-path=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421790 5492 flags.go:33] FLAG: --bootstrap-kubeconfig=\"/etc/kubernetes/kubeconfig\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421793 5492 flags.go:33] FLAG: --cert-dir=\"/var/lib/kubelet/pki\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421797 5492 flags.go:33] FLAG: --cgroup-driver=\"cgroupfs\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421800 5492 flags.go:33] FLAG: --cgroup-root=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421803 5492 flags.go:33] FLAG: --cgroups-per-qos=\"true\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421806 5492 flags.go:33] FLAG: --chaos-chance=\"0\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421811 5492 flags.go:33] FLAG: --client-ca-file=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421814 5492 flags.go:33] FLAG: --cloud-config=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421817 5492 flags.go:33] FLAG: --cloud-provider=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421820 5492 flags.go:33] FLAG: --cluster-dns=\"[]\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421824 5492 flags.go:33] FLAG: --cluster-domain=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421828 5492 flags.go:33] FLAG: --cni-bin-dir=\"/opt/cni/bin\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421831 5492 flags.go:33] FLAG: --cni-cache-dir=\"/var/lib/cni/cache\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421834 5492 flags.go:33] FLAG: --cni-conf-dir=\"/etc/cni/net.d\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421838 5492 flags.go:33] FLAG: --config=\"/etc/kubernetes/kubelet.conf\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421842 5492 flags.go:33] FLAG: --container-hints=\"/etc/cadvisor/container_hints.json\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421845 5492 flags.go:33] FLAG: --container-log-max-files=\"5\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421849 5492 flags.go:33] FLAG: --container-log-max-size=\"10Mi\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421852 5492 flags.go:33] FLAG: --container-runtime=\"remote\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421865 5492 flags.go:33] FLAG: --container-runtime-endpoint=\"/var/run/crio/crio.sock\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421868 5492 flags.go:33] FLAG: --containerd=\"/run/containerd/containerd.sock\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421872 5492 flags.go:33] FLAG: --contention-profiling=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421876 5492 flags.go:33] FLAG: --cpu-cfs-quota=\"true\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421879 5492 flags.go:33] FLAG: --cpu-cfs-quota-period=\"100ms\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421883 5492 flags.go:33] FLAG: --cpu-manager-policy=\"none\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421886 5492 flags.go:33] FLAG: --cpu-manager-reconcile-period=\"10s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421889 5492 flags.go:33] FLAG: --docker=\"unix:///var/run/docker.sock\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421893 5492 flags.go:33] FLAG: --docker-endpoint=\"unix:///var/run/docker.sock\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421896 5492 flags.go:33] FLAG: --docker-env-metadata-whitelist=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421899 5492 flags.go:33] FLAG: --docker-only=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421902 5492 flags.go:33] FLAG: --docker-root=\"/var/lib/docker\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421905 5492 flags.go:33] FLAG: --docker-tls=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421908 5492 flags.go:33] FLAG: --docker-tls-ca=\"ca.pem\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421911 5492 flags.go:33] FLAG: --docker-tls-cert=\"cert.pem\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421914 5492 flags.go:33] FLAG: --docker-tls-key=\"key.pem\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421917 5492 flags.go:33] FLAG: --dynamic-config-dir=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421921 5492 flags.go:33] FLAG: --enable-cadvisor-json-endpoints=\"true\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421924 5492 flags.go:33] FLAG: --enable-controller-attach-detach=\"true\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421929 5492 flags.go:33] FLAG: --enable-debugging-handlers=\"true\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421932 5492 flags.go:33] FLAG: --enable-load-reader=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421951 5492 flags.go:33] FLAG: --enable-server=\"true\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421954 5492 flags.go:33] FLAG: --enforce-node-allocatable=\"[pods]\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421958 5492 flags.go:33] FLAG: --event-burst=\"10\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421961 5492 flags.go:33] FLAG: --event-qps=\"5\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421963 5492 flags.go:33] FLAG: --event-storage-age-limit=\"default=0\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421966 5492 flags.go:33] FLAG: --event-storage-event-limit=\"default=0\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421969 5492 flags.go:33] FLAG: --eviction-hard=\"imagefs.available<15%,memory.available<100Mi,nodefs.available<10%,nodefs.inodesFree<5%\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421979 5492 flags.go:33] FLAG: --eviction-max-pod-grace-period=\"0\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421982 5492 flags.go:33] FLAG: --eviction-minimum-reclaim=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421986 5492 flags.go:33] FLAG: --eviction-pressure-transition-period=\"5m0s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.421990 5492 flags.go:33] FLAG: --eviction-soft=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422000 5492 flags.go:33] FLAG: --eviction-soft-grace-period=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422003 5492 flags.go:33] FLAG: --exit-on-lock-contention=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422006 5492 flags.go:33] FLAG: --experimental-allocatable-ignore-eviction=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422009 5492 flags.go:33] FLAG: --experimental-bootstrap-kubeconfig=\"/etc/kubernetes/kubeconfig\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422012 5492 flags.go:33] FLAG: --experimental-check-node-capabilities-before-mount=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422015 5492 flags.go:33] FLAG: --experimental-dockershim=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422018 5492 flags.go:33] FLAG: --experimental-dockershim-root-directory=\"/var/lib/dockershim\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422021 5492 flags.go:33] FLAG: --experimental-kernel-memcg-notification=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422024 5492 flags.go:33] FLAG: --experimental-mounter-path=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422027 5492 flags.go:33] FLAG: --fail-swap-on=\"true\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422030 5492 flags.go:33] FLAG: --feature-gates=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422034 5492 flags.go:33] FLAG: --file-check-frequency=\"20s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422037 5492 flags.go:33] FLAG: --global-housekeeping-interval=\"1m0s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422040 5492 flags.go:33] FLAG: --hairpin-mode=\"promiscuous-bridge\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422043 5492 flags.go:33] FLAG: --healthz-bind-address=\"127.0.0.1\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422046 5492 flags.go:33] FLAG: --healthz-port=\"10248\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422049 5492 flags.go:33] FLAG: --help=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422052 5492 flags.go:33] FLAG: --hostname-override=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422055 5492 flags.go:33] FLAG: --housekeeping-interval=\"10s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422058 5492 flags.go:33] FLAG: --http-check-frequency=\"20s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422060 5492 flags.go:33] FLAG: --image-gc-high-threshold=\"85\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422063 5492 flags.go:33] FLAG: --image-gc-low-threshold=\"80\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422066 5492 flags.go:33] FLAG: --image-pull-progress-deadline=\"1m0s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422069 5492 flags.go:33] FLAG: --image-service-endpoint=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422072 5492 flags.go:33] FLAG: --iptables-drop-bit=\"15\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422075 5492 flags.go:33] FLAG: --iptables-masquerade-bit=\"14\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422078 5492 flags.go:33] FLAG: --keep-terminated-pod-volumes=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422081 5492 flags.go:33] FLAG: --kube-api-burst=\"10\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422084 5492 flags.go:33] FLAG: --kube-api-content-type=\"application/vnd.kubernetes.protobuf\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422087 5492 flags.go:33] FLAG: --kube-api-qps=\"5\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422090 5492 flags.go:33] FLAG: --kube-reserved=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422093 5492 flags.go:33] FLAG: --kube-reserved-cgroup=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422103 5492 flags.go:33] FLAG: --kubeconfig=\"/var/lib/kubelet/kubeconfig\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422106 5492 flags.go:33] FLAG: --kubelet-cgroups=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422109 5492 flags.go:33] FLAG: --lock-file=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422112 5492 flags.go:33] FLAG: --log-backtrace-at=\":0\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422115 5492 flags.go:33] FLAG: --log-cadvisor-usage=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422118 5492 flags.go:33] FLAG: --log-dir=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422121 5492 flags.go:33] FLAG: --log-file=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422124 5492 flags.go:33] FLAG: --log-file-max-size=\"1800\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422127 5492 flags.go:33] FLAG: --log-flush-frequency=\"5s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422130 5492 flags.go:33] FLAG: --logtostderr=\"true\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422133 5492 flags.go:33] FLAG: --machine-id-file=\"/etc/machine-id,/var/lib/dbus/machine-id\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422136 5492 flags.go:33] FLAG: --make-iptables-util-chains=\"true\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422139 5492 flags.go:33] FLAG: --manifest-url=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422142 5492 flags.go:33] FLAG: --manifest-url-header=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422146 5492 flags.go:33] FLAG: --master-service-namespace=\"default\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422149 5492 flags.go:33] FLAG: --max-open-files=\"1000000\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422153 5492 flags.go:33] FLAG: --max-pods=\"110\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422156 5492 flags.go:33] FLAG: --maximum-dead-containers=\"-1\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422159 5492 flags.go:33] FLAG: --maximum-dead-containers-per-container=\"1\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422162 5492 flags.go:33] FLAG: --minimum-container-ttl-duration=\"6m0s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422165 5492 flags.go:33] FLAG: --minimum-image-ttl-duration=\"2m0s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422168 5492 flags.go:33] FLAG: --network-plugin=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422170 5492 flags.go:33] FLAG: --network-plugin-mtu=\"0\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422173 5492 flags.go:33] FLAG: --node-ip=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422176 5492 flags.go:33] FLAG: --node-labels=\"node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422182 5492 flags.go:33] FLAG: --node-status-max-images=\"50\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422185 5492 flags.go:33] FLAG: --node-status-update-frequency=\"10s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422188 5492 flags.go:33] FLAG: --non-masquerade-cidr=\"10.0.0.0/8\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422191 5492 flags.go:33] FLAG: --oom-score-adj=\"-999\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422194 5492 flags.go:33] FLAG: --pod-cidr=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422196 5492 flags.go:33] FLAG: --pod-infra-container-image=\"k8s.gcr.io/pause:3.1\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422200 5492 flags.go:33] FLAG: --pod-manifest-path=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422211 5492 flags.go:33] FLAG: --pod-max-pids=\"-1\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422214 5492 flags.go:33] FLAG: --pods-per-core=\"0\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422217 5492 flags.go:33] FLAG: --port=\"10250\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422219 5492 flags.go:33] FLAG: --protect-kernel-defaults=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422222 5492 flags.go:33] FLAG: --provider-id=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422225 5492 flags.go:33] FLAG: --qos-reserved=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422228 5492 flags.go:33] FLAG: --read-only-port=\"10255\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422231 5492 flags.go:33] FLAG: --really-crash-for-testing=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422234 5492 flags.go:33] FLAG: --redirect-container-streaming=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422237 5492 flags.go:33] FLAG: --register-node=\"true\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422240 5492 flags.go:33] FLAG: --register-schedulable=\"true\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422243 5492 flags.go:33] FLAG: --register-with-taints=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422246 5492 flags.go:33] FLAG: --registry-burst=\"10\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422249 5492 flags.go:33] FLAG: --registry-qps=\"5\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422252 5492 flags.go:33] FLAG: --reserved-cpus=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422255 5492 flags.go:33] FLAG: --resolv-conf=\"/etc/resolv.conf\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422258 5492 flags.go:33] FLAG: --root-dir=\"/var/lib/kubelet\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422261 5492 flags.go:33] FLAG: --rotate-certificates=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422264 5492 flags.go:33] FLAG: --rotate-server-certificates=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422266 5492 flags.go:33] FLAG: --runonce=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422269 5492 flags.go:33] FLAG: --runtime-cgroups=\"/system.slice/crio.service\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422272 5492 flags.go:33] FLAG: --runtime-request-timeout=\"2m0s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422275 5492 flags.go:33] FLAG: --seccomp-profile-root=\"/var/lib/kubelet/seccomp\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422278 5492 flags.go:33] FLAG: --serialize-image-pulls=\"true\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422281 5492 flags.go:33] FLAG: --skip-headers=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422284 5492 flags.go:33] FLAG: --skip-log-headers=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422287 5492 flags.go:33] FLAG: --stderrthreshold=\"2\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422290 5492 flags.go:33] FLAG: --storage-driver-buffer-duration=\"1m0s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422293 5492 flags.go:33] FLAG: --storage-driver-db=\"cadvisor\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422296 5492 flags.go:33] FLAG: --storage-driver-host=\"localhost:8086\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422299 5492 flags.go:33] FLAG: --storage-driver-password=\"root\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422301 5492 flags.go:33] FLAG: --storage-driver-secure=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422312 5492 flags.go:33] FLAG: --storage-driver-table=\"stats\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422315 5492 flags.go:33] FLAG: --storage-driver-user=\"root\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422318 5492 flags.go:33] FLAG: --streaming-connection-idle-timeout=\"4h0m0s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422321 5492 flags.go:33] FLAG: --sync-frequency=\"1m0s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422324 5492 flags.go:33] FLAG: --system-cgroups=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422326 5492 flags.go:33] FLAG: --system-reserved=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422329 5492 flags.go:33] FLAG: --system-reserved-cgroup=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422333 5492 flags.go:33] FLAG: --tls-cert-file=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422336 5492 flags.go:33] FLAG: --tls-cipher-suites=\"[]\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422340 5492 flags.go:33] FLAG: --tls-min-version=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422343 5492 flags.go:33] FLAG: --tls-private-key-file=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422345 5492 flags.go:33] FLAG: --topology-manager-policy=\"none\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422349 5492 flags.go:33] FLAG: --v=\"3\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422352 5492 flags.go:33] FLAG: --version=\"false\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422361 5492 flags.go:33] FLAG: --vmodule=\"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422365 5492 flags.go:33] FLAG: --volume-plugin-dir=\"/etc/kubernetes/kubelet-plugins/volume/exec\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422369 5492 flags.go:33] FLAG: --volume-stats-agg-period=\"1m0s\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.422394 5492 feature_gate.go:244] feature gates: &{map[]} Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: Flag --minimum-container-ttl-duration has been deprecated, Use --eviction-hard or --eviction-soft instead. Will be removed in a future version. Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.423859 5492 feature_gate.go:244] feature gates: &{map[LegacyNodeRoleBehavior:false NodeDisruptionExclusion:true RotateKubeletServerCertificate:true SCTPSupport:true ServiceNodeExclusion:true SupportPodPidsLimit:true]} Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.423940 5492 feature_gate.go:244] feature gates: &{map[LegacyNodeRoleBehavior:false NodeDisruptionExclusion:true RotateKubeletServerCertificate:true SCTPSupport:true ServiceNodeExclusion:true SupportPodPidsLimit:true]} Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.433150 5492 mount_linux.go:168] Detected OS with systemd Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.433271 5492 server.go:424] Version: v1.17.1 Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.433323 5492 feature_gate.go:244] feature gates: &{map[LegacyNodeRoleBehavior:false NodeDisruptionExclusion:true RotateKubeletServerCertificate:true SCTPSupport:true ServiceNodeExclusion:true SupportPodPidsLimit:true]} Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.433385 5492 feature_gate.go:244] feature gates: &{map[LegacyNodeRoleBehavior:false NodeDisruptionExclusion:true RotateKubeletServerCertificate:true SCTPSupport:true ServiceNodeExclusion:true SupportPodPidsLimit:true]} Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.433491 5492 plugins.go:100] No cloud provider specified. Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.433510 5492 server.go:540] No cloud provider specified: \"\" from the config file: \"\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.433519 5492 server.go:830] Client rotation is on, will bootstrap in background Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.445886 5492 bootstrap.go:84] Current kubeconfig file contents are still valid, no bootstrap necessary Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.445957 5492 certificate_store.go:129] Loading cert/key pair from \"/var/lib/kubelet/pki/kubelet-client-current.pem\". Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.446355 5492 server.go:857] Starting client certificate rotation. Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.446367 5492 certificate_manager.go:285] Certificate rotation is enabled. Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.446467 5492 certificate_manager.go:556] Certificate expiration is 2020-07-26 09:59:31 +0000 UTC, rotation deadline is 2020-07-20 19:25:44.002114354 +0000 UTC Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.446497 5492 certificate_manager.go:291] Waiting 576h20m33.555623075s for next certificate rotation Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.447060 5492 dynamic_cafile_content.go:129] Loaded a new CA Bundle and Verifier for \"client-ca-bundle::/etc/kubernetes/kubelet-ca.crt\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.447171 5492 dynamic_cafile_content.go:167] Starting client-ca-bundle::/etc/kubernetes/kubelet-ca.crt Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.447357 5492 manager.go:146] cAdvisor running in container: \"/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service\" Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.448446 5492 fs.go:125] Filesystem UUIDs: map[00000000-0000-4000-a000-000000000002:/dev/sda4 1ccc3ac6-b59d-46bb-9850-229cd8b4e007:/dev/dm-0 676b3306-2850-478c-a817-3f723d49377d:/dev/sda1 D802-CD71:/dev/sda2] Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.448470 5492 fs.go:126] Filesystem partitions: map[/dev/mapper/coreos-luks-root-nocrypt:{mountpoint:/var major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:ext4 blockSize:0} /dev/shm:{mountpoint:/dev/shm major:0 minor:22 fsType:tmpfs blockSize:0} /run:{mountpoint:/run major:0 minor:24 fsType:tmpfs blockSize:0} /run/user/1000:{mountpoint:/run/user/1000 major:0 minor:44 fsType:tmpfs blockSize:0} /sys/fs/cgroup:{mountpoint:/sys/fs/cgroup major:0 minor:25 fsType:tmpfs blockSize:0}] Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.453331 5492 manager.go:193] Machine: {NumCores:40 CpuFrequency:3100000 MemoryCapacity:135090417664 HugePages:[{PageSize:1048576 NumPages:0} {PageSize:2048 NumPages:0}] MachineID:21668e85e1264ac78ea115b2fe79408e SystemUUID:4c4c4544-004b-5a10-8050-cac04f484832 BootID:16f2699c-8a76-444a-8f56-e32057152db4 Filesystems:[{Device:/run/user/1000 DeviceMajor:0 DeviceMinor:44 Capacity:13509038080 Type:vfs Inodes:16490529 HasInodes:true} {Device:/dev/shm DeviceMajor:0 DeviceMinor:22 Capacity:67545206784 Type:vfs Inodes:16490529 HasInodes:true} {Device:/run DeviceMajor:0 DeviceMinor:24 Capacity:67545206784 Type:vfs Inodes:16490529 HasInodes:true} {Device:/sys/fs/cgroup DeviceMajor:0 DeviceMinor:25 Capacity:67545206784 Type:vfs Inodes:16490529 HasInodes:true} {Device:/dev/mapper/coreos-luks-root-nocrypt DeviceMajor:253 DeviceMinor:0 Capacity:598985388032 Type:vfs Inodes:292478400 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:381549568 Type:vfs Inodes:98304 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:598995877376 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:599550590976 Scheduler:mq-deadline}] NetworkDevices:[{Name:eno1 MacAddress:18:66:da:9f:b1:0a Speed:1000 Mtu:1500} {Name:eno2 MacAddress:18:66:da:9f:b1:0b Speed:-1 Mtu:1500} {Name:eno3 MacAddress:18:66:da:9f:b1:0c Speed:-1 Mtu:1500} {Name:eno4 MacAddress:18:66:da:9f:b1:0d Speed:1000 Mtu:1500} {Name:enp4s0f0 MacAddress:a0:36:9f:e5:e9:fc Speed:10000 Mtu:1500} {Name:enp4s0f1 MacAddress:a0:36:9f:e5:e9:fe Speed:10000 Mtu:1500} {Name:enp5s0f0 MacAddress:a0:36:9f:e5:e2:a8 Speed:10000 Mtu:1500} {Name:enp5s0f1 MacAddress:a0:36:9f:e5:e2:aa Speed:10000 Mtu:1500}] Topology:[{Id:0 Memory:67476070400 HugePages:[{PageSize:1048576 NumPages:0} {PageSize:2048 NumPages:0}] Cores:[{Id:0 Threads:[0 20] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:1 Threads:[2 22] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:2 Threads:[4 24] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:3 Threads:[6 26] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:4 Threads:[8 28] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:8 Threads:[10 30] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:9 Threads:[12 32] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:10 Threads:[14 34] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:11 Threads:[16 36] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:12 Threads:[18 38] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:26214400 Type:Unified Level:3}]} {Id:1 Memory:67614347264 HugePages:[{PageSize:1048576 NumPages:0} {PageSize:2048 NumPages:0}] Cores:[{Id:0 Threads:[1 21] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:1 Threads:[3 23] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:2 Threads:[5 25] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:3 Threads:[7 27] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:4 Threads:[9 29] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:8 Threads:[11 31] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:9 Threads:[13 33] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:10 Threads:[15 35] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:11 Threads:[17 37] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]} {Id:12 Threads:[19 39] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Instruction Level:1} {Size:262144 Type:Unified Level:2}]}] Caches:[{Size:26214400 Type:Unified Level:3}]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None} Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.453643 5492 manager.go:199] Version: {KernelVersion:4.18.0-147.8.1.el8_1.x86_64 ContainerOsVersion:Red Hat Enterprise Linux CoreOS 44.81.202005250830-0 (Ootpa) DockerVersion:Unknown DockerAPIVersion:Unknown CadvisorVersion: CadvisorRevision:} Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.454069 5492 container_manager_linux.go:265] container manager verified user specified cgroup-root exists: [] Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.454102 5492 container_manager_linux.go:270] Creating Container Manager object based on Node Config: {RuntimeCgroupsName:/system.slice/crio.service SystemCgroupsName:/system.slice KubeletCgroupsName: ContainerRuntime:remote CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:systemd KubeletRootDir:/var/lib/kubelet ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: ReservedSystemCPUs: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[cpu:{i:{value:0 scale:0} d:{Dec:<nil>} s:0 Format:DecimalSI} ephemeral-storage:{i:{value:10737418240 scale:0} d:{Dec:<nil>} s:10Gi Format:BinarySI} memory:{i:{value:1073741824 scale:0} d:{Dec:<nil>} s:1Gi Format:BinarySI}] HardEvictionThresholds:[{Signal:memory.available Operator:LessThan Value:{Quantity:100Mi Percentage:0} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.1} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.inodesFree Operator:LessThan Value:{Quantity:<nil> Percentage:0.05} GracePeriod:0s MinReclaim:<nil>} {Signal:imagefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.15} GracePeriod:0s MinReclaim:<nil>}]} QOSReserved:map[] ExperimentalCPUManagerPolicy:static ExperimentalCPUManagerReconcilePeriod:5s ExperimentalPodPidsLimit:-1 EnforceCPULimits:true CPUCFSQuotaPeriod:100ms ExperimentalTopologyManagerPolicy:none} Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.454183 5492 fake_topology_manager.go:29] [fake topologymanager] NewFakeManager Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.454189 5492 container_manager_linux.go:305] Creating device plugin manager: true Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.454201 5492 manager.go:126] Creating Device Plugin manager at /var/lib/kubelet/device-plugins/kubelet.sock Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.454215 5492 fake_topology_manager.go:39] [fake topologymanager] AddHintProvider HintProvider: &{kubelet.sock /var/lib/kubelet/device-plugins/ map[] {0 0} <nil> {{} [0 0 0]} 0x1b64c30 0x72dc9c8 0x1b65500 map[] map[] map[] map[] map[] 0xc000853ce0 [0 1] 0x72dc9c8} Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: I0626 19:05:10.454252 5492 cpu_manager.go:131] [cpumanager] detected CPU topology: &{40 20 2 map[0:{0 0 0} 1:{1 1 1} 2:{0 0 2} 3:{1 1 3} 4:{0 0 4} 5:{1 1 5} 6:{0 0 6} 7:{1 1 7} 8:{0 0 8} 9:{1 1 9} 10:{0 0 10} 11:{1 1 11} 12:{0 0 12} 13:{1 1 13} 14:{0 0 14} 15:{1 1 15} 16:{0 0 16} 17:{1 1 17} 18:{0 0 18} 19:{1 1 19} 20:{0 0 0} 21:{1 1 1} 22:{0 0 2} 23:{1 1 3} 24:{0 0 4} 25:{1 1 5} 26:{0 0 6} 27:{1 1 7} 28:{0 0 8} 29:{1 1 9} 30:{0 0 10} 31:{1 1 11} 32:{0 0 12} 33:{1 1 13} 34:{0 0 14} 35:{1 1 15} 36:{0 0 16} 37:{1 1 17} 38:{0 0 18} 39:{1 1 19}]} Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: E0626 19:05:10.454300 5492 container_manager_linux.go:329] failed to initialize cpu manager: [cpumanager] unable to determine reserved CPU resources for static policy Jun 26 19:05:10 openshift-worker-1.example.com hyperkube[5492]: F0626 19:05:10.454309 5492 server.go:273] failed to run Kubelet: [cpumanager] unable to determine reserved CPU resources for static policy Jun 26 19:05:10 openshift-worker-1.example.com systemd[1]: kubelet.service: Main process exited, code=exited, status=255/n/a Jun 26 19:05:10 openshift-worker-1.example.com systemd[1]: kubelet.service: Failed with result 'exit-code'. Jun 26 19:05:10 openshift-worker-1.example.com systemd[1]: Failed to start Kubernetes Kubelet. Jun 26 19:05:10 openshift-worker-1.example.com systemd[1]: kubelet.service: Consumed 117ms CPU time And the node will never become Ready in the node list.","title":"Reserving 0 CPUs for the host"},{"location":"openshift/cpu-manager-with-custom-machine-config-pool/#references","text":"https://www.redhat.com/en/blog/openshift-container-platform-4-how-does-machine-config-pool-work https://docs.openshift.com/container-platform/4.4/scalability_and_performance/using-cpu-manager.html https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/","title":"References"},{"location":"openshift/crio-conmon-runc/","text":"Crio vs conmon vs runc https://insujang.github.io/2019-11-18/interactions-between-crio-and-conmon/#how-conmon-is-used-in-cri-o","title":"Crio vs conmon vs runc"},{"location":"openshift/crio-conmon-runc/#crio-vs-conmon-vs-runc","text":"https://insujang.github.io/2019-11-18/interactions-between-crio-and-conmon/#how-conmon-is-used-in-cri-o","title":"Crio vs conmon vs runc"},{"location":"openshift/ingress-controller-sharding-on-separate-vip/","text":"Ingress Controller Sharding on separate VIP It is important to know that each node can only host one ingress router with HostNetwork networking. You will need a specific role for each INGRESS type and different INGRESS types cannot be hosted on the same worker node. This is due to the particularities of the HostNetwork network type. The following example uses the OpenStack IPI. Step 1: Scale out by 2 more worker nodes Scale out the worker role. This step might have to be improved. [cloud-user@akaris-jump-server cluster-logging]$ oc scale -n openshift-machine-api --replicas=5 machineset akaris-osc-6klvk-worker machineset.machine.openshift.io/akaris-osc-6klvk-worker scaled [cloud-user@akaris-jump-server cluster-logging]$ [cloud-user@akaris-jump-server openshift]$ oc get nodes NAME STATUS ROLES AGE VERSION akaris-osc-6klvk-master-0 Ready master 5h18m v1.18.3+012b3ec akaris-osc-6klvk-master-1 Ready master 5h18m v1.18.3+012b3ec akaris-osc-6klvk-master-2 Ready master 5h19m v1.18.3+012b3ec akaris-osc-6klvk-worker-g4q6z Ready worker 5h4m v1.18.3+012b3ec akaris-osc-6klvk-worker-hp2hz Ready worker 2m12s v1.18.3+012b3ec akaris-osc-6klvk-worker-l726s Ready worker 5h1m v1.18.3+012b3ec akaris-osc-6klvk-worker-lp2df Ready worker 5h6m v1.18.3+012b3ec akaris-osc-6klvk-worker-zhs67 Ready worker 2m57s v1.18.3+012b3ec Step 2: Add additional interface to additional nodes This step will be executed within the underlying infrastructure provider. In this example case, OpenShift is hosted on top of OpenStack. Make sure that the subnet runs a DHCP server: [cloud-user@akaris-jump-server openshift]$ openstack subnet set --dhcp 41c073c0-5b6a-4639-9023-115ce3d0e378 [cloud-user@akaris-jump-server openshift]$ openstack subnet show 41c073c0-5b6a-4639-9023-115ce3d0e378 +-------------------+--------------------------------------+ | Field | Value | +-------------------+--------------------------------------+ | allocation_pools | 172.31.254.2-172.31.254.254 | | cidr | 172.31.254.0/24 | | created_at | 2020-01-15T17:13:39Z | | description | | | dns_nameservers | | | enable_dhcp | True | | gateway_ip | 172.31.254.1 | | host_routes | | | id | 41c073c0-5b6a-4639-9023-115ce3d0e378 | | ip_version | 4 | | ipv6_address_mode | None | | ipv6_ra_mode | None | | name | akaris-backend-subnet | | network_id | aa68c208-b6c9-402e-b508-1b9d82f3baa4 | | prefix_length | None | | project_id | ea78dddee3e240c7af1779b417c64618 | | revision_number | 1 | | segment_id | None | | service_types | | | subnetpool_id | None | | tags | | | updated_at | 2020-08-11T15:19:17Z | +-------------------+--------------------------------------+ And create a router that can host floating IPs: openstack router create akaris-backend openstack router set --external-gateway 5cd089f9-8ed2-46bc-8ea7-4e1cdb5262ba akaris-backend openstack router add subnet akaris-backend akaris-backend-subnet [cloud-user@akaris-jump-server openshift]$ openstack port create --network aa68c208-b6c9-402e-b508-1b9d82f3baa4 akaris-osc-6klvk-worker-hp2hz-akaris-backend +-----------------------+------------------------------------------------------------------------------+ | Field | Value | +-----------------------+------------------------------------------------------------------------------+ | admin_state_up | UP | | allowed_address_pairs | | | binding_host_id | None | | binding_profile | None | | binding_vif_details | None | | binding_vif_type | None | | binding_vnic_type | normal | | created_at | 2020-08-11T14:47:17Z | | data_plane_status | None | | description | | | device_id | | | device_owner | | | dns_assignment | None | | dns_name | None | | extra_dhcp_opts | | | fixed_ips | ip_address='172.31.254.48', subnet_id='41c073c0-5b6a-4639-9023-115ce3d0e378' | | id | 8e737c31-0450-40d0-a5e0-54871a735428 | | ip_address | None | | mac_address | fa:16:3e:07:66:12 | | name | akaris-osc-6klvk-worker-hp2hz-akaris-backend | | network_id | aa68c208-b6c9-402e-b508-1b9d82f3baa4 | | option_name | None | | option_value | None | | port_security_enabled | True | | project_id | ea78dddee3e240c7af1779b417c64618 | | qos_policy_id | None | | revision_number | 6 | | security_group_ids | ae5a722f-aeea-4acd-8260-a04f5dceb624 | | status | DOWN | | subnet_id | None | | tags | | | trunk_details | None | | updated_at | 2020-08-11T14:47:17Z | +-----------------------+------------------------------------------------------------------------------+ [cloud-user@akaris-jump-server openshift]$ openstack port create --network aa68c208-b6c9-402e-b508-1b9d82f3baa4 akaris-osc-6klvk-worker-zhs67-akaris-backend +-----------------------+-------------------------------------------------------------------------------+ | Field | Value | +-----------------------+-------------------------------------------------------------------------------+ | admin_state_up | UP | | allowed_address_pairs | | | binding_host_id | None | | binding_profile | None | | binding_vif_details | None | | binding_vif_type | None | | binding_vnic_type | normal | | created_at | 2020-08-11T14:47:40Z | | data_plane_status | None | | description | | | device_id | | | device_owner | | | dns_assignment | None | | dns_name | None | | extra_dhcp_opts | | | fixed_ips | ip_address='172.31.254.207', subnet_id='41c073c0-5b6a-4639-9023-115ce3d0e378' | | id | c82401f3-fa76-449e-ba16-75fe4b0f5583 | | ip_address | None | | mac_address | fa:16:3e:66:d2:28 | | name | akaris-osc-6klvk-worker-zhs67-akaris-backend | | network_id | aa68c208-b6c9-402e-b508-1b9d82f3baa4 | | option_name | None | | option_value | None | | port_security_enabled | True | | project_id | ea78dddee3e240c7af1779b417c64618 | | qos_policy_id | None | | revision_number | 6 | | security_group_ids | ae5a722f-aeea-4acd-8260-a04f5dceb624 | | status | DOWN | | subnet_id | None | | tags | | | trunk_details | None | | updated_at | 2020-08-11T14:47:40Z | +-----------------------+-------------------------------------------------------------------------------+ [cloud-user@akaris-jump-server openshift]$ openstack port create --network aa68c208-b6c9-402e-b508-1b9d82f3baa4 vip-akaris-backend +-----------------------+-------------------------------------------------------------------------------+ | Field | Value | +-----------------------+-------------------------------------------------------------------------------+ | admin_state_up | UP | | allowed_address_pairs | | | binding_host_id | None | | binding_profile | None | | binding_vif_details | None | | binding_vif_type | None | | binding_vnic_type | normal | | created_at | 2020-08-11T18:32:53Z | | data_plane_status | None | | description | | | device_id | | | device_owner | | | dns_assignment | None | | dns_name | None | | extra_dhcp_opts | | | fixed_ips | ip_address='172.31.254.118', subnet_id='41c073c0-5b6a-4639-9023-115ce3d0e378' | | id | 4d0b32f5-203b-4e2b-b92b-b25f349b1d83 | | ip_address | None | | mac_address | fa:16:3e:0d:3e:cd | | name | vip-akaris-backend | | network_id | aa68c208-b6c9-402e-b508-1b9d82f3baa4 | | option_name | None | | option_value | None | | port_security_enabled | True | | project_id | ea78dddee3e240c7af1779b417c64618 | | qos_policy_id | None | | revision_number | 6 | | security_group_ids | ae5a722f-aeea-4acd-8260-a04f5dceb624 | | status | DOWN | | subnet_id | None | | tags | | | trunk_details | None | | updated_at | 2020-08-11T18:32:53Z | +-----------------------+-------------------------------------------------------------------------------+ [cloud-user@akaris-jump-server openshift]$ openstack server add port akaris-osc-6klvk-worker-hp2hz akaris-osc-6klvk-worker-hp2hz-akaris-backend [cloud-user@akaris-jump-server openshift]$ openstack server add port akaris-osc-6klvk-worker-zhs67 akaris-osc-6klvk-worker-zhs67-akaris-backend [cloud-user@akaris-jump-server openshift]$ openstack floating ip set --port akaris-osc-6klvk-worker-hp2hz-akaris-backend 10.0.92.194 [cloud-user@akaris-jump-server openshift]$ openstack floating ip set --port akaris-osc-6klvk-worker-zhs67-akaris-backend 10.0.92.205 [cloud-user@akaris-jump-server openshift]$ openstack floating ip set --port vip-akaris-backend 10.0.89.89 After this, the floating IPs look like this in OpenStack: [cloud-user@akaris-jump-server openshift]$ openstack floating ip list | grep 172.31.254 | dd055cc4-af00-4dbd-8e4a-94fac680299d | 10.0.89.89 | 172.31.254.118 | 4d0b32f5-203b-4e2b-b92b-b25f349b1d83 | 5cd089f9-8ed2-46bc-8ea7-4e1cdb5262ba | ea78dddee3e240c7af1779b417c64618 | | e586c5c8-1cd4-4bd5-adb7-790e895b5af8 | 10.0.92.194 | 172.31.254.48 | 8e737c31-0450-40d0-a5e0-54871a735428 | 5cd089f9-8ed2-46bc-8ea7-4e1cdb5262ba | ea78dddee3e240c7af1779b417c64618 | | f7e082f1-7880-4f07-8a05-3fdb0585eb32 | 10.0.92.205 | 172.31.254.207 | c82401f3-fa76-449e-ba16-75fe4b0f5583 | 5cd089f9-8ed2-46bc-8ea7-4e1cdb5262ba | ea78dddee3e240c7af1779b417c64618 | Last but not least, disable port security for these ports so that VRRP can work with these OpenStack ports: for port in akaris-osc-6klvk-worker-hp2hz-akaris-backend akaris-osc-6klvk-worker-zhs67-akaris-backend vip-akaris-backend ; do echo === $port === openstack port set --no-security-group $port openstack port set --disable-port-security $port done The VRRP floating IP is: IP 172.31.254.118 MAC address: fa:16:3e:0d:3e:cd VIP: 10.0.89.89 Verify that the port was added to the workers: [cloud-user@akaris-jump-server openshift]$ oc debug node/akaris-osc-6klvk-worker-hp2hz Starting pod/akaris-osc-6klvk-worker-hp2hz-debug ... To use host binaries, run `chroot /host` Pod IP: 192.168.1.160 If you don't see a command prompt, try pressing enter. sh-4.2# chroot /host sh-4.4# toolbox Trying to pull registry.redhat.io/rhel8/support-tools... Getting image source signatures Copying blob 77c58f19bd6e done Copying blob 47db82df7f3f done Copying blob cdc5441bd52d done Copying config 5ef2aab094 done Writing manifest to image destination Storing signatures 5ef2aab094514cc5561fa94b0bb52d75379ecf8a36355e891017f3982bac278c Spawning a container 'toolbox-' with image 'registry.redhat.io/rhel8/support-tools' Detected RUN label in the container image. Using that as the default... command: podman run -it --name toolbox- --privileged --ipc=host --net=host --pid=host -e HOST=/host -e NAME=toolbox- -e IMAGE=registry.redhat.io/rhel8/support-tools:latest -v /run:/run -v /var/log:/var/log -v /etc/machine-id:/etc/machine-id -v /etc/localtime:/etc/localtime -v /:/host registry.redhat.io/rhel8/support-tools:latest [root@akaris-osc-6klvk-worker-hp2hz /]# yum install iproute -y (...) Installed: iproute-5.3.0-1.el8.x86_64 libmnl-1.0.4-6.el8.x86_64 Complete! [root@akaris-osc-6klvk-worker-hp2hz /]# dmesg | tail [ 745.078322] pci 0000:00:06.0: BAR 0: assigned [io 0x1000-0x103f] [ 745.080319] virtio-pci 0000:00:06.0: enabling device (0000 -> 0003) [ 745.100662] PCI Interrupt Link [LNKB] enabled at IRQ 11 [ 745.109702] virtio_net virtio3 ens6: renamed from eth0 [ 745.156833] IPv6: ADDRCONF(NETDEV_UP): ens6: link is not ready [ 745.159290] IPv6: ADDRCONF(NETDEV_UP): ens6: link is not ready [ 745.165677] IPv6: ADDRCONF(NETDEV_UP): ens6: link is not ready [ 745.172694] IPv6: ADDRCONF(NETDEV_UP): ens6: link is not ready [ 746.143079] IPv6: ADDRCONF(NETDEV_CHANGE): ens6: link becomes ready [ 861.279527] SELinux: mount invalid. Same superblock, different security settings for (dev mqueue, type mqueue) [root@akaris-osc-6klvk-worker-hp2hz /]# ip link ls dev ens6 10: ens6: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc fq_codel state UP mode DEFAULT group default qlen 1000 link/ether fa:16:3e:07:66:12 brd ff:ff:ff:ff:ff:ff Repeat the same on the other node: [cloud-user@akaris-jump-server openshift]$ oc debug node/akaris-osc-6klvk-worker-zhs67 (...) Step 3: create new role and assign new nodes to the role Follow https://www.redhat.com/en/blog/openshift-container-platform-4-how-does-machine-config-pool-work with modifications . Run this via a script that's triggered by a systemd unit file: cat <<'EOF' | oc apply -f - apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfigPool metadata: name: staging spec: machineConfigSelector: matchExpressions: - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,staging]} nodeSelector: matchLabels: node-role.kubernetes.io/staging: \"\" paused: false EOF cat <<'FOE' > network-script.sh #!/bin/bash cat <<'EOF' > /etc/sysconfig/network-scripts/ifcfg-ens6 NAME=\"System ens6\" BOOTPROTO=dhcp DEFROUTE=no IPV6INIT=no DEVICE=ens6 ONBOOT=yes EOF # none of the following will work given that we're using DHCP # cat <<'EOF' > /etc/sysconfig/network-scripts/rule-ens6 # from 172.31.254.0/24 lookup 5000 # EOF # cat <<'EOF' > /etc/sysconfig/network-scripts/route-ens6 # default via 172.31.254.1 table 5000 # EOF # the following will not work either # ip rule add from 172.31.254.0/24 lookup 5000 # ip route add default via 172.31.254.1 table 5000 cat <<'EOF' > /etc/NetworkManager/dispatcher.d/ifup-local #!/bin/sh interface=\"$1\" status=\"$2\" if [ \"$interface\" == \"ens6\" ] && [ \"$status\" == \"up\" ]; then logger \"Interface $interface $status - setting ip rule and ip route\" ip rule add from 172.31.254.0/24 lookup 5000 ip route add default via 172.31.254.1 table 5000 fi EOF chmod +x /etc/NetworkManager/dispatcher.d/ifup-local FOE cat <<EOF | oc apply -f - apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: creationTimestamp: null labels: machineconfiguration.openshift.io/role: staging name: 01-staging-additional-interface spec: config: ignition: version: 2.2.0 storage: files: - contents: source: data:;base64,$(cat network-script.sh | base64 -w 0) path: /usr/local/bin/network-script.sh filesystem: root mode: 0755 systemd: units: - contents: \"[Unit]\\\\nDescription=Configure network\\\\nAfter=ignition-firstboot-complete.service\\\\nBefore=nodeip-configuration.service kubelet.service crio.service\\\\n\\\\n[Service]\\\\nType=oneshot\\\\nExecStart=/usr/local/bin/network-script.sh\\\\n\\\\n[Install]\\\\nWantedBy=multi-user.target\" enabled: true name: network-script.service EOF rm -f network-script.sh Label the nodes as staging: oc label node akaris-osc-6klvk-worker-hp2hz node-role.kubernetes.io/worker- node-role.kubernetes.io/staging= oc label node akaris-osc-6klvk-worker-zhs67 node-role.kubernetes.io/worker- node-role.kubernetes.io/staging= The nodes will now reboot with the new setting, so wait: [cloud-user@akaris-jump-server openshift]$ oc get nodes NAME STATUS ROLES AGE VERSION akaris-osc-6klvk-master-0 Ready master 5h48m v1.18.3+012b3ec akaris-osc-6klvk-master-1 Ready master 5h48m v1.18.3+012b3ec akaris-osc-6klvk-master-2 Ready master 5h49m v1.18.3+012b3ec akaris-osc-6klvk-worker-g4q6z Ready worker 5h34m v1.18.3+012b3ec akaris-osc-6klvk-worker-hp2hz Ready staging 32m v1.18.3+012b3ec akaris-osc-6klvk-worker-l726s Ready worker 5h31m v1.18.3+012b3ec akaris-osc-6klvk-worker-lp2df Ready worker 5h36m v1.18.3+012b3ec akaris-osc-6klvk-worker-zhs67 NotReady,SchedulingDisabled staging 32m v1.18.3+012b3ec [cloud-user@akaris-jump-server openshift]$ Make sure that the interface was created: [cloud-user@akaris-jump-server openshift]$ oc debug node/akaris-osc-6klvk-worker-zhs67 Starting pod/akaris-osc-6klvk-worker-zhs67-debug ... To use host binaries, run `chroot /host` Pod IP: 192.168.2.48 If you don't see a command prompt, try pressing enter. sh-4.4# cat /etc/sysconfig/network-scripts/ifcfg-ens6 NAME=\"System ens6\" BOOTPROTO=dhcp DEFROUTE=no IPV6INIT=no DEVICE=ens6 ONBOOT=yes TYPE=Ethernet PROXY_METHOD=none BROWSER_ONLY=no ETHTOOL_OPTS=\"-K ens6 tx-checksum-ip-generic off\" IPV4_FAILURE_FATAL=no UUID=9325ba04-7907-b4a4-1414-177267ba3519 sh-4.2# chroot /host sh-4.4# ip a ls dev ens6 3: ens6: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc fq_codel state UP group default qlen 1000 link/ether fa:16:3e:66:d2:28 brd ff:ff:ff:ff:ff:ff inet 172.31.254.207/24 brd 172.31.254.255 scope global dynamic noprefixroute ens6 valid_lft 86276sec preferred_lft 86276sec inet6 fe80::f816:3eff:fe66:d228/64 scope link valid_lft forever preferred_lft forever sh-4.4# ip route ls table 5000 default via 172.31.254.1 dev ens6 sh-4.4# ip rule ls | grep 5000 32765: from 172.31.254.0/24 lookup 5000 Run the same verification on the other worker. Ping both floating IPs and make sure that policy routing works: [cloud-user@akaris-jump-server openshift]$ ping 10.0.92.194 PING 10.0.92.194 (10.0.92.194) 56(84) bytes of data. 64 bytes from 10.0.92.194: icmp_seq=1 ttl=62 time=13.8 ms ^C --- 10.0.92.194 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 13.897/13.897/13.897/0.000 ms [cloud-user@akaris-jump-server openshift]$ ping 10.0.92.205 PING 10.0.92.205 (10.0.92.205) 56(84) bytes of data. 64 bytes from 10.0.92.205: icmp_seq=1 ttl=62 time=1.24 ms ^C --- 10.0.92.205 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 1.242/1.242/1.242/0.000 ms Step 4: Configure keepalived template for the staging node cat <<'EOF' > keepalived.conf.tmpl # TODO: Improve this check. The port is assumed to be alive. # Need to assess what is the ramification if the port is not there. vrrp_script chk_ingress { script \"/usr/bin/curl -o /dev/null -kLfs http://0:1936/healthz\" interval 1 weight 50 } vrrp_instance staging_INGRESS { state BACKUP interface ens6 virtual_router_id 100 priority 40 advert_int 1 authentication { auth_type PASS auth_pass staging_vip } virtual_ipaddress { 172.31.254.118/24 } track_script { chk_ingress } } EOF cat <<EOF | oc apply -f - apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: labels: machineconfiguration.openshift.io/role: staging name: 99-staging-keepalived-vip spec: config: ignition: config: {} security: tls: {} timeouts: {} version: 2.2.0 networkd: {} passwd: {} storage: files: - contents: source: data:text/plain;charset=utf-8;base64,$(cat keepalived.conf.tmpl | base64 -w0) verification: {} filesystem: root mode: 420 path: /etc/kubernetes/static-pod-resources/keepalived/keepalived.conf.tmpl systemd: {} EOF rm -f keepalived.conf.tmpl Verify: [root@akaris-osc-6klvk-worker-zhs67 ~]# cat /etc/keepalived/keepalived.conf # TODO: Improve this check. The port is assumed to be alive. # Need to assess what is the ramification if the port is not there. vrrp_script chk_ingress { script \"/usr/bin/curl -o /dev/null -kLfs http://0:1936/healthz\" interval 1 weight 50 } vrrp_instance staging_INGRESS { state BACKUP interface ens6 virtual_router_id 100 priority 40 advert_int 1 authentication { auth_type PASS auth_pass staging_vip } virtual_ipaddress { 172.31.254.118/24 } track_script { chk_ingress } } [root@akaris-osc-6klvk-worker-zhs67 ~]# ip a | gre^C [root@akaris-osc-6klvk-worker-zhs67 ~]# ip a ls dev ens6 3: ens6: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc fq_codel state UP group default qlen 1000 link/ether fa:16:3e:66:d2:28 brd ff:ff:ff:ff:ff:ff inet 172.31.254.207/24 brd 172.31.254.255 scope global dynamic noprefixroute ens6 valid_lft 86337sec preferred_lft 86337sec inet 172.31.254.118/24 scope global secondary ens6 valid_lft forever preferred_lft forever inet6 fe80::f816:3eff:fe66:d228/64 scope link valid_lft forever preferred_lft forever And ping the VIP: [cloud-user@akaris-jump-server ~]$ ping 10.0.89.89 PING 10.0.89.89 (10.0.89.89) 56(84) bytes of data. 64 bytes from 10.0.89.89: icmp_seq=1 ttl=62 time=15.7 ms ^C --- 10.0.89.89 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 15.791/15.791/15.791/0.000 ms [cloud-user@akaris-jump-server ~]$ And SSH into the VIP: [cloud-user@akaris-jump-server ~]$ ssh core@10.0.89.89 The authenticity of host '10.0.89.89 (10.0.89.89)' can't be established. ECDSA key fingerprint is SHA256:XB+FYE3vAxFZNI4HakM5WJkHmf74iMdFgI8x+dLx03A. ECDSA key fingerprint is MD5:a2:54:c1:52:ee:05:67:52:7c:2a:a9:b8:35:a9:a5:ee. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added '10.0.89.89' (ECDSA) to the list of known hosts. Red Hat Enterprise Linux CoreOS 45.82.202007240629-0 Part of OpenShift 4.5, RHCOS is a Kubernetes native operating system managed by the Machine Config Operator (`clusteroperator/machine-config`). WARNING: Direct SSH access to machines is not recommended; instead, make configuration changes via `machineconfig` objects: https://docs.openshift.com/container-platform/4.5/architecture/architecture-rhcos.html --- Last login: Tue Aug 11 19:14:19 2020 from 10.0.88.55 [core@akaris-osc-6klvk-worker-zhs67 ~]$ Step 5: Configure an Ingress, use Ingress controller sharding Use: https://docs.openshift.com/container-platform/4.5/networking/ingress-operator.html#nw-ingress-sharding_configuring-ingress For example, the following Ingress route should work and be served by the 2 staging workers: 10.0.89.99 test.staging.akaris-osc.... Configure the default IngressController and have it run only on worker nodes - you can also exclude namespaces which shall not be served by the default IngressController: cat <<'EOF' > patch.yaml spec: replicas: 2 # namespaceSelector: # matchExpressions: # - key: type # operator: NotIn # values: # - test1 # - test2 nodePlacement: nodeSelector: matchLabels: node-role.kubernetes.io/worker: \"\" EOF oc patch -n openshift-ingress-operator ingresscontroller default --type=\"merge\" -p \"$(cat patch.yaml)\" rm -f patch.yaml Configure the staging IngressController and have it run only on sharded nodes: cat <<'EOF' | oc apply -f - apiVersion: v1 items: - apiVersion: operator.openshift.io/v1 kind: IngressController metadata: name: staging namespace: openshift-ingress-operator spec: replicas: 2 domain: staging.akaris-osc.... nodePlacement: nodeSelector: matchLabels: node-role.kubernetes.io/staging: \"\" # --- either --- # routeSelector: # matchLabels: # type: staging # --- or --- # namespaceSelector: # matchLabels: # type: test1 endpointPublishingStrategy: type: HostNetwork status: {} kind: List metadata: resourceVersion: \"\" selfLink: \"\" EOF Verify generation and modification of the IngressController and OpenShift router pods: [cloud-user@akaris-jump-server openshift]$ oc get nodes NAME STATUS ROLES AGE VERSION akaris-osc-6klvk-master-0 Ready master 24h v1.18.3+012b3ec akaris-osc-6klvk-master-1 Ready master 24h v1.18.3+012b3ec akaris-osc-6klvk-master-2 Ready master 24h v1.18.3+012b3ec akaris-osc-6klvk-worker-g4q6z Ready worker 24h v1.18.3+012b3ec akaris-osc-6klvk-worker-hp2hz Ready staging 19h v1.18.3+012b3ec akaris-osc-6klvk-worker-l726s Ready worker 24h v1.18.3+012b3ec akaris-osc-6klvk-worker-lp2df Ready worker 24h v1.18.3+012b3ec akaris-osc-6klvk-worker-zhs67 Ready staging 19h v1.18.3+012b3ec [cloud-user@akaris-jump-server openshift]$ oc get ingresscontroller -n openshift-ingress-operator NAME AGE default 24h staging 13m [cloud-user@akaris-jump-server openshift]$ oc get pods -n openshift-ingress -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES router-default-7fd7bdd4fc-jf55h 1/1 Running 0 2m5s 192.168.0.114 akaris-osc-6klvk-worker-lp2df <none> <none> router-default-7fd7bdd4fc-mc572 1/1 Running 0 106s 192.168.0.251 akaris-osc-6klvk-worker-l726s <none> <none> router-staging-86c864549c-7stx8 1/1 Running 0 14m 192.168.1.160 akaris-osc-6klvk-worker-hp2hz <none> <none> router-staging-86c864549c-nkz28 1/1 Running 0 14m 192.168.2.48 akaris-osc-6klvk-worker-zhs67 <none> <none> Step 6: Expose route oc new-project test oc adm policy add-scc-to-user anyuid -z default cat <<'EOF' > httpbin-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: httpbin-ingress annotations: kubernetes.io/ingress.allow-http: \"true\" spec: rules: - host: httpbin-ingress.staging.akaris-osc.... http: paths: - path: / backend: serviceName: httpbin-service servicePort: 80 --- apiVersion: v1 kind: Service metadata: name: httpbin-service labels: app: httpbin-deployment spec: selector: app: httpbin-pod ports: - protocol: TCP port: 80 targetPort: 80 --- apiVersion: apps/v1 kind: Deployment metadata: name: httpbin-deployment labels: app: httpbin-deployment spec: replicas: 1 selector: matchLabels: app: httpbin-pod template: metadata: labels: app: httpbin-pod spec: containers: - name: tshark image: danielguerra/alpine-tshark command: - \"tshark\" - \"-i\" - \"eth0\" - \"-Y\" - \"http\" - \"-V\" - \"dst\" - \"port\" - \"80\" - name: httpbin image: kennethreitz/httpbin imagePullPolicy: Always command: - \"gunicorn\" - \"-b\" - \"0.0.0.0:80\" - \"httpbin:app\" - \"-k\" - \"gevent\" - \"--capture-output\" - \"--error-logfile\" - \"-\" - \"--access-logfile\" - \"-\" - \"--access-logformat\" - \"'%(h)s %(t)s %(r)s %(s)s Host: %({Host}i)s} Header-i: %({Header}i)s Header-o: %({Header}o)s'\" EOF oc apply -f httpbin-ingress.yaml [cloud-user@akaris-jump-server ~]$ oc get routes NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD httpbin-ingress-z9tgf httpbin-ingress.staging.akaris-osc.... ... 1 more / httpbin-service <all> None [cloud-user@akaris-jump-server ~]$ curl httpbin-ingress.staging.akaris-osc.... { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Forwarded\": \"for=10.0.88.55;host=httpbin-ingress.staging.akaris-osc....;proto=http\", \"Host\": \"httpbin-ingress.staging.akaris-osc....\", \"User-Agent\": \"curl/7.29.0\", \"X-Forwarded-Host\": \"httpbin-ingress.staging.akaris-osc....\" }, \"origin\": \"10.0.88.55\", \"url\": \"http://httpbin-ingress.staging.akaris-osc..../get\" } Step 7: Security hardening This is out of scope of this PoC, but note that at the moment, the default Ingress can still handle all domains and thus if the client configures its resolver appropriately, can reach the application via the default VIP. For example, take this mapping: 10.0.88.28 *.apps.akaris-osc.... 10.0.89.89 *.staging.akaris-osc.... Then one can still use the other ingress router by modifying the client resolver: [cloud-user@akaris-jump-server ~]$ curl --resolve httpbin-ingress.staging.akaris-osc..../get:80:10.0.88.28 httpbin-ingress.staging.akaris-osc..../get { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Forwarded\": \"for=10.0.88.55;host=httpbin-ingress.staging.akaris-osc....;proto=http\", \"Host\": \"httpbin-ingress.staging.akaris-osc...., \"User-Agent\": \"curl/7.29.0\", \"X-Forwarded-Host\": \"httpbin-ingress.staging.akaris-osc...\" }, \"origin\": \"10.0.88.55\", \"url\": \"http://httpbin-ingress.staging.akaris-osc..../get\" } [cloud-user@akaris-jump-server ~]$ curl --resolve httpbin-ingress.staging.akaris-osc..../get:80:10.0.89.89 httpbin-ingress.staging.akaris-osc.../get { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Forwarded\": \"for=10.0.88.55;host=httpbin-ingress.staging.akaris-osc....;proto=http\", \"Host\": \"httpbin-ingress.staging.akaris-osc...\", \"User-Agent\": \"curl/7.29.0\", \"X-Forwarded-Host\": \"httpbin-ingress.staging.akaris-osc...\" }, \"origin\": \"10.0.88.55\", \"url\": \"http://httpbin-ingress.staging.akaris-osc.../get\" } And the same goes for the other way around: [cloud-user@akaris-jump-server ~]$ curl --resolve nginx-deployment-application.apps.akaris-osc....:80:10.0.88.28 nginx-deployment-application.apps.akaris-osc.... Nginx A [cloud-user@akaris-jump-server ~]$ curl --resolve nginx-deployment-application.apps.akaris-osc....:80:10.0.89.89 nginx-deployment-application.apps.akaris-osc.... Nginx A Read the documentation about IngressController sharding for further details. Step 8: Troubleshooting Again, a bit outside of the scope of this PoC - the debug pods for my staging nodes stopped working overnight. This would need some further troubleshooting: [cloud-user@akaris-jump-server ~]$ oc get nodes NAME STATUS ROLES AGE VERSION akaris-osc-6klvk-master-0 Ready master 25h v1.18.3+012b3ec akaris-osc-6klvk-master-1 Ready master 25h v1.18.3+012b3ec akaris-osc-6klvk-master-2 Ready master 25h v1.18.3+012b3ec akaris-osc-6klvk-worker-g4q6z Ready worker 24h v1.18.3+012b3ec akaris-osc-6klvk-worker-hp2hz Ready staging 19h v1.18.3+012b3ec akaris-osc-6klvk-worker-l726s Ready worker 24h v1.18.3+012b3ec akaris-osc-6klvk-worker-lp2df Ready worker 24h v1.18.3+012b3ec akaris-osc-6klvk-worker-zhs67 Ready staging 19h v1.18.3+012b3ec [cloud-user@akaris-jump-server ~]$ oc debug node/akaris-osc-6klvk-worker-zhs67 Starting pod/akaris-osc-6klvk-worker-zhs67-debug ... To use host binaries, run `chroot /host` Pod IP: 192.168.2.48 If you don't see a command prompt, try pressing enter. Removing debug pod ... Error from server: error dialing backend: remote error: tls: internal error [cloud-user@akaris-jump-server ~]$","title":"Ingress Controller Sharding on separate VIP"},{"location":"openshift/ingress-controller-sharding-on-separate-vip/#ingress-controller-sharding-on-separate-vip","text":"It is important to know that each node can only host one ingress router with HostNetwork networking. You will need a specific role for each INGRESS type and different INGRESS types cannot be hosted on the same worker node. This is due to the particularities of the HostNetwork network type. The following example uses the OpenStack IPI.","title":"Ingress Controller Sharding on separate VIP"},{"location":"openshift/ingress-controller-sharding-on-separate-vip/#step-1-scale-out-by-2-more-worker-nodes","text":"Scale out the worker role. This step might have to be improved. [cloud-user@akaris-jump-server cluster-logging]$ oc scale -n openshift-machine-api --replicas=5 machineset akaris-osc-6klvk-worker machineset.machine.openshift.io/akaris-osc-6klvk-worker scaled [cloud-user@akaris-jump-server cluster-logging]$ [cloud-user@akaris-jump-server openshift]$ oc get nodes NAME STATUS ROLES AGE VERSION akaris-osc-6klvk-master-0 Ready master 5h18m v1.18.3+012b3ec akaris-osc-6klvk-master-1 Ready master 5h18m v1.18.3+012b3ec akaris-osc-6klvk-master-2 Ready master 5h19m v1.18.3+012b3ec akaris-osc-6klvk-worker-g4q6z Ready worker 5h4m v1.18.3+012b3ec akaris-osc-6klvk-worker-hp2hz Ready worker 2m12s v1.18.3+012b3ec akaris-osc-6klvk-worker-l726s Ready worker 5h1m v1.18.3+012b3ec akaris-osc-6klvk-worker-lp2df Ready worker 5h6m v1.18.3+012b3ec akaris-osc-6klvk-worker-zhs67 Ready worker 2m57s v1.18.3+012b3ec","title":"Step 1: Scale out by 2 more worker nodes"},{"location":"openshift/ingress-controller-sharding-on-separate-vip/#step-2-add-additional-interface-to-additional-nodes","text":"This step will be executed within the underlying infrastructure provider. In this example case, OpenShift is hosted on top of OpenStack. Make sure that the subnet runs a DHCP server: [cloud-user@akaris-jump-server openshift]$ openstack subnet set --dhcp 41c073c0-5b6a-4639-9023-115ce3d0e378 [cloud-user@akaris-jump-server openshift]$ openstack subnet show 41c073c0-5b6a-4639-9023-115ce3d0e378 +-------------------+--------------------------------------+ | Field | Value | +-------------------+--------------------------------------+ | allocation_pools | 172.31.254.2-172.31.254.254 | | cidr | 172.31.254.0/24 | | created_at | 2020-01-15T17:13:39Z | | description | | | dns_nameservers | | | enable_dhcp | True | | gateway_ip | 172.31.254.1 | | host_routes | | | id | 41c073c0-5b6a-4639-9023-115ce3d0e378 | | ip_version | 4 | | ipv6_address_mode | None | | ipv6_ra_mode | None | | name | akaris-backend-subnet | | network_id | aa68c208-b6c9-402e-b508-1b9d82f3baa4 | | prefix_length | None | | project_id | ea78dddee3e240c7af1779b417c64618 | | revision_number | 1 | | segment_id | None | | service_types | | | subnetpool_id | None | | tags | | | updated_at | 2020-08-11T15:19:17Z | +-------------------+--------------------------------------+ And create a router that can host floating IPs: openstack router create akaris-backend openstack router set --external-gateway 5cd089f9-8ed2-46bc-8ea7-4e1cdb5262ba akaris-backend openstack router add subnet akaris-backend akaris-backend-subnet [cloud-user@akaris-jump-server openshift]$ openstack port create --network aa68c208-b6c9-402e-b508-1b9d82f3baa4 akaris-osc-6klvk-worker-hp2hz-akaris-backend +-----------------------+------------------------------------------------------------------------------+ | Field | Value | +-----------------------+------------------------------------------------------------------------------+ | admin_state_up | UP | | allowed_address_pairs | | | binding_host_id | None | | binding_profile | None | | binding_vif_details | None | | binding_vif_type | None | | binding_vnic_type | normal | | created_at | 2020-08-11T14:47:17Z | | data_plane_status | None | | description | | | device_id | | | device_owner | | | dns_assignment | None | | dns_name | None | | extra_dhcp_opts | | | fixed_ips | ip_address='172.31.254.48', subnet_id='41c073c0-5b6a-4639-9023-115ce3d0e378' | | id | 8e737c31-0450-40d0-a5e0-54871a735428 | | ip_address | None | | mac_address | fa:16:3e:07:66:12 | | name | akaris-osc-6klvk-worker-hp2hz-akaris-backend | | network_id | aa68c208-b6c9-402e-b508-1b9d82f3baa4 | | option_name | None | | option_value | None | | port_security_enabled | True | | project_id | ea78dddee3e240c7af1779b417c64618 | | qos_policy_id | None | | revision_number | 6 | | security_group_ids | ae5a722f-aeea-4acd-8260-a04f5dceb624 | | status | DOWN | | subnet_id | None | | tags | | | trunk_details | None | | updated_at | 2020-08-11T14:47:17Z | +-----------------------+------------------------------------------------------------------------------+ [cloud-user@akaris-jump-server openshift]$ openstack port create --network aa68c208-b6c9-402e-b508-1b9d82f3baa4 akaris-osc-6klvk-worker-zhs67-akaris-backend +-----------------------+-------------------------------------------------------------------------------+ | Field | Value | +-----------------------+-------------------------------------------------------------------------------+ | admin_state_up | UP | | allowed_address_pairs | | | binding_host_id | None | | binding_profile | None | | binding_vif_details | None | | binding_vif_type | None | | binding_vnic_type | normal | | created_at | 2020-08-11T14:47:40Z | | data_plane_status | None | | description | | | device_id | | | device_owner | | | dns_assignment | None | | dns_name | None | | extra_dhcp_opts | | | fixed_ips | ip_address='172.31.254.207', subnet_id='41c073c0-5b6a-4639-9023-115ce3d0e378' | | id | c82401f3-fa76-449e-ba16-75fe4b0f5583 | | ip_address | None | | mac_address | fa:16:3e:66:d2:28 | | name | akaris-osc-6klvk-worker-zhs67-akaris-backend | | network_id | aa68c208-b6c9-402e-b508-1b9d82f3baa4 | | option_name | None | | option_value | None | | port_security_enabled | True | | project_id | ea78dddee3e240c7af1779b417c64618 | | qos_policy_id | None | | revision_number | 6 | | security_group_ids | ae5a722f-aeea-4acd-8260-a04f5dceb624 | | status | DOWN | | subnet_id | None | | tags | | | trunk_details | None | | updated_at | 2020-08-11T14:47:40Z | +-----------------------+-------------------------------------------------------------------------------+ [cloud-user@akaris-jump-server openshift]$ openstack port create --network aa68c208-b6c9-402e-b508-1b9d82f3baa4 vip-akaris-backend +-----------------------+-------------------------------------------------------------------------------+ | Field | Value | +-----------------------+-------------------------------------------------------------------------------+ | admin_state_up | UP | | allowed_address_pairs | | | binding_host_id | None | | binding_profile | None | | binding_vif_details | None | | binding_vif_type | None | | binding_vnic_type | normal | | created_at | 2020-08-11T18:32:53Z | | data_plane_status | None | | description | | | device_id | | | device_owner | | | dns_assignment | None | | dns_name | None | | extra_dhcp_opts | | | fixed_ips | ip_address='172.31.254.118', subnet_id='41c073c0-5b6a-4639-9023-115ce3d0e378' | | id | 4d0b32f5-203b-4e2b-b92b-b25f349b1d83 | | ip_address | None | | mac_address | fa:16:3e:0d:3e:cd | | name | vip-akaris-backend | | network_id | aa68c208-b6c9-402e-b508-1b9d82f3baa4 | | option_name | None | | option_value | None | | port_security_enabled | True | | project_id | ea78dddee3e240c7af1779b417c64618 | | qos_policy_id | None | | revision_number | 6 | | security_group_ids | ae5a722f-aeea-4acd-8260-a04f5dceb624 | | status | DOWN | | subnet_id | None | | tags | | | trunk_details | None | | updated_at | 2020-08-11T18:32:53Z | +-----------------------+-------------------------------------------------------------------------------+ [cloud-user@akaris-jump-server openshift]$ openstack server add port akaris-osc-6klvk-worker-hp2hz akaris-osc-6klvk-worker-hp2hz-akaris-backend [cloud-user@akaris-jump-server openshift]$ openstack server add port akaris-osc-6klvk-worker-zhs67 akaris-osc-6klvk-worker-zhs67-akaris-backend [cloud-user@akaris-jump-server openshift]$ openstack floating ip set --port akaris-osc-6klvk-worker-hp2hz-akaris-backend 10.0.92.194 [cloud-user@akaris-jump-server openshift]$ openstack floating ip set --port akaris-osc-6klvk-worker-zhs67-akaris-backend 10.0.92.205 [cloud-user@akaris-jump-server openshift]$ openstack floating ip set --port vip-akaris-backend 10.0.89.89 After this, the floating IPs look like this in OpenStack: [cloud-user@akaris-jump-server openshift]$ openstack floating ip list | grep 172.31.254 | dd055cc4-af00-4dbd-8e4a-94fac680299d | 10.0.89.89 | 172.31.254.118 | 4d0b32f5-203b-4e2b-b92b-b25f349b1d83 | 5cd089f9-8ed2-46bc-8ea7-4e1cdb5262ba | ea78dddee3e240c7af1779b417c64618 | | e586c5c8-1cd4-4bd5-adb7-790e895b5af8 | 10.0.92.194 | 172.31.254.48 | 8e737c31-0450-40d0-a5e0-54871a735428 | 5cd089f9-8ed2-46bc-8ea7-4e1cdb5262ba | ea78dddee3e240c7af1779b417c64618 | | f7e082f1-7880-4f07-8a05-3fdb0585eb32 | 10.0.92.205 | 172.31.254.207 | c82401f3-fa76-449e-ba16-75fe4b0f5583 | 5cd089f9-8ed2-46bc-8ea7-4e1cdb5262ba | ea78dddee3e240c7af1779b417c64618 | Last but not least, disable port security for these ports so that VRRP can work with these OpenStack ports: for port in akaris-osc-6klvk-worker-hp2hz-akaris-backend akaris-osc-6klvk-worker-zhs67-akaris-backend vip-akaris-backend ; do echo === $port === openstack port set --no-security-group $port openstack port set --disable-port-security $port done The VRRP floating IP is: IP 172.31.254.118 MAC address: fa:16:3e:0d:3e:cd VIP: 10.0.89.89 Verify that the port was added to the workers: [cloud-user@akaris-jump-server openshift]$ oc debug node/akaris-osc-6klvk-worker-hp2hz Starting pod/akaris-osc-6klvk-worker-hp2hz-debug ... To use host binaries, run `chroot /host` Pod IP: 192.168.1.160 If you don't see a command prompt, try pressing enter. sh-4.2# chroot /host sh-4.4# toolbox Trying to pull registry.redhat.io/rhel8/support-tools... Getting image source signatures Copying blob 77c58f19bd6e done Copying blob 47db82df7f3f done Copying blob cdc5441bd52d done Copying config 5ef2aab094 done Writing manifest to image destination Storing signatures 5ef2aab094514cc5561fa94b0bb52d75379ecf8a36355e891017f3982bac278c Spawning a container 'toolbox-' with image 'registry.redhat.io/rhel8/support-tools' Detected RUN label in the container image. Using that as the default... command: podman run -it --name toolbox- --privileged --ipc=host --net=host --pid=host -e HOST=/host -e NAME=toolbox- -e IMAGE=registry.redhat.io/rhel8/support-tools:latest -v /run:/run -v /var/log:/var/log -v /etc/machine-id:/etc/machine-id -v /etc/localtime:/etc/localtime -v /:/host registry.redhat.io/rhel8/support-tools:latest [root@akaris-osc-6klvk-worker-hp2hz /]# yum install iproute -y (...) Installed: iproute-5.3.0-1.el8.x86_64 libmnl-1.0.4-6.el8.x86_64 Complete! [root@akaris-osc-6klvk-worker-hp2hz /]# dmesg | tail [ 745.078322] pci 0000:00:06.0: BAR 0: assigned [io 0x1000-0x103f] [ 745.080319] virtio-pci 0000:00:06.0: enabling device (0000 -> 0003) [ 745.100662] PCI Interrupt Link [LNKB] enabled at IRQ 11 [ 745.109702] virtio_net virtio3 ens6: renamed from eth0 [ 745.156833] IPv6: ADDRCONF(NETDEV_UP): ens6: link is not ready [ 745.159290] IPv6: ADDRCONF(NETDEV_UP): ens6: link is not ready [ 745.165677] IPv6: ADDRCONF(NETDEV_UP): ens6: link is not ready [ 745.172694] IPv6: ADDRCONF(NETDEV_UP): ens6: link is not ready [ 746.143079] IPv6: ADDRCONF(NETDEV_CHANGE): ens6: link becomes ready [ 861.279527] SELinux: mount invalid. Same superblock, different security settings for (dev mqueue, type mqueue) [root@akaris-osc-6klvk-worker-hp2hz /]# ip link ls dev ens6 10: ens6: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc fq_codel state UP mode DEFAULT group default qlen 1000 link/ether fa:16:3e:07:66:12 brd ff:ff:ff:ff:ff:ff Repeat the same on the other node: [cloud-user@akaris-jump-server openshift]$ oc debug node/akaris-osc-6klvk-worker-zhs67 (...)","title":"Step 2: Add additional interface to additional nodes"},{"location":"openshift/ingress-controller-sharding-on-separate-vip/#step-3-create-new-role-and-assign-new-nodes-to-the-role","text":"Follow https://www.redhat.com/en/blog/openshift-container-platform-4-how-does-machine-config-pool-work with modifications . Run this via a script that's triggered by a systemd unit file: cat <<'EOF' | oc apply -f - apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfigPool metadata: name: staging spec: machineConfigSelector: matchExpressions: - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,staging]} nodeSelector: matchLabels: node-role.kubernetes.io/staging: \"\" paused: false EOF cat <<'FOE' > network-script.sh #!/bin/bash cat <<'EOF' > /etc/sysconfig/network-scripts/ifcfg-ens6 NAME=\"System ens6\" BOOTPROTO=dhcp DEFROUTE=no IPV6INIT=no DEVICE=ens6 ONBOOT=yes EOF # none of the following will work given that we're using DHCP # cat <<'EOF' > /etc/sysconfig/network-scripts/rule-ens6 # from 172.31.254.0/24 lookup 5000 # EOF # cat <<'EOF' > /etc/sysconfig/network-scripts/route-ens6 # default via 172.31.254.1 table 5000 # EOF # the following will not work either # ip rule add from 172.31.254.0/24 lookup 5000 # ip route add default via 172.31.254.1 table 5000 cat <<'EOF' > /etc/NetworkManager/dispatcher.d/ifup-local #!/bin/sh interface=\"$1\" status=\"$2\" if [ \"$interface\" == \"ens6\" ] && [ \"$status\" == \"up\" ]; then logger \"Interface $interface $status - setting ip rule and ip route\" ip rule add from 172.31.254.0/24 lookup 5000 ip route add default via 172.31.254.1 table 5000 fi EOF chmod +x /etc/NetworkManager/dispatcher.d/ifup-local FOE cat <<EOF | oc apply -f - apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: creationTimestamp: null labels: machineconfiguration.openshift.io/role: staging name: 01-staging-additional-interface spec: config: ignition: version: 2.2.0 storage: files: - contents: source: data:;base64,$(cat network-script.sh | base64 -w 0) path: /usr/local/bin/network-script.sh filesystem: root mode: 0755 systemd: units: - contents: \"[Unit]\\\\nDescription=Configure network\\\\nAfter=ignition-firstboot-complete.service\\\\nBefore=nodeip-configuration.service kubelet.service crio.service\\\\n\\\\n[Service]\\\\nType=oneshot\\\\nExecStart=/usr/local/bin/network-script.sh\\\\n\\\\n[Install]\\\\nWantedBy=multi-user.target\" enabled: true name: network-script.service EOF rm -f network-script.sh Label the nodes as staging: oc label node akaris-osc-6klvk-worker-hp2hz node-role.kubernetes.io/worker- node-role.kubernetes.io/staging= oc label node akaris-osc-6klvk-worker-zhs67 node-role.kubernetes.io/worker- node-role.kubernetes.io/staging= The nodes will now reboot with the new setting, so wait: [cloud-user@akaris-jump-server openshift]$ oc get nodes NAME STATUS ROLES AGE VERSION akaris-osc-6klvk-master-0 Ready master 5h48m v1.18.3+012b3ec akaris-osc-6klvk-master-1 Ready master 5h48m v1.18.3+012b3ec akaris-osc-6klvk-master-2 Ready master 5h49m v1.18.3+012b3ec akaris-osc-6klvk-worker-g4q6z Ready worker 5h34m v1.18.3+012b3ec akaris-osc-6klvk-worker-hp2hz Ready staging 32m v1.18.3+012b3ec akaris-osc-6klvk-worker-l726s Ready worker 5h31m v1.18.3+012b3ec akaris-osc-6klvk-worker-lp2df Ready worker 5h36m v1.18.3+012b3ec akaris-osc-6klvk-worker-zhs67 NotReady,SchedulingDisabled staging 32m v1.18.3+012b3ec [cloud-user@akaris-jump-server openshift]$ Make sure that the interface was created: [cloud-user@akaris-jump-server openshift]$ oc debug node/akaris-osc-6klvk-worker-zhs67 Starting pod/akaris-osc-6klvk-worker-zhs67-debug ... To use host binaries, run `chroot /host` Pod IP: 192.168.2.48 If you don't see a command prompt, try pressing enter. sh-4.4# cat /etc/sysconfig/network-scripts/ifcfg-ens6 NAME=\"System ens6\" BOOTPROTO=dhcp DEFROUTE=no IPV6INIT=no DEVICE=ens6 ONBOOT=yes TYPE=Ethernet PROXY_METHOD=none BROWSER_ONLY=no ETHTOOL_OPTS=\"-K ens6 tx-checksum-ip-generic off\" IPV4_FAILURE_FATAL=no UUID=9325ba04-7907-b4a4-1414-177267ba3519 sh-4.2# chroot /host sh-4.4# ip a ls dev ens6 3: ens6: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc fq_codel state UP group default qlen 1000 link/ether fa:16:3e:66:d2:28 brd ff:ff:ff:ff:ff:ff inet 172.31.254.207/24 brd 172.31.254.255 scope global dynamic noprefixroute ens6 valid_lft 86276sec preferred_lft 86276sec inet6 fe80::f816:3eff:fe66:d228/64 scope link valid_lft forever preferred_lft forever sh-4.4# ip route ls table 5000 default via 172.31.254.1 dev ens6 sh-4.4# ip rule ls | grep 5000 32765: from 172.31.254.0/24 lookup 5000 Run the same verification on the other worker. Ping both floating IPs and make sure that policy routing works: [cloud-user@akaris-jump-server openshift]$ ping 10.0.92.194 PING 10.0.92.194 (10.0.92.194) 56(84) bytes of data. 64 bytes from 10.0.92.194: icmp_seq=1 ttl=62 time=13.8 ms ^C --- 10.0.92.194 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 13.897/13.897/13.897/0.000 ms [cloud-user@akaris-jump-server openshift]$ ping 10.0.92.205 PING 10.0.92.205 (10.0.92.205) 56(84) bytes of data. 64 bytes from 10.0.92.205: icmp_seq=1 ttl=62 time=1.24 ms ^C --- 10.0.92.205 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 1.242/1.242/1.242/0.000 ms","title":"Step 3: create new role and assign new nodes to the role"},{"location":"openshift/ingress-controller-sharding-on-separate-vip/#step-4-configure-keepalived-template-for-the-staging-node","text":"cat <<'EOF' > keepalived.conf.tmpl # TODO: Improve this check. The port is assumed to be alive. # Need to assess what is the ramification if the port is not there. vrrp_script chk_ingress { script \"/usr/bin/curl -o /dev/null -kLfs http://0:1936/healthz\" interval 1 weight 50 } vrrp_instance staging_INGRESS { state BACKUP interface ens6 virtual_router_id 100 priority 40 advert_int 1 authentication { auth_type PASS auth_pass staging_vip } virtual_ipaddress { 172.31.254.118/24 } track_script { chk_ingress } } EOF cat <<EOF | oc apply -f - apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: labels: machineconfiguration.openshift.io/role: staging name: 99-staging-keepalived-vip spec: config: ignition: config: {} security: tls: {} timeouts: {} version: 2.2.0 networkd: {} passwd: {} storage: files: - contents: source: data:text/plain;charset=utf-8;base64,$(cat keepalived.conf.tmpl | base64 -w0) verification: {} filesystem: root mode: 420 path: /etc/kubernetes/static-pod-resources/keepalived/keepalived.conf.tmpl systemd: {} EOF rm -f keepalived.conf.tmpl Verify: [root@akaris-osc-6klvk-worker-zhs67 ~]# cat /etc/keepalived/keepalived.conf # TODO: Improve this check. The port is assumed to be alive. # Need to assess what is the ramification if the port is not there. vrrp_script chk_ingress { script \"/usr/bin/curl -o /dev/null -kLfs http://0:1936/healthz\" interval 1 weight 50 } vrrp_instance staging_INGRESS { state BACKUP interface ens6 virtual_router_id 100 priority 40 advert_int 1 authentication { auth_type PASS auth_pass staging_vip } virtual_ipaddress { 172.31.254.118/24 } track_script { chk_ingress } } [root@akaris-osc-6klvk-worker-zhs67 ~]# ip a | gre^C [root@akaris-osc-6klvk-worker-zhs67 ~]# ip a ls dev ens6 3: ens6: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc fq_codel state UP group default qlen 1000 link/ether fa:16:3e:66:d2:28 brd ff:ff:ff:ff:ff:ff inet 172.31.254.207/24 brd 172.31.254.255 scope global dynamic noprefixroute ens6 valid_lft 86337sec preferred_lft 86337sec inet 172.31.254.118/24 scope global secondary ens6 valid_lft forever preferred_lft forever inet6 fe80::f816:3eff:fe66:d228/64 scope link valid_lft forever preferred_lft forever And ping the VIP: [cloud-user@akaris-jump-server ~]$ ping 10.0.89.89 PING 10.0.89.89 (10.0.89.89) 56(84) bytes of data. 64 bytes from 10.0.89.89: icmp_seq=1 ttl=62 time=15.7 ms ^C --- 10.0.89.89 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 15.791/15.791/15.791/0.000 ms [cloud-user@akaris-jump-server ~]$ And SSH into the VIP: [cloud-user@akaris-jump-server ~]$ ssh core@10.0.89.89 The authenticity of host '10.0.89.89 (10.0.89.89)' can't be established. ECDSA key fingerprint is SHA256:XB+FYE3vAxFZNI4HakM5WJkHmf74iMdFgI8x+dLx03A. ECDSA key fingerprint is MD5:a2:54:c1:52:ee:05:67:52:7c:2a:a9:b8:35:a9:a5:ee. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added '10.0.89.89' (ECDSA) to the list of known hosts. Red Hat Enterprise Linux CoreOS 45.82.202007240629-0 Part of OpenShift 4.5, RHCOS is a Kubernetes native operating system managed by the Machine Config Operator (`clusteroperator/machine-config`). WARNING: Direct SSH access to machines is not recommended; instead, make configuration changes via `machineconfig` objects: https://docs.openshift.com/container-platform/4.5/architecture/architecture-rhcos.html --- Last login: Tue Aug 11 19:14:19 2020 from 10.0.88.55 [core@akaris-osc-6klvk-worker-zhs67 ~]$","title":"Step 4: Configure keepalived template for the staging node"},{"location":"openshift/ingress-controller-sharding-on-separate-vip/#step-5-configure-an-ingress-use-ingress-controller-sharding","text":"Use: https://docs.openshift.com/container-platform/4.5/networking/ingress-operator.html#nw-ingress-sharding_configuring-ingress For example, the following Ingress route should work and be served by the 2 staging workers: 10.0.89.99 test.staging.akaris-osc.... Configure the default IngressController and have it run only on worker nodes - you can also exclude namespaces which shall not be served by the default IngressController: cat <<'EOF' > patch.yaml spec: replicas: 2 # namespaceSelector: # matchExpressions: # - key: type # operator: NotIn # values: # - test1 # - test2 nodePlacement: nodeSelector: matchLabels: node-role.kubernetes.io/worker: \"\" EOF oc patch -n openshift-ingress-operator ingresscontroller default --type=\"merge\" -p \"$(cat patch.yaml)\" rm -f patch.yaml Configure the staging IngressController and have it run only on sharded nodes: cat <<'EOF' | oc apply -f - apiVersion: v1 items: - apiVersion: operator.openshift.io/v1 kind: IngressController metadata: name: staging namespace: openshift-ingress-operator spec: replicas: 2 domain: staging.akaris-osc.... nodePlacement: nodeSelector: matchLabels: node-role.kubernetes.io/staging: \"\" # --- either --- # routeSelector: # matchLabels: # type: staging # --- or --- # namespaceSelector: # matchLabels: # type: test1 endpointPublishingStrategy: type: HostNetwork status: {} kind: List metadata: resourceVersion: \"\" selfLink: \"\" EOF Verify generation and modification of the IngressController and OpenShift router pods: [cloud-user@akaris-jump-server openshift]$ oc get nodes NAME STATUS ROLES AGE VERSION akaris-osc-6klvk-master-0 Ready master 24h v1.18.3+012b3ec akaris-osc-6klvk-master-1 Ready master 24h v1.18.3+012b3ec akaris-osc-6klvk-master-2 Ready master 24h v1.18.3+012b3ec akaris-osc-6klvk-worker-g4q6z Ready worker 24h v1.18.3+012b3ec akaris-osc-6klvk-worker-hp2hz Ready staging 19h v1.18.3+012b3ec akaris-osc-6klvk-worker-l726s Ready worker 24h v1.18.3+012b3ec akaris-osc-6klvk-worker-lp2df Ready worker 24h v1.18.3+012b3ec akaris-osc-6klvk-worker-zhs67 Ready staging 19h v1.18.3+012b3ec [cloud-user@akaris-jump-server openshift]$ oc get ingresscontroller -n openshift-ingress-operator NAME AGE default 24h staging 13m [cloud-user@akaris-jump-server openshift]$ oc get pods -n openshift-ingress -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES router-default-7fd7bdd4fc-jf55h 1/1 Running 0 2m5s 192.168.0.114 akaris-osc-6klvk-worker-lp2df <none> <none> router-default-7fd7bdd4fc-mc572 1/1 Running 0 106s 192.168.0.251 akaris-osc-6klvk-worker-l726s <none> <none> router-staging-86c864549c-7stx8 1/1 Running 0 14m 192.168.1.160 akaris-osc-6klvk-worker-hp2hz <none> <none> router-staging-86c864549c-nkz28 1/1 Running 0 14m 192.168.2.48 akaris-osc-6klvk-worker-zhs67 <none> <none>","title":"Step 5: Configure an Ingress, use Ingress controller sharding"},{"location":"openshift/ingress-controller-sharding-on-separate-vip/#step-6-expose-route","text":"oc new-project test oc adm policy add-scc-to-user anyuid -z default cat <<'EOF' > httpbin-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: httpbin-ingress annotations: kubernetes.io/ingress.allow-http: \"true\" spec: rules: - host: httpbin-ingress.staging.akaris-osc.... http: paths: - path: / backend: serviceName: httpbin-service servicePort: 80 --- apiVersion: v1 kind: Service metadata: name: httpbin-service labels: app: httpbin-deployment spec: selector: app: httpbin-pod ports: - protocol: TCP port: 80 targetPort: 80 --- apiVersion: apps/v1 kind: Deployment metadata: name: httpbin-deployment labels: app: httpbin-deployment spec: replicas: 1 selector: matchLabels: app: httpbin-pod template: metadata: labels: app: httpbin-pod spec: containers: - name: tshark image: danielguerra/alpine-tshark command: - \"tshark\" - \"-i\" - \"eth0\" - \"-Y\" - \"http\" - \"-V\" - \"dst\" - \"port\" - \"80\" - name: httpbin image: kennethreitz/httpbin imagePullPolicy: Always command: - \"gunicorn\" - \"-b\" - \"0.0.0.0:80\" - \"httpbin:app\" - \"-k\" - \"gevent\" - \"--capture-output\" - \"--error-logfile\" - \"-\" - \"--access-logfile\" - \"-\" - \"--access-logformat\" - \"'%(h)s %(t)s %(r)s %(s)s Host: %({Host}i)s} Header-i: %({Header}i)s Header-o: %({Header}o)s'\" EOF oc apply -f httpbin-ingress.yaml [cloud-user@akaris-jump-server ~]$ oc get routes NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD httpbin-ingress-z9tgf httpbin-ingress.staging.akaris-osc.... ... 1 more / httpbin-service <all> None [cloud-user@akaris-jump-server ~]$ curl httpbin-ingress.staging.akaris-osc.... { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Forwarded\": \"for=10.0.88.55;host=httpbin-ingress.staging.akaris-osc....;proto=http\", \"Host\": \"httpbin-ingress.staging.akaris-osc....\", \"User-Agent\": \"curl/7.29.0\", \"X-Forwarded-Host\": \"httpbin-ingress.staging.akaris-osc....\" }, \"origin\": \"10.0.88.55\", \"url\": \"http://httpbin-ingress.staging.akaris-osc..../get\" }","title":"Step 6: Expose route"},{"location":"openshift/ingress-controller-sharding-on-separate-vip/#step-7-security-hardening","text":"This is out of scope of this PoC, but note that at the moment, the default Ingress can still handle all domains and thus if the client configures its resolver appropriately, can reach the application via the default VIP. For example, take this mapping: 10.0.88.28 *.apps.akaris-osc.... 10.0.89.89 *.staging.akaris-osc.... Then one can still use the other ingress router by modifying the client resolver: [cloud-user@akaris-jump-server ~]$ curl --resolve httpbin-ingress.staging.akaris-osc..../get:80:10.0.88.28 httpbin-ingress.staging.akaris-osc..../get { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Forwarded\": \"for=10.0.88.55;host=httpbin-ingress.staging.akaris-osc....;proto=http\", \"Host\": \"httpbin-ingress.staging.akaris-osc...., \"User-Agent\": \"curl/7.29.0\", \"X-Forwarded-Host\": \"httpbin-ingress.staging.akaris-osc...\" }, \"origin\": \"10.0.88.55\", \"url\": \"http://httpbin-ingress.staging.akaris-osc..../get\" } [cloud-user@akaris-jump-server ~]$ curl --resolve httpbin-ingress.staging.akaris-osc..../get:80:10.0.89.89 httpbin-ingress.staging.akaris-osc.../get { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Forwarded\": \"for=10.0.88.55;host=httpbin-ingress.staging.akaris-osc....;proto=http\", \"Host\": \"httpbin-ingress.staging.akaris-osc...\", \"User-Agent\": \"curl/7.29.0\", \"X-Forwarded-Host\": \"httpbin-ingress.staging.akaris-osc...\" }, \"origin\": \"10.0.88.55\", \"url\": \"http://httpbin-ingress.staging.akaris-osc.../get\" } And the same goes for the other way around: [cloud-user@akaris-jump-server ~]$ curl --resolve nginx-deployment-application.apps.akaris-osc....:80:10.0.88.28 nginx-deployment-application.apps.akaris-osc.... Nginx A [cloud-user@akaris-jump-server ~]$ curl --resolve nginx-deployment-application.apps.akaris-osc....:80:10.0.89.89 nginx-deployment-application.apps.akaris-osc.... Nginx A Read the documentation about IngressController sharding for further details.","title":"Step 7: Security hardening"},{"location":"openshift/ingress-controller-sharding-on-separate-vip/#step-8-troubleshooting","text":"Again, a bit outside of the scope of this PoC - the debug pods for my staging nodes stopped working overnight. This would need some further troubleshooting: [cloud-user@akaris-jump-server ~]$ oc get nodes NAME STATUS ROLES AGE VERSION akaris-osc-6klvk-master-0 Ready master 25h v1.18.3+012b3ec akaris-osc-6klvk-master-1 Ready master 25h v1.18.3+012b3ec akaris-osc-6klvk-master-2 Ready master 25h v1.18.3+012b3ec akaris-osc-6klvk-worker-g4q6z Ready worker 24h v1.18.3+012b3ec akaris-osc-6klvk-worker-hp2hz Ready staging 19h v1.18.3+012b3ec akaris-osc-6klvk-worker-l726s Ready worker 24h v1.18.3+012b3ec akaris-osc-6klvk-worker-lp2df Ready worker 24h v1.18.3+012b3ec akaris-osc-6klvk-worker-zhs67 Ready staging 19h v1.18.3+012b3ec [cloud-user@akaris-jump-server ~]$ oc debug node/akaris-osc-6klvk-worker-zhs67 Starting pod/akaris-osc-6klvk-worker-zhs67-debug ... To use host binaries, run `chroot /host` Pod IP: 192.168.2.48 If you don't see a command prompt, try pressing enter. Removing debug pod ... Error from server: error dialing backend: remote error: tls: internal error [cloud-user@akaris-jump-server ~]$","title":"Step 8: Troubleshooting"},{"location":"openshift/ingresscontroller_router_sharding_ocp_on_osp/","text":"Ingress Controller sharding for OpenShfit on OpenStack with Octavia How to configure IngressController router sharding with OpenShift 4.x on top of OpenStack Platform 13 and Octavia and default HostNetwork endpointPublishingStrategy Preface The official documentation describes how to configure router sharding: https://docs.openshift.com/container-platform/4.3/networking/configuring_ingress_cluster_traffic/configuring-ingress-cluster-traffic-ingress-controller.html#nw-ingress-sharding-namespace-labels_configuring-ingress-cluster-traffic-ingress-controller However, the default endpointPublishingStrategy is HostNetwork and by default, OpenShift configures only a single VIP for installations in OpenStack. It is possible to configure Kuryr (supported) or Octavia loadbalancers (unsupported, see https://access.redhat.com/solutions/4722521 ). In that case, the IngressController's endpoint publishing strategy needs to be set to LoadBalancerService : spec: endpointPublishingStrategy: type: LoadBalancerService But it is also possible to keep the default endpointPublishingStrategy as HostNetwork and configure additional load balancers. This is a similar approach as chosen for vSphere deployments: https://docs.openshift.com/container-platform/4.3/installing/installing_vsphere/installing-vsphere.html The following examples use 3 namespaces, default , test1 , test2 . Each namespace will be served by one IngressController. The environment hosts 3 worker nodes. Due to limitations of the HostNetwork strategy, each worker node can host exactly one IngressController's router instance. Meaning that the replica count for each controller needs to be set to 1 in an environment with 3 worker nodes. Prerequisites Make sure that the environment was configured with Octavia. Configuring test projects and the default project IngressControllers Create 2 new projects and label them: # Each ingresscontroller runs 1 replica = this works with 3 workers # default ingress will match ns: # oc get ns -l type!=test1,type!=test2 # test1 ingress will match # oc get ns -l type=test1 # test2 ingress will match # oc get ns -l type=test2 oc new-project test1 oc new-project test2 oc label namespace test1 \"type=test1\" oc label namespace test2 \"type=test2\" Create a label on each worker node to pin IngressController routers to a particular node: oc label node cluster-7n7w9-worker-zpq85 \"ingressoperator=default\" oc label node cluster-7n7w9-worker-zpq84 \"ingressoperator=test1\" oc label node cluster-7n7w9-worker-zpq83 \"ingressoperator=test2\" Configuring IngressControllers Now, patch the default IngressController to force node selection and to make sure that it will not serve namespaces test1 and test2: operator-default-patch.yaml spec: replicas: 1 namespaceSelector: matchExpressions: - key: type operator: NotIn values: - test1 - test2 nodePlacement: nodeSelector: matchLabels: ingressoperator: default oc project default oc patch -n openshift-ingress-operator ingresscontroller default --type=\"merge\" -p \"$(cat operator-default-patch.yaml)\" Now, create IngressOperators for namespaces test1 and test2: operator-test1.yaml apiVersion: v1 items: - apiVersion: operator.openshift.io/v1 kind: IngressController metadata: name: test1-ingress-controller namespace: openshift-ingress-operator spec: replicas: 1 domain: test1.cluster.example.com nodePlacement: nodeSelector: matchLabels: node-role.kubernetes.io/worker: \"\" ingressoperator: \"test1\" namespaceSelector: matchLabels: type: test1 endpointPublishingStrategy: type: HostNetwork #type: LoadBalancerService status: {} kind: List metadata: resourceVersion: \"\" selfLink: \"\" operator-test2.yaml apiVersion: v1 items: - apiVersion: operator.openshift.io/v1 kind: IngressController metadata: name: test2-ingress-controller namespace: openshift-ingress-operator spec: replicas: 1 domain: test2.cluster.example.com nodePlacement: nodeSelector: matchLabels: node-role.kubernetes.io/worker: \"\" ingressoperator: test2 namespaceSelector: matchLabels: type: test2 status: {} kind: List metadata: resourceVersion: \"\" selfLink: \"\" oc project test1 oc apply -f operator-test1.yaml oc project test2 oc apply -f operator-test2.yaml Note that at this point, the default IngressController routing will be broken. OCP on OSP runs a keepalived service which can be on either of the worker nodes. Only one of the 3 IngressControllers will be served at this moment. We will change this in a moment. Creating test builds and services oc project default oc apply -f fh-build.yaml oc start-build fh-build --follow oc apply -f fh-ingress.yaml oc project test1 oc adm policy add-scc-to-user anyuid -z default oc apply -f fh-test1-build.yaml oc start-build fh-test1-build --follow oc apply -f fh-test1-ingress.yaml oc project test2 oc adm policy add-scc-to-user anyuid -z default oc apply -f fh-test2-build.yaml oc start-build fh-test2-build --follow oc apply -f fh-test2-ingress.yaml fh-build.yaml apiVersion: image.openshift.io/v1 kind: ImageStream metadata: creationTimestamp: null generation: 1 name: fh selfLink: /apis/image.openshift.io/v1/namespaces/default/imagestreams/fh spec: lookupPolicy: local: false status: dockerImageRepository: docker-registry.default.svc:5000/fh --- apiVersion: v1 data: run-apache.sh: | #!/bin/bash /usr/sbin/httpd $OPTIONS -DFOREGROUND kind: ConfigMap metadata: creationTimestamp: null name: run-apache selfLink: /api/v1/namespaces/default/configmaps/run-apache --- apiVersion: v1 kind: BuildConfig metadata: name: fh-build spec: source: configMaps: - configMap: name: run-apache dockerfile: | FROM fedora EXPOSE 8080 RUN yum install httpd -y RUN yum install tcpdump -y RUN yum install iproute -y RUN yum install procps-ng -y RUN echo \"Apache default\" >> /var/www/html/index.html ADD run-apache.sh /usr/share/httpd/run-apache.sh RUN chown apache. /run/httpd/ -R RUN chmod -v +rx /usr/share/httpd/run-apache.sh RUN chown apache. /usr/share/httpd/run-apache.sh RUN usermod apache -s /bin/bash RUN sed -i 's/Listen 80/Listen 8080/' /etc/httpd/conf/httpd.conf RUN chown apache. /etc/httpd/logs/ -R USER apache CMD [\"/usr/share/httpd/run-apache.sh\"] strategy: dockerStrategy: noCache: true output: to: kind: ImageStreamTag name: fh:latest fh-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: fhingress-ingress annotations: kubernetes.io/ingress.allow-http: \"true\" spec: rules: - host: fh.apps.cluster.example.com http: paths: - path: / backend: serviceName: fhingress-service servicePort: 80 --- apiVersion: v1 kind: Service metadata: name: fhingress-service labels: app: fhingress-deploymentconfig spec: selector: app: fhingress-pod ports: - protocol: TCP port: 80 targetPort: 8080 --- apiVersion: apps/v1 kind: Deployment metadata: name: fhingress-deployment labels: app: fhingress-deployment spec: replicas: 3 selector: matchLabels: app: fhingress-pod template: metadata: labels: app: fhingress-pod spec: containers: - name: fhingress image: image-registry.openshift-image-registry.svc:5000/default/fh imagePullPolicy: Always fh-test1-build.yaml apiVersion: image.openshift.io/v1 kind: ImageStream metadata: creationTimestamp: null generation: 1 name: fh-test1 selfLink: /apis/image.openshift.io/v1/namespaces/default/imagestreams/fh-test1 spec: lookupPolicy: local: false status: dockerImageRepository: docker-registry.default.svc:5000/fh-test1 --- apiVersion: v1 data: run-apache.sh: | #!/bin/bash /usr/sbin/httpd $OPTIONS -DFOREGROUND kind: ConfigMap metadata: creationTimestamp: null name: run-apache selfLink: /api/v1/namespaces/default/configmaps/run-apache --- apiVersion: v1 kind: BuildConfig metadata: name: fh-test1-build spec: source: configMaps: - configMap: name: run-apache dockerfile: | FROM fedora EXPOSE 8080 RUN yum install httpd -y RUN yum install tcpdump -y RUN yum install iproute -y RUN yum install procps-ng -y RUN echo \"Apache test1\" >> /var/www/html/index.html ADD run-apache.sh /usr/share/httpd/run-apache.sh RUN chown apache. /run/httpd/ -R RUN chmod -v +rx /usr/share/httpd/run-apache.sh RUN chown apache. /usr/share/httpd/run-apache.sh RUN usermod apache -s /bin/bash RUN sed -i 's/Listen 80/Listen 8080/' /etc/httpd/conf/httpd.conf RUN chown apache. /etc/httpd/logs/ -R USER apache CMD [\"/usr/share/httpd/run-apache.sh\"] strategy: dockerStrategy: noCache: true output: to: kind: ImageStreamTag name: fh-test1:latest fh-test1-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: fh-test1ingress-ingress annotations: kubernetes.io/ingress.allow-http: \"true\" spec: rules: - host: fh.test1.cluster.example.com http: paths: - path: / backend: serviceName: fh-test1ingress-service servicePort: 80 --- apiVersion: v1 kind: Service metadata: name: fh-test1ingress-service labels: app: fh-test1ingress-deploymentconfig spec: selector: app: fh-test1ingress-pod ports: - protocol: TCP port: 80 targetPort: 8080 --- apiVersion: apps/v1 kind: Deployment metadata: name: fh-test1ingress-deployment labels: app: fh-test1ingress-deployment spec: replicas: 3 selector: matchLabels: app: fh-test1ingress-pod template: metadata: labels: app: fh-test1ingress-pod spec: containers: - name: fh-test1ingress image: image-registry.openshift-image-registry.svc:5000/test1/fh-test1 imagePullPolicy: Always fh-test2-build.yaml apiVersion: image.openshift.io/v1 kind: ImageStream metadata: creationTimestamp: null generation: 1 name: fh-test2 selfLink: /apis/image.openshift.io/v1/namespaces/default/imagestreams/fh-test2 spec: lookupPolicy: local: false status: dockerImageRepository: docker-registry.default.svc:5000/fh-test2 --- apiVersion: v1 data: run-apache.sh: | #!/bin/bash /usr/sbin/httpd $OPTIONS -DFOREGROUND kind: ConfigMap metadata: creationTimestamp: null name: run-apache selfLink: /api/v1/namespaces/default/configmaps/run-apache --- apiVersion: v1 kind: BuildConfig metadata: name: fh-test2-build spec: source: configMaps: - configMap: name: run-apache dockerfile: | FROM fedora EXPOSE 8080 RUN yum install httpd -y RUN yum install tcpdump -y RUN yum install iproute -y RUN yum install procps-ng -y RUN echo \"Apache test2\" >> /var/www/html/index.html ADD run-apache.sh /usr/share/httpd/run-apache.sh RUN chown apache. /run/httpd/ -R RUN chmod -v +rx /usr/share/httpd/run-apache.sh RUN chown apache. /usr/share/httpd/run-apache.sh RUN usermod apache -s /bin/bash RUN sed -i 's/Listen 80/Listen 8080/' /etc/httpd/conf/httpd.conf RUN chown apache. /etc/httpd/logs/ -R USER apache CMD [\"/usr/share/httpd/run-apache.sh\"] strategy: dockerStrategy: noCache: true output: to: kind: ImageStreamTag name: fh-test2:latest fh-test2-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: fh-test2ingress-ingress annotations: kubernetes.io/ingress.allow-http: \"true\" spec: rules: - host: fh.test2.cluster.example.com http: paths: - path: / backend: serviceName: fh-test2ingress-service servicePort: 80 --- apiVersion: v1 kind: Service metadata: name: fh-test2ingress-service labels: app: fh-test2ingress-deploymentconfig spec: selector: app: fh-test2ingress-pod ports: - protocol: TCP port: 80 targetPort: 8080 --- apiVersion: apps/v1 kind: Deployment metadata: name: fh-test2ingress-deployment labels: app: fh-test2ingress-deployment spec: replicas: 3 selector: matchLabels: app: fh-test2ingress-pod template: metadata: labels: app: fh-test2ingress-pod spec: containers: - name: fh-test2ingress image: image-registry.openshift-image-registry.svc:5000/test2/fh-test2 imagePullPolicy: Always Configuring Octavia loadbalancers for server IngressControllers Instead of using the default keepalived VIP, we are going to create one Octavia loadbalancer per IngressController: (overcloud) [stack@undercloud-0 ~]$ openstack server list +--------------------------------------+----------------------------+--------+--------------------------------------------------------------------------+---------------------+--------------+ | ID | Name | Status | Networks | Image | Flavor | +--------------------------------------+----------------------------+--------+--------------------------------------------------------------------------+---------------------+--------------+ | 2e5bc035-e6da-4807-8c96-268c2587c759 | cluster-7n7w9-worker-fkv5p | ACTIVE | cluster-7n7w9-openshift=172.31.0.45 | cluster-7n7w9-rhcos | m1.openshift | | 14406e18-74bd-4723-81b1-ea38080d3e72 | cluster-7n7w9-worker-ggxds | ACTIVE | cluster-7n7w9-openshift=172.31.0.13 | cluster-7n7w9-rhcos | m1.openshift | | 7ad9432b-bb4f-4cc1-b842-1e63a323a3b5 | cluster-7n7w9-worker-zpq85 | ACTIVE | cluster-7n7w9-openshift=172.31.0.14 | cluster-7n7w9-rhcos | m1.openshift | | dc4db4f7-d410-4404-9166-66204931538c | cluster-7n7w9-master-1 | ACTIVE | cluster-7n7w9-openshift=172.31.0.15 | cluster-7n7w9-rhcos | m1.openshift | | e0e3a4cd-4d1e-4496-9e59-ba8c0d62b54a | cluster-7n7w9-master-2 | ACTIVE | cluster-7n7w9-openshift=172.31.0.23 | cluster-7n7w9-rhcos | m1.openshift | | 1b76ade6-ad53-43c8-9f15-fd3d550e0a17 | cluster-7n7w9-master-0 | ACTIVE | cluster-7n7w9-openshift=172.31.0.39 | cluster-7n7w9-rhcos | m1.openshift | | 43810d22-74ce-4f5a-a615-d77d1628dde7 | rhel-test1 | ACTIVE | private1=2000:192:168:0:f816:3eff:fea4:39e1, 192.168.0.108, 172.16.0.208 | rhel | m1.small | +--------------------------------------+----------------------------+--------+--------------------------------------------------------------------------+---------------------+--------------+ (overcloud) [stack@undercloud-0 ~]$ openstack server list --all +--------------------------------------+----------------------------------------------+--------+--------------------------------------------------------------------------+----------------------------------------+--------------+ | ID | Name | Status | Networks | Image | Flavor | +--------------------------------------+----------------------------------------------+--------+--------------------------------------------------------------------------+----------------------------------------+--------------+ | 2bdd7c68-2b00-4824-9ebe-6cc3babdfe77 | amphora-da1a5888-1052-4a6b-a05d-fc21c22a5cf5 | ACTIVE | lb-mgmt-net=172.24.0.13; cluster-7n7w9-openshift=172.31.0.20 | octavia-amphora-13.0-20200323.2.x86_64 | | | 2e5bc035-e6da-4807-8c96-268c2587c759 | cluster-7n7w9-worker-fkv5p | ACTIVE | cluster-7n7w9-openshift=172.31.0.45 | cluster-7n7w9-rhcos | m1.openshift | | 14406e18-74bd-4723-81b1-ea38080d3e72 | cluster-7n7w9-worker-ggxds | ACTIVE | cluster-7n7w9-openshift=172.31.0.13 | cluster-7n7w9-rhcos | m1.openshift | | 7ad9432b-bb4f-4cc1-b842-1e63a323a3b5 | cluster-7n7w9-worker-zpq85 | ACTIVE | cluster-7n7w9-openshift=172.31.0.14 | cluster-7n7w9-rhcos | m1.openshift | | dc4db4f7-d410-4404-9166-66204931538c | cluster-7n7w9-master-1 | ACTIVE | cluster-7n7w9-openshift=172.31.0.15 | cluster-7n7w9-rhcos | m1.openshift | | e0e3a4cd-4d1e-4496-9e59-ba8c0d62b54a | cluster-7n7w9-master-2 | ACTIVE | cluster-7n7w9-openshift=172.31.0.23 | cluster-7n7w9-rhcos | m1.openshift | | 1b76ade6-ad53-43c8-9f15-fd3d550e0a17 | cluster-7n7w9-master-0 | ACTIVE | cluster-7n7w9-openshift=172.31.0.39 | cluster-7n7w9-rhcos | m1.openshift | | 43810d22-74ce-4f5a-a615-d77d1628dde7 | rhel-test1 | ACTIVE | private1=2000:192:168:0:f816:3eff:fea4:39e1, 192.168.0.108, 172.16.0.208 | rhel | m1.small | +--------------------------------------+----------------------------------------------+--------+--------------------------------------------------------------------------+----------------------------------------+--------------+ (overcloud) [stack@undercloud-0 ~]$ # openstack loadbalancer create --name ingress-controller-default --vip-subnet-id (overcloud) [stack@undercloud-0 ~]$ openstack subnet list +--------------------------------------+---------------------------------------------------+--------------------------------------+----------------------+ | ID | Name | Network | Subnet | +--------------------------------------+---------------------------------------------------+--------------------------------------+----------------------+ | 14a3bd37-7a3a-4f14-bd03-26d4860d91db | provider1-subnet | d14c0815-22b5-4cdf-9db1-5da7951f1e0a | 172.16.0.0/24 | | 5aef701a-c93f-4210-8722-3fdf8ef1ddf1 | provider1-ipv6-subnet | d14c0815-22b5-4cdf-9db1-5da7951f1e0a | 2000:10::/64 | | 63950c5e-fc70-49ad-8d6b-092b8ba39016 | cluster-7n7w9-nodes | ac9c0bd9-3793-48d3-a12e-eaa06028a366 | 172.31.0.0/16 | | 7f65d115-8077-4219-9ef8-3b0820ba8b52 | private1-ipv6-subnet | 0af094e6-1dac-4479-9ebc-88f8f7f0b254 | 2000:192:168::/64 | | 9fa85591-718c-4fb9-88d8-0c2e5b0771ee | private1-subnet | 0af094e6-1dac-4479-9ebc-88f8f7f0b254 | 192.168.0.0/24 | | a0827588-7460-4f2f-9f29-12bb1be49c76 | lb-mgmt-subnet | 85cd5be1-3d8b-4929-b9ac-2e770cb12ba9 | 172.24.0.0/16 | | a5e9841d-38a7-47a0-8db2-f302a67a9845 | private2-subnet | fdb07e70-7055-41a9-a8fa-10b2dce988b3 | 192.168.1.0/24 | | af925c70-979e-4fa9-8704-72ec683a31f8 | private-mgmt-ipv6-subnet | 0d74f260-c3d6-4a77-8881-438c3bfaaa7b | 2000:192:168:10::/64 | | c815a934-98ff-4daa-8b5e-5a0e5d91806f | private2-ipv6-subnet | fdb07e70-7055-41a9-a8fa-10b2dce988b3 | 2000:192:168:1::/64 | | df05dd9e-6583-42a0-afee-e8f65d813e6f | private-mgmt-subnet | 0d74f260-c3d6-4a77-8881-438c3bfaaa7b | 192.168.10.0/24 | | e1ebbac2-3d7c-4355-9624-f0920d705f76 | HA subnet tenant a416f556938f454f849da42faa317cd3 | 5079d963-7301-4218-855b-f7d33fce1081 | 169.254.192.0/18 | +--------------------------------------+---------------------------------------------------+--------------------------------------+----------------------+ (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer create --name ingress-controller-default --vip-subnet-id cluster-7n7w9-nodes +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | admin_state_up | True | | created_at | 2020-04-08T10:27:46 | | description | | | flavor | | | id | e5feb42d-77e4-428b-9ad0-55fe6d30f3b3 | | listeners | | | name | ingress-controller-default | | operating_status | OFFLINE | | pools | | | project_id | a416f556938f454f849da42faa317cd3 | | provider | octavia | | provisioning_status | PENDING_CREATE | | updated_at | None | | vip_address | 172.31.0.21 | | vip_network_id | ac9c0bd9-3793-48d3-a12e-eaa06028a366 | | vip_port_id | f336d8c6-8f47-4d61-a865-19f34a671ffa | | vip_qos_policy_id | None | | vip_subnet_id | 63950c5e-fc70-49ad-8d6b-092b8ba39016 | +---------------------+--------------------------------------+ (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer create --name ingress-controller-test1 --vip-subnet-id cluster-7n7w9-nodes +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | admin_state_up | True | | created_at | 2020-04-08T10:28:33 | | description | | | flavor | | | id | a636c15a-92a2-4253-8945-8875c58aae96 | | listeners | | | name | ingress-controller-test1 | | operating_status | OFFLINE | | pools | | | project_id | a416f556938f454f849da42faa317cd3 | | provider | octavia | | provisioning_status | PENDING_CREATE | | updated_at | None | | vip_address | 172.31.0.24 | | vip_network_id | ac9c0bd9-3793-48d3-a12e-eaa06028a366 | | vip_port_id | 7d85d059-65c2-47af-90a2-35337d4396f8 | | vip_qos_policy_id | None | | vip_subnet_id | 63950c5e-fc70-49ad-8d6b-092b8ba39016 | +---------------------+--------------------------------------+ (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer create --name ingress-controller-test2 --vip-subnet-id cluster-7n7w9-nodes +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | admin_state_up | True | | created_at | 2020-04-08T10:28:44 | | description | | | flavor | | | id | c2041396-66f5-4726-ad47-9ce7d2f7904f | | listeners | | | name | ingress-controller-test2 | | operating_status | OFFLINE | | pools | | | project_id | a416f556938f454f849da42faa317cd3 | | provider | octavia | | provisioning_status | PENDING_CREATE | | updated_at | None | | vip_address | 172.31.0.27 | | vip_network_id | ac9c0bd9-3793-48d3-a12e-eaa06028a366 | | vip_port_id | ce0129ca-9b39-4ee1-8d12-bb0685967f65 | | vip_qos_policy_id | None | | vip_subnet_id | 63950c5e-fc70-49ad-8d6b-092b8ba39016 | +---------------------+--------------------------------------+ # wait until all loadbalancers are active (overcloud) [stack@undercloud-0 ~]$ watch openstack loadbalancer list (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer list +--------------------------------------+----------------------------+----------------------------------+-------------+---------------------+----------+ | id | name | project_id | vip_address | provisioning_status | provider | +--------------------------------------+----------------------------+----------------------------------+-------------+---------------------+----------+ | e5feb42d-77e4-428b-9ad0-55fe6d30f3b3 | ingress-controller-default | a416f556938f454f849da42faa317cd3 | 172.31.0.21 | ACTIVE | octavia | | a636c15a-92a2-4253-8945-8875c58aae96 | ingress-controller-test1 | a416f556938f454f849da42faa317cd3 | 172.31.0.24 | ACTIVE | octavia | | c2041396-66f5-4726-ad47-9ce7d2f7904f | ingress-controller-test2 | a416f556938f454f849da42faa317cd3 | 172.31.0.27 | ACTIVE | octavia | +--------------------------------------+----------------------------+----------------------------------+-------------+---------------------+----------+ (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer listener create --name ingress-controller-default-listener --protocol HTTP --protocol-port 80 ingress-controller-default +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | True | | connection_limit | -1 | | created_at | 2020-04-08T11:00:01 | | default_pool_id | None | | default_tls_container_ref | None | | description | | | id | be0d73e6-1162-4363-ad62-0d74e2c16f6b | | insert_headers | None | | l7policies | | | loadbalancers | e5feb42d-77e4-428b-9ad0-55fe6d30f3b3 | | name | ingress-controller-default-listener | | operating_status | OFFLINE | | project_id | a416f556938f454f849da42faa317cd3 | | protocol | HTTP | | protocol_port | 80 | | provisioning_status | PENDING_CREATE | | sni_container_refs | [] | | updated_at | None | +---------------------------+--------------------------------------+ (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer pool create --name ingress-controller-default-pool --lb-algorithm ROUND_ROBIN --listener ingress-controller-default-listener --protocol HTTP +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | admin_state_up | True | | created_at | 2020-04-08T11:00:13 | | description | | | healthmonitor_id | | | id | 47dc4567-f182-4c16-a2ec-aa306e40779d | | lb_algorithm | ROUND_ROBIN | | listeners | be0d73e6-1162-4363-ad62-0d74e2c16f6b | | loadbalancers | e5feb42d-77e4-428b-9ad0-55fe6d30f3b3 | | members | | | name | ingress-controller-default-pool | | operating_status | OFFLINE | | project_id | a416f556938f454f849da42faa317cd3 | | protocol | HTTP | | provisioning_status | PENDING_CREATE | | session_persistence | None | | updated_at | None | +---------------------+--------------------------------------+ (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer member create --subnet-id cluster-7n7w9-nodes --address 172.31.0.45 --protocol-port 80 ingress-controller-default-pool +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | address | 172.31.0.45 | | admin_state_up | True | | created_at | 2020-04-08T11:02:06 | | id | 431aa74f-e98d-41fd-aab9-bf907f1e3993 | | name | | | operating_status | NO_MONITOR | | project_id | a416f556938f454f849da42faa317cd3 | | protocol_port | 80 | | provisioning_status | PENDING_CREATE | | subnet_id | 63950c5e-fc70-49ad-8d6b-092b8ba39016 | | updated_at | None | | weight | 1 | | monitor_port | None | | monitor_address | None | +---------------------+--------------------------------------+ (overcloud) [stack@undercloud-0 ~]$ openstack floating ip create provider1 +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | created_at | 2020-04-08T11:02:34Z | | description | | | fixed_ip_address | None | | floating_ip_address | 172.16.0.211 | | floating_network_id | d14c0815-22b5-4cdf-9db1-5da7951f1e0a | | id | 44720e51-dc95-453b-9993-1cac1fbf40b9 | | name | 172.16.0.211 | | port_id | None | | project_id | a416f556938f454f849da42faa317cd3 | | qos_policy_id | None | | revision_number | 0 | | router_id | None | | status | DOWN | | subnet_id | None | | updated_at | 2020-04-08T11:02:34Z | +---------------------+--------------------------------------+ oad(overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer list +--------------------------------------+----------------------------+----------------------------------+-------------+---------------------+----------+ | id | name | project_id | vip_address | provisioning_status | provider | +--------------------------------------+----------------------------+----------------------------------+-------------+---------------------+----------+ | e5feb42d-77e4-428b-9ad0-55fe6d30f3b3 | ingress-controller-default | a416f556938f454f849da42faa317cd3 | 172.31.0.21 | ACTIVE | octavia | | a636c15a-92a2-4253-8945-8875c58aae96 | ingress-controller-test1 | a416f556938f454f849da42faa317cd3 | 172.31.0.24 | ACTIVE | octavia | | c2041396-66f5-4726-ad47-9ce7d2f7904f | ingress-controller-test2 | a416f556938f454f849da42faa317cd3 | 172.31.0.27 | ACTIVE | octavia | +--------------------------------------+----------------------------+----------------------------------+-------------+---------------------+----------+ (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer listener create --name ingress-controller-test1-listener --protocol HTTP --protocol-port 80 ingress-controller-test1 +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | True | | connection_limit | -1 | | created_at | 2020-04-08T11:03:33 | | default_pool_id | None | | default_tls_container_ref | None | | description | | | id | 9e7236f5-cb15-4c8c-8ffb-a00182c44871 | | insert_headers | None | | l7policies | | | loadbalancers | a636c15a-92a2-4253-8945-8875c58aae96 | | name | ingress-controller-test1-listener | | operating_status | OFFLINE | | project_id | a416f556938f454f849da42faa317cd3 | | protocol | HTTP | | protocol_port | 80 | | provisioning_status | PENDING_CREATE | | sni_container_refs | [] | | updated_at | None | +---------------------------+--------------------------------------+ (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer listener create --name ingress-controller-test2-listener --protocol HTTP --protocol-port 80 ingress-controller-test2 +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | True | | connection_limit | -1 | | created_at | 2020-04-08T11:03:42 | | default_pool_id | None | | default_tls_container_ref | None | | description | | | id | 0cd99fe3-3750-4506-bafa-57cf4a15c7e8 | | insert_headers | None | | l7policies | | | loadbalancers | c2041396-66f5-4726-ad47-9ce7d2f7904f | | name | ingress-controller-test2-listener | | operating_status | OFFLINE | | project_id | a416f556938f454f849da42faa317cd3 | | protocol | HTTP | | protocol_port | 80 | | provisioning_status | PENDING_CREATE | | sni_container_refs | [] | | updated_at | None | +---------------------------+--------------------------------------+ (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer pool create --name ingress-controller-test1-pool --lb-algorithm ROUND_ROBIN --listener ingress-controller-test1-listener --protocol HTTP +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | admin_state_up | True | | created_at | 2020-04-08T11:04:00 | | description | | | healthmonitor_id | | | id | d7e93f06-3afd-467d-b43a-4018e59d6be1 | | lb_algorithm | ROUND_ROBIN | | listeners | 9e7236f5-cb15-4c8c-8ffb-a00182c44871 | | loadbalancers | a636c15a-92a2-4253-8945-8875c58aae96 | | members | | | name | ingress-controller-test1-pool | | operating_status | OFFLINE | | project_id | a416f556938f454f849da42faa317cd3 | | protocol | HTTP | | provisioning_status | PENDING_CREATE | | session_persistence | None | | updated_at | None | +---------------------+--------------------------------------+ (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer pool create --name ingress-controller-test2-pool --lb-algorithm ROUND_ROBIN --listener ingress-controller-test2-listener --protocol HTTP +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | admin_state_up | True | | created_at | 2020-04-08T11:04:10 | | description | | | healthmonitor_id | | | id | c6a4c5dd-836f-42fe-91bf-5a71d3109691 | | lb_algorithm | ROUND_ROBIN | | listeners | 0cd99fe3-3750-4506-bafa-57cf4a15c7e8 | | loadbalancers | c2041396-66f5-4726-ad47-9ce7d2f7904f | | members | | | name | ingress-controller-test2-pool | | operating_status | OFFLINE | | project_id | a416f556938f454f849da42faa317cd3 | | protocol | HTTP | | provisioning_status | PENDING_CREATE | | session_persistence | None | | updated_at | None | +---------------------+--------------------------------------+ (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer member create --subnet-id cluster-7n7w9-nodes --address 172.31.0.13 --protocol-port 80 ingress-controller-test1-pool +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | address | 172.31.0.13 | | admin_state_up | True | | created_at | 2020-04-08T11:05:02 | | id | 584a9a8b-eb1c-41b4-9308-0a1cd9f91122 | | name | | | operating_status | NO_MONITOR | | project_id | a416f556938f454f849da42faa317cd3 | | protocol_port | 80 | | provisioning_status | PENDING_CREATE | | subnet_id | 63950c5e-fc70-49ad-8d6b-092b8ba39016 | | updated_at | None | | weight | 1 | | monitor_port | None | | monitor_address | None | +---------------------+--------------------------------------+ (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer member create --subnet-id cluster-7n7w9-nodes --address 172.31.0.14 --protocol-port 80 ingress-controller-test2-pool openstack floating ip creat+---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | address | 172.31.0.14 | | admin_state_up | True | | created_at | 2020-04-08T11:05:14 | | id | 8d859649-fcc1-4ffe-b9be-0ed6540346e1 | | name | | | operating_status | NO_MONITOR | | project_id | a416f556938f454f849da42faa317cd3 | | protocol_port | 80 | | provisioning_status | PENDING_CREATE | | subnet_id | 63950c5e-fc70-49ad-8d6b-092b8ba39016 | | updated_at | None | | weight | 1 | | monitor_port | None | | monitor_address | None | +---------------------+--------------------------------------+ (overcloud) [stack@undercloud-0 ~]$ openstack floating ip create provider1 o[+---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | created_at | 2020-04-08T11:05:27Z | | description | | | fixed_ip_address | None | | floating_ip_address | 172.16.0.204 | | floating_network_id | d14c0815-22b5-4cdf-9db1-5da7951f1e0a | | id | 72181227-12f0-4790-bd7c-208785e05918 | | name | 172.16.0.204 | | port_id | None | | project_id | a416f556938f454f849da42faa317cd3 | | qos_policy_id | None | | revision_number | 0 | | router_id | None | | status | DOWN | | subnet_id | None | | updated_at | 2020-04-08T11:05:27Z | +---------------------+--------------------------------------+ (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer list openstack floating ip +--------------------------------------+----------------------------+----------------------------------+-------------+---------------------+----------+ | id | name | project_id | vip_address | provisioning_status | provider | +--------------------------------------+----------------------------+----------------------------------+-------------+---------------------+----------+ | e5feb42d-77e4-428b-9ad0-55fe6d30f3b3 | ingress-controller-default | a416f556938f454f849da42faa317cd3 | 172.31.0.21 | ACTIVE | octavia | | a636c15a-92a2-4253-8945-8875c58aae96 | ingress-controller-test1 | a416f556938f454f849da42faa317cd3 | 172.31.0.24 | ACTIVE | octavia | | c2041396-66f5-4726-ad47-9ce7d2f7904f | ingress-controller-test2 | a416f556938f454f849da42faa317cd3 | 172.31.0.27 | ACTIVE | octavia | +--------------------------------------+----------------------------+----------------------------------+-------------+---------------------+----------+ l(overcloud) [stack@undercloud-0 ~]$ openstack floating ip list +--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+ | ID | Floating IP Address | Fixed IP Address | Port | Floating Network | Project | +--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+ | 0a3e9fa7-577c-47f9-b9e4-f583bd3ce180 | 172.16.0.208 | 192.168.0.108 | 03d91ec5-8de6-415b-9581-d00501bb40ed | d14c0815-22b5-4cdf-9db1-5da7951f1e0a | a416f556938f454f849da42faa317cd3 | | 44720e51-dc95-453b-9993-1cac1fbf40b9 | 172.16.0.211 | None | None | d14c0815-22b5-4cdf-9db1-5da7951f1e0a | a416f556938f454f849da42faa317cd3 | | 497c6667-22b3-4ed7-b6af-1c5091b14209 | 172.16.0.213 | 172.31.0.5 | 3bc4d5e1-90c9-4da1-86cd-d5e5d6fe985d | d14c0815-22b5-4cdf-9db1-5da7951f1e0a | a416f556938f454f849da42faa317cd3 | | 4b9e9daf-4e7f-4338-ad70-6206e7e56367 | 172.16.0.214 | 172.31.0.7 | ee68b77c-95af-4bb2-ba62-c4d06d728c47 | d14c0815-22b5-4cdf-9db1-5da7951f1e0a | a416f556938f454f849da42faa317cd3 | | 72181227-12f0-4790-bd7c-208785e05918 | 172.16.0.204 | None | None | d14c0815-22b5-4cdf-9db1-5da7951f1e0a | a416f556938f454f849da42faa317cd3 | | a2693cfc-4379-4d9b-bb90-a86c3fcafd9c | 172.16.0.217 | None | None | d14c0815-22b5-4cdf-9db1-5da7951f1e0a | a416f556938f454f849da42faa317cd3 | +--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+ (overcloud) [stack@undercloud-0 ~]$ openstack port list | grep 172.31.0.21 | f336d8c6-8f47-4d61-a865-19f34a671ffa | octavia-lb-e5feb42d-77e4-428b-9ad0-55fe6d30f3b3 | fa:16:3e:f1:04:50 | ip_address='172.31.0.21', subnet_id='63950c5e-fc70-49ad-8d6b-092b8ba39016' | DOWN | (overcloud) [stack@undercloud-0 ~]$ openstack port list | grep 172.31.0.24 | 7d85d059-65c2-47af-90a2-35337d4396f8 | octavia-lb-a636c15a-92a2-4253-8945-8875c58aae96 | fa:16:3e:8c:32:69 | ip_address='172.31.0.24', subnet_id='63950c5e-fc70-49ad-8d6b-092b8ba39016' | DOWN | (overcloud) [stack@undercloud-0 ~]$ openstack port list | grep 172.31.0.27 | ce0129ca-9b39-4ee1-8d12-bb0685967f65 | octavia-lb-c2041396-66f5-4726-ad47-9ce7d2f7904f | fa:16:3e:e0:d7:bd | ip_address='172.31.0.27', subnet_id='63950c5e-fc70-49ad-8d6b-092b8ba39016' | DOWN | (overcloud) [stack@undercloud-0 ~]$ openstack floating ip set 172.16.0.211 --port octavia-lb-e5feb42d-77e4-428b-9ad0-55fe6d30f3b3 (overcloud) [stack@undercloud-0 ~]$ openstack floating ip set 172.16.0.204 --port octavia-lb-a636c15a-92a2-4253-8945-8875c58aae96 (overcloud) [stack@undercloud-0 ~]$ openstack floating ip set 172.16.0.217 --port octavia-lb-c2041396-66f5-4726-ad47-9ce7d2f7904f (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer member list ingress-controller-default-pool +--------------------------------------+------+----------------------------------+---------------------+-------------+---------------+------------------+--------+ | id | name | project_id | provisioning_status | address | protocol_port | operating_status | weight | +--------------------------------------+------+----------------------------------+---------------------+-------------+---------------+------------------+--------+ | 431aa74f-e98d-41fd-aab9-bf907f1e3993 | | a416f556938f454f849da42faa317cd3 | ACTIVE | 172.31.0.45 | 80 | NO_MONITOR | 1 | +--------------------------------------+------+----------------------------------+---------------------+-------------+---------------+------------------+--------+ (overcloud) [stack@undercloud-0 ~]$ curl ^C Once this is done, update your DNS entries. In this lab case, I'm simply changing /etc/hosts : (overcloud) [stack@undercloud-0 ~]$ cat /etc/hosts 127.0.0.1 undercloud-0.redhat.local undercloud-0 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 # API cluster VIP 172.16.0.213 api.cluster.example.com # original VIP # 172.16.0.214 oauth-openshift.apps.cluster.example.com console-openshift-console.apps.cluster.example.com downloads-openshift-console.apps.cluster.example.com alertmanager-main-openshift-monitoring.apps.cluster.example.com grafana-openshift-monitoring.apps.cluster.example.com prometheus-k8s-openshift-monitoring.apps.cluster.example.com thanos-querier-openshift-monitoring.apps.cluster.example.com fh.apps.cluster.example.com fh.test2.cluster.example.com fh.test1.cluster.example.com # new Octavia VIPs 172.16.0.211 oauth-openshift.apps.cluster.example.com console-openshift-console.apps.cluster.example.com downloads-openshift-console.apps.cluster.example.com alertmanager-main-openshift-monitoring.apps.cluster.example.com grafana-openshift-monitoring.apps.cluster.example.com prometheus-k8s-openshift-monitoring.apps.cluster.example.com thanos-querier-openshift-monitoring.apps.cluster.example.com fh.apps.cluster.example.com 172.16.0.204 fh.test1.cluster.example.com 172.16.0.217 fh.test2.cluster.example.com Testing This should now work: (overcloud) [stack@undercloud-0 ~]$ curl fh.apps.cluster.example.com Apache default (overcloud) [stack@undercloud-0 ~]$ curl fh.test1.cluster.example.com Apache test1 (overcloud) [stack@undercloud-0 ~]$ curl fh.test2.cluster.example.com Apache test2","title":"Ingress Controller Sharding OCP on OSP"},{"location":"openshift/ingresscontroller_router_sharding_ocp_on_osp/#ingress-controller-sharding-for-openshfit-on-openstack-with-octavia","text":"How to configure IngressController router sharding with OpenShift 4.x on top of OpenStack Platform 13 and Octavia and default HostNetwork endpointPublishingStrategy","title":"Ingress Controller sharding for OpenShfit on OpenStack with Octavia"},{"location":"openshift/ingresscontroller_router_sharding_ocp_on_osp/#preface","text":"The official documentation describes how to configure router sharding: https://docs.openshift.com/container-platform/4.3/networking/configuring_ingress_cluster_traffic/configuring-ingress-cluster-traffic-ingress-controller.html#nw-ingress-sharding-namespace-labels_configuring-ingress-cluster-traffic-ingress-controller However, the default endpointPublishingStrategy is HostNetwork and by default, OpenShift configures only a single VIP for installations in OpenStack. It is possible to configure Kuryr (supported) or Octavia loadbalancers (unsupported, see https://access.redhat.com/solutions/4722521 ). In that case, the IngressController's endpoint publishing strategy needs to be set to LoadBalancerService : spec: endpointPublishingStrategy: type: LoadBalancerService But it is also possible to keep the default endpointPublishingStrategy as HostNetwork and configure additional load balancers. This is a similar approach as chosen for vSphere deployments: https://docs.openshift.com/container-platform/4.3/installing/installing_vsphere/installing-vsphere.html The following examples use 3 namespaces, default , test1 , test2 . Each namespace will be served by one IngressController. The environment hosts 3 worker nodes. Due to limitations of the HostNetwork strategy, each worker node can host exactly one IngressController's router instance. Meaning that the replica count for each controller needs to be set to 1 in an environment with 3 worker nodes.","title":"Preface"},{"location":"openshift/ingresscontroller_router_sharding_ocp_on_osp/#prerequisites","text":"Make sure that the environment was configured with Octavia.","title":"Prerequisites"},{"location":"openshift/ingresscontroller_router_sharding_ocp_on_osp/#configuring-test-projects-and-the-default-project-ingresscontrollers","text":"Create 2 new projects and label them: # Each ingresscontroller runs 1 replica = this works with 3 workers # default ingress will match ns: # oc get ns -l type!=test1,type!=test2 # test1 ingress will match # oc get ns -l type=test1 # test2 ingress will match # oc get ns -l type=test2 oc new-project test1 oc new-project test2 oc label namespace test1 \"type=test1\" oc label namespace test2 \"type=test2\" Create a label on each worker node to pin IngressController routers to a particular node: oc label node cluster-7n7w9-worker-zpq85 \"ingressoperator=default\" oc label node cluster-7n7w9-worker-zpq84 \"ingressoperator=test1\" oc label node cluster-7n7w9-worker-zpq83 \"ingressoperator=test2\"","title":"Configuring test projects and the default project IngressControllers"},{"location":"openshift/ingresscontroller_router_sharding_ocp_on_osp/#configuring-ingresscontrollers","text":"Now, patch the default IngressController to force node selection and to make sure that it will not serve namespaces test1 and test2: operator-default-patch.yaml spec: replicas: 1 namespaceSelector: matchExpressions: - key: type operator: NotIn values: - test1 - test2 nodePlacement: nodeSelector: matchLabels: ingressoperator: default oc project default oc patch -n openshift-ingress-operator ingresscontroller default --type=\"merge\" -p \"$(cat operator-default-patch.yaml)\" Now, create IngressOperators for namespaces test1 and test2: operator-test1.yaml apiVersion: v1 items: - apiVersion: operator.openshift.io/v1 kind: IngressController metadata: name: test1-ingress-controller namespace: openshift-ingress-operator spec: replicas: 1 domain: test1.cluster.example.com nodePlacement: nodeSelector: matchLabels: node-role.kubernetes.io/worker: \"\" ingressoperator: \"test1\" namespaceSelector: matchLabels: type: test1 endpointPublishingStrategy: type: HostNetwork #type: LoadBalancerService status: {} kind: List metadata: resourceVersion: \"\" selfLink: \"\" operator-test2.yaml apiVersion: v1 items: - apiVersion: operator.openshift.io/v1 kind: IngressController metadata: name: test2-ingress-controller namespace: openshift-ingress-operator spec: replicas: 1 domain: test2.cluster.example.com nodePlacement: nodeSelector: matchLabels: node-role.kubernetes.io/worker: \"\" ingressoperator: test2 namespaceSelector: matchLabels: type: test2 status: {} kind: List metadata: resourceVersion: \"\" selfLink: \"\" oc project test1 oc apply -f operator-test1.yaml oc project test2 oc apply -f operator-test2.yaml Note that at this point, the default IngressController routing will be broken. OCP on OSP runs a keepalived service which can be on either of the worker nodes. Only one of the 3 IngressControllers will be served at this moment. We will change this in a moment.","title":"Configuring IngressControllers"},{"location":"openshift/ingresscontroller_router_sharding_ocp_on_osp/#creating-test-builds-and-services","text":"oc project default oc apply -f fh-build.yaml oc start-build fh-build --follow oc apply -f fh-ingress.yaml oc project test1 oc adm policy add-scc-to-user anyuid -z default oc apply -f fh-test1-build.yaml oc start-build fh-test1-build --follow oc apply -f fh-test1-ingress.yaml oc project test2 oc adm policy add-scc-to-user anyuid -z default oc apply -f fh-test2-build.yaml oc start-build fh-test2-build --follow oc apply -f fh-test2-ingress.yaml fh-build.yaml apiVersion: image.openshift.io/v1 kind: ImageStream metadata: creationTimestamp: null generation: 1 name: fh selfLink: /apis/image.openshift.io/v1/namespaces/default/imagestreams/fh spec: lookupPolicy: local: false status: dockerImageRepository: docker-registry.default.svc:5000/fh --- apiVersion: v1 data: run-apache.sh: | #!/bin/bash /usr/sbin/httpd $OPTIONS -DFOREGROUND kind: ConfigMap metadata: creationTimestamp: null name: run-apache selfLink: /api/v1/namespaces/default/configmaps/run-apache --- apiVersion: v1 kind: BuildConfig metadata: name: fh-build spec: source: configMaps: - configMap: name: run-apache dockerfile: | FROM fedora EXPOSE 8080 RUN yum install httpd -y RUN yum install tcpdump -y RUN yum install iproute -y RUN yum install procps-ng -y RUN echo \"Apache default\" >> /var/www/html/index.html ADD run-apache.sh /usr/share/httpd/run-apache.sh RUN chown apache. /run/httpd/ -R RUN chmod -v +rx /usr/share/httpd/run-apache.sh RUN chown apache. /usr/share/httpd/run-apache.sh RUN usermod apache -s /bin/bash RUN sed -i 's/Listen 80/Listen 8080/' /etc/httpd/conf/httpd.conf RUN chown apache. /etc/httpd/logs/ -R USER apache CMD [\"/usr/share/httpd/run-apache.sh\"] strategy: dockerStrategy: noCache: true output: to: kind: ImageStreamTag name: fh:latest fh-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: fhingress-ingress annotations: kubernetes.io/ingress.allow-http: \"true\" spec: rules: - host: fh.apps.cluster.example.com http: paths: - path: / backend: serviceName: fhingress-service servicePort: 80 --- apiVersion: v1 kind: Service metadata: name: fhingress-service labels: app: fhingress-deploymentconfig spec: selector: app: fhingress-pod ports: - protocol: TCP port: 80 targetPort: 8080 --- apiVersion: apps/v1 kind: Deployment metadata: name: fhingress-deployment labels: app: fhingress-deployment spec: replicas: 3 selector: matchLabels: app: fhingress-pod template: metadata: labels: app: fhingress-pod spec: containers: - name: fhingress image: image-registry.openshift-image-registry.svc:5000/default/fh imagePullPolicy: Always fh-test1-build.yaml apiVersion: image.openshift.io/v1 kind: ImageStream metadata: creationTimestamp: null generation: 1 name: fh-test1 selfLink: /apis/image.openshift.io/v1/namespaces/default/imagestreams/fh-test1 spec: lookupPolicy: local: false status: dockerImageRepository: docker-registry.default.svc:5000/fh-test1 --- apiVersion: v1 data: run-apache.sh: | #!/bin/bash /usr/sbin/httpd $OPTIONS -DFOREGROUND kind: ConfigMap metadata: creationTimestamp: null name: run-apache selfLink: /api/v1/namespaces/default/configmaps/run-apache --- apiVersion: v1 kind: BuildConfig metadata: name: fh-test1-build spec: source: configMaps: - configMap: name: run-apache dockerfile: | FROM fedora EXPOSE 8080 RUN yum install httpd -y RUN yum install tcpdump -y RUN yum install iproute -y RUN yum install procps-ng -y RUN echo \"Apache test1\" >> /var/www/html/index.html ADD run-apache.sh /usr/share/httpd/run-apache.sh RUN chown apache. /run/httpd/ -R RUN chmod -v +rx /usr/share/httpd/run-apache.sh RUN chown apache. /usr/share/httpd/run-apache.sh RUN usermod apache -s /bin/bash RUN sed -i 's/Listen 80/Listen 8080/' /etc/httpd/conf/httpd.conf RUN chown apache. /etc/httpd/logs/ -R USER apache CMD [\"/usr/share/httpd/run-apache.sh\"] strategy: dockerStrategy: noCache: true output: to: kind: ImageStreamTag name: fh-test1:latest fh-test1-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: fh-test1ingress-ingress annotations: kubernetes.io/ingress.allow-http: \"true\" spec: rules: - host: fh.test1.cluster.example.com http: paths: - path: / backend: serviceName: fh-test1ingress-service servicePort: 80 --- apiVersion: v1 kind: Service metadata: name: fh-test1ingress-service labels: app: fh-test1ingress-deploymentconfig spec: selector: app: fh-test1ingress-pod ports: - protocol: TCP port: 80 targetPort: 8080 --- apiVersion: apps/v1 kind: Deployment metadata: name: fh-test1ingress-deployment labels: app: fh-test1ingress-deployment spec: replicas: 3 selector: matchLabels: app: fh-test1ingress-pod template: metadata: labels: app: fh-test1ingress-pod spec: containers: - name: fh-test1ingress image: image-registry.openshift-image-registry.svc:5000/test1/fh-test1 imagePullPolicy: Always fh-test2-build.yaml apiVersion: image.openshift.io/v1 kind: ImageStream metadata: creationTimestamp: null generation: 1 name: fh-test2 selfLink: /apis/image.openshift.io/v1/namespaces/default/imagestreams/fh-test2 spec: lookupPolicy: local: false status: dockerImageRepository: docker-registry.default.svc:5000/fh-test2 --- apiVersion: v1 data: run-apache.sh: | #!/bin/bash /usr/sbin/httpd $OPTIONS -DFOREGROUND kind: ConfigMap metadata: creationTimestamp: null name: run-apache selfLink: /api/v1/namespaces/default/configmaps/run-apache --- apiVersion: v1 kind: BuildConfig metadata: name: fh-test2-build spec: source: configMaps: - configMap: name: run-apache dockerfile: | FROM fedora EXPOSE 8080 RUN yum install httpd -y RUN yum install tcpdump -y RUN yum install iproute -y RUN yum install procps-ng -y RUN echo \"Apache test2\" >> /var/www/html/index.html ADD run-apache.sh /usr/share/httpd/run-apache.sh RUN chown apache. /run/httpd/ -R RUN chmod -v +rx /usr/share/httpd/run-apache.sh RUN chown apache. /usr/share/httpd/run-apache.sh RUN usermod apache -s /bin/bash RUN sed -i 's/Listen 80/Listen 8080/' /etc/httpd/conf/httpd.conf RUN chown apache. /etc/httpd/logs/ -R USER apache CMD [\"/usr/share/httpd/run-apache.sh\"] strategy: dockerStrategy: noCache: true output: to: kind: ImageStreamTag name: fh-test2:latest fh-test2-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: fh-test2ingress-ingress annotations: kubernetes.io/ingress.allow-http: \"true\" spec: rules: - host: fh.test2.cluster.example.com http: paths: - path: / backend: serviceName: fh-test2ingress-service servicePort: 80 --- apiVersion: v1 kind: Service metadata: name: fh-test2ingress-service labels: app: fh-test2ingress-deploymentconfig spec: selector: app: fh-test2ingress-pod ports: - protocol: TCP port: 80 targetPort: 8080 --- apiVersion: apps/v1 kind: Deployment metadata: name: fh-test2ingress-deployment labels: app: fh-test2ingress-deployment spec: replicas: 3 selector: matchLabels: app: fh-test2ingress-pod template: metadata: labels: app: fh-test2ingress-pod spec: containers: - name: fh-test2ingress image: image-registry.openshift-image-registry.svc:5000/test2/fh-test2 imagePullPolicy: Always","title":"Creating test builds and services"},{"location":"openshift/ingresscontroller_router_sharding_ocp_on_osp/#configuring-octavia-loadbalancers-for-server-ingresscontrollers","text":"Instead of using the default keepalived VIP, we are going to create one Octavia loadbalancer per IngressController: (overcloud) [stack@undercloud-0 ~]$ openstack server list +--------------------------------------+----------------------------+--------+--------------------------------------------------------------------------+---------------------+--------------+ | ID | Name | Status | Networks | Image | Flavor | +--------------------------------------+----------------------------+--------+--------------------------------------------------------------------------+---------------------+--------------+ | 2e5bc035-e6da-4807-8c96-268c2587c759 | cluster-7n7w9-worker-fkv5p | ACTIVE | cluster-7n7w9-openshift=172.31.0.45 | cluster-7n7w9-rhcos | m1.openshift | | 14406e18-74bd-4723-81b1-ea38080d3e72 | cluster-7n7w9-worker-ggxds | ACTIVE | cluster-7n7w9-openshift=172.31.0.13 | cluster-7n7w9-rhcos | m1.openshift | | 7ad9432b-bb4f-4cc1-b842-1e63a323a3b5 | cluster-7n7w9-worker-zpq85 | ACTIVE | cluster-7n7w9-openshift=172.31.0.14 | cluster-7n7w9-rhcos | m1.openshift | | dc4db4f7-d410-4404-9166-66204931538c | cluster-7n7w9-master-1 | ACTIVE | cluster-7n7w9-openshift=172.31.0.15 | cluster-7n7w9-rhcos | m1.openshift | | e0e3a4cd-4d1e-4496-9e59-ba8c0d62b54a | cluster-7n7w9-master-2 | ACTIVE | cluster-7n7w9-openshift=172.31.0.23 | cluster-7n7w9-rhcos | m1.openshift | | 1b76ade6-ad53-43c8-9f15-fd3d550e0a17 | cluster-7n7w9-master-0 | ACTIVE | cluster-7n7w9-openshift=172.31.0.39 | cluster-7n7w9-rhcos | m1.openshift | | 43810d22-74ce-4f5a-a615-d77d1628dde7 | rhel-test1 | ACTIVE | private1=2000:192:168:0:f816:3eff:fea4:39e1, 192.168.0.108, 172.16.0.208 | rhel | m1.small | +--------------------------------------+----------------------------+--------+--------------------------------------------------------------------------+---------------------+--------------+ (overcloud) [stack@undercloud-0 ~]$ openstack server list --all +--------------------------------------+----------------------------------------------+--------+--------------------------------------------------------------------------+----------------------------------------+--------------+ | ID | Name | Status | Networks | Image | Flavor | +--------------------------------------+----------------------------------------------+--------+--------------------------------------------------------------------------+----------------------------------------+--------------+ | 2bdd7c68-2b00-4824-9ebe-6cc3babdfe77 | amphora-da1a5888-1052-4a6b-a05d-fc21c22a5cf5 | ACTIVE | lb-mgmt-net=172.24.0.13; cluster-7n7w9-openshift=172.31.0.20 | octavia-amphora-13.0-20200323.2.x86_64 | | | 2e5bc035-e6da-4807-8c96-268c2587c759 | cluster-7n7w9-worker-fkv5p | ACTIVE | cluster-7n7w9-openshift=172.31.0.45 | cluster-7n7w9-rhcos | m1.openshift | | 14406e18-74bd-4723-81b1-ea38080d3e72 | cluster-7n7w9-worker-ggxds | ACTIVE | cluster-7n7w9-openshift=172.31.0.13 | cluster-7n7w9-rhcos | m1.openshift | | 7ad9432b-bb4f-4cc1-b842-1e63a323a3b5 | cluster-7n7w9-worker-zpq85 | ACTIVE | cluster-7n7w9-openshift=172.31.0.14 | cluster-7n7w9-rhcos | m1.openshift | | dc4db4f7-d410-4404-9166-66204931538c | cluster-7n7w9-master-1 | ACTIVE | cluster-7n7w9-openshift=172.31.0.15 | cluster-7n7w9-rhcos | m1.openshift | | e0e3a4cd-4d1e-4496-9e59-ba8c0d62b54a | cluster-7n7w9-master-2 | ACTIVE | cluster-7n7w9-openshift=172.31.0.23 | cluster-7n7w9-rhcos | m1.openshift | | 1b76ade6-ad53-43c8-9f15-fd3d550e0a17 | cluster-7n7w9-master-0 | ACTIVE | cluster-7n7w9-openshift=172.31.0.39 | cluster-7n7w9-rhcos | m1.openshift | | 43810d22-74ce-4f5a-a615-d77d1628dde7 | rhel-test1 | ACTIVE | private1=2000:192:168:0:f816:3eff:fea4:39e1, 192.168.0.108, 172.16.0.208 | rhel | m1.small | +--------------------------------------+----------------------------------------------+--------+--------------------------------------------------------------------------+----------------------------------------+--------------+ (overcloud) [stack@undercloud-0 ~]$ # openstack loadbalancer create --name ingress-controller-default --vip-subnet-id (overcloud) [stack@undercloud-0 ~]$ openstack subnet list +--------------------------------------+---------------------------------------------------+--------------------------------------+----------------------+ | ID | Name | Network | Subnet | +--------------------------------------+---------------------------------------------------+--------------------------------------+----------------------+ | 14a3bd37-7a3a-4f14-bd03-26d4860d91db | provider1-subnet | d14c0815-22b5-4cdf-9db1-5da7951f1e0a | 172.16.0.0/24 | | 5aef701a-c93f-4210-8722-3fdf8ef1ddf1 | provider1-ipv6-subnet | d14c0815-22b5-4cdf-9db1-5da7951f1e0a | 2000:10::/64 | | 63950c5e-fc70-49ad-8d6b-092b8ba39016 | cluster-7n7w9-nodes | ac9c0bd9-3793-48d3-a12e-eaa06028a366 | 172.31.0.0/16 | | 7f65d115-8077-4219-9ef8-3b0820ba8b52 | private1-ipv6-subnet | 0af094e6-1dac-4479-9ebc-88f8f7f0b254 | 2000:192:168::/64 | | 9fa85591-718c-4fb9-88d8-0c2e5b0771ee | private1-subnet | 0af094e6-1dac-4479-9ebc-88f8f7f0b254 | 192.168.0.0/24 | | a0827588-7460-4f2f-9f29-12bb1be49c76 | lb-mgmt-subnet | 85cd5be1-3d8b-4929-b9ac-2e770cb12ba9 | 172.24.0.0/16 | | a5e9841d-38a7-47a0-8db2-f302a67a9845 | private2-subnet | fdb07e70-7055-41a9-a8fa-10b2dce988b3 | 192.168.1.0/24 | | af925c70-979e-4fa9-8704-72ec683a31f8 | private-mgmt-ipv6-subnet | 0d74f260-c3d6-4a77-8881-438c3bfaaa7b | 2000:192:168:10::/64 | | c815a934-98ff-4daa-8b5e-5a0e5d91806f | private2-ipv6-subnet | fdb07e70-7055-41a9-a8fa-10b2dce988b3 | 2000:192:168:1::/64 | | df05dd9e-6583-42a0-afee-e8f65d813e6f | private-mgmt-subnet | 0d74f260-c3d6-4a77-8881-438c3bfaaa7b | 192.168.10.0/24 | | e1ebbac2-3d7c-4355-9624-f0920d705f76 | HA subnet tenant a416f556938f454f849da42faa317cd3 | 5079d963-7301-4218-855b-f7d33fce1081 | 169.254.192.0/18 | +--------------------------------------+---------------------------------------------------+--------------------------------------+----------------------+ (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer create --name ingress-controller-default --vip-subnet-id cluster-7n7w9-nodes +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | admin_state_up | True | | created_at | 2020-04-08T10:27:46 | | description | | | flavor | | | id | e5feb42d-77e4-428b-9ad0-55fe6d30f3b3 | | listeners | | | name | ingress-controller-default | | operating_status | OFFLINE | | pools | | | project_id | a416f556938f454f849da42faa317cd3 | | provider | octavia | | provisioning_status | PENDING_CREATE | | updated_at | None | | vip_address | 172.31.0.21 | | vip_network_id | ac9c0bd9-3793-48d3-a12e-eaa06028a366 | | vip_port_id | f336d8c6-8f47-4d61-a865-19f34a671ffa | | vip_qos_policy_id | None | | vip_subnet_id | 63950c5e-fc70-49ad-8d6b-092b8ba39016 | +---------------------+--------------------------------------+ (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer create --name ingress-controller-test1 --vip-subnet-id cluster-7n7w9-nodes +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | admin_state_up | True | | created_at | 2020-04-08T10:28:33 | | description | | | flavor | | | id | a636c15a-92a2-4253-8945-8875c58aae96 | | listeners | | | name | ingress-controller-test1 | | operating_status | OFFLINE | | pools | | | project_id | a416f556938f454f849da42faa317cd3 | | provider | octavia | | provisioning_status | PENDING_CREATE | | updated_at | None | | vip_address | 172.31.0.24 | | vip_network_id | ac9c0bd9-3793-48d3-a12e-eaa06028a366 | | vip_port_id | 7d85d059-65c2-47af-90a2-35337d4396f8 | | vip_qos_policy_id | None | | vip_subnet_id | 63950c5e-fc70-49ad-8d6b-092b8ba39016 | +---------------------+--------------------------------------+ (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer create --name ingress-controller-test2 --vip-subnet-id cluster-7n7w9-nodes +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | admin_state_up | True | | created_at | 2020-04-08T10:28:44 | | description | | | flavor | | | id | c2041396-66f5-4726-ad47-9ce7d2f7904f | | listeners | | | name | ingress-controller-test2 | | operating_status | OFFLINE | | pools | | | project_id | a416f556938f454f849da42faa317cd3 | | provider | octavia | | provisioning_status | PENDING_CREATE | | updated_at | None | | vip_address | 172.31.0.27 | | vip_network_id | ac9c0bd9-3793-48d3-a12e-eaa06028a366 | | vip_port_id | ce0129ca-9b39-4ee1-8d12-bb0685967f65 | | vip_qos_policy_id | None | | vip_subnet_id | 63950c5e-fc70-49ad-8d6b-092b8ba39016 | +---------------------+--------------------------------------+ # wait until all loadbalancers are active (overcloud) [stack@undercloud-0 ~]$ watch openstack loadbalancer list (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer list +--------------------------------------+----------------------------+----------------------------------+-------------+---------------------+----------+ | id | name | project_id | vip_address | provisioning_status | provider | +--------------------------------------+----------------------------+----------------------------------+-------------+---------------------+----------+ | e5feb42d-77e4-428b-9ad0-55fe6d30f3b3 | ingress-controller-default | a416f556938f454f849da42faa317cd3 | 172.31.0.21 | ACTIVE | octavia | | a636c15a-92a2-4253-8945-8875c58aae96 | ingress-controller-test1 | a416f556938f454f849da42faa317cd3 | 172.31.0.24 | ACTIVE | octavia | | c2041396-66f5-4726-ad47-9ce7d2f7904f | ingress-controller-test2 | a416f556938f454f849da42faa317cd3 | 172.31.0.27 | ACTIVE | octavia | +--------------------------------------+----------------------------+----------------------------------+-------------+---------------------+----------+ (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer listener create --name ingress-controller-default-listener --protocol HTTP --protocol-port 80 ingress-controller-default +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | True | | connection_limit | -1 | | created_at | 2020-04-08T11:00:01 | | default_pool_id | None | | default_tls_container_ref | None | | description | | | id | be0d73e6-1162-4363-ad62-0d74e2c16f6b | | insert_headers | None | | l7policies | | | loadbalancers | e5feb42d-77e4-428b-9ad0-55fe6d30f3b3 | | name | ingress-controller-default-listener | | operating_status | OFFLINE | | project_id | a416f556938f454f849da42faa317cd3 | | protocol | HTTP | | protocol_port | 80 | | provisioning_status | PENDING_CREATE | | sni_container_refs | [] | | updated_at | None | +---------------------------+--------------------------------------+ (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer pool create --name ingress-controller-default-pool --lb-algorithm ROUND_ROBIN --listener ingress-controller-default-listener --protocol HTTP +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | admin_state_up | True | | created_at | 2020-04-08T11:00:13 | | description | | | healthmonitor_id | | | id | 47dc4567-f182-4c16-a2ec-aa306e40779d | | lb_algorithm | ROUND_ROBIN | | listeners | be0d73e6-1162-4363-ad62-0d74e2c16f6b | | loadbalancers | e5feb42d-77e4-428b-9ad0-55fe6d30f3b3 | | members | | | name | ingress-controller-default-pool | | operating_status | OFFLINE | | project_id | a416f556938f454f849da42faa317cd3 | | protocol | HTTP | | provisioning_status | PENDING_CREATE | | session_persistence | None | | updated_at | None | +---------------------+--------------------------------------+ (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer member create --subnet-id cluster-7n7w9-nodes --address 172.31.0.45 --protocol-port 80 ingress-controller-default-pool +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | address | 172.31.0.45 | | admin_state_up | True | | created_at | 2020-04-08T11:02:06 | | id | 431aa74f-e98d-41fd-aab9-bf907f1e3993 | | name | | | operating_status | NO_MONITOR | | project_id | a416f556938f454f849da42faa317cd3 | | protocol_port | 80 | | provisioning_status | PENDING_CREATE | | subnet_id | 63950c5e-fc70-49ad-8d6b-092b8ba39016 | | updated_at | None | | weight | 1 | | monitor_port | None | | monitor_address | None | +---------------------+--------------------------------------+ (overcloud) [stack@undercloud-0 ~]$ openstack floating ip create provider1 +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | created_at | 2020-04-08T11:02:34Z | | description | | | fixed_ip_address | None | | floating_ip_address | 172.16.0.211 | | floating_network_id | d14c0815-22b5-4cdf-9db1-5da7951f1e0a | | id | 44720e51-dc95-453b-9993-1cac1fbf40b9 | | name | 172.16.0.211 | | port_id | None | | project_id | a416f556938f454f849da42faa317cd3 | | qos_policy_id | None | | revision_number | 0 | | router_id | None | | status | DOWN | | subnet_id | None | | updated_at | 2020-04-08T11:02:34Z | +---------------------+--------------------------------------+ oad(overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer list +--------------------------------------+----------------------------+----------------------------------+-------------+---------------------+----------+ | id | name | project_id | vip_address | provisioning_status | provider | +--------------------------------------+----------------------------+----------------------------------+-------------+---------------------+----------+ | e5feb42d-77e4-428b-9ad0-55fe6d30f3b3 | ingress-controller-default | a416f556938f454f849da42faa317cd3 | 172.31.0.21 | ACTIVE | octavia | | a636c15a-92a2-4253-8945-8875c58aae96 | ingress-controller-test1 | a416f556938f454f849da42faa317cd3 | 172.31.0.24 | ACTIVE | octavia | | c2041396-66f5-4726-ad47-9ce7d2f7904f | ingress-controller-test2 | a416f556938f454f849da42faa317cd3 | 172.31.0.27 | ACTIVE | octavia | +--------------------------------------+----------------------------+----------------------------------+-------------+---------------------+----------+ (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer listener create --name ingress-controller-test1-listener --protocol HTTP --protocol-port 80 ingress-controller-test1 +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | True | | connection_limit | -1 | | created_at | 2020-04-08T11:03:33 | | default_pool_id | None | | default_tls_container_ref | None | | description | | | id | 9e7236f5-cb15-4c8c-8ffb-a00182c44871 | | insert_headers | None | | l7policies | | | loadbalancers | a636c15a-92a2-4253-8945-8875c58aae96 | | name | ingress-controller-test1-listener | | operating_status | OFFLINE | | project_id | a416f556938f454f849da42faa317cd3 | | protocol | HTTP | | protocol_port | 80 | | provisioning_status | PENDING_CREATE | | sni_container_refs | [] | | updated_at | None | +---------------------------+--------------------------------------+ (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer listener create --name ingress-controller-test2-listener --protocol HTTP --protocol-port 80 ingress-controller-test2 +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | True | | connection_limit | -1 | | created_at | 2020-04-08T11:03:42 | | default_pool_id | None | | default_tls_container_ref | None | | description | | | id | 0cd99fe3-3750-4506-bafa-57cf4a15c7e8 | | insert_headers | None | | l7policies | | | loadbalancers | c2041396-66f5-4726-ad47-9ce7d2f7904f | | name | ingress-controller-test2-listener | | operating_status | OFFLINE | | project_id | a416f556938f454f849da42faa317cd3 | | protocol | HTTP | | protocol_port | 80 | | provisioning_status | PENDING_CREATE | | sni_container_refs | [] | | updated_at | None | +---------------------------+--------------------------------------+ (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer pool create --name ingress-controller-test1-pool --lb-algorithm ROUND_ROBIN --listener ingress-controller-test1-listener --protocol HTTP +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | admin_state_up | True | | created_at | 2020-04-08T11:04:00 | | description | | | healthmonitor_id | | | id | d7e93f06-3afd-467d-b43a-4018e59d6be1 | | lb_algorithm | ROUND_ROBIN | | listeners | 9e7236f5-cb15-4c8c-8ffb-a00182c44871 | | loadbalancers | a636c15a-92a2-4253-8945-8875c58aae96 | | members | | | name | ingress-controller-test1-pool | | operating_status | OFFLINE | | project_id | a416f556938f454f849da42faa317cd3 | | protocol | HTTP | | provisioning_status | PENDING_CREATE | | session_persistence | None | | updated_at | None | +---------------------+--------------------------------------+ (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer pool create --name ingress-controller-test2-pool --lb-algorithm ROUND_ROBIN --listener ingress-controller-test2-listener --protocol HTTP +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | admin_state_up | True | | created_at | 2020-04-08T11:04:10 | | description | | | healthmonitor_id | | | id | c6a4c5dd-836f-42fe-91bf-5a71d3109691 | | lb_algorithm | ROUND_ROBIN | | listeners | 0cd99fe3-3750-4506-bafa-57cf4a15c7e8 | | loadbalancers | c2041396-66f5-4726-ad47-9ce7d2f7904f | | members | | | name | ingress-controller-test2-pool | | operating_status | OFFLINE | | project_id | a416f556938f454f849da42faa317cd3 | | protocol | HTTP | | provisioning_status | PENDING_CREATE | | session_persistence | None | | updated_at | None | +---------------------+--------------------------------------+ (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer member create --subnet-id cluster-7n7w9-nodes --address 172.31.0.13 --protocol-port 80 ingress-controller-test1-pool +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | address | 172.31.0.13 | | admin_state_up | True | | created_at | 2020-04-08T11:05:02 | | id | 584a9a8b-eb1c-41b4-9308-0a1cd9f91122 | | name | | | operating_status | NO_MONITOR | | project_id | a416f556938f454f849da42faa317cd3 | | protocol_port | 80 | | provisioning_status | PENDING_CREATE | | subnet_id | 63950c5e-fc70-49ad-8d6b-092b8ba39016 | | updated_at | None | | weight | 1 | | monitor_port | None | | monitor_address | None | +---------------------+--------------------------------------+ (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer member create --subnet-id cluster-7n7w9-nodes --address 172.31.0.14 --protocol-port 80 ingress-controller-test2-pool openstack floating ip creat+---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | address | 172.31.0.14 | | admin_state_up | True | | created_at | 2020-04-08T11:05:14 | | id | 8d859649-fcc1-4ffe-b9be-0ed6540346e1 | | name | | | operating_status | NO_MONITOR | | project_id | a416f556938f454f849da42faa317cd3 | | protocol_port | 80 | | provisioning_status | PENDING_CREATE | | subnet_id | 63950c5e-fc70-49ad-8d6b-092b8ba39016 | | updated_at | None | | weight | 1 | | monitor_port | None | | monitor_address | None | +---------------------+--------------------------------------+ (overcloud) [stack@undercloud-0 ~]$ openstack floating ip create provider1 o[+---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | created_at | 2020-04-08T11:05:27Z | | description | | | fixed_ip_address | None | | floating_ip_address | 172.16.0.204 | | floating_network_id | d14c0815-22b5-4cdf-9db1-5da7951f1e0a | | id | 72181227-12f0-4790-bd7c-208785e05918 | | name | 172.16.0.204 | | port_id | None | | project_id | a416f556938f454f849da42faa317cd3 | | qos_policy_id | None | | revision_number | 0 | | router_id | None | | status | DOWN | | subnet_id | None | | updated_at | 2020-04-08T11:05:27Z | +---------------------+--------------------------------------+ (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer list openstack floating ip +--------------------------------------+----------------------------+----------------------------------+-------------+---------------------+----------+ | id | name | project_id | vip_address | provisioning_status | provider | +--------------------------------------+----------------------------+----------------------------------+-------------+---------------------+----------+ | e5feb42d-77e4-428b-9ad0-55fe6d30f3b3 | ingress-controller-default | a416f556938f454f849da42faa317cd3 | 172.31.0.21 | ACTIVE | octavia | | a636c15a-92a2-4253-8945-8875c58aae96 | ingress-controller-test1 | a416f556938f454f849da42faa317cd3 | 172.31.0.24 | ACTIVE | octavia | | c2041396-66f5-4726-ad47-9ce7d2f7904f | ingress-controller-test2 | a416f556938f454f849da42faa317cd3 | 172.31.0.27 | ACTIVE | octavia | +--------------------------------------+----------------------------+----------------------------------+-------------+---------------------+----------+ l(overcloud) [stack@undercloud-0 ~]$ openstack floating ip list +--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+ | ID | Floating IP Address | Fixed IP Address | Port | Floating Network | Project | +--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+ | 0a3e9fa7-577c-47f9-b9e4-f583bd3ce180 | 172.16.0.208 | 192.168.0.108 | 03d91ec5-8de6-415b-9581-d00501bb40ed | d14c0815-22b5-4cdf-9db1-5da7951f1e0a | a416f556938f454f849da42faa317cd3 | | 44720e51-dc95-453b-9993-1cac1fbf40b9 | 172.16.0.211 | None | None | d14c0815-22b5-4cdf-9db1-5da7951f1e0a | a416f556938f454f849da42faa317cd3 | | 497c6667-22b3-4ed7-b6af-1c5091b14209 | 172.16.0.213 | 172.31.0.5 | 3bc4d5e1-90c9-4da1-86cd-d5e5d6fe985d | d14c0815-22b5-4cdf-9db1-5da7951f1e0a | a416f556938f454f849da42faa317cd3 | | 4b9e9daf-4e7f-4338-ad70-6206e7e56367 | 172.16.0.214 | 172.31.0.7 | ee68b77c-95af-4bb2-ba62-c4d06d728c47 | d14c0815-22b5-4cdf-9db1-5da7951f1e0a | a416f556938f454f849da42faa317cd3 | | 72181227-12f0-4790-bd7c-208785e05918 | 172.16.0.204 | None | None | d14c0815-22b5-4cdf-9db1-5da7951f1e0a | a416f556938f454f849da42faa317cd3 | | a2693cfc-4379-4d9b-bb90-a86c3fcafd9c | 172.16.0.217 | None | None | d14c0815-22b5-4cdf-9db1-5da7951f1e0a | a416f556938f454f849da42faa317cd3 | +--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+ (overcloud) [stack@undercloud-0 ~]$ openstack port list | grep 172.31.0.21 | f336d8c6-8f47-4d61-a865-19f34a671ffa | octavia-lb-e5feb42d-77e4-428b-9ad0-55fe6d30f3b3 | fa:16:3e:f1:04:50 | ip_address='172.31.0.21', subnet_id='63950c5e-fc70-49ad-8d6b-092b8ba39016' | DOWN | (overcloud) [stack@undercloud-0 ~]$ openstack port list | grep 172.31.0.24 | 7d85d059-65c2-47af-90a2-35337d4396f8 | octavia-lb-a636c15a-92a2-4253-8945-8875c58aae96 | fa:16:3e:8c:32:69 | ip_address='172.31.0.24', subnet_id='63950c5e-fc70-49ad-8d6b-092b8ba39016' | DOWN | (overcloud) [stack@undercloud-0 ~]$ openstack port list | grep 172.31.0.27 | ce0129ca-9b39-4ee1-8d12-bb0685967f65 | octavia-lb-c2041396-66f5-4726-ad47-9ce7d2f7904f | fa:16:3e:e0:d7:bd | ip_address='172.31.0.27', subnet_id='63950c5e-fc70-49ad-8d6b-092b8ba39016' | DOWN | (overcloud) [stack@undercloud-0 ~]$ openstack floating ip set 172.16.0.211 --port octavia-lb-e5feb42d-77e4-428b-9ad0-55fe6d30f3b3 (overcloud) [stack@undercloud-0 ~]$ openstack floating ip set 172.16.0.204 --port octavia-lb-a636c15a-92a2-4253-8945-8875c58aae96 (overcloud) [stack@undercloud-0 ~]$ openstack floating ip set 172.16.0.217 --port octavia-lb-c2041396-66f5-4726-ad47-9ce7d2f7904f (overcloud) [stack@undercloud-0 ~]$ openstack loadbalancer member list ingress-controller-default-pool +--------------------------------------+------+----------------------------------+---------------------+-------------+---------------+------------------+--------+ | id | name | project_id | provisioning_status | address | protocol_port | operating_status | weight | +--------------------------------------+------+----------------------------------+---------------------+-------------+---------------+------------------+--------+ | 431aa74f-e98d-41fd-aab9-bf907f1e3993 | | a416f556938f454f849da42faa317cd3 | ACTIVE | 172.31.0.45 | 80 | NO_MONITOR | 1 | +--------------------------------------+------+----------------------------------+---------------------+-------------+---------------+------------------+--------+ (overcloud) [stack@undercloud-0 ~]$ curl ^C Once this is done, update your DNS entries. In this lab case, I'm simply changing /etc/hosts : (overcloud) [stack@undercloud-0 ~]$ cat /etc/hosts 127.0.0.1 undercloud-0.redhat.local undercloud-0 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 # API cluster VIP 172.16.0.213 api.cluster.example.com # original VIP # 172.16.0.214 oauth-openshift.apps.cluster.example.com console-openshift-console.apps.cluster.example.com downloads-openshift-console.apps.cluster.example.com alertmanager-main-openshift-monitoring.apps.cluster.example.com grafana-openshift-monitoring.apps.cluster.example.com prometheus-k8s-openshift-monitoring.apps.cluster.example.com thanos-querier-openshift-monitoring.apps.cluster.example.com fh.apps.cluster.example.com fh.test2.cluster.example.com fh.test1.cluster.example.com # new Octavia VIPs 172.16.0.211 oauth-openshift.apps.cluster.example.com console-openshift-console.apps.cluster.example.com downloads-openshift-console.apps.cluster.example.com alertmanager-main-openshift-monitoring.apps.cluster.example.com grafana-openshift-monitoring.apps.cluster.example.com prometheus-k8s-openshift-monitoring.apps.cluster.example.com thanos-querier-openshift-monitoring.apps.cluster.example.com fh.apps.cluster.example.com 172.16.0.204 fh.test1.cluster.example.com 172.16.0.217 fh.test2.cluster.example.com","title":"Configuring Octavia loadbalancers for server IngressControllers"},{"location":"openshift/ingresscontroller_router_sharding_ocp_on_osp/#testing","text":"This should now work: (overcloud) [stack@undercloud-0 ~]$ curl fh.apps.cluster.example.com Apache default (overcloud) [stack@undercloud-0 ~]$ curl fh.test1.cluster.example.com Apache test1 (overcloud) [stack@undercloud-0 ~]$ curl fh.test2.cluster.example.com Apache test2","title":"Testing"},{"location":"openshift/istio-1.6-on-ocp.4.x/","text":"Istio 1.6 on OpenShfit 4.x How to install upstream istio 1.6 on OCP 4.x on AWS Resources Instructions for upstream Istio with OpenShift: https://istio.io/latest/docs/setup/platform-setup/openshift/ I then installed istioctl: https://istio.io/latest/docs/ops/diagnostic-tools/istioctl/ And then DO NOT follow: https://istio.io/latest/docs/setup/install/standalone-operator/ The standalone operator configuration does not work, so instead follow: https://istio.io/latest/docs/setup/additional-setup/cni/ But be aware of: https://istio.io/latest/docs/setup/additional-setup/cni/#hosted-kubernetes-settings Install the bookinfo application: https://istio.io/latest/docs/examples/bookinfo/ Installation instructions Make sure the cluster was correctly installed: [akaris@linux upi]$ oc get clusterversion NAME VERSION AVAILABLE PROGRESSING SINCE STATUS version 4.4.16 True False 2d1h Cluster version is 4.4.16 [akaris@linux upi]$ oc new-project istio-system Now using project \"istio-system\" on server \"https://api.akaris-upi.focused-solutions.support:6443\". You can add applications to this project with the 'new-app' command. For example, try: oc new-app django-psql-example to build a new example application in Python. Or use kubectl to deploy a simple Kubernetes application: kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node [akaris@linux upi]$ oc adm policy add-scc-to-group anyuid system:serviceaccounts:istio-system securitycontextconstraints.security.openshift.io/anyuid added to groups: [\"system:serviceaccounts:istio-system\"] Install istiocli: [akaris@linux upi]$ curl -sL https://istio.io/downloadIstioctl | sh - Downloading istioctl-1.6.8 from https://github.com/istio/istio/releases/download/1.6.8/istioctl-1.6.8-linux.tar.gz ... Failed. Trying with TARGET_ARCH. Downloading istioctl-1.6.8 from https://github.com/istio/istio/releases/download/1.6.8/istioctl-1.6.8-linux-amd64.tar.gz ... istioctl-1.6.8-linux-amd64.tar.gz download complete! Add the istioctl to your path with: export PATH=$PATH:$HOME/.istioctl/bin Begin the Istio pre-installation verification check by running: istioctl verify-install Need more information? Visit https://istio.io/docs/reference/commands/istioctl/ [akaris@linux upi]$ export PATH=$PATH:$HOME/.istioctl/bin^C [akaris@linux upi]$ export PATH=$PATH:$HOME/.istioctl/bin [akaris@linux upi]$ istioctl verify-install Error: could not load IstioOperator from cluster: the server could not find the requested resource. Use --filename The above error is normal. Nothing was installed yet. Follow https://istio.io/latest/docs/setup/additional-setup/cni/ : $ cat <<EOF > istio-cni.yaml apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: components: cni: enabled: true values: cni: excludeNamespaces: - istio-system - kube-system logLevel: info EOF For OpenShift, then follow: https://istio.io/latest/docs/setup/additional-setup/cni/#hosted-kubernetes-settings : Therefore, run: [akaris@linux upi]$ istioctl install -f istio-cni.yaml --set components.cni.namespace=kube-system --set values.cni.cniBinDir=/var/lib/cni/bin --set values.cni.cniConfDir=/etc/cni/multus/net.d --set values.cni.chained=false --set values.cni.cniConfFileName=\"istio-cni.conf\" --set values.sidecarInjectorWebhook.injectedAnnotations.\"k8s\\.v1\\.cni\\.cncf\\.io/networks\"=istio-cni \u2714 Istio core installed \u2714 Istiod installed \u2714 Addons installed \u2714 Ingress gateways installed \u2714 CNI installed - Pruning removed resources Pruned object DaemonSet:istio-system:istio-cni-node. Pruned object ConfigMap:istio-system:istio-cni-config. Pruned object ServiceAccount:istio-system:istio-cni. \u2714 Installation complete Verify the installation: [akaris@linux upi]$ istioctl verify-install ClusterRole: istio-cni.default checked successfully ClusterRole: istio-cni-repair-role.default checked successfully ClusterRoleBinding: istio-cni.default checked successfully ClusterRoleBinding: istio-cni-repair-rolebinding.default checked successfully ConfigMap: istio-cni-config.kube-system checked successfully DaemonSet: istio-cni-node.kube-system checked successfully ServiceAccount: istio-cni.kube-system checked successfully HorizontalPodAutoscaler: istio-ingressgateway.istio-system checked successfully Deployment: istio-ingressgateway.istio-system checked successfully PodDisruptionBudget: istio-ingressgateway.istio-system checked successfully Role: istio-ingressgateway-sds.istio-system checked successfully RoleBinding: istio-ingressgateway-sds.istio-system checked successfully Service: istio-ingressgateway.istio-system checked successfully ServiceAccount: istio-ingressgateway-service-account.istio-system checked successfully ClusterRole: prometheus-istio-system.default checked successfully ClusterRoleBinding: prometheus-istio-system.default checked successfully ConfigMap: prometheus.istio-system checked successfully Deployment: prometheus.istio-system checked successfully Service: prometheus.istio-system checked successfully ServiceAccount: prometheus.istio-system checked successfully HorizontalPodAutoscaler: istiod.istio-system checked successfully ConfigMap: istio.istio-system checked successfully Deployment: istiod.istio-system checked successfully ConfigMap: istio-sidecar-injector.istio-system checked successfully MutatingWebhookConfiguration: istio-sidecar-injector.default checked successfully PodDisruptionBudget: istiod.istio-system checked successfully Service: istiod.istio-system checked successfully EnvoyFilter: metadata-exchange-1.4.istio-system checked successfully EnvoyFilter: stats-filter-1.4.istio-system checked successfully EnvoyFilter: metadata-exchange-1.5.istio-system checked successfully EnvoyFilter: tcp-metadata-exchange-1.5.istio-system checked successfully EnvoyFilter: stats-filter-1.5.istio-system checked successfully EnvoyFilter: tcp-stats-filter-1.5.istio-system checked successfully EnvoyFilter: metadata-exchange-1.6.istio-system checked successfully EnvoyFilter: tcp-metadata-exchange-1.6.istio-system checked successfully EnvoyFilter: stats-filter-1.6.istio-system checked successfully EnvoyFilter: tcp-stats-filter-1.6.istio-system checked successfully ClusterRole: istiod-istio-system.default checked successfully ClusterRole: istio-reader-istio-system.default checked successfully ClusterRoleBinding: istio-reader-istio-system.default checked successfully ClusterRoleBinding: istiod-pilot-istio-system.default checked successfully ServiceAccount: istio-reader-service-account.istio-system checked successfully ServiceAccount: istiod-service-account.istio-system checked successfully ValidatingWebhookConfiguration: istiod-istio-system.default checked successfully CustomResourceDefinition: httpapispecs.config.istio.io.default checked successfully CustomResourceDefinition: httpapispecbindings.config.istio.io.default checked successfully CustomResourceDefinition: quotaspecs.config.istio.io.default checked successfully CustomResourceDefinition: quotaspecbindings.config.istio.io.default checked successfully CustomResourceDefinition: destinationrules.networking.istio.io.default checked successfully CustomResourceDefinition: envoyfilters.networking.istio.io.default checked successfully CustomResourceDefinition: gateways.networking.istio.io.default checked successfully CustomResourceDefinition: serviceentries.networking.istio.io.default checked successfully CustomResourceDefinition: sidecars.networking.istio.io.default checked successfully CustomResourceDefinition: virtualservices.networking.istio.io.default checked successfully CustomResourceDefinition: workloadentries.networking.istio.io.default checked successfully CustomResourceDefinition: attributemanifests.config.istio.io.default checked successfully CustomResourceDefinition: handlers.config.istio.io.default checked successfully CustomResourceDefinition: instances.config.istio.io.default checked successfully CustomResourceDefinition: rules.config.istio.io.default checked successfully CustomResourceDefinition: clusterrbacconfigs.rbac.istio.io.default checked successfully CustomResourceDefinition: rbacconfigs.rbac.istio.io.default checked successfully CustomResourceDefinition: serviceroles.rbac.istio.io.default checked successfully CustomResourceDefinition: servicerolebindings.rbac.istio.io.default checked successfully CustomResourceDefinition: authorizationpolicies.security.istio.io.default checked successfully CustomResourceDefinition: peerauthentications.security.istio.io.default checked successfully CustomResourceDefinition: requestauthentications.security.istio.io.default checked successfully CustomResourceDefinition: adapters.config.istio.io.default checked successfully CustomResourceDefinition: templates.config.istio.io.default checked successfully CustomResourceDefinition: istiooperators.install.istio.io.default checked successfully Checked 25 custom resource definitions Checked 1 Istio Deployments Istio is installed successfully [akaris@linux upi]$ That will create the istio pods that are needed for the CNI plugin: [akaris@linux upi]$ oc get pods -A | grep istio istio-system istio-ingressgateway-6c77d7f498-d58gx 1/1 Running 0 33m istio-system istiod-58f84ffddc-r2bj9 1/1 Running 0 33m istio-system prometheus-5db67458fb-5m67n 2/2 Running 0 33m kube-system istio-cni-node-52ktn 2/2 Running 0 56s kube-system istio-cni-node-794mr 2/2 Running 0 56s kube-system istio-cni-node-gggvm 2/2 Running 0 56s kube-system istio-cni-node-nd72d 2/2 Running 0 56s kube-system istio-cni-node-p2w5f 2/2 Running 0 56s kube-system istio-cni-node-vqjkb 2/2 Running 0 56s Create the bookinfo app according to https://istio.io/latest/docs/examples/bookinfo/ : [akaris@linux upi]$ oc new-project bookinfo Now using project \"bookinfo\" on server \"https://api.akaris-upi.focused-solutions.support:6443\". You can add applications to this project with the 'new-app' command. For example, try: oc new-app django-psql-example to build a new example application in Python. Or use kubectl to deploy a simple Kubernetes application: kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node Adjust SCCs according to https://istio.io/latest/docs/setup/platform-setup/openshift/#privileged-security-context-constraints-for-application-sidecars : The Istio sidecar injected into each application pod runs with user ID 1337, which is not allowed by default in OpenShift. To allow this user ID to be used, execute the following commands. Replace <target-namespace> with the appropriate namespace. So execute: [akaris@linux upi]$ oc adm policy add-scc-to-group privileged system:serviceaccounts:bookinfo securitycontextconstraints.security.openshift.io/privileged added to groups: [\"system:serviceaccounts:bookinfo\"] [akaris@linux upi]$ oc adm policy add-scc-to-group anyuid system:serviceaccounts:bookinfo securitycontextconstraints.security.openshift.io/anyuid added to groups: [\"system:serviceaccounts:bookinfo\"] Verify SCC configuration - in SCC 4.5, check oc get clusterrolebindings | grep scc instead. [akaris@linux upi]$ oc get scc anyuid -o yaml | grep serv -C3 type: RunAsAny groups: - system:cluster-admins - system:serviceaccounts:istio-system kind: SecurityContextConstraints metadata: annotations: Now, create the application: oc apply -f https://raw.githubusercontent.com/istio/istio/release-1.6/samples/bookinfo/platform/kube/bookinfo.yaml [akaris@linux upi]$ oc get pods NAME READY STATUS RESTARTS AGE details-v1-5974b67c8-nd7kl 2/2 Running 0 83s productpage-v1-64794f5db4-gl7c9 2/2 Running 0 83s ratings-v1-c6cdf8d98-z987h 2/2 Running 0 83s reviews-v1-7f6558b974-6z84q 2/2 Running 0 83s reviews-v2-6cb6ccd848-l8s56 2/2 Running 0 83s reviews-v3-cc56b578-9vvqv 2/2 Running 0 83s Now, create the gateway: [akaris@linux upi]$ oc apply -f https://raw.githubusercontent.com/istio/istio/release-1.6/samples/bookinfo/networking/bookinfo-gateway.yaml gateway.networking.istio.io/bookinfo-gateway created [akaris@linux upi]$ oc get svc istio-ingressgateway -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway LoadBalancer 172.30.44.45 af0e3e39e3f544f55a59da6578212ff5-1911792852.eu-west-3.elb.amazonaws.com 15021:31538/TCP,80:31367/TCP,443:31276/TCP,15443:30398/TCP 47m Get variables to connect to the gateway: https://istio.io/latest/docs/tasks/traffic-management/ingress/ingress-control/#determining-the-ingress-ip-and-ports There's an issue in the instructions for the INGRESS_HOST . With AWS, it needs to be .status.loadBalancer.ingress[0].hostname and not .status.loadBalancer.ingress[0].ip : [akaris@linux upi]$ export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}') [akaris@linux upi]$ export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\"http2\")].port}') [akaris@linux upi]$ export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\"https\")].port}') [akaris@linux upi]$ export TCP_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\"tcp\")].port}') [akaris@linux upi]$ export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT Now, connect to the application with curl: [akaris@linux upi]$ curl -s \"http://${GATEWAY_URL}/productpage\" | grep -o \"<title>.*</title>\" <title>Simple Bookstore App</title>","title":"Istio 1.6 on OpenShift 4.x"},{"location":"openshift/istio-1.6-on-ocp.4.x/#istio-16-on-openshfit-4x","text":"How to install upstream istio 1.6 on OCP 4.x on AWS","title":"Istio 1.6 on OpenShfit 4.x"},{"location":"openshift/istio-1.6-on-ocp.4.x/#resources","text":"Instructions for upstream Istio with OpenShift: https://istio.io/latest/docs/setup/platform-setup/openshift/ I then installed istioctl: https://istio.io/latest/docs/ops/diagnostic-tools/istioctl/ And then DO NOT follow: https://istio.io/latest/docs/setup/install/standalone-operator/ The standalone operator configuration does not work, so instead follow: https://istio.io/latest/docs/setup/additional-setup/cni/ But be aware of: https://istio.io/latest/docs/setup/additional-setup/cni/#hosted-kubernetes-settings Install the bookinfo application: https://istio.io/latest/docs/examples/bookinfo/","title":"Resources"},{"location":"openshift/istio-1.6-on-ocp.4.x/#installation-instructions","text":"Make sure the cluster was correctly installed: [akaris@linux upi]$ oc get clusterversion NAME VERSION AVAILABLE PROGRESSING SINCE STATUS version 4.4.16 True False 2d1h Cluster version is 4.4.16 [akaris@linux upi]$ oc new-project istio-system Now using project \"istio-system\" on server \"https://api.akaris-upi.focused-solutions.support:6443\". You can add applications to this project with the 'new-app' command. For example, try: oc new-app django-psql-example to build a new example application in Python. Or use kubectl to deploy a simple Kubernetes application: kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node [akaris@linux upi]$ oc adm policy add-scc-to-group anyuid system:serviceaccounts:istio-system securitycontextconstraints.security.openshift.io/anyuid added to groups: [\"system:serviceaccounts:istio-system\"] Install istiocli: [akaris@linux upi]$ curl -sL https://istio.io/downloadIstioctl | sh - Downloading istioctl-1.6.8 from https://github.com/istio/istio/releases/download/1.6.8/istioctl-1.6.8-linux.tar.gz ... Failed. Trying with TARGET_ARCH. Downloading istioctl-1.6.8 from https://github.com/istio/istio/releases/download/1.6.8/istioctl-1.6.8-linux-amd64.tar.gz ... istioctl-1.6.8-linux-amd64.tar.gz download complete! Add the istioctl to your path with: export PATH=$PATH:$HOME/.istioctl/bin Begin the Istio pre-installation verification check by running: istioctl verify-install Need more information? Visit https://istio.io/docs/reference/commands/istioctl/ [akaris@linux upi]$ export PATH=$PATH:$HOME/.istioctl/bin^C [akaris@linux upi]$ export PATH=$PATH:$HOME/.istioctl/bin [akaris@linux upi]$ istioctl verify-install Error: could not load IstioOperator from cluster: the server could not find the requested resource. Use --filename The above error is normal. Nothing was installed yet. Follow https://istio.io/latest/docs/setup/additional-setup/cni/ : $ cat <<EOF > istio-cni.yaml apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: components: cni: enabled: true values: cni: excludeNamespaces: - istio-system - kube-system logLevel: info EOF For OpenShift, then follow: https://istio.io/latest/docs/setup/additional-setup/cni/#hosted-kubernetes-settings : Therefore, run: [akaris@linux upi]$ istioctl install -f istio-cni.yaml --set components.cni.namespace=kube-system --set values.cni.cniBinDir=/var/lib/cni/bin --set values.cni.cniConfDir=/etc/cni/multus/net.d --set values.cni.chained=false --set values.cni.cniConfFileName=\"istio-cni.conf\" --set values.sidecarInjectorWebhook.injectedAnnotations.\"k8s\\.v1\\.cni\\.cncf\\.io/networks\"=istio-cni \u2714 Istio core installed \u2714 Istiod installed \u2714 Addons installed \u2714 Ingress gateways installed \u2714 CNI installed - Pruning removed resources Pruned object DaemonSet:istio-system:istio-cni-node. Pruned object ConfigMap:istio-system:istio-cni-config. Pruned object ServiceAccount:istio-system:istio-cni. \u2714 Installation complete Verify the installation: [akaris@linux upi]$ istioctl verify-install ClusterRole: istio-cni.default checked successfully ClusterRole: istio-cni-repair-role.default checked successfully ClusterRoleBinding: istio-cni.default checked successfully ClusterRoleBinding: istio-cni-repair-rolebinding.default checked successfully ConfigMap: istio-cni-config.kube-system checked successfully DaemonSet: istio-cni-node.kube-system checked successfully ServiceAccount: istio-cni.kube-system checked successfully HorizontalPodAutoscaler: istio-ingressgateway.istio-system checked successfully Deployment: istio-ingressgateway.istio-system checked successfully PodDisruptionBudget: istio-ingressgateway.istio-system checked successfully Role: istio-ingressgateway-sds.istio-system checked successfully RoleBinding: istio-ingressgateway-sds.istio-system checked successfully Service: istio-ingressgateway.istio-system checked successfully ServiceAccount: istio-ingressgateway-service-account.istio-system checked successfully ClusterRole: prometheus-istio-system.default checked successfully ClusterRoleBinding: prometheus-istio-system.default checked successfully ConfigMap: prometheus.istio-system checked successfully Deployment: prometheus.istio-system checked successfully Service: prometheus.istio-system checked successfully ServiceAccount: prometheus.istio-system checked successfully HorizontalPodAutoscaler: istiod.istio-system checked successfully ConfigMap: istio.istio-system checked successfully Deployment: istiod.istio-system checked successfully ConfigMap: istio-sidecar-injector.istio-system checked successfully MutatingWebhookConfiguration: istio-sidecar-injector.default checked successfully PodDisruptionBudget: istiod.istio-system checked successfully Service: istiod.istio-system checked successfully EnvoyFilter: metadata-exchange-1.4.istio-system checked successfully EnvoyFilter: stats-filter-1.4.istio-system checked successfully EnvoyFilter: metadata-exchange-1.5.istio-system checked successfully EnvoyFilter: tcp-metadata-exchange-1.5.istio-system checked successfully EnvoyFilter: stats-filter-1.5.istio-system checked successfully EnvoyFilter: tcp-stats-filter-1.5.istio-system checked successfully EnvoyFilter: metadata-exchange-1.6.istio-system checked successfully EnvoyFilter: tcp-metadata-exchange-1.6.istio-system checked successfully EnvoyFilter: stats-filter-1.6.istio-system checked successfully EnvoyFilter: tcp-stats-filter-1.6.istio-system checked successfully ClusterRole: istiod-istio-system.default checked successfully ClusterRole: istio-reader-istio-system.default checked successfully ClusterRoleBinding: istio-reader-istio-system.default checked successfully ClusterRoleBinding: istiod-pilot-istio-system.default checked successfully ServiceAccount: istio-reader-service-account.istio-system checked successfully ServiceAccount: istiod-service-account.istio-system checked successfully ValidatingWebhookConfiguration: istiod-istio-system.default checked successfully CustomResourceDefinition: httpapispecs.config.istio.io.default checked successfully CustomResourceDefinition: httpapispecbindings.config.istio.io.default checked successfully CustomResourceDefinition: quotaspecs.config.istio.io.default checked successfully CustomResourceDefinition: quotaspecbindings.config.istio.io.default checked successfully CustomResourceDefinition: destinationrules.networking.istio.io.default checked successfully CustomResourceDefinition: envoyfilters.networking.istio.io.default checked successfully CustomResourceDefinition: gateways.networking.istio.io.default checked successfully CustomResourceDefinition: serviceentries.networking.istio.io.default checked successfully CustomResourceDefinition: sidecars.networking.istio.io.default checked successfully CustomResourceDefinition: virtualservices.networking.istio.io.default checked successfully CustomResourceDefinition: workloadentries.networking.istio.io.default checked successfully CustomResourceDefinition: attributemanifests.config.istio.io.default checked successfully CustomResourceDefinition: handlers.config.istio.io.default checked successfully CustomResourceDefinition: instances.config.istio.io.default checked successfully CustomResourceDefinition: rules.config.istio.io.default checked successfully CustomResourceDefinition: clusterrbacconfigs.rbac.istio.io.default checked successfully CustomResourceDefinition: rbacconfigs.rbac.istio.io.default checked successfully CustomResourceDefinition: serviceroles.rbac.istio.io.default checked successfully CustomResourceDefinition: servicerolebindings.rbac.istio.io.default checked successfully CustomResourceDefinition: authorizationpolicies.security.istio.io.default checked successfully CustomResourceDefinition: peerauthentications.security.istio.io.default checked successfully CustomResourceDefinition: requestauthentications.security.istio.io.default checked successfully CustomResourceDefinition: adapters.config.istio.io.default checked successfully CustomResourceDefinition: templates.config.istio.io.default checked successfully CustomResourceDefinition: istiooperators.install.istio.io.default checked successfully Checked 25 custom resource definitions Checked 1 Istio Deployments Istio is installed successfully [akaris@linux upi]$ That will create the istio pods that are needed for the CNI plugin: [akaris@linux upi]$ oc get pods -A | grep istio istio-system istio-ingressgateway-6c77d7f498-d58gx 1/1 Running 0 33m istio-system istiod-58f84ffddc-r2bj9 1/1 Running 0 33m istio-system prometheus-5db67458fb-5m67n 2/2 Running 0 33m kube-system istio-cni-node-52ktn 2/2 Running 0 56s kube-system istio-cni-node-794mr 2/2 Running 0 56s kube-system istio-cni-node-gggvm 2/2 Running 0 56s kube-system istio-cni-node-nd72d 2/2 Running 0 56s kube-system istio-cni-node-p2w5f 2/2 Running 0 56s kube-system istio-cni-node-vqjkb 2/2 Running 0 56s Create the bookinfo app according to https://istio.io/latest/docs/examples/bookinfo/ : [akaris@linux upi]$ oc new-project bookinfo Now using project \"bookinfo\" on server \"https://api.akaris-upi.focused-solutions.support:6443\". You can add applications to this project with the 'new-app' command. For example, try: oc new-app django-psql-example to build a new example application in Python. Or use kubectl to deploy a simple Kubernetes application: kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node Adjust SCCs according to https://istio.io/latest/docs/setup/platform-setup/openshift/#privileged-security-context-constraints-for-application-sidecars : The Istio sidecar injected into each application pod runs with user ID 1337, which is not allowed by default in OpenShift. To allow this user ID to be used, execute the following commands. Replace <target-namespace> with the appropriate namespace. So execute: [akaris@linux upi]$ oc adm policy add-scc-to-group privileged system:serviceaccounts:bookinfo securitycontextconstraints.security.openshift.io/privileged added to groups: [\"system:serviceaccounts:bookinfo\"] [akaris@linux upi]$ oc adm policy add-scc-to-group anyuid system:serviceaccounts:bookinfo securitycontextconstraints.security.openshift.io/anyuid added to groups: [\"system:serviceaccounts:bookinfo\"] Verify SCC configuration - in SCC 4.5, check oc get clusterrolebindings | grep scc instead. [akaris@linux upi]$ oc get scc anyuid -o yaml | grep serv -C3 type: RunAsAny groups: - system:cluster-admins - system:serviceaccounts:istio-system kind: SecurityContextConstraints metadata: annotations: Now, create the application: oc apply -f https://raw.githubusercontent.com/istio/istio/release-1.6/samples/bookinfo/platform/kube/bookinfo.yaml [akaris@linux upi]$ oc get pods NAME READY STATUS RESTARTS AGE details-v1-5974b67c8-nd7kl 2/2 Running 0 83s productpage-v1-64794f5db4-gl7c9 2/2 Running 0 83s ratings-v1-c6cdf8d98-z987h 2/2 Running 0 83s reviews-v1-7f6558b974-6z84q 2/2 Running 0 83s reviews-v2-6cb6ccd848-l8s56 2/2 Running 0 83s reviews-v3-cc56b578-9vvqv 2/2 Running 0 83s Now, create the gateway: [akaris@linux upi]$ oc apply -f https://raw.githubusercontent.com/istio/istio/release-1.6/samples/bookinfo/networking/bookinfo-gateway.yaml gateway.networking.istio.io/bookinfo-gateway created [akaris@linux upi]$ oc get svc istio-ingressgateway -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway LoadBalancer 172.30.44.45 af0e3e39e3f544f55a59da6578212ff5-1911792852.eu-west-3.elb.amazonaws.com 15021:31538/TCP,80:31367/TCP,443:31276/TCP,15443:30398/TCP 47m Get variables to connect to the gateway: https://istio.io/latest/docs/tasks/traffic-management/ingress/ingress-control/#determining-the-ingress-ip-and-ports There's an issue in the instructions for the INGRESS_HOST . With AWS, it needs to be .status.loadBalancer.ingress[0].hostname and not .status.loadBalancer.ingress[0].ip : [akaris@linux upi]$ export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}') [akaris@linux upi]$ export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\"http2\")].port}') [akaris@linux upi]$ export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\"https\")].port}') [akaris@linux upi]$ export TCP_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\"tcp\")].port}') [akaris@linux upi]$ export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT Now, connect to the application with curl: [akaris@linux upi]$ curl -s \"http://${GATEWAY_URL}/productpage\" | grep -o \"<title>.*</title>\" <title>Simple Bookstore App</title>","title":"Installation instructions"},{"location":"openshift/kata/","text":"kata containers and the kata operator This post looks at kata containers on top of OpenShift as deployed with the kata-operator ( https://github.com/harche/kata-operator ). kata-operator kata-operator will not install At time of this writing, you will have to hack the worker node once the kata-operator and CR were installed and configured the worker: https://github.com/harche/kata-operator/issues/38 VM configuration [root@openshift-worker-2 ~]# ps aux | grep qemu-kvm root 28450 0.1 0.2 2599904 351472 ? Sl Jul26 1:08 /usr/libexec/qemu-kvm -name sandbox-dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103 -uuid 6f03e56e-efa1-4f0f-9b8f-492cf1824c83 -machine q35,accel=kvm,kernel_irqchip -cpu host -qmp unix:/run/vc/vm/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/qmp.sock,server,nowait -m 2048M,slots=10,maxmem=129853M -device pci-bridge,bus=pcie.0,id=pci-bridge-0,chassis_nr=1,shpc=on,addr=2,romfile= -device virtio-serial-pci,disable-modern=false,id=serial0,romfile= -device virtconsole,chardev=charconsole0,id=console0 -chardev socket,id=charconsole0,path=/run/vc/vm/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/console.sock,server,nowait -device virtio-scsi-pci,id=scsi0,disable-modern=false,romfile= -object rng-random,id=rng0,filename=/dev/urandom -device virtio-rng-pci,rng=rng0,romfile= -device vhost-vsock-pci,disable-modern=false,vhostfd=3,id=vsock-555233078,guest-cid=555233078,romfile= -chardev socket,id=char-1082a328d9d79959,path=/run/vc/vm/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/vhost-fs.sock -device vhost-user-fs-pci,chardev=char-1082a328d9d79959,tag=kataShared,romfile= -netdev tap,id=network-0,vhost=on,vhostfds=4,fds=5 -device driver=virtio-net-pci,netdev=network-0,mac=d6:96:23:1b:02:05,disable-modern=false,mq=on,vectors=4,romfile= -global kvm-pit.lost_tick_policy=discard -vga none -no-user-config -nodefaults -nographic -daemonize -object memory-backend-file,id=dimm1,size=2048M,mem-path=/dev/shm,share=on -numa node,memdev=dimm1 -kernel /usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/vmlinuz -initrd /var/cache/kata-containers/osbuilder-images/4.18.0-193.13.2.el8_2.x86_64/\"rhcos\"-kata-4.18.0-193.13.2.el8_2.x86_64.initrd -append tsc=reliable no_timer_check rcupdate.rcu_expedited=1 i8042.direct=1 i8042.dumbkbd=1 i8042.nopnp=1 i8042.noaux=1 noreplace-smp reboot=k console=hvc0 console=hvc1 iommu=off cryptomgr.notests net.ifnames=0 pci=lastbus=0 quiet panic=1 nr_cpus=40 agent.use_vsock=true scsi_mod.scan=none -pidfile /run/vc/vm/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/pid -smp 1,cores=1,threads=1,sockets=40,maxcpus=40 Initrd build process kata-operator builds an initrd image with /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh . This script is executed by service kata-osbuilder-generate.service upon start and restart. For further details, also see: https://github.com/kata-containers/osbuilder : [root@openshift-worker-2 ~]# systemctl list-unit-files | grep kata kata-osbuilder-generate.service enabled [root@openshift-worker-2 ~]# systemctl status kata-osbuilder-generate.service \u25cf kata-osbuilder-generate.service - Hacky service to enable kata-osbuilder-generate.service Loaded: loaded (/etc/systemd/system/kata-osbuilder-generate.service; enabled; vendor preset: disabled) Active: inactive (dead) since Sun 2020-07-26 15:54:05 UTC; 18h ago Process: 12437 ExecStart=/usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh (code=exited, status=0/SUCCESS) Main PID: 12437 (code=exited, status=0/SUCCESS) CPU: 23.046s Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: INFO: Create /etc/resolv.conf file in rootfs if not exist Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: INFO: Creating summary file Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: INFO: Created summary file '/var/lib/osbuilder/osbuilder.yaml' inside rootfs Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: + Calling osbuilder initrd_builder.sh Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: [OK] init is installed Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: [OK] Agent is installed Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: INFO: Creating /tmp/kata-dracut-images-jeAfIG/kata-containers-initrd.img based on rootfs at /tmp/kata-dracut-rootfs-qPP8lc Jul 26 15:54:05 openshift-worker-2.example.com kata-osbuilder.sh[12437]: 133725 blocks Jul 26 15:54:05 openshift-worker-2.example.com systemd[1]: Started Hacky service to enable kata-osbuilder-generate.service. Jul 26 15:54:05 openshift-worker-2.example.com systemd[1]: kata-osbuilder-generate.service: Consumed 23.046s CPU time [root@openshift-worker-2 ~]# cat /etc/systemd/system/kata-osbuilder-generate.service [Unit] Description=Hacky service to enable kata-osbuilder-generate.service ConditionPathExists=/usr/lib/systemd/system/kata-osbuilder-generate.service [Service] Type=oneshot ExecStart=/usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh ExecRestart=/usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh [Install] WantedBy=multi-user.target [root@openshift-worker-2 ~]# journalctl -u kata-osbuilder-generate.service | cat -- Logs begin at Thu 2020-07-23 14:27:02 UTC, end at Mon 2020-07-27 10:12:56 UTC. -- Jul 26 15:46:48 openshift-worker-2.example.com systemd[1]: Starting Hacky service to enable kata-osbuilder-generate.service... Jul 26 15:46:48 openshift-worker-2.example.com systemd[1]: kata-osbuilder-generate.service: Main process exited, code=exited, status=203/EXEC Jul 26 15:46:48 openshift-worker-2.example.com systemd[1]: kata-osbuilder-generate.service: Failed with result 'exit-code'. Jul 26 15:46:48 openshift-worker-2.example.com systemd[1]: Failed to start Hacky service to enable kata-osbuilder-generate.service. Jul 26 15:46:48 openshift-worker-2.example.com systemd[1]: kata-osbuilder-generate.service: Consumed 944us CPU time Jul 26 15:53:28 openshift-worker-2.example.com systemd[1]: Starting Hacky service to enable kata-osbuilder-generate.service... Jul 26 15:53:28 openshift-worker-2.example.com systemd[1]: kata-osbuilder-generate.service: Main process exited, code=exited, status=203/EXEC Jul 26 15:53:28 openshift-worker-2.example.com systemd[1]: kata-osbuilder-generate.service: Failed with result 'exit-code'. Jul 26 15:53:28 openshift-worker-2.example.com systemd[1]: Failed to start Hacky service to enable kata-osbuilder-generate.service. Jul 26 15:53:28 openshift-worker-2.example.com systemd[1]: kata-osbuilder-generate.service: Consumed 1ms CPU time Jul 26 15:53:34 openshift-worker-2.example.com systemd[1]: /etc/systemd/system/kata-osbuilder-generate.service:8: Unknown lvalue 'ExecRestart' in section 'Service' Jul 26 15:53:41 openshift-worker-2.example.com systemd[1]: Starting Hacky service to enable kata-osbuilder-generate.service... Jul 26 15:53:41 openshift-worker-2.example.com kata-osbuilder.sh[12437]: + Building dracut initrd Jul 26 15:53:41 openshift-worker-2.example.com dracut[12466]: Executing: /usr/bin/dracut --confdir ./dracut/dracut.conf.d --no-compress --conf /dev/null /tmp/kata-dracut-images-jeAfIG/tmp.dHnQ14COyx 4.18.0-193.13.2.el8_2.x86_64 Jul 26 15:53:42 openshift-worker-2.example.com dracut[12466]: *** Including module: bash *** Jul 26 15:53:42 openshift-worker-2.example.com dracut[12466]: *** Including module: systemd *** Jul 26 15:53:43 openshift-worker-2.example.com dracut[12466]: *** Including module: rescue *** Jul 26 15:53:43 openshift-worker-2.example.com dracut[12466]: *** Including module: nss-softokn *** Jul 26 15:53:43 openshift-worker-2.example.com dracut[12466]: *** Including module: kernel-modules *** Jul 26 15:53:43 openshift-worker-2.example.com dracut[12466]: *** Including module: udev-rules *** Jul 26 15:53:43 openshift-worker-2.example.com dracut[12466]: Skipping udev rule: 91-permissions.rules Jul 26 15:53:43 openshift-worker-2.example.com dracut[12466]: Skipping udev rule: 80-drivers-modprobe.rules Jul 26 15:53:43 openshift-worker-2.example.com dracut[12466]: *** Including module: syslog *** Jul 26 15:53:43 openshift-worker-2.example.com kata-osbuilder.sh[12437]: dracut: Could not find any syslog binary although the syslogmodule is selected to be installed. Please check. Jul 26 15:53:43 openshift-worker-2.example.com dracut[12466]: Could not find any syslog binary although the syslogmodule is selected to be installed. Please check. Jul 26 15:53:43 openshift-worker-2.example.com dracut[12466]: *** Including modules done *** Jul 26 15:53:43 openshift-worker-2.example.com dracut[12466]: *** Installing kernel module dependencies *** Jul 26 15:53:43 openshift-worker-2.example.com dracut[12466]: *** Installing kernel module dependencies done *** Jul 26 15:53:43 openshift-worker-2.example.com dracut[12466]: *** Resolving executable dependencies *** Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: *** Resolving executable dependencies done*** Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: Could not find 'strip'. Not stripping the initramfs. Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: *** Store current command line parameters *** Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: *** Creating image file '/tmp/kata-dracut-images-jeAfIG/tmp.dHnQ14COyx' *** Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: Image: /var/tmp/dracut.9gy4Wx/initramfs.img: 39M Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: ======================================================================== Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: Version: Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: Arguments: --confdir './dracut/dracut.conf.d' --no-compress --conf '/dev/null' Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: dracut modules: Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: bash Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: systemd Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: rescue Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: nss-softokn Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: kernel-modules Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: udev-rules Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: syslog Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: ======================================================================== Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: crw-r--r-- 1 root root 5, 1 Jan 1 1970 dev/console Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: crw-r--r-- 1 root root 1, 11 Jan 1 1970 dev/kmsg Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: crw-r--r-- 1 root root 1, 3 Jan 1 1970 dev/null Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: crw-r--r-- 1 root root 1, 8 Jan 1 1970 dev/random Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: crw-r--r-- 1 root root 1, 9 Jan 1 1970 dev/urandom Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 11 root root 0 Jan 1 1970 . Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 7 Jan 1 1970 bin -> usr/bin Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 dev Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 7 root root 0 Jan 1 1970 etc Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 etc/cmdline.d Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 etc/conf.d Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 124 Jan 1 1970 etc/conf.d/systemd.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 116 Jan 1 1970 etc/group Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 4335 Jan 1 1970 etc/ld.so.cache Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 28 Jan 1 1970 etc/ld.so.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 etc/ld.so.conf.d Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 26 Jan 1 1970 etc/ld.so.conf.d/bind-export-x86_64.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -r--r--r-- 1 root root 67 Jan 1 1970 etc/ld.so.conf.d/kernel-4.18.0-193.13.2.el8_2.x86_64.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 0 Jan 1 1970 etc/machine-id Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 0 Jan 1 1970 etc/passwd Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 etc/systemd Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 64 Jan 1 1970 etc/systemd/journald.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 3 root root 0 Jan 1 1970 etc/udev Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 etc/udev/rules.d Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 297 Jan 1 1970 etc/udev/rules.d/59-persistent-storage.rules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 1030 Jan 1 1970 etc/udev/rules.d/61-persistent-storage.rules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 215 Jan 1 1970 etc/udev/udev.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 1204 Jan 1 1970 etc/virc Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 23 Jan 1 1970 init -> usr/lib/systemd/systemd Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 7 Jan 1 1970 lib -> usr/lib Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 9 Jan 1 1970 lib64 -> usr/lib64 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 proc Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 12 Jan 1 1970 root -> var/roothome Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 run Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 8 Jan 1 1970 sbin -> usr/sbin Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 sys Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 sysroot Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 tmp Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 6 root root 0 Jan 1 1970 usr Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/bin Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 1596592 Jan 1 1970 usr/bin/bash Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 38504 Jan 1 1970 usr/bin/cat Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 38392 Jan 1 1970 usr/bin/echo Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 22496 Jan 1 1970 usr/bin/free Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 193768 Jan 1 1970 usr/bin/grep Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 78712 Jan 1 1970 usr/bin/journalctl Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 163744 Jan 1 1970 usr/bin/kmod Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 4 Jan 1 1970 usr/bin/loginctl -> true Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 46112 Jan 1 1970 usr/bin/more Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwsr-xr-x 1 root root 50456 Jan 1 1970 usr/bin/mount Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 180280 Jan 1 1970 usr/bin/netstat Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 80136 Jan 1 1970 usr/bin/ping Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 141240 Jan 1 1970 usr/bin/ps Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 72216 Jan 1 1970 usr/bin/rm Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 33552 Jan 1 1970 usr/bin/rpcinfo Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 171464 Jan 1 1970 usr/bin/scp Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 4 Jan 1 1970 usr/bin/sh -> bash Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 1040320 Jan 1 1970 usr/bin/ssh Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 1505288 Jan 1 1970 usr/bin/strace Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 215336 Jan 1 1970 usr/bin/systemctl Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 16632 Jan 1 1970 usr/bin/systemd-cgls Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 16608 Jan 1 1970 usr/bin/systemd-escape Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 49984 Jan 1 1970 usr/bin/systemd-run Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 74936 Jan 1 1970 usr/bin/systemd-tmpfiles Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 34288 Jan 1 1970 usr/bin/true Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 434592 Jan 1 1970 usr/bin/udevadm Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwsr-xr-x 1 root root 33640 Jan 1 1970 usr/bin/umount Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 38384 Jan 1 1970 usr/bin/uname Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 1416744 Jan 1 1970 usr/bin/vi Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 9 root root 0 Jan 1 1970 usr/lib Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 3 root root 0 Jan 1 1970 usr/lib/dracut Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 68 Jan 1 1970 usr/lib/dracut/build-parameter.txt Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 15 root root 0 Jan 1 1970 usr/lib/dracut/hooks Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/cleanup Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/cmdline Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/emergency Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 6 root root 0 Jan 1 1970 usr/lib/dracut/hooks/initqueue Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/initqueue/finished Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/initqueue/online Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/initqueue/settled Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/initqueue/timeout Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/mount Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/netroot Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/pre-mount Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/pre-pivot Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/pre-shutdown Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/pre-trigger Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/pre-udev Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/shutdown Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/shutdown-emergency Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 65 Jan 1 1970 usr/lib/dracut/modules.txt Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 0 Jan 1 1970 usr/lib/dracut/need-initqueue Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/modprobe.d Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 382 Jan 1 1970 usr/lib/modprobe.d/dist-alsa.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 884 Jan 1 1970 usr/lib/modprobe.d/dist-blacklist.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 559 Jan 1 1970 usr/lib/modprobe.d/libmlx4.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 765 Jan 1 1970 usr/lib/modprobe.d/systemd.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 3 root root 0 Jan 1 1970 usr/lib/modules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 3 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 7 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 3 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/arch Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 3 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/arch/x86 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 3 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/arch/x86/crypto Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/arch/x86/crypto/sha256-mb Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 9256 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/arch/x86/crypto/sha256-mb/sha256-mb.ko.xz Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/crypto Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 6756 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/crypto/mcryptd.ko.xz Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 6 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/drivers Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/drivers/block Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 8924 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/drivers/block/virtio_blk.ko.xz Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/drivers/char Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 14796 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/drivers/char/virtio_console.ko.xz Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/drivers/net Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 6776 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/drivers/net/net_failover.ko.xz Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 24512 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/drivers/net/virtio_net.ko.xz Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/drivers/scsi Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 20836 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/drivers/scsi/sg.ko.xz Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 8744 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/drivers/scsi/virtio_scsi.ko.xz Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 3 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/fs Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/fs/fuse Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 57012 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/fs/fuse/fuse.ko.xz Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 11132 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/fs/fuse/virtiofs.ko.xz Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 4 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/net Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/net/core Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 4100 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/net/core/failover.ko.xz Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/net/vmw_vsock Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 6884 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/net/vmw_vsock/vmw_vsock_virtio_transport.ko.xz Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 12144 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/net/vmw_vsock/vmw_vsock_virtio_transport_common.ko.xz Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 13696 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/net/vmw_vsock/vsock.ko.xz Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 552 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/modules.alias Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 777 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/modules.alias.bin Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 7534 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/modules.builtin Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 9748 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/modules.builtin.bin Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 827 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/modules.dep Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 1374 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/modules.dep.bin Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 70 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/modules.devname Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 100570 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/modules.order Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 55 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/modules.softdep Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 5544 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/modules.symbols Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 5720 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/modules.symbols.bin Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/sysctl.d Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 122 Jan 1 1970 usr/lib/sysctl.d/10-coreos-ratelimit-kmsg.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 1810 Jan 1 1970 usr/lib/sysctl.d/10-default-yama-scope.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 524 Jan 1 1970 usr/lib/sysctl.d/50-coredump.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 1270 Jan 1 1970 usr/lib/sysctl.d/50-default.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 246 Jan 1 1970 usr/lib/sysctl.d/50-libkcapi-optmem_max.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 636 Jan 1 1970 usr/lib/sysctl.d/50-pid-max.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 5 root root 0 Jan 1 1970 usr/lib/systemd Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 2707624 Jan 1 1970 usr/lib/systemd/libsystemd-shared-239.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/systemd/network Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 412 Jan 1 1970 usr/lib/systemd/network/99-default.link Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 8 root root 0 Jan 1 1970 usr/lib/systemd/system Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/systemd/system-generators Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 16648 Jan 1 1970 usr/lib/systemd/system-generators/systemd-debug-generator Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 46080 Jan 1 1970 usr/lib/systemd/system-generators/systemd-fstab-generator Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 1024 Jan 1 1970 usr/lib/systemd/system/basic.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 412 Jan 1 1970 usr/lib/systemd/system/cryptsetup.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 13 Jan 1 1970 usr/lib/systemd/system/ctrl-alt-del.target -> reboot.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 1084 Jan 1 1970 usr/lib/systemd/system/debug-shell.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 17 Jan 1 1970 usr/lib/systemd/system/default.target -> multi-user.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 471 Jan 1 1970 usr/lib/systemd/system/emergency.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/systemd/system/emergency.target.wants Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 33 Jan 1 1970 usr/lib/systemd/system/emergency.target.wants/systemd-vconsole-setup.service -> ../systemd-vconsole-setup.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 480 Jan 1 1970 usr/lib/systemd/system/final.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 527 Jan 1 1970 usr/lib/systemd/system/halt.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 541 Jan 1 1970 usr/lib/systemd/system/kexec.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 721 Jan 1 1970 usr/lib/systemd/system/kmod-static-nodes.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 435 Jan 1 1970 usr/lib/systemd/system/local-fs-pre.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 547 Jan 1 1970 usr/lib/systemd/system/local-fs.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 532 Jan 1 1970 usr/lib/systemd/system/multi-user.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 505 Jan 1 1970 usr/lib/systemd/system/network-online.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 502 Jan 1 1970 usr/lib/systemd/system/network-pre.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 521 Jan 1 1970 usr/lib/systemd/system/network.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 554 Jan 1 1970 usr/lib/systemd/system/nss-lookup.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 513 Jan 1 1970 usr/lib/systemd/system/nss-user-lookup.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 394 Jan 1 1970 usr/lib/systemd/system/paths.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 592 Jan 1 1970 usr/lib/systemd/system/poweroff.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 583 Jan 1 1970 usr/lib/systemd/system/reboot.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 436 Jan 1 1970 usr/lib/systemd/system/remote-fs-pre.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 522 Jan 1 1970 usr/lib/systemd/system/remote-fs.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 492 Jan 1 1970 usr/lib/systemd/system/rescue.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/systemd/system/rescue.target.wants Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 33 Jan 1 1970 usr/lib/systemd/system/rescue.target.wants/systemd-vconsole-setup.service -> ../systemd-vconsole-setup.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 540 Jan 1 1970 usr/lib/systemd/system/rpcbind.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 442 Jan 1 1970 usr/lib/systemd/system/shutdown.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 402 Jan 1 1970 usr/lib/systemd/system/sigpwr.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 449 Jan 1 1970 usr/lib/systemd/system/slices.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 396 Jan 1 1970 usr/lib/systemd/system/sockets.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/systemd/system/sockets.target.wants Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 34 Jan 1 1970 usr/lib/systemd/system/sockets.target.wants/systemd-journald-dev-log.socket -> ../systemd-journald-dev-log.socket Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 26 Jan 1 1970 usr/lib/systemd/system/sockets.target.wants/systemd-journald.socket -> ../systemd-journald.socket Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 31 Jan 1 1970 usr/lib/systemd/system/sockets.target.wants/systemd-udevd-control.socket -> ../systemd-udevd-control.socket Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 30 Jan 1 1970 usr/lib/systemd/system/sockets.target.wants/systemd-udevd-kernel.socket -> ../systemd-udevd-kernel.socket Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 393 Jan 1 1970 usr/lib/systemd/system/swap.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 767 Jan 1 1970 usr/lib/systemd/system/sys-kernel-config.mount Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 558 Jan 1 1970 usr/lib/systemd/system/sysinit.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/systemd/system/sysinit.target.wants Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 28 Jan 1 1970 usr/lib/systemd/system/sysinit.target.wants/kmod-static-nodes.service -> ../kmod-static-nodes.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 36 Jan 1 1970 usr/lib/systemd/system/sysinit.target.wants/systemd-ask-password-console.path -> ../systemd-ask-password-console.path Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 27 Jan 1 1970 usr/lib/systemd/system/sysinit.target.wants/systemd-journald.service -> ../systemd-journald.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 31 Jan 1 1970 usr/lib/systemd/system/sysinit.target.wants/systemd-modules-load.service -> ../systemd-modules-load.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 25 Jan 1 1970 usr/lib/systemd/system/sysinit.target.wants/systemd-sysctl.service -> ../systemd-sysctl.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 37 Jan 1 1970 usr/lib/systemd/system/sysinit.target.wants/systemd-tmpfiles-setup-dev.service -> ../systemd-tmpfiles-setup-dev.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 33 Jan 1 1970 usr/lib/systemd/system/sysinit.target.wants/systemd-tmpfiles-setup.service -> ../systemd-tmpfiles-setup.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 31 Jan 1 1970 usr/lib/systemd/system/sysinit.target.wants/systemd-udev-trigger.service -> ../systemd-udev-trigger.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 24 Jan 1 1970 usr/lib/systemd/system/sysinit.target.wants/systemd-udevd.service -> ../systemd-udevd.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 1407 Jan 1 1970 usr/lib/systemd/system/syslog.socket Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 704 Jan 1 1970 usr/lib/systemd/system/systemd-ask-password-console.path Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 728 Jan 1 1970 usr/lib/systemd/system/systemd-ask-password-console.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/systemd/system/systemd-ask-password-console.service.wants Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 33 Jan 1 1970 usr/lib/systemd/system/systemd-ask-password-console.service.wants/systemd-vconsole-setup.service -> ../systemd-vconsole-setup.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/systemd/system/systemd-ask-password-plymouth.service.wants Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 33 Jan 1 1970 usr/lib/systemd/system/systemd-ask-password-plymouth.service.wants/systemd-vconsole-setup.service -> ../systemd-vconsole-setup.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 671 Jan 1 1970 usr/lib/systemd/system/systemd-fsck@.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 588 Jan 1 1970 usr/lib/systemd/system/systemd-halt.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 647 Jan 1 1970 usr/lib/systemd/system/systemd-journald-audit.socket Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 1130 Jan 1 1970 usr/lib/systemd/system/systemd-journald-dev-log.socket Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 1537 Jan 1 1970 usr/lib/systemd/system/systemd-journald.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 882 Jan 1 1970 usr/lib/systemd/system/systemd-journald.socket Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 601 Jan 1 1970 usr/lib/systemd/system/systemd-kexec.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 1011 Jan 1 1970 usr/lib/systemd/system/systemd-modules-load.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 597 Jan 1 1970 usr/lib/systemd/system/systemd-poweroff.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 800 Jan 1 1970 usr/lib/systemd/system/systemd-random-seed.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 592 Jan 1 1970 usr/lib/systemd/system/systemd-reboot.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 697 Jan 1 1970 usr/lib/systemd/system/systemd-sysctl.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 771 Jan 1 1970 usr/lib/systemd/system/systemd-tmpfiles-setup-dev.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 751 Jan 1 1970 usr/lib/systemd/system/systemd-tmpfiles-setup.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 867 Jan 1 1970 usr/lib/systemd/system/systemd-udev-settle.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 771 Jan 1 1970 usr/lib/systemd/system/systemd-udev-trigger.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 635 Jan 1 1970 usr/lib/systemd/system/systemd-udevd-control.socket Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 610 Jan 1 1970 usr/lib/systemd/system/systemd-udevd-kernel.socket Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 1070 Jan 1 1970 usr/lib/systemd/system/systemd-udevd.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 622 Jan 1 1970 usr/lib/systemd/system/systemd-vconsole-setup.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 445 Jan 1 1970 usr/lib/systemd/system/timers.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 457 Jan 1 1970 usr/lib/systemd/system/umount.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 1559672 Jan 1 1970 usr/lib/systemd/systemd Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 12504 Jan 1 1970 usr/lib/systemd/systemd-cgroups-agent Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 62488 Jan 1 1970 usr/lib/systemd/systemd-coredump Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 25192 Jan 1 1970 usr/lib/systemd/systemd-fsck Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 153912 Jan 1 1970 usr/lib/systemd/systemd-journald Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 21192 Jan 1 1970 usr/lib/systemd/systemd-modules-load Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 12512 Jan 1 1970 usr/lib/systemd/systemd-reply-password Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 62504 Jan 1 1970 usr/lib/systemd/systemd-shutdown Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 16640 Jan 1 1970 usr/lib/systemd/systemd-sysctl Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 418112 Jan 1 1970 usr/lib/systemd/systemd-udevd Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 21104 Jan 1 1970 usr/lib/systemd/systemd-vconsole-setup Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/tmpfiles.d Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 1676 Jan 1 1970 usr/lib/tmpfiles.d/systemd.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 3 root root 0 Jan 1 1970 usr/lib/udev Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 16592 Jan 1 1970 usr/lib/udev/ata_id Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 33312 Jan 1 1970 usr/lib/udev/cdrom_id Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/udev/rules.d Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 1834 Jan 1 1970 usr/lib/udev/rules.d/40-redhat.rules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 3679 Jan 1 1970 usr/lib/udev/rules.d/50-udev-default.rules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 626 Jan 1 1970 usr/lib/udev/rules.d/60-block.rules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 6528 Jan 1 1970 usr/lib/udev/rules.d/60-persistent-storage.rules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 2671 Jan 1 1970 usr/lib/udev/rules.d/70-uaccess.rules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 2758 Jan 1 1970 usr/lib/udev/rules.d/71-seat.rules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 636 Jan 1 1970 usr/lib/udev/rules.d/73-seat-late.rules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 452 Jan 1 1970 usr/lib/udev/rules.d/75-net-description.rules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 615 Jan 1 1970 usr/lib/udev/rules.d/80-drivers.rules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 292 Jan 1 1970 usr/lib/udev/rules.d/80-net-setup-link.rules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 510 Jan 1 1970 usr/lib/udev/rules.d/90-vconsole.rules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 4367 Jan 1 1970 usr/lib/udev/rules.d/99-systemd.rules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 34072 Jan 1 1970 usr/lib/udev/scsi_id Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib64 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 252288 Jan 1 1970 usr/lib64/ld-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 10 Jan 1 1970 usr/lib64/ld-linux-x86-64.so.2 -> ld-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 18 Jan 1 1970 usr/lib64/libacl.so.1 -> libacl.so.1.1.2253 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 54816 Jan 1 1970 usr/lib64/libacl.so.1.1.2253 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 19 Jan 1 1970 usr/lib64/libattr.so.1 -> libattr.so.1.1.2448 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 26192 Jan 1 1970 usr/lib64/libattr.so.1.1.2448 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 17 Jan 1 1970 usr/lib64/libaudit.so.1 -> libaudit.so.1.0.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 123336 Jan 1 1970 usr/lib64/libaudit.so.1.0.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 17 Jan 1 1970 usr/lib64/libblkid.so.1 -> libblkid.so.1.1.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 339272 Jan 1 1970 usr/lib64/libblkid.so.1.1.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 15 Jan 1 1970 usr/lib64/libbz2.so.1 -> libbz2.so.1.0.6 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 74744 Jan 1 1970 usr/lib64/libbz2.so.1.0.6 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 3149120 Jan 1 1970 usr/lib64/libc-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 12 Jan 1 1970 usr/lib64/libc.so.6 -> libc-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 18 Jan 1 1970 usr/lib64/libcap-ng.so.0 -> libcap-ng.so.0.0.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 30256 Jan 1 1970 usr/lib64/libcap-ng.so.0.0.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 14 Jan 1 1970 usr/lib64/libcap.so.2 -> libcap.so.2.26 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 37952 Jan 1 1970 usr/lib64/libcap.so.2.26 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 17 Jan 1 1970 usr/lib64/libcom_err.so.2 -> libcom_err.so.2.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 17336 Jan 1 1970 usr/lib64/libcom_err.so.2.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 17 Jan 1 1970 usr/lib64/libcrypt.so.1 -> libcrypt.so.1.1.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 142712 Jan 1 1970 usr/lib64/libcrypt.so.1.1.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 19 Jan 1 1970 usr/lib64/libcrypto.so.1.1 -> libcrypto.so.1.1.1c Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 3058976 Jan 1 1970 usr/lib64/libcrypto.so.1.1.1c Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 23 Jan 1 1970 usr/lib64/libcryptsetup.so.12 -> libcryptsetup.so.12.5.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 485432 Jan 1 1970 usr/lib64/libcryptsetup.so.12.5.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -r-xr-xr-x 1 root root 371736 Jan 1 1970 usr/lib64/libdevmapper.so.1.02 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 28968 Jan 1 1970 usr/lib64/libdl-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 13 Jan 1 1970 usr/lib64/libdl.so.2 -> libdl-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 658352 Jan 1 1970 usr/lib64/libdw-0.178.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 14 Jan 1 1970 usr/lib64/libdw.so.1 -> libdw-0.178.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 13 Jan 1 1970 usr/lib64/libe2p.so.2 -> libe2p.so.2.3 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 38896 Jan 1 1970 usr/lib64/libe2p.so.2.3 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 104552 Jan 1 1970 usr/lib64/libelf-0.178.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 15 Jan 1 1970 usr/lib64/libelf.so.1 -> libelf-0.178.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 16 Jan 1 1970 usr/lib64/libext2fs.so.2 -> libext2fs.so.2.4 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 428784 Jan 1 1970 usr/lib64/libext2fs.so.2.4 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 21 Jan 1 1970 usr/lib64/libfipscheck.so.1 -> libfipscheck.so.1.2.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 12200 Jan 1 1970 usr/lib64/libfipscheck.so.1.2.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 12112 Jan 1 1970 usr/lib64/libfreebl3.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 899 Jan 1 1970 usr/lib64/libfreeblpriv3.chk Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 552696 Jan 1 1970 usr/lib64/libfreeblpriv3.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 99688 Jan 1 1970 usr/lib64/libgcc_s-8-20191121.so.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 24 Jan 1 1970 usr/lib64/libgcc_s.so.1 -> libgcc_s-8-20191121.so.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 19 Jan 1 1970 usr/lib64/libgcrypt.so.20 -> libgcrypt.so.20.2.3 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 1385560 Jan 1 1970 usr/lib64/libgcrypt.so.20.2.3 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 22 Jan 1 1970 usr/lib64/libgpg-error.so.0 -> libgpg-error.so.0.24.2 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 144392 Jan 1 1970 usr/lib64/libgpg-error.so.0.24.2 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 21 Jan 1 1970 usr/lib64/libgssapi_krb5.so.2 -> libgssapi_krb5.so.2.2 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 335040 Jan 1 1970 usr/lib64/libgssapi_krb5.so.2.2 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 16 Jan 1 1970 usr/lib64/libidn2.so.0 -> libidn2.so.0.3.6 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 162224 Jan 1 1970 usr/lib64/libidn2.so.0.3.6 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 17 Jan 1 1970 usr/lib64/libip4tc.so.2 -> libip4tc.so.2.0.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 33240 Jan 1 1970 usr/lib64/libip4tc.so.2.0.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 18 Jan 1 1970 usr/lib64/libjson-c.so.4 -> libjson-c.so.4.0.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 70432 Jan 1 1970 usr/lib64/libjson-c.so.4.0.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 18 Jan 1 1970 usr/lib64/libk5crypto.so.3 -> libk5crypto.so.3.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 117216 Jan 1 1970 usr/lib64/libk5crypto.so.3.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 18 Jan 1 1970 usr/lib64/libkeyutils.so.1 -> libkeyutils.so.1.6 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 16240 Jan 1 1970 usr/lib64/libkeyutils.so.1.6 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 16 Jan 1 1970 usr/lib64/libkmod.so.2 -> libkmod.so.2.3.3 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 100200 Jan 1 1970 usr/lib64/libkmod.so.2.3.3 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 14 Jan 1 1970 usr/lib64/libkrb5.so.3 -> libkrb5.so.3.3 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 997600 Jan 1 1970 usr/lib64/libkrb5.so.3.3 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 21 Jan 1 1970 usr/lib64/libkrb5support.so.0 -> libkrb5support.so.0.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 71624 Jan 1 1970 usr/lib64/libkrb5support.so.0.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 15 Jan 1 1970 usr/lib64/liblz4.so.1 -> liblz4.so.1.8.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 95728 Jan 1 1970 usr/lib64/liblz4.so.1.8.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 16 Jan 1 1970 usr/lib64/liblzma.so.5 -> liblzma.so.5.2.4 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 192016 Jan 1 1970 usr/lib64/liblzma.so.5.2.4 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 2191792 Jan 1 1970 usr/lib64/libm-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 12 Jan 1 1970 usr/lib64/libm.so.6 -> libm-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 17 Jan 1 1970 usr/lib64/libmount.so.1 -> libmount.so.1.1.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 371208 Jan 1 1970 usr/lib64/libmount.so.1.1.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 76872 Jan 1 1970 usr/lib64/libnss_files-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 20 Jan 1 1970 usr/lib64/libnss_files.so.2 -> libnss_files-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 16 Jan 1 1970 usr/lib64/libpam.so.0 -> libpam.so.0.84.2 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 66272 Jan 1 1970 usr/lib64/libpam.so.0.84.2 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 16 Jan 1 1970 usr/lib64/libpcap.so.1 -> libpcap.so.1.9.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 356920 Jan 1 1970 usr/lib64/libpcap.so.1.9.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 17 Jan 1 1970 usr/lib64/libpcre.so.1 -> libpcre.so.1.2.10 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 471224 Jan 1 1970 usr/lib64/libpcre.so.1.2.10 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 19 Jan 1 1970 usr/lib64/libpcre2-8.so.0 -> libpcre2-8.so.0.7.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 553480 Jan 1 1970 usr/lib64/libpcre2-8.so.0.7.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 18 Jan 1 1970 usr/lib64/libprocps.so.7 -> libprocps.so.7.1.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 86912 Jan 1 1970 usr/lib64/libprocps.so.7.1.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 322816 Jan 1 1970 usr/lib64/libpthread-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 18 Jan 1 1970 usr/lib64/libpthread.so.0 -> libpthread-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 123320 Jan 1 1970 usr/lib64/libresolv-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 17 Jan 1 1970 usr/lib64/libresolv.so.2 -> libresolv-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 74976 Jan 1 1970 usr/lib64/librt-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 13 Jan 1 1970 usr/lib64/librt.so.1 -> librt-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 19 Jan 1 1970 usr/lib64/libseccomp.so.2 -> libseccomp.so.2.4.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 357336 Jan 1 1970 usr/lib64/libseccomp.so.2.4.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 172640 Jan 1 1970 usr/lib64/libselinux.so.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 766120 Jan 1 1970 usr/lib64/libsepol.so.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 16 Jan 1 1970 usr/lib64/libssl.so.1.1 -> libssl.so.1.1.1c Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 615504 Jan 1 1970 usr/lib64/libssl.so.1.1.1c Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 20 Jan 1 1970 usr/lib64/libsystemd.so.0 -> libsystemd.so.0.23.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 1354792 Jan 1 1970 usr/lib64/libsystemd.so.0.23.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 15 Jan 1 1970 usr/lib64/libtinfo.so.6 -> libtinfo.so.6.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 208616 Jan 1 1970 usr/lib64/libtinfo.so.6.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 17 Jan 1 1970 usr/lib64/libtirpc.so.3 -> libtirpc.so.3.0.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 371032 Jan 1 1970 usr/lib64/libtirpc.so.3.0.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 17 Jan 1 1970 usr/lib64/libudev.so.1 -> libudev.so.1.6.11 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 628696 Jan 1 1970 usr/lib64/libudev.so.1.6.11 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 21 Jan 1 1970 usr/lib64/libunistring.so.2 -> libunistring.so.2.1.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 1760264 Jan 1 1970 usr/lib64/libunistring.so.2.1.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 23392 Jan 1 1970 usr/lib64/libutil-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 15 Jan 1 1970 usr/lib64/libutil.so.1 -> libutil-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 16 Jan 1 1970 usr/lib64/libuuid.so.1 -> libuuid.so.1.3.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 33224 Jan 1 1970 usr/lib64/libuuid.so.1.3.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 14 Jan 1 1970 usr/lib64/libz.so.1 -> libz.so.1.2.11 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 97136 Jan 1 1970 usr/lib64/libz.so.1.2.11 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/sbin Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 97008 Jan 1 1970 usr/sbin/blkid Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 11 Jan 1 1970 usr/sbin/depmod -> ../bin/kmod Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 336512 Jan 1 1970 usr/sbin/e2fsck Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 54872 Jan 1 1970 usr/sbin/fsck Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 336512 Jan 1 1970 usr/sbin/fsck.ext2 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 336512 Jan 1 1970 usr/sbin/fsck.ext3 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 336512 Jan 1 1970 usr/sbin/fsck.ext4 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 66688 Jan 1 1970 usr/sbin/fsck.fat Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 8 Jan 1 1970 usr/sbin/fsck.vfat -> fsck.fat Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 22 Jan 1 1970 usr/sbin/init -> ../lib/systemd/systemd Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 11 Jan 1 1970 usr/sbin/insmod -> ../bin/kmod Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 193 Jan 1 1970 usr/sbin/insmodpost.sh Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 11 Jan 1 1970 usr/sbin/lsmod -> ../bin/kmod Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 11 Jan 1 1970 usr/sbin/modinfo -> ../bin/kmod Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 11 Jan 1 1970 usr/sbin/modprobe -> ../bin/kmod Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 12320 Jan 1 1970 usr/sbin/nologin Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 11 Jan 1 1970 usr/sbin/ping -> ../bin/ping Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 11 Jan 1 1970 usr/sbin/ping6 -> ../bin/ping Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 16 Jan 1 1970 usr/sbin/poweroff -> ../bin/systemctl Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 16 Jan 1 1970 usr/sbin/reboot -> ../bin/systemctl Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 11 Jan 1 1970 usr/sbin/rmmod -> ../bin/kmod Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 14 Jan 1 1970 usr/sbin/rpcinfo -> ../bin/rpcinfo Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 21672 Jan 1 1970 usr/sbin/showmount Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 21360 Jan 1 1970 usr/sbin/swapoff Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 14 Jan 1 1970 usr/sbin/udevadm -> ../bin/udevadm Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 4 root root 0 Jan 1 1970 var Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 11 Jan 1 1970 var/lock -> ../run/lock Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwx------ 2 root root 0 Jan 1 1970 var/roothome Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 6 Jan 1 1970 var/run -> ../run Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 var/tmp Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: ======================================================================== Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: *** Creating initramfs image file '/tmp/kata-dracut-images-jeAfIG/tmp.dHnQ14COyx' done *** Jul 26 15:53:45 openshift-worker-2.example.com kata-osbuilder.sh[12437]: Warning no default label for /tmp/kata-dracut-images-jeAfIG/tmp.dHnQ14COyx Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: dracut: warning: could not fsfreeze /tmp/kata-dracut-images-jeAfIG Jul 26 15:53:45 openshift-worker-2.example.com kata-osbuilder.sh[12437]: + Extracting dracut initrd rootfs Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: 78145 blocks Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: + Copying agent directory tree into place Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: Calling osbuilder rootfs.sh on extracted rootfs Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: INFO: Create symlink to /tmp in /var to create private temporal directories with systemd Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: INFO: Install tmp.mount in ./etc/systemd/system Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: cp: cannot stat './usr/share/systemd/tmp.mount': No such file or directory Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: INFO: Create /tmp/kata-dracut-rootfs-qPP8lc/etc Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: INFO: Configure chrony file /tmp/kata-dracut-rootfs-qPP8lc/etc/chrony.conf Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: [OK] cp /usr/libexec/kata-containers/agent/usr/bin/kata-agent /tmp/kata-dracut-rootfs-qPP8lc/usr/bin/kata-agent Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: [OK] Agent installed Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: INFO: Check init is installed Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: [OK] init is installed Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: INFO: Create /etc/resolv.conf file in rootfs if not exist Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: INFO: Creating summary file Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: INFO: Created summary file '/var/lib/osbuilder/osbuilder.yaml' inside rootfs Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: + Calling osbuilder initrd_builder.sh Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: [OK] init is installed Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: [OK] Agent is installed Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: INFO: Creating /tmp/kata-dracut-images-jeAfIG/kata-containers-initrd.img based on rootfs at /tmp/kata-dracut-rootfs-qPP8lc Jul 26 15:54:05 openshift-worker-2.example.com kata-osbuilder.sh[12437]: 133725 blocks Jul 26 15:54:05 openshift-worker-2.example.com systemd[1]: Started Hacky service to enable kata-osbuilder-generate.service. Jul 26 15:54:05 openshift-worker-2.example.com systemd[1]: kata-osbuilder-generate.service: Consumed 23.046s CPU time [root@openshift-worker-2 ~]# [root@openshift-worker-2 ~]# grep IMAGE_TOPDIR /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh readonly IMAGE_TOPDIR=\"/var/cache/kata-containers\" readonly KERNEL_SYMLINK=\"${IMAGE_TOPDIR}/vmlinuz.container\" stable symlink paths in ${IMAGE_TOPDIR} local image_osbuilder_dir=\"${IMAGE_TOPDIR}/osbuilder-images\" local image_dest_link=\"${IMAGE_TOPDIR}/kata-containers.img\" ln -sf ${initrd_dest_path} ${IMAGE_TOPDIR}/kata-containers-initrd.img [root@openshift-worker-2 ~]# grep image_osbuilder_dir ^C [root@openshift-worker-2 ~]# grep image_osbuilder_dir /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh local image_osbuilder_dir=\"${IMAGE_TOPDIR}/osbuilder-images\" local image_dir=\"${image_osbuilder_dir}/$KVERSION\" rm -rf \"${image_osbuilder_dir}\" [root@openshift-worker-2 ~]# grep image_dir /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh local image_dir=\"${image_osbuilder_dir}/$KVERSION\" local initrd_dest_path=\"${image_dir}/${DISTRO}-kata-${KVERSION}.initrd\" local image_dest_path=\"${image_dir}/${DISTRO}-kata-${KVERSION}.img\" mkdir -p \"${image_dir}\" [root@openshift-worker-2 ~]# grep initrd_dest_path /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh local initrd_dest_path=\"${image_dir}/${DISTRO}-kata-${KVERSION}.initrd\" mv -Z ${GENERATED_INITRD} ${initrd_dest_path} ln -sf ${initrd_dest_path} ${IMAGE_TOPDIR}/kata-containers-initrd.img [root@openshift-worker-2 ~]# grep GENERATED_INITRD /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh readonly GENERATED_INITRD=\"${DRACUT_IMAGES}/kata-containers-initrd.img\" mv -Z ${GENERATED_INITRD} ${initrd_dest_path} ./initrd-builder/initrd_builder.sh -o ${GENERATED_INITRD} ${DRACUT_ROOTFS} [root@openshift-worker-2 ~]# grep image_dest_path /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh: local image_dest_path=\"${image_dir}/${DISTRO}-kata-${KVERSION}.img\" /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh: mv -Z ${GENERATED_IMAGE} ${image_dest_path} /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh: ln -sf ${image_dest_path} ${image_dest_link} /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh: local image_dest_path=\"${image_dir}/${DISTRO}-kata-${KVERSION}.img\" /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh: mv -Z ${GENERATED_IMAGE} ${image_dest_path} /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh: ln -sf ${image_dest_path} ${image_dest_link} [root@openshift-worker-2 ~]# grep GENERATED_IMAGE /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh readonly GENERATED_IMAGE=\"${DRACUT_IMAGES}/kata-containers.img\" mv -Z ${GENERATED_IMAGE} ${image_dest_path} -o ${GENERATED_IMAGE} ${DRACUT_ROOTFS} [root@openshift-worker-2 ~]# find /var/cache/kata-containers /var/cache/kata-containers /var/cache/kata-containers/osbuilder-images /var/cache/kata-containers/osbuilder-images/4.18.0-193.13.2.el8_2.x86_64 /var/cache/kata-containers/osbuilder-images/4.18.0-193.13.2.el8_2.x86_64/\"rhcos\"-kata-4.18.0-193.13.2.el8_2.x86_64.initrd /var/cache/kata-containers/vmlinuz.container /var/cache/kata-containers/kata-containers-initrd.img kata configuration - which kernel and initrd does the VM use? Looking at the VM, it uses: -kernel /usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/vmlinuz -initrd /var/cache/kata-containers/osbuilder-images/4.18.0-193.13.2.el8_2.x86_64/\"rhcos\"-kata-4.18.0-193.13.2.el8_2.x86_64.initrd And following the configuration for kata (and the symlinks): [root@openshift-worker-2 ~]# egrep '^kernel' /usr/share/kata-containers/defaults/configuration.toml kernel = \"/var/cache/kata-containers/vmlinuz.container\" kernel_params = \"\" kernel_modules=[] [root@openshift-worker-2 ~]# egrep '^initrd' /usr/share/kata-containers/defaults/configuration.toml initrd = \"/var/cache/kata-containers/kata-containers-initrd.img\" [root@openshift-worker-2 ~]# ls -al /var/cache/kata-containers/vmlinuz.container lrwxrwxrwx. 1 root root 50 Jul 26 15:54 /var/cache/kata-containers/vmlinuz.container -> /lib/modules/4.18.0-193.13.2.el8_2.x86_64//vmlinuz [root@openshift-worker-2 ~]# ls -al /var/cache/kata-containers/kata-containers-initrd.img lrwxrwxrwx. 1 root root 121 Jul 26 15:54 /var/cache/kata-containers/kata-containers-initrd.img -> '/var/cache/kata-containers/osbuilder-images/4.18.0-193.13.2.el8_2.x86_64/\"rhcos\"-kata-4.18.0-193.13.2.el8_2.x86_64.initrd' [root@openshift-worker-2 ~]# kata-runtime For further details, see: https://github.com/kata-containers/runtime Opened files: [root@openshift-worker-2 ~]# strace -e trace=file kata-runtime list 2>&1 | grep open openat(AT_FDCWD, \"/etc/ld.so.cache\", O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \"/lib64/libpthread.so.0\", O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \"/lib64/libdl.so.2\", O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \"/lib64/libc.so.6\", O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \"/sys/kernel/mm/transparent_hugepage/hpage_pmd_size\", O_RDONLY) = 3 openat(AT_FDCWD, \"/etc/ld.so.cache\", O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \"/lib64/libcrypto.so.1.1\", O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \"/lib64/libz.so.1\", O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \"/proc/self/uid_map\", O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \"/sys/kernel/mm/hugepages\", O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \"/dev/null\", O_WRONLY|O_CREAT|O_APPEND|O_SYNC|O_CLOEXEC, 0640) = 3 openat(AT_FDCWD, \"/usr/share/kata-containers/defaults/configuration.toml\", O_RDONLY|O_CLOEXEC) = 5 openat(AT_FDCWD, \"/etc//localtime\", O_RDONLY) = 6 openat(AT_FDCWD, \"/proc/self/uid_map\", O_RDONLY|O_CLOEXEC) = 6 openat(AT_FDCWD, \"/run/vc/sbs\", O_RDONLY|O_CLOEXEC) = 6 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103\", O_RDONLY|O_CLOEXEC) = 7 openat(AT_FDCWD, \"/var/lib/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/config.json\", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/persist.json\", O_RDONLY|O_CLOEXEC) = 8 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103\", O_RDONLY|O_CLOEXEC) = 9 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/6b41cdf410d12e75f99c37c7cadc4c968b1061d388db379aecacf381f46af56a/persist.json\", O_RDONLY|O_CLOEXEC) = 9 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/persist.json\", O_RDONLY|O_CLOEXEC) = 10 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/persist.json\", O_RDONLY|O_CLOEXEC) = 8 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103\", O_RDONLY|O_CLOEXEC) = 9 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/6b41cdf410d12e75f99c37c7cadc4c968b1061d388db379aecacf381f46af56a/persist.json\", O_RDONLY|O_CLOEXEC) = 9 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/persist.json\", O_RDONLY|O_CLOEXEC) = 10 openat(AT_FDCWD, \"/proc/cpuinfo\", O_RDONLY|O_CLOEXEC) = 8 openat(AT_FDCWD, \"/proc/meminfo\", O_RDONLY|O_CLOEXEC) = 8 openat(AT_FDCWD, \"/run/containers/storage/overlay-containers/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/userdata/config.json\", O_RDONLY|O_CLOEXEC) = 8 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/persist.json\", O_RDONLY|O_CLOEXEC) = 8 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103\", O_RDONLY|O_CLOEXEC) = 9 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/6b41cdf410d12e75f99c37c7cadc4c968b1061d388db379aecacf381f46af56a/persist.json\", O_RDONLY|O_CLOEXEC) = 9 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/persist.json\", O_RDONLY|O_CLOEXEC) = 10 openat(AT_FDCWD, \"/run/containers/storage/overlay-containers/6b41cdf410d12e75f99c37c7cadc4c968b1061d388db379aecacf381f46af56a/userdata/config.json\", O_RDONLY|O_CLOEXEC) = 8 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/persist.json\", O_RDONLY|O_CLOEXEC) = 8 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103\", O_RDONLY|O_CLOEXEC) = 9 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/6b41cdf410d12e75f99c37c7cadc4c968b1061d388db379aecacf381f46af56a/persist.json\", O_RDONLY|O_CLOEXEC) = 9 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/persist.json\", O_RDONLY|O_CLOEXEC) = 10 Listing VMs / containers and verifying runtime state: [root@openshift-worker-2 ~]# kata-runtime kata-check System is capable of running Kata Containers System can currently create Kata Containers [root@openshift-worker-2 ~]# kata-runtime list ID PID STATUS BUNDLE CREATED OWNER dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103 -1 running /run/containers/storage/overlay-containers/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/userdata 2020-07-26T16:04:25.183088432Z #0 6b41cdf410d12e75f99c37c7cadc4c968b1061d388db379aecacf381f46af56a -1 running /run/containers/storage/overlay-containers/6b41cdf410d12e75f99c37c7cadc4c968b1061d388db379aecacf381f46af56a/userdata 2020-07-26T16:04:25.467048412Z #0 [root@openshift-worker-2 ~]# ps aux | grep dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103 root 28433 0.0 0.0 878648 30720 ? Sl Jul26 0:59 /usr/bin/containerd-shim-kata-v2 -namespace default -address -publish-binary /usr/bin/crio -id dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103 root 28445 0.0 0.0 68424 5664 ? S Jul26 0:00 /usr/libexec/virtiofsd --fd=3 -o source=/run/kata-containers/shared/sandboxes/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/shared -o cache=always --syslog -o no_posix_lock -f root 28450 0.1 0.2 2599904 351472 ? Sl Jul26 1:10 /usr/libexec/qemu-kvm -name sandbox-dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103 -uuid 6f03e56e-efa1-4f0f-9b8f-492cf1824c83 -machine q35,accel=kvm,kernel_irqchip -cpu host -qmp unix:/run/vc/vm/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/qmp.sock,server,nowait -m 2048M,slots=10,maxmem=129853M -device pci-bridge,bus=pcie.0,id=pci-bridge-0,chassis_nr=1,shpc=on,addr=2,romfile= -device virtio-serial-pci,disable-modern=false,id=serial0,romfile= -device virtconsole,chardev=charconsole0,id=console0 -chardev socket,id=charconsole0,path=/run/vc/vm/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/console.sock,server,nowait -device virtio-scsi-pci,id=scsi0,disable-modern=false,romfile= -object rng-random,id=rng0,filename=/dev/urandom -device virtio-rng-pci,rng=rng0,romfile= -device vhost-vsock-pci,disable-modern=false,vhostfd=3,id=vsock-555233078,guest-cid=555233078,romfile= -chardev socket,id=char-1082a328d9d79959,path=/run/vc/vm/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/vhost-fs.sock -device vhost-user-fs-pci,chardev=char-1082a328d9d79959,tag=kataShared,romfile= -netdev tap,id=network-0,vhost=on,vhostfds=4,fds=5 -device driver=virtio-net-pci,netdev=network-0,mac=d6:96:23:1b:02:05,disable-modern=false,mq=on,vectors=4,romfile= -global kvm-pit.lost_tick_policy=discard -vga none -no-user-config -nodefaults -nographic -daemonize -object memory-backend-file,id=dimm1,size=2048M,mem-path=/dev/shm,share=on -numa node,memdev=dimm1 -kernel /usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/vmlinuz -initrd /var/cache/kata-containers/osbuilder-images/4.18.0-193.13.2.el8_2.x86_64/\"rhcos\"-kata-4.18.0-193.13.2.el8_2.x86_64.initrd -append tsc=reliable no_timer_check rcupdate.rcu_expedited=1 i8042.direct=1 i8042.dumbkbd=1 i8042.nopnp=1 i8042.noaux=1 noreplace-smp reboot=k console=hvc0 console=hvc1 iommu=off cryptomgr.notests net.ifnames=0 pci=lastbus=0 quiet panic=1 nr_cpus=40 agent.use_vsock=true scsi_mod.scan=none -pidfile /run/vc/vm/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/pid -smp 1,cores=1,threads=1,sockets=40,maxcpus=40 root 28454 0.0 0.0 4215264 17304 ? Sl Jul26 0:00 /usr/libexec/virtiofsd --fd=3 -o source=/run/kata-containers/shared/sandboxes/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/shared -o cache=always --syslog -o no_posix_lock -f root 1366797 0.0 0.0 12920 2360 pts/0 S+ 10:41 0:00 grep --color=auto dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103 [root@openshift-worker-2 ~]# And to verify the kata-runtime configuration: [root@openshift-worker-2 ~]# kata-runtime --kata-show-default-config-paths /etc/kata-containers/configuration.toml /usr/share/kata-containers/defaults/configuration.toml [root@openshift-worker-2 ~]# ls /etc/kata-containers ls: cannot access '/etc/kata-containers': No such file or directory [root@openshift-worker-2 ~]# kata-runtime kata-env [Meta] Version = \"1.0.24\" [Runtime] Debug = false Trace = false DisableGuestSeccomp = true DisableNewNetNs = false SandboxCgroupOnly = true Path = \"/usr/bin/kata-runtime\" [Runtime.Version] OCI = \"1.0.1-dev\" [Runtime.Version.Version] Semver = \"1.11.1\" Major = 1 Minor = 11 Patch = 1 Commit = \"\" [Runtime.Config] Path = \"/usr/share/kata-containers/defaults/configuration.toml\" [Hypervisor] MachineType = \"q35\" Version = \"QEMU emulator version 4.2.0 (qemu-kvm-4.2.0-19.el8)\\nCopyright (c) 2003-2019 Fabrice Bellard and the QEMU Project developers\" Path = \"/usr/libexec/qemu-kvm\" BlockDeviceDriver = \"virtio-scsi\" EntropySource = \"/dev/urandom\" SharedFS = \"virtio-fs\" VirtioFSDaemon = \"/usr/libexec/virtiofsd\" Msize9p = 8192 MemorySlots = 10 PCIeRootPort = 0 HotplugVFIOOnRootBus = false Debug = false UseVSock = true [Image] Path = \"\" [Kernel] Path = \"/usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/vmlinuz\" Parameters = \"scsi_mod.scan=none\" [Initrd] Path = \"/var/cache/kata-containers/osbuilder-images/4.18.0-193.13.2.el8_2.x86_64/\\\"rhcos\\\"-kata-4.18.0-193.13.2.el8_2.x86_64.initrd\" [Proxy] Type = \"noProxy\" Path = \"\" Debug = false [Proxy.Version] Semver = \"\" Major = 0 Minor = 0 Patch = 0 Commit = \"\" [Shim] Type = \"kataShim\" Path = \"/usr/libexec/kata-containers/kata-shim\" Debug = false [Shim.Version] Semver = \"1.11.1\" Major = 1 Minor = 11 Patch = 1 Commit = \"<<unknown>>\" [Agent] Type = \"kata\" Debug = false Trace = false TraceMode = \"\" TraceType = \"\" [Host] Kernel = \"4.18.0-193.13.2.el8_2.x86_64\" Architecture = \"amd64\" VMContainerCapable = true SupportVSocks = true [Host.Distro] Name = \"Red Hat Enterprise Linux CoreOS\" Version = \"4.5\" [Host.CPU] Vendor = \"GenuineIntel\" Model = \"Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz\" [Netmon] Path = \"/usr/libexec/kata-containers/kata-netmon\" Debug = false Enable = false [Netmon.Version] Semver = \"1.11.1\" Major = 1 Minor = 11 Patch = 1 Commit = \"<<unknown>>\" And for logging: [root@openshift-worker-2 ~]# journalctl -t kata-runtime -- Logs begin at Thu 2020-07-23 14:27:02 UTC, end at Mon 2020-07-27 10:47:03 UTC. -- Jul 27 10:33:25 openshift-worker-2.example.com kata-runtime[1356869]: time=\"2020-07-27T10:33:25.151041313Z\" level=info msg=\"loaded configuration\" arch=amd64 command=list file=/usr/share/kata-containers/defaults/configuration.toml format=TOML name=kata-runtime pid=1356869> Jul 27 10:33:25 openshift-worker-2.example.com kata-runtime[1356869]: time=\"2020-07-27T10:33:25.151232547Z\" level=info msg=\"vsock supported\" arch=amd64 command=list name=kata-runtime pid=1356869 source=katautils Jul 27 10:33:25 openshift-worker-2.example.com kata-runtime[1356869]: time=\"2020-07-27T10:33:25.151287033Z\" level=info msg=\"VSOCK supported, configure to not use proxy\" arch=amd64 command=list name=kata-runtime pid=1356869 source=katautils Jul 27 10:33:25 openshift-worker-2.example.com kata-runtime[1356869]: time=\"2020-07-27T10:33:25.151325928Z\" level=info arch=amd64 arguments=\"\\\"list\\\"\" command=list commit= name=kata-runtime pid=1356869 source=runtime version=1.11.1 Jul 27 10:33:25 openshift-worker-2.example.com kata-runtime[1356869]: time=\"2020-07-27T10:33:25.151435298Z\" level=info msg=\"fetch sandbox\" arch=amd64 command=list name=kata-runtime pid=1356869 source=virtcontainers Jul 27 10:33:25 openshift-worker-2.example.com kata-runtime[1356869]: time=\"2020-07-27T10:33:25.206438572Z\" level=warning msg=\"failed to get sandbox config from old store: open /var/lib/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/config.json: > Jul 27 10:34:13 openshift-worker-2.example.com kata-runtime[1357888]: time=\"2020-07-27T10:34:13.491790968Z\" level=info msg=\"loaded configuration\" arch=amd64 command=state file=/usr/share/kata-containers/defaults/configuration.toml format=TOML name=kata-runtime pid=135788> Jul 27 10:34:13 openshift-worker-2.example.com kata-runtime[1357888]: time=\"2020-07-27T10:34:13.491979419Z\" level=info msg=\"vsock supported\" arch=amd64 command=state name=kata-runtime pid=1357888 source=katautils Jul 27 10:34:13 openshift-worker-2.example.com kata-runtime[1357888]: time=\"2020-07-27T10:34:13.49202632Z\" level=info msg=\"VSOCK supported, configure to not use proxy\" arch=amd64 command=state name=kata-runtime pid=1357888 source=katautils Jul 27 10:34:13 openshift-worker-2.example.com kata-runtime[1357888]: time=\"2020-07-27T10:34:13.492063527Z\" level=info arch=amd64 arguments=\"\\\"state dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103\\\"\" command=state commit= name=kata-runtime pid=1357888 so> Jul 27 10:34:13 openshift-worker-2.example.com kata-runtime[1357888]: time=\"2020-07-27T10:34:13.492166408Z\" level=error msg=\"Container ID (dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103) does not exist\" arch=amd64 command=state container=dc7a95dc3219fa5> Jul 27 10:34:29 openshift-worker-2.example.com kata-runtime[1358196]: time=\"2020-07-27T10:34:29.677420912Z\" level=info msg=\"loaded configuration\" arch=amd64 command=state file=/usr/share/kata-containers/defaults/configuration.toml format=TOML name=kata-runtime pid=135819> Jul 27 10:34:29 openshift-worker-2.example.com kata-runtime[1358196]: time=\"2020-07-27T10:34:29.677612114Z\" level=info msg=\"vsock supported\" arch=amd64 command=state name=kata-runtime pid=1358196 source=katautils Jul 27 10:34:29 openshift-worker-2.example.com kata-runtime[1358196]: time=\"2020-07-27T10:34:29.677660668Z\" level=info msg=\"VSOCK supported, configure to not use proxy\" arch=amd64 command=state name=kata-runtime pid=1358196 source=katautils Jul 27 10:34:29 openshift-worker-2.example.com kata-runtime[1358196]: time=\"2020-07-27T10:34:29.677699242Z\" level=info arch=amd64 arguments=\"\\\"state 6b41cdf410d12e75f99c37c7cadc4c968b1061d388db379aecacf381f46af56a\\\"\" command=state commit= name=kata-runtime pid=1358196 so> Jul 27 10:34:29 openshift-worker-2.example.com kata-runtime[1358196]: time=\"2020-07-27T10:34:29.677801888Z\" level=error msg=\"Container ID (6b41cdf410d12e75f99c37c7cadc4c968b1061d388db379aecacf381f46af56a) does not exist\" arch=amd64 command=state container=6b41cdf410d12e7> Jul 27 10:34:32 openshift-worker-2.example.com kata-runtime[1358313]: time=\"2020-07-27T10:34:32.680058234Z\" level=info msg=\"loaded configuration\" arch=amd64 command=ps file=/usr/share/kata-containers/defaults/configuration.toml format=TOML name=kata-runtime pid=1358313 s> Jul 27 10:34:32 openshift-worker-2.example.com kata-runtime[1358313]: time=\"2020-07-27T10:34:32.680258087Z\" level=info msg=\"vsock supported\" arch=amd64 command=ps name=kata-runtime pid=1358313 source=katautils Jul 27 10:34:32 openshift-worker-2.example.com kata-runtime[1358313]: time=\"2020-07-27T10:34:32.680309596Z\" level=info msg=\"VSOCK supported, configure to not use proxy\" arch=amd64 command=ps name=kata-runtime pid=1358313 source=katautils Jul 27 10:34:32 openshift-worker-2.example.com kata-runtime[1358313]: time=\"2020-07-27T10:34:32.680345567Z\" level=info arch=amd64 arguments=\"\\\"ps\\\"\" command=ps commit= name=kata-runtime pid=1358313 source=runtime version=1.11.1 Jul 27 10:34:32 openshift-worker-2.example.com kata-runtime[1358313]: time=\"2020-07-27T10:34:32.680388502Z\" level=error msg=\"Missing container ID, should at least provide one\" arch=amd64 command=ps name=kata-runtime pid=1358313 source=runtime Jul 27 10:34:35 openshift-worker-2.example.com kata-runtime[1358359]: time=\"2020-07-27T10:34:35.49846456Z\" level=info msg=\"loaded configuration\" arch=amd64 command=list file=/usr/share/kata-containers/defaults/configuration.toml format=TOML name=kata-runtime pid=1358359 > Jul 27 10:34:35 openshift-worker-2.example.com kata-runtime[1358359]: time=\"2020-07-27T10:34:35.498647076Z\" level=info msg=\"vsock supported\" arch=amd64 command=list name=kata-runtime pid=1358359 source=katautils Jul 27 10:34:35 openshift-worker-2.example.com kata-runtime[1358359]: time=\"2020-07-27T10:34:35.498689418Z\" level=info msg=\"VSOCK supported, configure to not use proxy\" arch=amd64 command=list name=kata-runtime pid=1358359 source=katautils Jul 27 10:34:35 openshift-worker-2.example.com kata-runtime[1358359]: time=\"2020-07-27T10:34:35.498725525Z\" level=info arch=amd64 arguments=\"\\\"list\\\"\" command=list commit= name=kata-runtime pid=1358359 source=runtime version=1.11.1 Jul 27 10:34:35 openshift-worker-2.example.com kata-runtime[1358359]: time=\"2020-07-27T10:34:35.498825313Z\" level=info msg=\"fetch sandbox\" arch=amd64 command=list name=kata-runtime pid=1358359 source=virtcontainers Jul 27 10:34:35 openshift-worker-2.example.com kata-runtime[1358359]: time=\"2020-07-27T10:34:35.498906984Z\" level=warning msg=\"failed to get sandbox config from old store: open /var/lib/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/config.json: > Jul 27 10:34:51 openshift-worker-2.example.com kata-runtime[1358694]: time=\"2020-07-27T10:34:51.733126256Z\" level=info msg=\"loaded configuration\" arch=amd64 command=events file=/usr/share/kata-containers/defaults/configuration.toml format=TOML name=kata-runtime pid=13586> Jul 27 10:34:51 openshift-worker-2.example.com kata-runtime[1358694]: time=\"2020-07-27T10:34:51.73331869Z\" level=info msg=\"vsock supported\" arch=amd64 command=events name=kata-runtime pid=1358694 source=katautils Jul 27 10:34:51 openshift-worker-2.example.com kata-runtime[1358694]: time=\"2020-07-27T10:34:51.733376204Z\" level=info msg=\"VSOCK supported, configure to not use proxy\" arch=amd64 command=events name=kata-runtime pid=1358694 source=katautils Jul 27 10:34:51 openshift-worker-2.example.com kata-runtime[1358694]: time=\"2020-07-27T10:34:51.733416598Z\" level=info arch=amd64 arguments=\"\\\"events\\\"\" command=events commit= name=kata-runtime pid=1358694 source=runtime version=1.11.1 Jul 27 10:34:51 openshift-worker-2.example.com kata-runtime[1358694]: time=\"2020-07-27T10:34:51.733458859Z\" level=error msg=\"container id cannot be empty\" arch=amd64 command=events name=kata-runtime pid=1358694 source=runtime Jul 27 10:34:56 openshift-worker-2.example.com kata-runtime[1358808]: time=\"2020-07-27T10:34:56.767961592Z\" level=info msg=\"loaded configuration\" arch=amd64 command=events file=/usr/share/kata-containers/defaults/configuration.toml format=TOML name=kata-runtime pid=13588> Jul 27 10:34:56 openshift-worker-2.example.com kata-runtime[1358808]: time=\"2020-07-27T10:34:56.768145404Z\" level=info msg=\"vsock supported\" arch=amd64 command=events name=kata-runtime pid=1358808 source=katautils Jul 27 10:34:56 openshift-worker-2.example.com kata-runtime[1358808]: time=\"2020-07-27T10:34:56.768196771Z\" level=info msg=\"VSOCK supported, configure to not use proxy\" arch=amd64 command=events name=kata-runtime pid=1358808 source=katautils Jul 27 10:34:56 openshift-worker-2.example.com kata-runtime[1358808]: time=\"2020-07-27T10:34:56.768235257Z\" level=info arch=amd64 arguments=\"\\\"events dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103\\\"\" command=events commit= name=kata-runtime pid=1358808 > Jul 27 10:34:56 openshift-worker-2.example.com kata-runtime[1358808]: time=\"2020-07-27T10:34:56.768401639Z\" level=error msg=\"Container ID (dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103) does not exist\" arch=amd64 command=events container=dc7a95dc3219fa> Jul 27 10:35:16 openshift-worker-2.example.com kata-runtime[1359217]: time=\"2020-07-27T10:35:16.137798572Z\" level=info msg=\"loaded configuration\" arch=amd64 command=list file=/usr/share/kata-containers/defaults/configuration.toml format=TOML name=kata-runtime pid=1359217> Jul 27 10:35:16 openshift-worker-2.example.com kata-runtime[1359217]: time=\"2020-07-27T10:35:16.137979805Z\" level=info msg=\"vsock supported\" arch=amd64 command=list name=kata-runtime pid=1359217 source=katautils Jul 27 10:35:16 openshift-worker-2.example.com kata-runtime[1359217]: time=\"2020-07-27T10:35:16.138022584Z\" level=info msg=\"VSOCK supported, configure to not use proxy\" arch=amd64 command=list name=kata-runtime pid=1359217 source=katautils Jul 27 10:35:16 openshift-worker-2.example.com kata-runtime[1359217]: time=\"2020-07-27T10:35:16.138057713Z\" level=info arch=amd64 arguments=\"\\\"list\\\"\" command=list commit= name=kata-runtime pid=1359217 source=runtime version=1.11.1 Jul 27 10:35:16 openshift-worker-2.example.com kata-runtime[1359217]: time=\"2020-07-27T10:35:16.138166603Z\" level=info msg=\"fetch sandbox\" arch=amd64 command=list name=kata-runtime pid=1359217 source=virtcontainers Jul 27 10:35:16 openshift-worker-2.example.com kata-runtime[1359217]: time=\"2020-07-27T10:35:16.138244704Z\" level=warning msg=\"failed to get sandbox config from old store: open /var/lib/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/config.json: > Jul 27 10:35:20 openshift-worker-2.example.com kata-runtime[1359329]: time=\"2020-07-27T10:35:20.836297534Z\" level=info msg=\"loaded configuration\" arch=amd64 command=state file=/usr/share/kata-containers/defaults/configuration.toml format=TOML name=kata-runtime pid=135932> Jul 27 10:35:20 openshift-worker-2.example.com kata-runtime[1359329]: time=\"2020-07-27T10:35:20.836493897Z\" level=info msg=\"vsock supported\" arch=amd64 command=state name=kata-runtime pid=1359329 source=katautils [root@openshift-worker-2 ~]# journalctl -t kata -- Logs begin at Thu 2020-07-23 14:27:02 UTC, end at Mon 2020-07-27 10:47:59 UTC. -- Jul 26 16:03:42 openshift-worker-2.example.com kata[27183]: time=\"2020-07-26T16:03:42.108410531Z\" level=error msg=\"post event\" error=\"failed to publish event: exit status 1\" Jul 26 16:03:42 openshift-worker-2.example.com kata[27183]: time=\"2020-07-26T16:03:42.172077874Z\" level=error msg=\"post event\" error=\"failed to publish event: exit status 1\" Jul 26 16:03:46 openshift-worker-2.example.com kata[27183]: time=\"2020-07-26T16:03:46.380966347Z\" level=error msg=\"post event\" error=\"failed to publish event: exit status 1\" Jul 26 16:03:46 openshift-worker-2.example.com kata[27183]: time=\"2020-07-26T16:03:46.444520024Z\" level=error msg=\"post event\" error=\"failed to publish event: exit status 1\" Jul 26 16:03:53 openshift-worker-2.example.com kata[27183]: time=\"2020-07-26T16:03:53.458482832Z\" level=warning msg=\"Could not remove container share dir\" ID=f3bfda981bbafd53ed8f8e1d29a52e9297db0532ba3c564449b43fe79ec31506 error=\"no such file or directory\" sandbox=f3bfda> Jul 26 16:03:53 openshift-worker-2.example.com kata[27183]: time=\"2020-07-26T16:03:53.493291963Z\" level=warning msg=\"failed to cleanup rootfs mount\" error=\"no such file or directory\" Jul 26 16:03:53 openshift-worker-2.example.com kata[27183]: time=\"2020-07-26T16:03:53.525282501Z\" level=error msg=\"post event\" error=\"failed to publish event: exit status 1\" Jul 26 16:03:53 openshift-worker-2.example.com kata[27183]: time=\"2020-07-26T16:03:53.597489614Z\" level=warning msg=\"Could not remove container share dir\" ID=f3bfda981bbafd53ed8f8e1d29a52e9297db0532ba3c564449b43fe79ec31506 error=\"no such file or directory\" sandbox=f3bfda> Jul 26 16:03:53 openshift-worker-2.example.com kata[27183]: time=\"2020-07-26T16:03:53.599932291Z\" level=error msg=\"post event\" error=\"failed to publish event: exit status 1\" Jul 26 16:03:53 openshift-worker-2.example.com kata[27183]: time=\"2020-07-26T16:03:53.631820928Z\" level=error msg=\"Could not read qemu pid file\" ID=f3bfda981bbafd53ed8f8e1d29a52e9297db0532ba3c564449b43fe79ec31506 error=\"open /run/vc/vm/f3bfda981bbafd53ed8f8e1d29a52e9297d> Jul 26 16:03:53 openshift-worker-2.example.com kata[27183]: time=\"2020-07-26T16:03:53.632319378Z\" level=error msg=\"Could not read qemu pid file\" ID=f3bfda981bbafd53ed8f8e1d29a52e9297db0532ba3c564449b43fe79ec31506 error=\"open /run/vc/vm/f3bfda981bbafd53ed8f8e1d29a52e9297d> Jul 26 16:03:53 openshift-worker-2.example.com kata[27183]: time=\"2020-07-26T16:03:53.735993003Z\" level=error msg=\"post event\" error=\"failed to publish event: exit status 1\" Jul 26 16:04:25 openshift-worker-2.example.com kata[28433]: time=\"2020-07-26T16:04:25.25620997Z\" level=error msg=\"post event\" error=\"failed to publish event: exit status 1\" Jul 26 16:04:25 openshift-worker-2.example.com kata[28433]: time=\"2020-07-26T16:04:25.324699487Z\" level=error msg=\"post event\" error=\"failed to publish event: exit status 1\" Jul 26 16:04:25 openshift-worker-2.example.com kata[28433]: time=\"2020-07-26T16:04:25.532722656Z\" level=error msg=\"post event\" error=\"failed to publish event: exit status 1\" Jul 26 16:04:25 openshift-worker-2.example.com kata[28433]: time=\"2020-07-26T16:04:25.59569864Z\" level=error msg=\"post event\" error=\"failed to publish event: exit status 1\" Jul 26 16:04:37 openshift-worker-2.example.com kata[27183]: time=\"2020-07-26T16:04:37.109804951Z\" level=warning msg=\"failed to cleanup rootfs mount\" error=\"no such file or directory\" Jul 27 09:22:48 openshift-worker-2.example.com kata[28433]: time=\"2020-07-27T09:22:48.313239015Z\" level=error msg=\"post event\" error=\"failed to publish event: exit status 1\" Jul 27 09:22:48 openshift-worker-2.example.com kata[28433]: time=\"2020-07-27T09:22:48.461956579Z\" level=error msg=\"post event\" error=\"failed to publish event: exit status 1\" Jul 27 09:23:05 openshift-worker-2.example.com kata[28433]: time=\"2020-07-27T09:23:05.940817814Z\" level=error msg=\"post event\" error=\"failed to publish event: exit status 1\" [root@openshift-worker-2 ~]# How does kata containers plug the VM networking The namespace looks like this: sh-4.4# ip netns exec 317575d4-1143-4aab-b896-5767221326ec ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 3: eth0@if18: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1400 qdisc noqueue state UP group default qlen 1000 link/ether d6:96:23:1b:02:05 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.27.2.4/23 brd 172.27.3.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::bce2:c3ff:feca:293b/64 scope link valid_lft forever preferred_lft forever 4: tap0_kata: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1400 qdisc mq state UNKNOWN group default qlen 1000 link/ether 9a:b6:22:f7:1c:26 brd ff:ff:ff:ff:ff:ff inet6 fe80::98b6:22ff:fef7:1c26/64 scope link valid_lft forever preferred_lft forever And the VM's tap configuration looks like this, using ifindex 4 within the namespace: -netdev tap,id=network-0,vhost=on,vhostfds=4,fds=5 All traffic that arrives on tap0_kata will be mirrored to eth0 and vise versa. In the below case, the VM sends TCP SYNs. These SYNs enter on tap0_kata and are mirrored to eth0: sh-4.4# ip netns exec 317575d4-1143-4aab-b896-5767221326ec tcpdump -nne -i tap0_kata tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on tap0_kata, link-type EN10MB (Ethernet), capture size 262144 bytes 16:49:30.018482 d6:96:23:1b:02:05 > 0a:58:ac:1b:02:01, ethertype IPv4 (0x0800), length 74: 172.27.2.4.44268 > 172.30.0.10.80: Flags [S], seq 2995123630, win 27200, options [mss 1360,sackOK,TS val 3516836732 ecr 0,nop,wscale 7], length 0 16:49:31.038346 d6:96:23:1b:02:05 > 0a:58:ac:1b:02:01, ethertype IPv4 (0x0800), length 74: 172.27.2.4.44270 > 172.30.0.10.80: Flags [S], seq 1381681461, win 27200, options [mss 1360,sackOK,TS val 3516837752 ecr 0,nop,wscale 7], length 0 ^C 2 packets captured 2 packets received by filter 0 packets dropped by kernel sh-4.4# ip netns exec 317575d4-1143-4aab-b896-5767221326ec tcpdump -nne -i eth0 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 16:49:40.197817 d6:96:23:1b:02:05 > 0a:58:ac:1b:02:01, ethertype IPv4 (0x0800), length 74: 172.27.2.4.44288 > 172.30.0.10.80: Flags [S], seq 712892146, win 27200, options [mss 1360,sackOK,TS val 3516846912 ecr 0,nop,wscale 7], length 0 16:49:41.215135 d6:96:23:1b:02:05 > 0a:58:ac:1b:02:01, ethertype IPv4 (0x0800), length 74: 172.27.2.4.44290 > 172.30.0.10.80: Flags [S], seq 3944941969, win 27200, options [mss 1360,sackOK,TS val 3516847929 ecr 0,nop,wscale 7], length 0 ^C 2 packets captured 2 packets received by filter 0 packets dropped by kernel sh-4.4# The kata documentation explains how this works: https://github.com/kata-containers/documentation/blob/master/design/architecture.md#networking Containers will typically live in their own, possibly shared, networking namespace. At some point in a container lifecycle, container engines will set up that namespace to add the container to a network which is isolated from the host network, but which is shared between containers In order to do so, container engines will usually add one end of a virtual ethernet (veth) pair into the container networking namespace. The other end of the veth pair is added to the host networking namespace. This is a very namespace-centric approach as many hypervisors (in particular QEMU) cannot handle veth interfaces. Typically, TAP interfaces are created for VM connectivity. To overcome incompatibility between typical container engines expectations and virtual machines, kata-runtime networking transparently connects veth interfaces with TAP ones using MACVTAP: See the picture in the documentation for further details. eth0 and tap_kata0 are transparently connected via tc rules: # ingress qdisc configured for both interfaces sh-4.4# ip netns exec 317575d4-1143-4aab-b896-5767221326ec tc qdisc ls dev eth0 qdisc noqueue 0: root refcnt 2 qdisc ingress ffff: parent ffff:fff1 ---------------- sh-4.4# ip netns exec 317575d4-1143-4aab-b896-5767221326ec tc qdisc ls dev tap0_kata qdisc mq 0: root qdisc fq_codel 0: parent :1 limit 10240p flows 1024 quantum 1414 target 5.0ms interval 100.0ms memory_limit 32Mb ecn qdisc ingress ffff: parent ffff:fff1 ---------------- # filter configured for both interfaces sh-4.4# ip netns exec 317575d4-1143-4aab-b896-5767221326ec tc filter ls dev eth0 ingress filter protocol all pref 49152 u32 chain 0 filter protocol all pref 49152 u32 chain 0 fh 800: ht divisor 1 filter protocol all pref 49152 u32 chain 0 fh 800::800 order 2048 key ht 800 bkt 0 terminal flowid ??? not_in_hw match 00000000/00000000 at 0 action order 1: mirred (Egress Redirect to device tap0_kata) stolen index 1 ref 1 bind 1 sh-4.4# ip netns exec 317575d4-1143-4aab-b896-5767221326ec tc filter ls dev tap0_kata ingress filter protocol all pref 49152 u32 chain 0 filter protocol all pref 49152 u32 chain 0 fh 800: ht divisor 1 filter protocol all pref 49152 u32 chain 0 fh 800::800 order 2048 key ht 800 bkt 0 terminal flowid ??? not_in_hw match 00000000/00000000 at 0 action order 1: mirred (Egress Redirect to device eth0) stolen index 2 ref 1 bind 1 Further details about this in: * https://gist.github.com/mcastelino/7d85f4164ffdaf48242f9281bb1d0f9b * https://man7.org/linux/man-pages/man8/tc-mirred.8.html Selecting the appropriate runtime class per container Kubernetes allows users to choose the appropriate runtimeClass for their pod: https://kubernetes.io/docs/concepts/containers/runtime-class/ [root@openshift-jumpserver-0 ~]# oc explain runtimeclass KIND: RuntimeClass VERSION: node.k8s.io/v1beta1 DESCRIPTION: RuntimeClass defines a class of container runtime supported in the cluster. The RuntimeClass is used to determine which container runtime is used to run all containers in a pod. RuntimeClasses are (currently) manually defined by a user or cluster provisioner, and referenced in the PodSpec. The Kubelet is responsible for resolving the RuntimeClassName reference before running the pod. For more details, see https://git.k8s.io/enhancements/keps/sig-node/runtime-class.md FIELDS: apiVersion <string> APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources handler <string> -required- Handler specifies the underlying runtime and configuration that the CRI implementation will use to handle pods of this class. The possible values are specific to the node & CRI configuration. It is assumed that all handlers are available on every node, and handlers of the same name are equivalent on every node. For example, a handler called \"runc\" might specify that the runc OCI runtime (using native Linux containers) will be used to run the containers in a pod. The Handler must conform to the DNS Label (RFC 1123) requirements, and is immutable. kind <string> Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds metadata <Object> More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata overhead <Object> Overhead represents the resource overhead associated with running a pod for a given RuntimeClass. For more details, see https://git.k8s.io/enhancements/keps/sig-node/20190226-pod-overhead.md This field is alpha-level as of Kubernetes v1.15, and is only honored by servers that enable the PodOverhead feature. scheduling <Object> Scheduling holds the scheduling constraints to ensure that pods running with this RuntimeClass are scheduled to nodes that support it. If scheduling is nil, this RuntimeClass is assumed to be supported by all nodes. The runtime class definition looks like this: [root@openshift-jumpserver-0 ~]# oc get runtimeclass kata-oc -o yaml apiVersion: node.k8s.io/v1beta1 handler: kata-oc kind: RuntimeClass metadata: creationTimestamp: \"2020-07-26T15:48:38Z\" managedFields: - apiVersion: node.k8s.io/v1beta1 fieldsType: FieldsV1 fieldsV1: f:handler: {} f:metadata: f:ownerReferences: .: {} k:{\"uid\":\"e8e76d58-2615-4787-9d4c-7b89dd275131\"}: .: {} f:apiVersion: {} f:blockOwnerDeletion: {} f:controller: {} f:kind: {} f:name: {} f:uid: {} f:scheduling: .: {} f:nodeSelector: .: {} f:kata-containers: {} manager: kata-operator operation: Update time: \"2020-07-26T15:48:38Z\" name: kata-oc ownerReferences: - apiVersion: kataconfiguration.openshift.io/v1alpha1 blockOwnerDeletion: true controller: true kind: KataConfig name: example-kataconfig uid: e8e76d58-2615-4787-9d4c-7b89dd275131 resourceVersion: \"3111152\" selfLink: /apis/node.k8s.io/v1beta1/runtimeclasses/kata-oc uid: bd0fea1a-2d41-43aa-90ba-c9804aabaaa7 scheduling: nodeSelector: kata-containers: \"\" And the scheduling nodeSelector makes sure that kata pods will only be spawned on correctly configured workers: [root@openshift-jumpserver-0 ~]# oc get nodes -l kata-containers= NAME STATUS ROLES AGE VERSION openshift-worker-2.example.com Ready worker 4d19h v1.18.3+b74c5ed On these worker nodes, the kata operator configures the kata-oc handler in crio with: [root@openshift-worker-2 ~]# cat /etc/crio/crio.conf.d/kata-50.conf [crio.runtime] manage_ns_lifecycle = true [crio.runtime.runtimes.kata-oc] runtime_path = \"/usr/bin/containerd-shim-kata-v2\" runtime_type = \"vm\" runtime_root = \"/run/vc\" [crio.runtime.runtimes.runc] runtime_path = \"\" runtime_type = \"oci\" runtime_root = \"/run/runc\" And default runtime: [root@openshift-worker-2 ~]# grep default_runtime /etc/crio/ -R /etc/crio/crio.conf:# default_runtime is the _name_ of the OCI runtime to be used as the default. /etc/crio/crio.conf:default_runtime = \"runc\" [root@openshift-worker-2 ~]# Check the crio implementation for further details, e.g.: https://github.com/cri-o/cri-o/blob/d23a830d170ae177c922fe63611f8d1d3772ef8a/server/sandbox_run_linux.go#L494 // If using kata runtime, the process label should be set to container_kvm_t // Keep in mind that kata does *not* apply any process label to containers within the VM // Note: the requirement here is that the name used for the runtime class has \"kata\" in it // or the runtime_type is set to \"vm\" if runtimeType == libconfig.RuntimeTypeVM || strings.Contains(strings.ToLower(runtimeHandler), \"kata\") { processLabel, err = selinux.SELinuxKVMLabel(processLabel) if err != nil { return nil, err } g.SetProcessSelinuxLabel(processLabel) } For an explanation of the fields, see: https://github.com/cri-o/cri-o/blob/master/docs/crio.conf.5.md#crioruntime-table CRIO.RUNTIME.RUNTIMES TABLE The \"crio.runtime.runtimes\" table defines a list of OCI compatible runtimes. The runtime to use is picked based on the runtime_handler provided by the CRI. If no runtime_handler is provided, the runtime will be picked based on the level of trust of the workload. runtime_path=\"\" Path to the OCI compatible runtime used for this runtime handler. runtime_root=\"\" Root directory used to store runtime data runtime_type=\"oci\" Type of the runtime used for this runtime handler. \"oci\", \"vm\" The runtime class in a pod is then selected with runtimeClassName : [root@openshift-jumpserver-0 ~]# cat katapod.yaml apiVersion: v1 kind: Pod metadata: name: katapod spec: containers: - name: sample-container image: fedora:latest imagePullPolicy: IfNotPresent command: [\"sleep\", \"infinity\"] runtimeClassName: kata-oc crio - debugging kata container launch Change log level to debug: [root@openshift-worker-2 ~]# grep log_level /etc/crio/ -R /etc/crio/crio.conf:log_level = \"info\" /etc/crio/crio.conf.d/00-default:log_level = \"debug\" <----------------------- here [root@openshift-worker-2 ~]# systemctl restart crio Then, spawn a new pod, named katapod. The logs will look like this: [root@openshift-worker-2 ~]# journalctl -u crio --since \"7 minutes ago\" | grep kata | cut -b 1-250 Jul 28 10:09:11 openshift-worker-2.example.com crio[2156]: time=\"2020-07-28T10:09:11.335500711Z\" level=warning msg=\"Could not remove container share dir\" ID=dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103 error=\"no such file or direc Jul 28 10:09:11 openshift-worker-2.example.com crio[2156]: time=\"2020-07-28 10:09:11.360578238Z\" level=info msg=\"Stopped container 6b41cdf410d12e75f99c37c7cadc4c968b1061d388db379aecacf381f46af56a: katatest/katapod/sample-container\" id=2a25e783-dc3f-4 Jul 28 10:09:11 openshift-worker-2.example.com crio[2156]: time=\"2020-07-28 10:09:11.361758542Z\" level=info msg=\"Got pod network &{Name:katapod Namespace:katatest ID:dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103 NetNS:/var/run/netn Jul 28 10:09:11 openshift-worker-2.example.com crio[2156]: 2020-07-28T10:09:11Z [verbose] Del: katatest:katapod:a7633f9b-5ce7-4a97-9137-206ce44b58ac:ovn-kubernetes:eth0 {\"cniVersion\":\"0.4.0\",\"dns\":{},\"ipam\":{},\"logFile\":\"/var/log/ovn-kubernetes/ovn-k Jul 28 10:09:11 openshift-worker-2.example.com crio[2156]: time=\"2020-07-28T10:09:11.477479323Z\" level=warning msg=\"Could not remove container share dir\" ID=dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103 error=\"no such file or direc Jul 28 10:09:11 openshift-worker-2.example.com crio[2156]: time=\"2020-07-28 10:09:11.808648343Z\" level=info msg=\"Removed container 6b41cdf410d12e75f99c37c7cadc4c968b1061d388db379aecacf381f46af56a: katatest/katapod/sample-container\" id=573efd66-576b-4 Jul 28 10:10:03 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:03.191724317Z\" level=debug msg=\"Found valid runtime \\\"kata-oc\\\" for runtime_path \\\"/usr/bin/containerd-shim-kata-v2\\\"\" file=\"config/config.go:940\" Jul 28 10:10:03 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:03.225896842Z\" level=warning msg=\"Unable to delete container k8s_katapod_katatest_a7633f9b-5ce7-4a97-9137-206ce44b58ac_0: identifier is not a container\" file=\"server Jul 28 10:10:04 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:04.343526624Z\" level=debug msg=\"Response: &ListImagesResponse{Images:[]*Image{&Image{Id:aa9557bde2f3e1699a119eea4fe53bfef7232628a8c03597816b58a77cd47297,RepoTags:[qu Jul 28 10:10:14 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:14.380904505Z\" level=debug msg=\"Response: &ListImagesResponse{Images:[]*Image{&Image{Id:aa9557bde2f3e1699a119eea4fe53bfef7232628a8c03597816b58a77cd47297,RepoTags:[qu Jul 28 10:10:16 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:16.654826038Z\" level=debug msg=\"Response: &ListImagesResponse{Images:[]*Image{&Image{Id:aa9557bde2f3e1699a119eea4fe53bfef7232628a8c03597816b58a77cd47297,RepoTags:[qu Jul 28 10:10:20 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:20.903822858Z\" level=debug msg=\"Request: &RunPodSandboxRequest{Config:&PodSandboxConfig{Metadata:&PodSandboxMetadata{Name:katapod,Uid:fdc30295-d339-448c-9019-5a7cfc9 Jul 28 10:10:20 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:20.903981027Z\" level=info msg=\"Running pod sandbox: katatest/katapod/POD\" file=\"server/sandbox_run_linux.go:55\" id=06c123b2-8741-42d2-9e49-4a89bd681d1a name=/runtime Jul 28 10:10:20 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:20.917167736Z\" level=info msg=\"Got pod network &{Name:katapod Namespace:katatest ID:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 NetNS:/var/run/n Jul 28 10:10:22 openshift-worker-2.example.com crio[3067623]: 2020-07-28T10:10:22Z [verbose] Add: katatest:katapod:fdc30295-d339-448c-9019-5a7cfc9585c2:(ovn-kubernetes):eth0 {\"cniVersion\":\"0.4.0\",\"interfaces\":[{\"name\":\"9d8a2e790b7836e\",\"mac\":\"ea:0a:b Jul 28 10:10:22 openshift-worker-2.example.com crio[3067623]: I0728 10:10:22.448390 3068254 event.go:221] Event(v1.ObjectReference{Kind:\"Pod\", Namespace:\"katatest\", Name:\"katapod\", UID:\"fdc30295-d339-448c-9019-5a7cfc9585c2\", APIVersion:\"v1\", Resource Jul 28 10:10:22 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:22.462209769Z\" level=info msg=\"Got pod network &{Name:katapod Namespace:katatest ID:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 NetNS:/var/run/n Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.530538309Z\" level=info msg=\"loaded configuration\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 file=/usr/share/kata-containers/defaults/con Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.530688992Z\" level=info msg=\"vsock supported\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 source=katautils Jul 28 10:10:22 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28T10:10:22.530538309Z\" level=info msg=\"loaded configuration\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 file=/usr/share/kata-containers/defaults/con Jul 28 10:10:22 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28T10:10:22.530688992Z\" level=info msg=\"vsock supported\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 source=katautils Jul 28 10:10:22 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28T10:10:22.530725036Z\" level=info msg=\"VSOCK supported, configure to not use proxy\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 source=katautils Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.530725036Z\" level=info msg=\"VSOCK supported, configure to not use proxy\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 source=katautils Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.530785191Z\" level=info msg=\"shm-size detected: 67108864\" source=virtcontainers subsystem=oci Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.533182605Z\" level=info msg=\"adding volume\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 source=virtcontainers subsystem=qemu volume-type=vi Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.534052168Z\" level=info msg=\"Endpoints found after scan\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 endpoints=\"[0xc000592780]\" source=virt Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.534183103Z\" level=info msg=\"Attaching endpoint\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 endpoint-type=virtual hotplug=false source=vir Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.576732075Z\" level=info msg=\"Starting VM\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 sandbox=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92 Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.577702797Z\" level=info msg=\"Adding extra file [0xc0000109f8 0xc000010ad0 0xc000010ab8]\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 source Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.577783973Z\" level=info msg=\"launching /usr/libexec/qemu-kvm with: [-name sandbox-9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 -uuid 59a1fe1e-8 Jul 28 10:10:22 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28T10:10:22.577783973Z\" level=info msg=\"launching /usr/libexec/qemu-kvm with: [-name sandbox-9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 -uuid 59a1fe1e-8 Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.662797076Z\" level=info msg=\"{\\\"QMP\\\": {\\\"version\\\": {\\\"qemu\\\": {\\\"micro\\\": 0, \\\"minor\\\": 2, \\\"major\\\": 4}, \\\"package\\\": \\\"qemu-kvm-4.2.0-19.el8\\\"}, \\\"capabilities Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.663082377Z\" level=info msg=\"QMP details\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 qmp-capabilities=oob qmp-major-version=4 qmp-micro-ve Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.663188824Z\" level=info msg=\"{\\\"execute\\\":\\\"qmp_capabilities\\\"}\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 source=virtcontainers subsyste Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.669410267Z\" level=info msg=\"{\\\"return\\\": {}}\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 source=virtcontainers subsystem=qmp Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.669553509Z\" level=info msg=\"sanner return error: read unix @->/run/vc/vm/9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10/qmp.sock: use of closed Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.669631223Z\" level=info msg=\"VM started\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 sandbox=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.669728645Z\" level=info msg=\"proxy started\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 proxy-pid=3068393 proxy-url=\"vsock://3022826186:102 Jul 28 10:10:22 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28T10:10:22.669728645Z\" level=info msg=\"proxy started\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 proxy-pid=3068393 proxy-url=\"vsock://3022826186:102 Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.669812928Z\" level=info msg=\"New client\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 proxy=3068393 source=virtcontainers subsystem=kata_age Jul 28 10:10:22 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28T10:10:22.669812928Z\" level=info msg=\"New client\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 proxy=3068393 source=virtcontainers subsystem=kata_age Jul 28 10:10:24 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:24.400551122Z\" level=debug msg=\"Response: &ListImagesResponse{Images:[]*Image{&Image{Id:aa9557bde2f3e1699a119eea4fe53bfef7232628a8c03597816b58a77cd47297,RepoTags:[qu Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28T10:10:25.048437517Z\" level=info msg=\"Using sandbox shm\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 shm-size=67108864 source=virtcontainers subsyst Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28T10:10:25.048700740Z\" level=info msg=\"SELinux label from config will be applied to the hypervisor process, not the VM workload\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb0125 Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:25.343910261Z\" level=info msg=\"Ran pod sandbox 9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 with infra container: katatest/katapod/POD\" file=\"ser Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:25.344647979Z\" level=debug msg=\"Response: &PodSandboxStatusResponse{Status:&PodSandboxStatus{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10,Meta Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:25.349933952Z\" level=debug msg=\"Request: &CreateContainerRequest{PodSandboxId:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10,Config:&ContainerConfi Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:25.350113427Z\" level=info msg=\"Creating container: katatest/katapod/sample-container\" file=\"server/container_create.go:524\" id=ea52ea9c-e9ca-4d9e-8002-da3d3166fe7c n Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:25.362037581Z\" level=debug msg=\"Setting container's log_path = /var/log/pods/katatest_katapod_fdc30295-d339-448c-9019-5a7cfc9585c2, sbox.logdir = sample-container/0. Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28T10:10:25.402397904Z\" level=info msg=\"Using sandbox shm\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 shm-size=67108864 source=virtcontainers subsyst Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28T10:10:25.402627956Z\" level=info msg=\"SELinux label from config will be applied to the hypervisor process, not the VM workload\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb0125 Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:25.501604493Z\" level=info msg=\"Created container 8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6cc4: katatest/katapod/sample-container\" file=\"server/co Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:25.507785167Z\" level=info msg=\"Started container 8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6cc4: katatest/katapod/sample-container\" file=\"server/co Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:25.883031245Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:25.884143520Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:25.885476950Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:25.885897712Z\" level=debug msg=\"Response: &PodSandboxStatusResponse{Status:&PodSandboxStatus{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10,Meta Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:25.886321570Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:25.886665870Z\" level=debug msg=\"Response: &ContainerStatusResponse{Status:&ContainerStatus{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6cc4,Metada Jul 28 10:10:26 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:26.887642507Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:26 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:26.889329001Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:27 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:27.198982924Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:27 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:27.200308230Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:27 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:27.201527681Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:27 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:27.202571813Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:27 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:27.891804708Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:27 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:27.892945930Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:28 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:28.894628148Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:28 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:28.895748056Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:29 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:29.197921214Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:29 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:29.198882077Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:29 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:29.897301640Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:29 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:29.898454054Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:30 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:30.901632289Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:30 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:30.902863561Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:31 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:31.198043649Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:31 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:31.199211238Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:31 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:31.200295569Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:31 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:31.201321049Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:31 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:31.904501224Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:31 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:31.905741478Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:32 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:32.907397641Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:32 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:32.908636589Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:33 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:33.197869270Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:33 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:33.198924362Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:33 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:33.910225026Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:33 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:33.911466751Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:34 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:34.437957994Z\" level=debug msg=\"Response: &ListImagesResponse{Images:[]*Image{&Image{Id:aa9557bde2f3e1699a119eea4fe53bfef7232628a8c03597816b58a77cd47297,RepoTags:[qu Jul 28 10:10:34 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:34.913063858Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:34 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:34.914176789Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:35 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:35.197818930Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:35 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:35.198847621Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:35 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:35.199968036Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:35 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:35.200945896Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:35 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:35.491404744Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:35 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:35.492513493Z\" level=debug msg=\"Response: &ContainerStatusResponse{Status:&ContainerStatus{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6cc4,Metada Jul 28 10:10:35 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:35.915801132Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:35 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:35.918835240Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:36 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:36.920490279Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:36 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:36.921919929Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:37 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:37.197865644Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:37 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:37.198828183Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:37 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:37.199978674Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:37 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:37.201046190Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:37 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:37.923587550Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:37 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:37.924768896Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:38 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:38.926437831Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:38 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:38.927535330Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:39 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:39.197561502Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:39 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:39.198589011Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:39 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:39.929242132Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:39 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:39.930624954Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:40 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:40.932460677Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:40 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:40.933685014Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:41 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:41.198117596Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:41 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:41.199189319Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:41 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:41.200312331Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:41 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:41.201397260Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:41 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:41.935267335Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:41 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:41.936533454Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:42 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:42.938964939Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:42 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:42.940206741Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:43 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:43.198051418Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:43 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:43.199127960Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:43 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:43.941875134Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:43 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:43.943014454Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:44 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:44.477098929Z\" level=debug msg=\"Response: &ListImagesResponse{Images:[]*Image{&Image{Id:aa9557bde2f3e1699a119eea4fe53bfef7232628a8c03597816b58a77cd47297,RepoTags:[qu Jul 28 10:10:44 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:44.944623020Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:44 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:44.945858418Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:45 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:45.198112719Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:45 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:45.199403987Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:45 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:45.201342432Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:45 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:45.202303876Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:45 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:45.497788583Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:45 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:45.498908989Z\" level=debug msg=\"Response: &ContainerStatusResponse{Status:&ContainerStatus{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6cc4,Metada Jul 28 10:10:45 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:45.543711612Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:45 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:45.545123859Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:45 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:45.546270304Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:45 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:45.947309093Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:45 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:45.948492558Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:46 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:46.659075373Z\" level=debug msg=\"Response: &ListImagesResponse{Images:[]*Image{&Image{Id:aa9557bde2f3e1699a119eea4fe53bfef7232628a8c03597816b58a77cd47297,RepoTags:[qu Jul 28 10:10:46 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:46.950714682Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:46 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:46.951824792Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:47 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:47.198041957Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:47 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:47.198993669Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:47 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:47.953643279Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:47 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:47.954838431Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:48 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:48.956694271Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:48 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:48.957802676Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:49 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:49.200294004Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:49 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:49.201241261Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:49 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:49.202321480Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:49 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:49.203692827Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:49 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:49.959650132Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:49 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:49.960738848Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:50 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:50.962629963Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:50 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:50.964061786Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:51 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:51.198210094Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:51 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:51.199165518Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:51 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:51.965826824Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:51 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:51.967090960Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:52 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:52.968922540Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:52 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:52.970185474Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:53 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:53.197881649Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:53 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:53.199482168Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:53 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:53.201311069Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:53 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:53.202378219Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:53 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:53.971897478Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:53 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:53.973242764Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:54 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:54.491767193Z\" level=debug msg=\"Response: &ListImagesResponse{Images:[]*Image{&Image{Id:aa9557bde2f3e1699a119eea4fe53bfef7232628a8c03597816b58a77cd47297,RepoTags:[qu Jul 28 10:10:55 openshift-worker-2.example.com crio[3069288]: time=\"2020-07-28 10:10:55.236031546Z\" level=warning msg=\"Unable to delete container k8s_POD_katapod_katatest_fdc30295-d339-448c-9019-5a7cfc9585c2_0: 1 error occurred:\\n\\t* unlinkat /var/ru Jul 28 10:10:55 openshift-worker-2.example.com crio[3069288]: time=\"2020-07-28 10:10:55.236122983Z\" level=warning msg=\"Unable to delete container k8s_katapod_katatest_fdc30295-d339-448c-9019-5a7cfc9585c2_0: identifier is not a container\" Jul 28 10:10:55 openshift-worker-2.example.com crio[3069288]: time=\"2020-07-28 10:10:55.350843107Z\" level=warning msg=\"Unable to delete container k8s_sample-container_katapod_katatest_fdc30295-d339-448c-9019-5a7cfc9585c2_0: identifier is not a contai Jul 28 10:10:55 openshift-worker-2.example.com crio[3069288]: time=\"2020-07-28 10:10:55.977080875Z\" level=info msg=\"Running pod sandbox: katatest/katapod/POD\" id=00ec3ec8-8a02-4626-a4be-3eee173a95ed name=/runtime.v1alpha2.RuntimeService/RunPodSandbox Jul 28 10:10:55 openshift-worker-2.example.com crio[3069288]: time=\"2020-07-28 10:10:55.989280275Z\" level=info msg=\"Got pod network &{Name:katapod Namespace:katatest ID:a9765d6aa6a1ceb776f4726aab1ce901cbd4d10c2c41c008fe2d4579e36b4c3b NetNS:/var/run/n Jul 28 10:10:58 openshift-worker-2.example.com crio[3069288]: 2020-07-28T10:10:58Z [verbose] Add: katatest:katapod:fdc30295-d339-448c-9019-5a7cfc9585c2:(ovn-kubernetes):eth0 {\"cniVersion\":\"0.4.0\",\"interfaces\":[{\"name\":\"a9765d6aa6a1ceb\",\"mac\":\"72:e1:9 Jul 28 10:10:58 openshift-worker-2.example.com crio[3069288]: I0728 10:10:58.038289 3069588 event.go:221] Event(v1.ObjectReference{Kind:\"Pod\", Namespace:\"katatest\", Name:\"katapod\", UID:\"fdc30295-d339-448c-9019-5a7cfc9585c2\", APIVersion:\"v1\", Resource Jul 28 10:10:58 openshift-worker-2.example.com crio[3069288]: time=\"2020-07-28 10:10:58.051337687Z\" level=info msg=\"Got pod network &{Name:katapod Namespace:katatest ID:a9765d6aa6a1ceb776f4726aab1ce901cbd4d10c2c41c008fe2d4579e36b4c3b NetNS:/var/run/n Jul 28 10:11:00 openshift-worker-2.example.com crio[3069288]: time=\"2020-07-28 10:11:00.933784352Z\" level=info msg=\"Ran pod sandbox a9765d6aa6a1ceb776f4726aab1ce901cbd4d10c2c41c008fe2d4579e36b4c3b with infra container: katatest/katapod/POD\" id=00ec3e Jul 28 10:11:00 openshift-worker-2.example.com crio[3069288]: time=\"2020-07-28 10:11:00.938623997Z\" level=info msg=\"Creating container: katatest/katapod/sample-container\" id=25867b9e-a543-42e7-ba98-fb38ddd938db name=/runtime.v1alpha2.RuntimeService/C Jul 28 10:11:01 openshift-worker-2.example.com crio[3069288]: time=\"2020-07-28 10:11:01.091184087Z\" level=info msg=\"Created container ed470742207b8df91b5b194d07c0f0db2746470accdeeba28d90d37cfc3a8447: katatest/katapod/sample-container\" id=25867b9e-a54 Jul 28 10:11:01 openshift-worker-2.example.com crio[3069288]: time=\"2020-07-28 10:11:01.096407977Z\" level=info msg=\"Started container ed470742207b8df91b5b194d07c0f0db2746470accdeeba28d90d37cfc3a8447: katatest/katapod/sample-container\" id=5ec1b483-d18 Disable debugging after the test as crio logs quite a lot in debug mode. Finding the kata-containers socket WIP [root@openshift-worker-2 /]# ps aux | grep crio | grep -v conmon root 2156 0.5 0.1 4854356 145820 ? Ssl Jul26 13:56 /usr/bin/crio --enable-metrics=true --metrics-port=9537 root 2210 5.5 0.1 5937816 168356 ? Ssl Jul26 139:27 kubelet --config=/etc/kubernetes/kubelet.conf --bootstrap-kubeconfig=/etc/kubernetes/kubeconfig --kubeconfig=/var/lib/kubelet/kubeconfig --container-runtime=remote --container-runtime-endpoint=/var/run/crio/crio.sock --runtime-cgroups=/system.slice/crio.service --node-labels=node-role.kubernetes.io/worker,node.openshift.io/os_id=rhcos --minimum-container-ttl-duration=6m0s --volume-plugin-dir=/etc/kubernetes/kubelet-plugins/volume/exec --cloud-provider= --pod-infra-container-image=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:636ba95d1f2717a876ab2cb1b09ac9b40739db00d26b5cdb9c3a7d6114b75494 --v=4 root 28433 0.0 0.0 878648 31028 ? Sl Jul26 2:14 /usr/bin/containerd-shim-kata-v2 -namespace default -address -publish-binary /usr/bin/crio -id dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103 root 3047886 0.0 0.0 16368 972 pts/0 S+ 09:53 0:00 grep --color=auto crio [root@openshift-worker-2 /]# lsof -nn -p 2156 | grep sock crio 2156 root 10u unix 0xffff92a9d4622400 0t0 52558 /var/run/crio/crio.sock type=STREAM crio 2156 root 22u unix 0xffff92a9d3f10480 0t0 35195 /var/run/crio/crio.sock type=STREAM crio 2156 root 23u unix 0xffff92a9d3f5e300 0t0 35197 /var/run/crio/crio.sock type=STREAM crio 2156 root 36u sock 0,9 0t0 11508128 protocol: TCPv6 crio 2156 root 40u sock 0,9 0t0 13352222 protocol: TCPv6 crio 2156 root 46u unix 0xffff9299d3cb4c80 0t0 19430168 /var/run/crio/crio.sock type=STREAM crio 2156 root 64u unix 0xffff92a9c2873180 0t0 19407526 /var/run/crio/crio.sock type=STREAM [root@openshift-worker-2 /]# ps aux | grep kata root 28433 0.0 0.0 878648 31028 ? Sl Jul26 2:14 /usr/bin/containerd-shim-kata-v2 -namespace default -address -publish-binary /usr/bin/crio -id dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103 root 28445 0.0 0.0 68424 5664 ? S Jul26 0:00 /usr/libexec/virtiofsd --fd=3 -o source=/run/kata-containers/shared/sandboxes/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/shared -o cache=always --syslog -o no_posix_lock -f root 28450 0.1 0.7 2599904 975108 ? Sl Jul26 4:35 /usr/libexec/qemu-kvm -name sandbox-dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103 -uuid 6f03e56e-efa1-4f0f-9b8f-492cf1824c83 -machine q35,accel=kvm,kernel_irqchip -cpu host -qmp unix:/run/vc/vm/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/qmp.sock,server,nowait -m 2048M,slots=10,maxmem=129853M -device pci-bridge,bus=pcie.0,id=pci-bridge-0,chassis_nr=1,shpc=on,addr=2,romfile= -device virtio-serial-pci,disable-modern=false,id=serial0,romfile= -device virtconsole,chardev=charconsole0,id=console0 -chardev socket,id=charconsole0,path=/run/vc/vm/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/console.sock,server,nowait -device virtio-scsi-pci,id=scsi0,disable-modern=false,romfile= -object rng-random,id=rng0,filename=/dev/urandom -device virtio-rng-pci,rng=rng0,romfile= -device vhost-vsock-pci,disable-modern=false,vhostfd=3,id=vsock-555233078,guest-cid=555233078,romfile= -chardev socket,id=char-1082a328d9d79959,path=/run/vc/vm/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/vhost-fs.sock -device vhost-user-fs-pci,chardev=char-1082a328d9d79959,tag=kataShared,romfile= -netdev tap,id=network-0,vhost=on,vhostfds=4,fds=5 -device driver=virtio-net-pci,netdev=network-0,mac=d6:96:23:1b:02:05,disable-modern=false,mq=on,vectors=4,romfile= -global kvm-pit.lost_tick_policy=discard -vga none -no-user-config -nodefaults -nographic -daemonize -object memory-backend-file,id=dimm1,size=2048M,mem-path=/dev/shm,share=on -numa node,memdev=dimm1 -kernel /usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/vmlinuz -initrd /var/cache/kata-containers/osbuilder-images/4.18.0-193.13.2.el8_2.x86_64/\"rhcos\"-kata-4.18.0-193.13.2.el8_2.x86_64.initrd -append tsc=reliable no_timer_check rcupdate.rcu_expedited=1 i8042.direct=1 i8042.dumbkbd=1 i8042.nopnp=1 i8042.noaux=1 noreplace-smp reboot=k console=hvc0 console=hvc1 iommu=off cryptomgr.notests net.ifnames=0 pci=lastbus=0 quiet panic=1 nr_cpus=40 agent.use_vsock=true scsi_mod.scan=none -pidfile /run/vc/vm/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/pid -smp 1,cores=1,threads=1,sockets=40,maxcpus=40 root 28454 0.0 0.2 4216312 349024 ? Sl Jul26 0:23 /usr/libexec/virtiofsd --fd=3 -o source=/run/kata-containers/shared/sandboxes/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/shared -o cache=always --syslog -o no_posix_lock -f root 3048279 0.0 0.0 16368 972 pts/0 S+ 09:54 0:00 grep --color=auto kata [root@openshift-worker-2 /]# lsof -nn -p 28433 | grep sock container 28433 root 3u sock 0,9 0t0 3589421 protocol: AF_VSOCK container 28433 root 7u unix 0xffff92a9cbf2de80 0t0 3038410 @/containerd-shim/default/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/shim.sock@ type=STREAM container 28433 root 8u unix 0xffff92a9cbf08480 0t0 3131447 @/containerd-shim/default/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/shim.sock@ type=STREAM container 28433 root 17u unix 0xffff9299e58c5a00 0t0 3535834 @/containerd-shim/default/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/shim.sock@ type=STREAM [root@openshift-worker-2 /]# Connecting a debug console to the kata VM https://github.com/kata-containers/documentation/blob/master/Developer-Guide.md#set-up-a-debug-console Resources Kata operator https://github.com/harche/kata-operator https://www.youtube.com/watch?v=k7rLIU3L94w Kata containers https://katacontainers.io/ https://github.com/kata-containers/kata-containers/tree/2.0-dev/tools/packaging/kata-deploy https://github.com/kata-containers/kata-containers/blob/2.0-dev/tools/packaging/kata-deploy/scripts/kata-deploy.sh https://github.com/kata-containers/osbuilder https://github.com/kata-containers/runtime https://github.com/kata-containers/documentation/blob/master/design/architecture.md https://github.com/kata-containers/documentation/blob/master/how-to/run-kata-with-k8s.md https://github.com/kata-containers/documentation/blob/master/design/virtualization.md https://katacontainers.io/learn/ kubernetes https://kubernetes.io/docs/concepts/containers/runtime-class/ Firecracker https://medium.com/@gokulchandrapr/kata-containers-on-kubernetes-and-kata-firecracker-vmm-support-28abb3a196e7 https://github.com/kata-containers/documentation/wiki/Initial-release-of-Kata-Containers-with-Firecracker-support","title":"kata containers and the kata operatora"},{"location":"openshift/kata/#kata-containers-and-the-kata-operator","text":"This post looks at kata containers on top of OpenShift as deployed with the kata-operator ( https://github.com/harche/kata-operator ).","title":"kata containers and the kata operator"},{"location":"openshift/kata/#kata-operator","text":"","title":"kata-operator"},{"location":"openshift/kata/#kata-operator-will-not-install","text":"At time of this writing, you will have to hack the worker node once the kata-operator and CR were installed and configured the worker: https://github.com/harche/kata-operator/issues/38","title":"kata-operator will not install"},{"location":"openshift/kata/#vm-configuration","text":"[root@openshift-worker-2 ~]# ps aux | grep qemu-kvm root 28450 0.1 0.2 2599904 351472 ? Sl Jul26 1:08 /usr/libexec/qemu-kvm -name sandbox-dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103 -uuid 6f03e56e-efa1-4f0f-9b8f-492cf1824c83 -machine q35,accel=kvm,kernel_irqchip -cpu host -qmp unix:/run/vc/vm/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/qmp.sock,server,nowait -m 2048M,slots=10,maxmem=129853M -device pci-bridge,bus=pcie.0,id=pci-bridge-0,chassis_nr=1,shpc=on,addr=2,romfile= -device virtio-serial-pci,disable-modern=false,id=serial0,romfile= -device virtconsole,chardev=charconsole0,id=console0 -chardev socket,id=charconsole0,path=/run/vc/vm/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/console.sock,server,nowait -device virtio-scsi-pci,id=scsi0,disable-modern=false,romfile= -object rng-random,id=rng0,filename=/dev/urandom -device virtio-rng-pci,rng=rng0,romfile= -device vhost-vsock-pci,disable-modern=false,vhostfd=3,id=vsock-555233078,guest-cid=555233078,romfile= -chardev socket,id=char-1082a328d9d79959,path=/run/vc/vm/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/vhost-fs.sock -device vhost-user-fs-pci,chardev=char-1082a328d9d79959,tag=kataShared,romfile= -netdev tap,id=network-0,vhost=on,vhostfds=4,fds=5 -device driver=virtio-net-pci,netdev=network-0,mac=d6:96:23:1b:02:05,disable-modern=false,mq=on,vectors=4,romfile= -global kvm-pit.lost_tick_policy=discard -vga none -no-user-config -nodefaults -nographic -daemonize -object memory-backend-file,id=dimm1,size=2048M,mem-path=/dev/shm,share=on -numa node,memdev=dimm1 -kernel /usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/vmlinuz -initrd /var/cache/kata-containers/osbuilder-images/4.18.0-193.13.2.el8_2.x86_64/\"rhcos\"-kata-4.18.0-193.13.2.el8_2.x86_64.initrd -append tsc=reliable no_timer_check rcupdate.rcu_expedited=1 i8042.direct=1 i8042.dumbkbd=1 i8042.nopnp=1 i8042.noaux=1 noreplace-smp reboot=k console=hvc0 console=hvc1 iommu=off cryptomgr.notests net.ifnames=0 pci=lastbus=0 quiet panic=1 nr_cpus=40 agent.use_vsock=true scsi_mod.scan=none -pidfile /run/vc/vm/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/pid -smp 1,cores=1,threads=1,sockets=40,maxcpus=40","title":"VM configuration"},{"location":"openshift/kata/#initrd-build-process","text":"kata-operator builds an initrd image with /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh . This script is executed by service kata-osbuilder-generate.service upon start and restart. For further details, also see: https://github.com/kata-containers/osbuilder : [root@openshift-worker-2 ~]# systemctl list-unit-files | grep kata kata-osbuilder-generate.service enabled [root@openshift-worker-2 ~]# systemctl status kata-osbuilder-generate.service \u25cf kata-osbuilder-generate.service - Hacky service to enable kata-osbuilder-generate.service Loaded: loaded (/etc/systemd/system/kata-osbuilder-generate.service; enabled; vendor preset: disabled) Active: inactive (dead) since Sun 2020-07-26 15:54:05 UTC; 18h ago Process: 12437 ExecStart=/usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh (code=exited, status=0/SUCCESS) Main PID: 12437 (code=exited, status=0/SUCCESS) CPU: 23.046s Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: INFO: Create /etc/resolv.conf file in rootfs if not exist Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: INFO: Creating summary file Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: INFO: Created summary file '/var/lib/osbuilder/osbuilder.yaml' inside rootfs Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: + Calling osbuilder initrd_builder.sh Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: [OK] init is installed Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: [OK] Agent is installed Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: INFO: Creating /tmp/kata-dracut-images-jeAfIG/kata-containers-initrd.img based on rootfs at /tmp/kata-dracut-rootfs-qPP8lc Jul 26 15:54:05 openshift-worker-2.example.com kata-osbuilder.sh[12437]: 133725 blocks Jul 26 15:54:05 openshift-worker-2.example.com systemd[1]: Started Hacky service to enable kata-osbuilder-generate.service. Jul 26 15:54:05 openshift-worker-2.example.com systemd[1]: kata-osbuilder-generate.service: Consumed 23.046s CPU time [root@openshift-worker-2 ~]# cat /etc/systemd/system/kata-osbuilder-generate.service [Unit] Description=Hacky service to enable kata-osbuilder-generate.service ConditionPathExists=/usr/lib/systemd/system/kata-osbuilder-generate.service [Service] Type=oneshot ExecStart=/usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh ExecRestart=/usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh [Install] WantedBy=multi-user.target [root@openshift-worker-2 ~]# journalctl -u kata-osbuilder-generate.service | cat -- Logs begin at Thu 2020-07-23 14:27:02 UTC, end at Mon 2020-07-27 10:12:56 UTC. -- Jul 26 15:46:48 openshift-worker-2.example.com systemd[1]: Starting Hacky service to enable kata-osbuilder-generate.service... Jul 26 15:46:48 openshift-worker-2.example.com systemd[1]: kata-osbuilder-generate.service: Main process exited, code=exited, status=203/EXEC Jul 26 15:46:48 openshift-worker-2.example.com systemd[1]: kata-osbuilder-generate.service: Failed with result 'exit-code'. Jul 26 15:46:48 openshift-worker-2.example.com systemd[1]: Failed to start Hacky service to enable kata-osbuilder-generate.service. Jul 26 15:46:48 openshift-worker-2.example.com systemd[1]: kata-osbuilder-generate.service: Consumed 944us CPU time Jul 26 15:53:28 openshift-worker-2.example.com systemd[1]: Starting Hacky service to enable kata-osbuilder-generate.service... Jul 26 15:53:28 openshift-worker-2.example.com systemd[1]: kata-osbuilder-generate.service: Main process exited, code=exited, status=203/EXEC Jul 26 15:53:28 openshift-worker-2.example.com systemd[1]: kata-osbuilder-generate.service: Failed with result 'exit-code'. Jul 26 15:53:28 openshift-worker-2.example.com systemd[1]: Failed to start Hacky service to enable kata-osbuilder-generate.service. Jul 26 15:53:28 openshift-worker-2.example.com systemd[1]: kata-osbuilder-generate.service: Consumed 1ms CPU time Jul 26 15:53:34 openshift-worker-2.example.com systemd[1]: /etc/systemd/system/kata-osbuilder-generate.service:8: Unknown lvalue 'ExecRestart' in section 'Service' Jul 26 15:53:41 openshift-worker-2.example.com systemd[1]: Starting Hacky service to enable kata-osbuilder-generate.service... Jul 26 15:53:41 openshift-worker-2.example.com kata-osbuilder.sh[12437]: + Building dracut initrd Jul 26 15:53:41 openshift-worker-2.example.com dracut[12466]: Executing: /usr/bin/dracut --confdir ./dracut/dracut.conf.d --no-compress --conf /dev/null /tmp/kata-dracut-images-jeAfIG/tmp.dHnQ14COyx 4.18.0-193.13.2.el8_2.x86_64 Jul 26 15:53:42 openshift-worker-2.example.com dracut[12466]: *** Including module: bash *** Jul 26 15:53:42 openshift-worker-2.example.com dracut[12466]: *** Including module: systemd *** Jul 26 15:53:43 openshift-worker-2.example.com dracut[12466]: *** Including module: rescue *** Jul 26 15:53:43 openshift-worker-2.example.com dracut[12466]: *** Including module: nss-softokn *** Jul 26 15:53:43 openshift-worker-2.example.com dracut[12466]: *** Including module: kernel-modules *** Jul 26 15:53:43 openshift-worker-2.example.com dracut[12466]: *** Including module: udev-rules *** Jul 26 15:53:43 openshift-worker-2.example.com dracut[12466]: Skipping udev rule: 91-permissions.rules Jul 26 15:53:43 openshift-worker-2.example.com dracut[12466]: Skipping udev rule: 80-drivers-modprobe.rules Jul 26 15:53:43 openshift-worker-2.example.com dracut[12466]: *** Including module: syslog *** Jul 26 15:53:43 openshift-worker-2.example.com kata-osbuilder.sh[12437]: dracut: Could not find any syslog binary although the syslogmodule is selected to be installed. Please check. Jul 26 15:53:43 openshift-worker-2.example.com dracut[12466]: Could not find any syslog binary although the syslogmodule is selected to be installed. Please check. Jul 26 15:53:43 openshift-worker-2.example.com dracut[12466]: *** Including modules done *** Jul 26 15:53:43 openshift-worker-2.example.com dracut[12466]: *** Installing kernel module dependencies *** Jul 26 15:53:43 openshift-worker-2.example.com dracut[12466]: *** Installing kernel module dependencies done *** Jul 26 15:53:43 openshift-worker-2.example.com dracut[12466]: *** Resolving executable dependencies *** Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: *** Resolving executable dependencies done*** Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: Could not find 'strip'. Not stripping the initramfs. Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: *** Store current command line parameters *** Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: *** Creating image file '/tmp/kata-dracut-images-jeAfIG/tmp.dHnQ14COyx' *** Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: Image: /var/tmp/dracut.9gy4Wx/initramfs.img: 39M Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: ======================================================================== Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: Version: Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: Arguments: --confdir './dracut/dracut.conf.d' --no-compress --conf '/dev/null' Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: dracut modules: Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: bash Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: systemd Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: rescue Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: nss-softokn Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: kernel-modules Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: udev-rules Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: syslog Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: ======================================================================== Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: crw-r--r-- 1 root root 5, 1 Jan 1 1970 dev/console Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: crw-r--r-- 1 root root 1, 11 Jan 1 1970 dev/kmsg Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: crw-r--r-- 1 root root 1, 3 Jan 1 1970 dev/null Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: crw-r--r-- 1 root root 1, 8 Jan 1 1970 dev/random Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: crw-r--r-- 1 root root 1, 9 Jan 1 1970 dev/urandom Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 11 root root 0 Jan 1 1970 . Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 7 Jan 1 1970 bin -> usr/bin Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 dev Jul 26 15:53:44 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 7 root root 0 Jan 1 1970 etc Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 etc/cmdline.d Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 etc/conf.d Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 124 Jan 1 1970 etc/conf.d/systemd.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 116 Jan 1 1970 etc/group Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 4335 Jan 1 1970 etc/ld.so.cache Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 28 Jan 1 1970 etc/ld.so.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 etc/ld.so.conf.d Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 26 Jan 1 1970 etc/ld.so.conf.d/bind-export-x86_64.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -r--r--r-- 1 root root 67 Jan 1 1970 etc/ld.so.conf.d/kernel-4.18.0-193.13.2.el8_2.x86_64.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 0 Jan 1 1970 etc/machine-id Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 0 Jan 1 1970 etc/passwd Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 etc/systemd Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 64 Jan 1 1970 etc/systemd/journald.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 3 root root 0 Jan 1 1970 etc/udev Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 etc/udev/rules.d Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 297 Jan 1 1970 etc/udev/rules.d/59-persistent-storage.rules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 1030 Jan 1 1970 etc/udev/rules.d/61-persistent-storage.rules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 215 Jan 1 1970 etc/udev/udev.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 1204 Jan 1 1970 etc/virc Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 23 Jan 1 1970 init -> usr/lib/systemd/systemd Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 7 Jan 1 1970 lib -> usr/lib Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 9 Jan 1 1970 lib64 -> usr/lib64 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 proc Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 12 Jan 1 1970 root -> var/roothome Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 run Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 8 Jan 1 1970 sbin -> usr/sbin Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 sys Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 sysroot Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 tmp Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 6 root root 0 Jan 1 1970 usr Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/bin Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 1596592 Jan 1 1970 usr/bin/bash Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 38504 Jan 1 1970 usr/bin/cat Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 38392 Jan 1 1970 usr/bin/echo Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 22496 Jan 1 1970 usr/bin/free Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 193768 Jan 1 1970 usr/bin/grep Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 78712 Jan 1 1970 usr/bin/journalctl Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 163744 Jan 1 1970 usr/bin/kmod Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 4 Jan 1 1970 usr/bin/loginctl -> true Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 46112 Jan 1 1970 usr/bin/more Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwsr-xr-x 1 root root 50456 Jan 1 1970 usr/bin/mount Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 180280 Jan 1 1970 usr/bin/netstat Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 80136 Jan 1 1970 usr/bin/ping Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 141240 Jan 1 1970 usr/bin/ps Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 72216 Jan 1 1970 usr/bin/rm Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 33552 Jan 1 1970 usr/bin/rpcinfo Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 171464 Jan 1 1970 usr/bin/scp Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 4 Jan 1 1970 usr/bin/sh -> bash Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 1040320 Jan 1 1970 usr/bin/ssh Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 1505288 Jan 1 1970 usr/bin/strace Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 215336 Jan 1 1970 usr/bin/systemctl Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 16632 Jan 1 1970 usr/bin/systemd-cgls Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 16608 Jan 1 1970 usr/bin/systemd-escape Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 49984 Jan 1 1970 usr/bin/systemd-run Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 74936 Jan 1 1970 usr/bin/systemd-tmpfiles Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 34288 Jan 1 1970 usr/bin/true Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 434592 Jan 1 1970 usr/bin/udevadm Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwsr-xr-x 1 root root 33640 Jan 1 1970 usr/bin/umount Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 38384 Jan 1 1970 usr/bin/uname Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 1416744 Jan 1 1970 usr/bin/vi Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 9 root root 0 Jan 1 1970 usr/lib Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 3 root root 0 Jan 1 1970 usr/lib/dracut Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 68 Jan 1 1970 usr/lib/dracut/build-parameter.txt Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 15 root root 0 Jan 1 1970 usr/lib/dracut/hooks Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/cleanup Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/cmdline Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/emergency Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 6 root root 0 Jan 1 1970 usr/lib/dracut/hooks/initqueue Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/initqueue/finished Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/initqueue/online Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/initqueue/settled Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/initqueue/timeout Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/mount Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/netroot Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/pre-mount Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/pre-pivot Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/pre-shutdown Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/pre-trigger Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/pre-udev Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/shutdown Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/dracut/hooks/shutdown-emergency Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 65 Jan 1 1970 usr/lib/dracut/modules.txt Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 0 Jan 1 1970 usr/lib/dracut/need-initqueue Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/modprobe.d Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 382 Jan 1 1970 usr/lib/modprobe.d/dist-alsa.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 884 Jan 1 1970 usr/lib/modprobe.d/dist-blacklist.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 559 Jan 1 1970 usr/lib/modprobe.d/libmlx4.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 765 Jan 1 1970 usr/lib/modprobe.d/systemd.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 3 root root 0 Jan 1 1970 usr/lib/modules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 3 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 7 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 3 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/arch Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 3 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/arch/x86 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 3 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/arch/x86/crypto Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/arch/x86/crypto/sha256-mb Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 9256 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/arch/x86/crypto/sha256-mb/sha256-mb.ko.xz Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/crypto Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 6756 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/crypto/mcryptd.ko.xz Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 6 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/drivers Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/drivers/block Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 8924 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/drivers/block/virtio_blk.ko.xz Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/drivers/char Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 14796 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/drivers/char/virtio_console.ko.xz Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/drivers/net Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 6776 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/drivers/net/net_failover.ko.xz Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 24512 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/drivers/net/virtio_net.ko.xz Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/drivers/scsi Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 20836 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/drivers/scsi/sg.ko.xz Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 8744 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/drivers/scsi/virtio_scsi.ko.xz Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 3 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/fs Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/fs/fuse Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 57012 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/fs/fuse/fuse.ko.xz Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 11132 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/fs/fuse/virtiofs.ko.xz Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 4 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/net Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/net/core Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 4100 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/net/core/failover.ko.xz Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/net/vmw_vsock Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 6884 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/net/vmw_vsock/vmw_vsock_virtio_transport.ko.xz Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 12144 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/net/vmw_vsock/vmw_vsock_virtio_transport_common.ko.xz Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 13696 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/kernel/net/vmw_vsock/vsock.ko.xz Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 552 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/modules.alias Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 777 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/modules.alias.bin Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 7534 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/modules.builtin Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 9748 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/modules.builtin.bin Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 827 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/modules.dep Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 1374 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/modules.dep.bin Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 70 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/modules.devname Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 100570 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/modules.order Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 55 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/modules.softdep Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 5544 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/modules.symbols Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 5720 Jan 1 1970 usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/modules.symbols.bin Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/sysctl.d Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 122 Jan 1 1970 usr/lib/sysctl.d/10-coreos-ratelimit-kmsg.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 1810 Jan 1 1970 usr/lib/sysctl.d/10-default-yama-scope.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 524 Jan 1 1970 usr/lib/sysctl.d/50-coredump.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 1270 Jan 1 1970 usr/lib/sysctl.d/50-default.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 246 Jan 1 1970 usr/lib/sysctl.d/50-libkcapi-optmem_max.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 636 Jan 1 1970 usr/lib/sysctl.d/50-pid-max.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 5 root root 0 Jan 1 1970 usr/lib/systemd Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 2707624 Jan 1 1970 usr/lib/systemd/libsystemd-shared-239.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/systemd/network Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 412 Jan 1 1970 usr/lib/systemd/network/99-default.link Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 8 root root 0 Jan 1 1970 usr/lib/systemd/system Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/systemd/system-generators Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 16648 Jan 1 1970 usr/lib/systemd/system-generators/systemd-debug-generator Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 46080 Jan 1 1970 usr/lib/systemd/system-generators/systemd-fstab-generator Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 1024 Jan 1 1970 usr/lib/systemd/system/basic.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 412 Jan 1 1970 usr/lib/systemd/system/cryptsetup.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 13 Jan 1 1970 usr/lib/systemd/system/ctrl-alt-del.target -> reboot.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 1084 Jan 1 1970 usr/lib/systemd/system/debug-shell.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 17 Jan 1 1970 usr/lib/systemd/system/default.target -> multi-user.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 471 Jan 1 1970 usr/lib/systemd/system/emergency.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/systemd/system/emergency.target.wants Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 33 Jan 1 1970 usr/lib/systemd/system/emergency.target.wants/systemd-vconsole-setup.service -> ../systemd-vconsole-setup.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 480 Jan 1 1970 usr/lib/systemd/system/final.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 527 Jan 1 1970 usr/lib/systemd/system/halt.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 541 Jan 1 1970 usr/lib/systemd/system/kexec.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 721 Jan 1 1970 usr/lib/systemd/system/kmod-static-nodes.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 435 Jan 1 1970 usr/lib/systemd/system/local-fs-pre.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 547 Jan 1 1970 usr/lib/systemd/system/local-fs.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 532 Jan 1 1970 usr/lib/systemd/system/multi-user.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 505 Jan 1 1970 usr/lib/systemd/system/network-online.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 502 Jan 1 1970 usr/lib/systemd/system/network-pre.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 521 Jan 1 1970 usr/lib/systemd/system/network.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 554 Jan 1 1970 usr/lib/systemd/system/nss-lookup.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 513 Jan 1 1970 usr/lib/systemd/system/nss-user-lookup.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 394 Jan 1 1970 usr/lib/systemd/system/paths.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 592 Jan 1 1970 usr/lib/systemd/system/poweroff.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 583 Jan 1 1970 usr/lib/systemd/system/reboot.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 436 Jan 1 1970 usr/lib/systemd/system/remote-fs-pre.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 522 Jan 1 1970 usr/lib/systemd/system/remote-fs.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 492 Jan 1 1970 usr/lib/systemd/system/rescue.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/systemd/system/rescue.target.wants Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 33 Jan 1 1970 usr/lib/systemd/system/rescue.target.wants/systemd-vconsole-setup.service -> ../systemd-vconsole-setup.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 540 Jan 1 1970 usr/lib/systemd/system/rpcbind.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 442 Jan 1 1970 usr/lib/systemd/system/shutdown.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 402 Jan 1 1970 usr/lib/systemd/system/sigpwr.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 449 Jan 1 1970 usr/lib/systemd/system/slices.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 396 Jan 1 1970 usr/lib/systemd/system/sockets.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/systemd/system/sockets.target.wants Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 34 Jan 1 1970 usr/lib/systemd/system/sockets.target.wants/systemd-journald-dev-log.socket -> ../systemd-journald-dev-log.socket Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 26 Jan 1 1970 usr/lib/systemd/system/sockets.target.wants/systemd-journald.socket -> ../systemd-journald.socket Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 31 Jan 1 1970 usr/lib/systemd/system/sockets.target.wants/systemd-udevd-control.socket -> ../systemd-udevd-control.socket Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 30 Jan 1 1970 usr/lib/systemd/system/sockets.target.wants/systemd-udevd-kernel.socket -> ../systemd-udevd-kernel.socket Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 393 Jan 1 1970 usr/lib/systemd/system/swap.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 767 Jan 1 1970 usr/lib/systemd/system/sys-kernel-config.mount Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 558 Jan 1 1970 usr/lib/systemd/system/sysinit.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/systemd/system/sysinit.target.wants Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 28 Jan 1 1970 usr/lib/systemd/system/sysinit.target.wants/kmod-static-nodes.service -> ../kmod-static-nodes.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 36 Jan 1 1970 usr/lib/systemd/system/sysinit.target.wants/systemd-ask-password-console.path -> ../systemd-ask-password-console.path Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 27 Jan 1 1970 usr/lib/systemd/system/sysinit.target.wants/systemd-journald.service -> ../systemd-journald.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 31 Jan 1 1970 usr/lib/systemd/system/sysinit.target.wants/systemd-modules-load.service -> ../systemd-modules-load.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 25 Jan 1 1970 usr/lib/systemd/system/sysinit.target.wants/systemd-sysctl.service -> ../systemd-sysctl.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 37 Jan 1 1970 usr/lib/systemd/system/sysinit.target.wants/systemd-tmpfiles-setup-dev.service -> ../systemd-tmpfiles-setup-dev.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 33 Jan 1 1970 usr/lib/systemd/system/sysinit.target.wants/systemd-tmpfiles-setup.service -> ../systemd-tmpfiles-setup.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 31 Jan 1 1970 usr/lib/systemd/system/sysinit.target.wants/systemd-udev-trigger.service -> ../systemd-udev-trigger.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 24 Jan 1 1970 usr/lib/systemd/system/sysinit.target.wants/systemd-udevd.service -> ../systemd-udevd.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 1407 Jan 1 1970 usr/lib/systemd/system/syslog.socket Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 704 Jan 1 1970 usr/lib/systemd/system/systemd-ask-password-console.path Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 728 Jan 1 1970 usr/lib/systemd/system/systemd-ask-password-console.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/systemd/system/systemd-ask-password-console.service.wants Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 33 Jan 1 1970 usr/lib/systemd/system/systemd-ask-password-console.service.wants/systemd-vconsole-setup.service -> ../systemd-vconsole-setup.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/systemd/system/systemd-ask-password-plymouth.service.wants Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 33 Jan 1 1970 usr/lib/systemd/system/systemd-ask-password-plymouth.service.wants/systemd-vconsole-setup.service -> ../systemd-vconsole-setup.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 671 Jan 1 1970 usr/lib/systemd/system/systemd-fsck@.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 588 Jan 1 1970 usr/lib/systemd/system/systemd-halt.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 647 Jan 1 1970 usr/lib/systemd/system/systemd-journald-audit.socket Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 1130 Jan 1 1970 usr/lib/systemd/system/systemd-journald-dev-log.socket Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 1537 Jan 1 1970 usr/lib/systemd/system/systemd-journald.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 882 Jan 1 1970 usr/lib/systemd/system/systemd-journald.socket Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 601 Jan 1 1970 usr/lib/systemd/system/systemd-kexec.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 1011 Jan 1 1970 usr/lib/systemd/system/systemd-modules-load.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 597 Jan 1 1970 usr/lib/systemd/system/systemd-poweroff.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 800 Jan 1 1970 usr/lib/systemd/system/systemd-random-seed.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 592 Jan 1 1970 usr/lib/systemd/system/systemd-reboot.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 697 Jan 1 1970 usr/lib/systemd/system/systemd-sysctl.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 771 Jan 1 1970 usr/lib/systemd/system/systemd-tmpfiles-setup-dev.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 751 Jan 1 1970 usr/lib/systemd/system/systemd-tmpfiles-setup.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 867 Jan 1 1970 usr/lib/systemd/system/systemd-udev-settle.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 771 Jan 1 1970 usr/lib/systemd/system/systemd-udev-trigger.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 635 Jan 1 1970 usr/lib/systemd/system/systemd-udevd-control.socket Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 610 Jan 1 1970 usr/lib/systemd/system/systemd-udevd-kernel.socket Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 1070 Jan 1 1970 usr/lib/systemd/system/systemd-udevd.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 622 Jan 1 1970 usr/lib/systemd/system/systemd-vconsole-setup.service Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 445 Jan 1 1970 usr/lib/systemd/system/timers.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 457 Jan 1 1970 usr/lib/systemd/system/umount.target Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 1559672 Jan 1 1970 usr/lib/systemd/systemd Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 12504 Jan 1 1970 usr/lib/systemd/systemd-cgroups-agent Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 62488 Jan 1 1970 usr/lib/systemd/systemd-coredump Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 25192 Jan 1 1970 usr/lib/systemd/systemd-fsck Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 153912 Jan 1 1970 usr/lib/systemd/systemd-journald Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 21192 Jan 1 1970 usr/lib/systemd/systemd-modules-load Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 12512 Jan 1 1970 usr/lib/systemd/systemd-reply-password Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 62504 Jan 1 1970 usr/lib/systemd/systemd-shutdown Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 16640 Jan 1 1970 usr/lib/systemd/systemd-sysctl Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 418112 Jan 1 1970 usr/lib/systemd/systemd-udevd Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 21104 Jan 1 1970 usr/lib/systemd/systemd-vconsole-setup Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/tmpfiles.d Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 1676 Jan 1 1970 usr/lib/tmpfiles.d/systemd.conf Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 3 root root 0 Jan 1 1970 usr/lib/udev Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 16592 Jan 1 1970 usr/lib/udev/ata_id Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 33312 Jan 1 1970 usr/lib/udev/cdrom_id Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib/udev/rules.d Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 1834 Jan 1 1970 usr/lib/udev/rules.d/40-redhat.rules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 3679 Jan 1 1970 usr/lib/udev/rules.d/50-udev-default.rules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 626 Jan 1 1970 usr/lib/udev/rules.d/60-block.rules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 6528 Jan 1 1970 usr/lib/udev/rules.d/60-persistent-storage.rules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 2671 Jan 1 1970 usr/lib/udev/rules.d/70-uaccess.rules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 2758 Jan 1 1970 usr/lib/udev/rules.d/71-seat.rules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 636 Jan 1 1970 usr/lib/udev/rules.d/73-seat-late.rules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 452 Jan 1 1970 usr/lib/udev/rules.d/75-net-description.rules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 615 Jan 1 1970 usr/lib/udev/rules.d/80-drivers.rules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 292 Jan 1 1970 usr/lib/udev/rules.d/80-net-setup-link.rules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 510 Jan 1 1970 usr/lib/udev/rules.d/90-vconsole.rules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 4367 Jan 1 1970 usr/lib/udev/rules.d/99-systemd.rules Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 34072 Jan 1 1970 usr/lib/udev/scsi_id Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/lib64 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 252288 Jan 1 1970 usr/lib64/ld-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 10 Jan 1 1970 usr/lib64/ld-linux-x86-64.so.2 -> ld-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 18 Jan 1 1970 usr/lib64/libacl.so.1 -> libacl.so.1.1.2253 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 54816 Jan 1 1970 usr/lib64/libacl.so.1.1.2253 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 19 Jan 1 1970 usr/lib64/libattr.so.1 -> libattr.so.1.1.2448 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 26192 Jan 1 1970 usr/lib64/libattr.so.1.1.2448 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 17 Jan 1 1970 usr/lib64/libaudit.so.1 -> libaudit.so.1.0.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 123336 Jan 1 1970 usr/lib64/libaudit.so.1.0.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 17 Jan 1 1970 usr/lib64/libblkid.so.1 -> libblkid.so.1.1.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 339272 Jan 1 1970 usr/lib64/libblkid.so.1.1.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 15 Jan 1 1970 usr/lib64/libbz2.so.1 -> libbz2.so.1.0.6 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 74744 Jan 1 1970 usr/lib64/libbz2.so.1.0.6 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 3149120 Jan 1 1970 usr/lib64/libc-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 12 Jan 1 1970 usr/lib64/libc.so.6 -> libc-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 18 Jan 1 1970 usr/lib64/libcap-ng.so.0 -> libcap-ng.so.0.0.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 30256 Jan 1 1970 usr/lib64/libcap-ng.so.0.0.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 14 Jan 1 1970 usr/lib64/libcap.so.2 -> libcap.so.2.26 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 37952 Jan 1 1970 usr/lib64/libcap.so.2.26 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 17 Jan 1 1970 usr/lib64/libcom_err.so.2 -> libcom_err.so.2.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 17336 Jan 1 1970 usr/lib64/libcom_err.so.2.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 17 Jan 1 1970 usr/lib64/libcrypt.so.1 -> libcrypt.so.1.1.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 142712 Jan 1 1970 usr/lib64/libcrypt.so.1.1.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 19 Jan 1 1970 usr/lib64/libcrypto.so.1.1 -> libcrypto.so.1.1.1c Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 3058976 Jan 1 1970 usr/lib64/libcrypto.so.1.1.1c Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 23 Jan 1 1970 usr/lib64/libcryptsetup.so.12 -> libcryptsetup.so.12.5.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 485432 Jan 1 1970 usr/lib64/libcryptsetup.so.12.5.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -r-xr-xr-x 1 root root 371736 Jan 1 1970 usr/lib64/libdevmapper.so.1.02 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 28968 Jan 1 1970 usr/lib64/libdl-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 13 Jan 1 1970 usr/lib64/libdl.so.2 -> libdl-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 658352 Jan 1 1970 usr/lib64/libdw-0.178.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 14 Jan 1 1970 usr/lib64/libdw.so.1 -> libdw-0.178.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 13 Jan 1 1970 usr/lib64/libe2p.so.2 -> libe2p.so.2.3 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 38896 Jan 1 1970 usr/lib64/libe2p.so.2.3 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 104552 Jan 1 1970 usr/lib64/libelf-0.178.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 15 Jan 1 1970 usr/lib64/libelf.so.1 -> libelf-0.178.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 16 Jan 1 1970 usr/lib64/libext2fs.so.2 -> libext2fs.so.2.4 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 428784 Jan 1 1970 usr/lib64/libext2fs.so.2.4 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 21 Jan 1 1970 usr/lib64/libfipscheck.so.1 -> libfipscheck.so.1.2.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 12200 Jan 1 1970 usr/lib64/libfipscheck.so.1.2.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 12112 Jan 1 1970 usr/lib64/libfreebl3.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rw-r--r-- 1 root root 899 Jan 1 1970 usr/lib64/libfreeblpriv3.chk Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 552696 Jan 1 1970 usr/lib64/libfreeblpriv3.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 99688 Jan 1 1970 usr/lib64/libgcc_s-8-20191121.so.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 24 Jan 1 1970 usr/lib64/libgcc_s.so.1 -> libgcc_s-8-20191121.so.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 19 Jan 1 1970 usr/lib64/libgcrypt.so.20 -> libgcrypt.so.20.2.3 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 1385560 Jan 1 1970 usr/lib64/libgcrypt.so.20.2.3 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 22 Jan 1 1970 usr/lib64/libgpg-error.so.0 -> libgpg-error.so.0.24.2 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 144392 Jan 1 1970 usr/lib64/libgpg-error.so.0.24.2 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 21 Jan 1 1970 usr/lib64/libgssapi_krb5.so.2 -> libgssapi_krb5.so.2.2 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 335040 Jan 1 1970 usr/lib64/libgssapi_krb5.so.2.2 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 16 Jan 1 1970 usr/lib64/libidn2.so.0 -> libidn2.so.0.3.6 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 162224 Jan 1 1970 usr/lib64/libidn2.so.0.3.6 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 17 Jan 1 1970 usr/lib64/libip4tc.so.2 -> libip4tc.so.2.0.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 33240 Jan 1 1970 usr/lib64/libip4tc.so.2.0.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 18 Jan 1 1970 usr/lib64/libjson-c.so.4 -> libjson-c.so.4.0.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 70432 Jan 1 1970 usr/lib64/libjson-c.so.4.0.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 18 Jan 1 1970 usr/lib64/libk5crypto.so.3 -> libk5crypto.so.3.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 117216 Jan 1 1970 usr/lib64/libk5crypto.so.3.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 18 Jan 1 1970 usr/lib64/libkeyutils.so.1 -> libkeyutils.so.1.6 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 16240 Jan 1 1970 usr/lib64/libkeyutils.so.1.6 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 16 Jan 1 1970 usr/lib64/libkmod.so.2 -> libkmod.so.2.3.3 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 100200 Jan 1 1970 usr/lib64/libkmod.so.2.3.3 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 14 Jan 1 1970 usr/lib64/libkrb5.so.3 -> libkrb5.so.3.3 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 997600 Jan 1 1970 usr/lib64/libkrb5.so.3.3 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 21 Jan 1 1970 usr/lib64/libkrb5support.so.0 -> libkrb5support.so.0.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 71624 Jan 1 1970 usr/lib64/libkrb5support.so.0.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 15 Jan 1 1970 usr/lib64/liblz4.so.1 -> liblz4.so.1.8.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 95728 Jan 1 1970 usr/lib64/liblz4.so.1.8.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 16 Jan 1 1970 usr/lib64/liblzma.so.5 -> liblzma.so.5.2.4 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 192016 Jan 1 1970 usr/lib64/liblzma.so.5.2.4 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 2191792 Jan 1 1970 usr/lib64/libm-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 12 Jan 1 1970 usr/lib64/libm.so.6 -> libm-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 17 Jan 1 1970 usr/lib64/libmount.so.1 -> libmount.so.1.1.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 371208 Jan 1 1970 usr/lib64/libmount.so.1.1.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 76872 Jan 1 1970 usr/lib64/libnss_files-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 20 Jan 1 1970 usr/lib64/libnss_files.so.2 -> libnss_files-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 16 Jan 1 1970 usr/lib64/libpam.so.0 -> libpam.so.0.84.2 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 66272 Jan 1 1970 usr/lib64/libpam.so.0.84.2 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 16 Jan 1 1970 usr/lib64/libpcap.so.1 -> libpcap.so.1.9.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 356920 Jan 1 1970 usr/lib64/libpcap.so.1.9.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 17 Jan 1 1970 usr/lib64/libpcre.so.1 -> libpcre.so.1.2.10 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 471224 Jan 1 1970 usr/lib64/libpcre.so.1.2.10 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 19 Jan 1 1970 usr/lib64/libpcre2-8.so.0 -> libpcre2-8.so.0.7.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 553480 Jan 1 1970 usr/lib64/libpcre2-8.so.0.7.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 18 Jan 1 1970 usr/lib64/libprocps.so.7 -> libprocps.so.7.1.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 86912 Jan 1 1970 usr/lib64/libprocps.so.7.1.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 322816 Jan 1 1970 usr/lib64/libpthread-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 18 Jan 1 1970 usr/lib64/libpthread.so.0 -> libpthread-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 123320 Jan 1 1970 usr/lib64/libresolv-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 17 Jan 1 1970 usr/lib64/libresolv.so.2 -> libresolv-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 74976 Jan 1 1970 usr/lib64/librt-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 13 Jan 1 1970 usr/lib64/librt.so.1 -> librt-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 19 Jan 1 1970 usr/lib64/libseccomp.so.2 -> libseccomp.so.2.4.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 357336 Jan 1 1970 usr/lib64/libseccomp.so.2.4.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 172640 Jan 1 1970 usr/lib64/libselinux.so.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 766120 Jan 1 1970 usr/lib64/libsepol.so.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 16 Jan 1 1970 usr/lib64/libssl.so.1.1 -> libssl.so.1.1.1c Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 615504 Jan 1 1970 usr/lib64/libssl.so.1.1.1c Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 20 Jan 1 1970 usr/lib64/libsystemd.so.0 -> libsystemd.so.0.23.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 1354792 Jan 1 1970 usr/lib64/libsystemd.so.0.23.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 15 Jan 1 1970 usr/lib64/libtinfo.so.6 -> libtinfo.so.6.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 208616 Jan 1 1970 usr/lib64/libtinfo.so.6.1 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 17 Jan 1 1970 usr/lib64/libtirpc.so.3 -> libtirpc.so.3.0.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 371032 Jan 1 1970 usr/lib64/libtirpc.so.3.0.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 17 Jan 1 1970 usr/lib64/libudev.so.1 -> libudev.so.1.6.11 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 628696 Jan 1 1970 usr/lib64/libudev.so.1.6.11 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 21 Jan 1 1970 usr/lib64/libunistring.so.2 -> libunistring.so.2.1.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 1760264 Jan 1 1970 usr/lib64/libunistring.so.2.1.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 23392 Jan 1 1970 usr/lib64/libutil-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 15 Jan 1 1970 usr/lib64/libutil.so.1 -> libutil-2.28.so Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 16 Jan 1 1970 usr/lib64/libuuid.so.1 -> libuuid.so.1.3.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 33224 Jan 1 1970 usr/lib64/libuuid.so.1.3.0 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 14 Jan 1 1970 usr/lib64/libz.so.1 -> libz.so.1.2.11 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 97136 Jan 1 1970 usr/lib64/libz.so.1.2.11 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 usr/sbin Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 97008 Jan 1 1970 usr/sbin/blkid Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 11 Jan 1 1970 usr/sbin/depmod -> ../bin/kmod Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 336512 Jan 1 1970 usr/sbin/e2fsck Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 54872 Jan 1 1970 usr/sbin/fsck Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 336512 Jan 1 1970 usr/sbin/fsck.ext2 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 336512 Jan 1 1970 usr/sbin/fsck.ext3 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 336512 Jan 1 1970 usr/sbin/fsck.ext4 Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 66688 Jan 1 1970 usr/sbin/fsck.fat Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 8 Jan 1 1970 usr/sbin/fsck.vfat -> fsck.fat Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 22 Jan 1 1970 usr/sbin/init -> ../lib/systemd/systemd Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 11 Jan 1 1970 usr/sbin/insmod -> ../bin/kmod Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 193 Jan 1 1970 usr/sbin/insmodpost.sh Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 11 Jan 1 1970 usr/sbin/lsmod -> ../bin/kmod Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 11 Jan 1 1970 usr/sbin/modinfo -> ../bin/kmod Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 11 Jan 1 1970 usr/sbin/modprobe -> ../bin/kmod Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 12320 Jan 1 1970 usr/sbin/nologin Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 11 Jan 1 1970 usr/sbin/ping -> ../bin/ping Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 11 Jan 1 1970 usr/sbin/ping6 -> ../bin/ping Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 16 Jan 1 1970 usr/sbin/poweroff -> ../bin/systemctl Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 16 Jan 1 1970 usr/sbin/reboot -> ../bin/systemctl Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 11 Jan 1 1970 usr/sbin/rmmod -> ../bin/kmod Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 14 Jan 1 1970 usr/sbin/rpcinfo -> ../bin/rpcinfo Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 21672 Jan 1 1970 usr/sbin/showmount Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: -rwxr-xr-x 1 root root 21360 Jan 1 1970 usr/sbin/swapoff Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 14 Jan 1 1970 usr/sbin/udevadm -> ../bin/udevadm Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 4 root root 0 Jan 1 1970 var Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 11 Jan 1 1970 var/lock -> ../run/lock Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwx------ 2 root root 0 Jan 1 1970 var/roothome Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: lrwxrwxrwx 1 root root 6 Jan 1 1970 var/run -> ../run Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: drwxr-xr-x 2 root root 0 Jan 1 1970 var/tmp Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: ======================================================================== Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: *** Creating initramfs image file '/tmp/kata-dracut-images-jeAfIG/tmp.dHnQ14COyx' done *** Jul 26 15:53:45 openshift-worker-2.example.com kata-osbuilder.sh[12437]: Warning no default label for /tmp/kata-dracut-images-jeAfIG/tmp.dHnQ14COyx Jul 26 15:53:45 openshift-worker-2.example.com dracut[12466]: dracut: warning: could not fsfreeze /tmp/kata-dracut-images-jeAfIG Jul 26 15:53:45 openshift-worker-2.example.com kata-osbuilder.sh[12437]: + Extracting dracut initrd rootfs Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: 78145 blocks Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: + Copying agent directory tree into place Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: Calling osbuilder rootfs.sh on extracted rootfs Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: INFO: Create symlink to /tmp in /var to create private temporal directories with systemd Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: INFO: Install tmp.mount in ./etc/systemd/system Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: cp: cannot stat './usr/share/systemd/tmp.mount': No such file or directory Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: INFO: Create /tmp/kata-dracut-rootfs-qPP8lc/etc Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: INFO: Configure chrony file /tmp/kata-dracut-rootfs-qPP8lc/etc/chrony.conf Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: [OK] cp /usr/libexec/kata-containers/agent/usr/bin/kata-agent /tmp/kata-dracut-rootfs-qPP8lc/usr/bin/kata-agent Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: [OK] Agent installed Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: INFO: Check init is installed Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: [OK] init is installed Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: INFO: Create /etc/resolv.conf file in rootfs if not exist Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: INFO: Creating summary file Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: INFO: Created summary file '/var/lib/osbuilder/osbuilder.yaml' inside rootfs Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: + Calling osbuilder initrd_builder.sh Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: [OK] init is installed Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: [OK] Agent is installed Jul 26 15:53:46 openshift-worker-2.example.com kata-osbuilder.sh[12437]: INFO: Creating /tmp/kata-dracut-images-jeAfIG/kata-containers-initrd.img based on rootfs at /tmp/kata-dracut-rootfs-qPP8lc Jul 26 15:54:05 openshift-worker-2.example.com kata-osbuilder.sh[12437]: 133725 blocks Jul 26 15:54:05 openshift-worker-2.example.com systemd[1]: Started Hacky service to enable kata-osbuilder-generate.service. Jul 26 15:54:05 openshift-worker-2.example.com systemd[1]: kata-osbuilder-generate.service: Consumed 23.046s CPU time [root@openshift-worker-2 ~]# [root@openshift-worker-2 ~]# grep IMAGE_TOPDIR /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh readonly IMAGE_TOPDIR=\"/var/cache/kata-containers\" readonly KERNEL_SYMLINK=\"${IMAGE_TOPDIR}/vmlinuz.container\" stable symlink paths in ${IMAGE_TOPDIR} local image_osbuilder_dir=\"${IMAGE_TOPDIR}/osbuilder-images\" local image_dest_link=\"${IMAGE_TOPDIR}/kata-containers.img\" ln -sf ${initrd_dest_path} ${IMAGE_TOPDIR}/kata-containers-initrd.img [root@openshift-worker-2 ~]# grep image_osbuilder_dir ^C [root@openshift-worker-2 ~]# grep image_osbuilder_dir /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh local image_osbuilder_dir=\"${IMAGE_TOPDIR}/osbuilder-images\" local image_dir=\"${image_osbuilder_dir}/$KVERSION\" rm -rf \"${image_osbuilder_dir}\" [root@openshift-worker-2 ~]# grep image_dir /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh local image_dir=\"${image_osbuilder_dir}/$KVERSION\" local initrd_dest_path=\"${image_dir}/${DISTRO}-kata-${KVERSION}.initrd\" local image_dest_path=\"${image_dir}/${DISTRO}-kata-${KVERSION}.img\" mkdir -p \"${image_dir}\" [root@openshift-worker-2 ~]# grep initrd_dest_path /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh local initrd_dest_path=\"${image_dir}/${DISTRO}-kata-${KVERSION}.initrd\" mv -Z ${GENERATED_INITRD} ${initrd_dest_path} ln -sf ${initrd_dest_path} ${IMAGE_TOPDIR}/kata-containers-initrd.img [root@openshift-worker-2 ~]# grep GENERATED_INITRD /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh readonly GENERATED_INITRD=\"${DRACUT_IMAGES}/kata-containers-initrd.img\" mv -Z ${GENERATED_INITRD} ${initrd_dest_path} ./initrd-builder/initrd_builder.sh -o ${GENERATED_INITRD} ${DRACUT_ROOTFS} [root@openshift-worker-2 ~]# grep image_dest_path /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh: local image_dest_path=\"${image_dir}/${DISTRO}-kata-${KVERSION}.img\" /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh: mv -Z ${GENERATED_IMAGE} ${image_dest_path} /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh: ln -sf ${image_dest_path} ${image_dest_link} /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh: local image_dest_path=\"${image_dir}/${DISTRO}-kata-${KVERSION}.img\" /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh: mv -Z ${GENERATED_IMAGE} ${image_dest_path} /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh: ln -sf ${image_dest_path} ${image_dest_link} [root@openshift-worker-2 ~]# grep GENERATED_IMAGE /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh readonly GENERATED_IMAGE=\"${DRACUT_IMAGES}/kata-containers.img\" mv -Z ${GENERATED_IMAGE} ${image_dest_path} -o ${GENERATED_IMAGE} ${DRACUT_ROOTFS} [root@openshift-worker-2 ~]# find /var/cache/kata-containers /var/cache/kata-containers /var/cache/kata-containers/osbuilder-images /var/cache/kata-containers/osbuilder-images/4.18.0-193.13.2.el8_2.x86_64 /var/cache/kata-containers/osbuilder-images/4.18.0-193.13.2.el8_2.x86_64/\"rhcos\"-kata-4.18.0-193.13.2.el8_2.x86_64.initrd /var/cache/kata-containers/vmlinuz.container /var/cache/kata-containers/kata-containers-initrd.img","title":"Initrd build process"},{"location":"openshift/kata/#kata-configuration-which-kernel-and-initrd-does-the-vm-use","text":"Looking at the VM, it uses: -kernel /usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/vmlinuz -initrd /var/cache/kata-containers/osbuilder-images/4.18.0-193.13.2.el8_2.x86_64/\"rhcos\"-kata-4.18.0-193.13.2.el8_2.x86_64.initrd And following the configuration for kata (and the symlinks): [root@openshift-worker-2 ~]# egrep '^kernel' /usr/share/kata-containers/defaults/configuration.toml kernel = \"/var/cache/kata-containers/vmlinuz.container\" kernel_params = \"\" kernel_modules=[] [root@openshift-worker-2 ~]# egrep '^initrd' /usr/share/kata-containers/defaults/configuration.toml initrd = \"/var/cache/kata-containers/kata-containers-initrd.img\" [root@openshift-worker-2 ~]# ls -al /var/cache/kata-containers/vmlinuz.container lrwxrwxrwx. 1 root root 50 Jul 26 15:54 /var/cache/kata-containers/vmlinuz.container -> /lib/modules/4.18.0-193.13.2.el8_2.x86_64//vmlinuz [root@openshift-worker-2 ~]# ls -al /var/cache/kata-containers/kata-containers-initrd.img lrwxrwxrwx. 1 root root 121 Jul 26 15:54 /var/cache/kata-containers/kata-containers-initrd.img -> '/var/cache/kata-containers/osbuilder-images/4.18.0-193.13.2.el8_2.x86_64/\"rhcos\"-kata-4.18.0-193.13.2.el8_2.x86_64.initrd' [root@openshift-worker-2 ~]#","title":"kata configuration - which kernel and initrd does the VM use?"},{"location":"openshift/kata/#kata-runtime","text":"For further details, see: https://github.com/kata-containers/runtime Opened files: [root@openshift-worker-2 ~]# strace -e trace=file kata-runtime list 2>&1 | grep open openat(AT_FDCWD, \"/etc/ld.so.cache\", O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \"/lib64/libpthread.so.0\", O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \"/lib64/libdl.so.2\", O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \"/lib64/libc.so.6\", O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \"/sys/kernel/mm/transparent_hugepage/hpage_pmd_size\", O_RDONLY) = 3 openat(AT_FDCWD, \"/etc/ld.so.cache\", O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \"/lib64/libcrypto.so.1.1\", O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \"/lib64/libz.so.1\", O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \"/proc/self/uid_map\", O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \"/sys/kernel/mm/hugepages\", O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \"/dev/null\", O_WRONLY|O_CREAT|O_APPEND|O_SYNC|O_CLOEXEC, 0640) = 3 openat(AT_FDCWD, \"/usr/share/kata-containers/defaults/configuration.toml\", O_RDONLY|O_CLOEXEC) = 5 openat(AT_FDCWD, \"/etc//localtime\", O_RDONLY) = 6 openat(AT_FDCWD, \"/proc/self/uid_map\", O_RDONLY|O_CLOEXEC) = 6 openat(AT_FDCWD, \"/run/vc/sbs\", O_RDONLY|O_CLOEXEC) = 6 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103\", O_RDONLY|O_CLOEXEC) = 7 openat(AT_FDCWD, \"/var/lib/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/config.json\", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/persist.json\", O_RDONLY|O_CLOEXEC) = 8 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103\", O_RDONLY|O_CLOEXEC) = 9 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/6b41cdf410d12e75f99c37c7cadc4c968b1061d388db379aecacf381f46af56a/persist.json\", O_RDONLY|O_CLOEXEC) = 9 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/persist.json\", O_RDONLY|O_CLOEXEC) = 10 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/persist.json\", O_RDONLY|O_CLOEXEC) = 8 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103\", O_RDONLY|O_CLOEXEC) = 9 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/6b41cdf410d12e75f99c37c7cadc4c968b1061d388db379aecacf381f46af56a/persist.json\", O_RDONLY|O_CLOEXEC) = 9 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/persist.json\", O_RDONLY|O_CLOEXEC) = 10 openat(AT_FDCWD, \"/proc/cpuinfo\", O_RDONLY|O_CLOEXEC) = 8 openat(AT_FDCWD, \"/proc/meminfo\", O_RDONLY|O_CLOEXEC) = 8 openat(AT_FDCWD, \"/run/containers/storage/overlay-containers/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/userdata/config.json\", O_RDONLY|O_CLOEXEC) = 8 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/persist.json\", O_RDONLY|O_CLOEXEC) = 8 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103\", O_RDONLY|O_CLOEXEC) = 9 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/6b41cdf410d12e75f99c37c7cadc4c968b1061d388db379aecacf381f46af56a/persist.json\", O_RDONLY|O_CLOEXEC) = 9 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/persist.json\", O_RDONLY|O_CLOEXEC) = 10 openat(AT_FDCWD, \"/run/containers/storage/overlay-containers/6b41cdf410d12e75f99c37c7cadc4c968b1061d388db379aecacf381f46af56a/userdata/config.json\", O_RDONLY|O_CLOEXEC) = 8 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/persist.json\", O_RDONLY|O_CLOEXEC) = 8 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103\", O_RDONLY|O_CLOEXEC) = 9 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/6b41cdf410d12e75f99c37c7cadc4c968b1061d388db379aecacf381f46af56a/persist.json\", O_RDONLY|O_CLOEXEC) = 9 openat(AT_FDCWD, \"/run/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/persist.json\", O_RDONLY|O_CLOEXEC) = 10 Listing VMs / containers and verifying runtime state: [root@openshift-worker-2 ~]# kata-runtime kata-check System is capable of running Kata Containers System can currently create Kata Containers [root@openshift-worker-2 ~]# kata-runtime list ID PID STATUS BUNDLE CREATED OWNER dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103 -1 running /run/containers/storage/overlay-containers/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/userdata 2020-07-26T16:04:25.183088432Z #0 6b41cdf410d12e75f99c37c7cadc4c968b1061d388db379aecacf381f46af56a -1 running /run/containers/storage/overlay-containers/6b41cdf410d12e75f99c37c7cadc4c968b1061d388db379aecacf381f46af56a/userdata 2020-07-26T16:04:25.467048412Z #0 [root@openshift-worker-2 ~]# ps aux | grep dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103 root 28433 0.0 0.0 878648 30720 ? Sl Jul26 0:59 /usr/bin/containerd-shim-kata-v2 -namespace default -address -publish-binary /usr/bin/crio -id dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103 root 28445 0.0 0.0 68424 5664 ? S Jul26 0:00 /usr/libexec/virtiofsd --fd=3 -o source=/run/kata-containers/shared/sandboxes/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/shared -o cache=always --syslog -o no_posix_lock -f root 28450 0.1 0.2 2599904 351472 ? Sl Jul26 1:10 /usr/libexec/qemu-kvm -name sandbox-dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103 -uuid 6f03e56e-efa1-4f0f-9b8f-492cf1824c83 -machine q35,accel=kvm,kernel_irqchip -cpu host -qmp unix:/run/vc/vm/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/qmp.sock,server,nowait -m 2048M,slots=10,maxmem=129853M -device pci-bridge,bus=pcie.0,id=pci-bridge-0,chassis_nr=1,shpc=on,addr=2,romfile= -device virtio-serial-pci,disable-modern=false,id=serial0,romfile= -device virtconsole,chardev=charconsole0,id=console0 -chardev socket,id=charconsole0,path=/run/vc/vm/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/console.sock,server,nowait -device virtio-scsi-pci,id=scsi0,disable-modern=false,romfile= -object rng-random,id=rng0,filename=/dev/urandom -device virtio-rng-pci,rng=rng0,romfile= -device vhost-vsock-pci,disable-modern=false,vhostfd=3,id=vsock-555233078,guest-cid=555233078,romfile= -chardev socket,id=char-1082a328d9d79959,path=/run/vc/vm/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/vhost-fs.sock -device vhost-user-fs-pci,chardev=char-1082a328d9d79959,tag=kataShared,romfile= -netdev tap,id=network-0,vhost=on,vhostfds=4,fds=5 -device driver=virtio-net-pci,netdev=network-0,mac=d6:96:23:1b:02:05,disable-modern=false,mq=on,vectors=4,romfile= -global kvm-pit.lost_tick_policy=discard -vga none -no-user-config -nodefaults -nographic -daemonize -object memory-backend-file,id=dimm1,size=2048M,mem-path=/dev/shm,share=on -numa node,memdev=dimm1 -kernel /usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/vmlinuz -initrd /var/cache/kata-containers/osbuilder-images/4.18.0-193.13.2.el8_2.x86_64/\"rhcos\"-kata-4.18.0-193.13.2.el8_2.x86_64.initrd -append tsc=reliable no_timer_check rcupdate.rcu_expedited=1 i8042.direct=1 i8042.dumbkbd=1 i8042.nopnp=1 i8042.noaux=1 noreplace-smp reboot=k console=hvc0 console=hvc1 iommu=off cryptomgr.notests net.ifnames=0 pci=lastbus=0 quiet panic=1 nr_cpus=40 agent.use_vsock=true scsi_mod.scan=none -pidfile /run/vc/vm/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/pid -smp 1,cores=1,threads=1,sockets=40,maxcpus=40 root 28454 0.0 0.0 4215264 17304 ? Sl Jul26 0:00 /usr/libexec/virtiofsd --fd=3 -o source=/run/kata-containers/shared/sandboxes/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/shared -o cache=always --syslog -o no_posix_lock -f root 1366797 0.0 0.0 12920 2360 pts/0 S+ 10:41 0:00 grep --color=auto dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103 [root@openshift-worker-2 ~]# And to verify the kata-runtime configuration: [root@openshift-worker-2 ~]# kata-runtime --kata-show-default-config-paths /etc/kata-containers/configuration.toml /usr/share/kata-containers/defaults/configuration.toml [root@openshift-worker-2 ~]# ls /etc/kata-containers ls: cannot access '/etc/kata-containers': No such file or directory [root@openshift-worker-2 ~]# kata-runtime kata-env [Meta] Version = \"1.0.24\" [Runtime] Debug = false Trace = false DisableGuestSeccomp = true DisableNewNetNs = false SandboxCgroupOnly = true Path = \"/usr/bin/kata-runtime\" [Runtime.Version] OCI = \"1.0.1-dev\" [Runtime.Version.Version] Semver = \"1.11.1\" Major = 1 Minor = 11 Patch = 1 Commit = \"\" [Runtime.Config] Path = \"/usr/share/kata-containers/defaults/configuration.toml\" [Hypervisor] MachineType = \"q35\" Version = \"QEMU emulator version 4.2.0 (qemu-kvm-4.2.0-19.el8)\\nCopyright (c) 2003-2019 Fabrice Bellard and the QEMU Project developers\" Path = \"/usr/libexec/qemu-kvm\" BlockDeviceDriver = \"virtio-scsi\" EntropySource = \"/dev/urandom\" SharedFS = \"virtio-fs\" VirtioFSDaemon = \"/usr/libexec/virtiofsd\" Msize9p = 8192 MemorySlots = 10 PCIeRootPort = 0 HotplugVFIOOnRootBus = false Debug = false UseVSock = true [Image] Path = \"\" [Kernel] Path = \"/usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/vmlinuz\" Parameters = \"scsi_mod.scan=none\" [Initrd] Path = \"/var/cache/kata-containers/osbuilder-images/4.18.0-193.13.2.el8_2.x86_64/\\\"rhcos\\\"-kata-4.18.0-193.13.2.el8_2.x86_64.initrd\" [Proxy] Type = \"noProxy\" Path = \"\" Debug = false [Proxy.Version] Semver = \"\" Major = 0 Minor = 0 Patch = 0 Commit = \"\" [Shim] Type = \"kataShim\" Path = \"/usr/libexec/kata-containers/kata-shim\" Debug = false [Shim.Version] Semver = \"1.11.1\" Major = 1 Minor = 11 Patch = 1 Commit = \"<<unknown>>\" [Agent] Type = \"kata\" Debug = false Trace = false TraceMode = \"\" TraceType = \"\" [Host] Kernel = \"4.18.0-193.13.2.el8_2.x86_64\" Architecture = \"amd64\" VMContainerCapable = true SupportVSocks = true [Host.Distro] Name = \"Red Hat Enterprise Linux CoreOS\" Version = \"4.5\" [Host.CPU] Vendor = \"GenuineIntel\" Model = \"Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz\" [Netmon] Path = \"/usr/libexec/kata-containers/kata-netmon\" Debug = false Enable = false [Netmon.Version] Semver = \"1.11.1\" Major = 1 Minor = 11 Patch = 1 Commit = \"<<unknown>>\" And for logging: [root@openshift-worker-2 ~]# journalctl -t kata-runtime -- Logs begin at Thu 2020-07-23 14:27:02 UTC, end at Mon 2020-07-27 10:47:03 UTC. -- Jul 27 10:33:25 openshift-worker-2.example.com kata-runtime[1356869]: time=\"2020-07-27T10:33:25.151041313Z\" level=info msg=\"loaded configuration\" arch=amd64 command=list file=/usr/share/kata-containers/defaults/configuration.toml format=TOML name=kata-runtime pid=1356869> Jul 27 10:33:25 openshift-worker-2.example.com kata-runtime[1356869]: time=\"2020-07-27T10:33:25.151232547Z\" level=info msg=\"vsock supported\" arch=amd64 command=list name=kata-runtime pid=1356869 source=katautils Jul 27 10:33:25 openshift-worker-2.example.com kata-runtime[1356869]: time=\"2020-07-27T10:33:25.151287033Z\" level=info msg=\"VSOCK supported, configure to not use proxy\" arch=amd64 command=list name=kata-runtime pid=1356869 source=katautils Jul 27 10:33:25 openshift-worker-2.example.com kata-runtime[1356869]: time=\"2020-07-27T10:33:25.151325928Z\" level=info arch=amd64 arguments=\"\\\"list\\\"\" command=list commit= name=kata-runtime pid=1356869 source=runtime version=1.11.1 Jul 27 10:33:25 openshift-worker-2.example.com kata-runtime[1356869]: time=\"2020-07-27T10:33:25.151435298Z\" level=info msg=\"fetch sandbox\" arch=amd64 command=list name=kata-runtime pid=1356869 source=virtcontainers Jul 27 10:33:25 openshift-worker-2.example.com kata-runtime[1356869]: time=\"2020-07-27T10:33:25.206438572Z\" level=warning msg=\"failed to get sandbox config from old store: open /var/lib/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/config.json: > Jul 27 10:34:13 openshift-worker-2.example.com kata-runtime[1357888]: time=\"2020-07-27T10:34:13.491790968Z\" level=info msg=\"loaded configuration\" arch=amd64 command=state file=/usr/share/kata-containers/defaults/configuration.toml format=TOML name=kata-runtime pid=135788> Jul 27 10:34:13 openshift-worker-2.example.com kata-runtime[1357888]: time=\"2020-07-27T10:34:13.491979419Z\" level=info msg=\"vsock supported\" arch=amd64 command=state name=kata-runtime pid=1357888 source=katautils Jul 27 10:34:13 openshift-worker-2.example.com kata-runtime[1357888]: time=\"2020-07-27T10:34:13.49202632Z\" level=info msg=\"VSOCK supported, configure to not use proxy\" arch=amd64 command=state name=kata-runtime pid=1357888 source=katautils Jul 27 10:34:13 openshift-worker-2.example.com kata-runtime[1357888]: time=\"2020-07-27T10:34:13.492063527Z\" level=info arch=amd64 arguments=\"\\\"state dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103\\\"\" command=state commit= name=kata-runtime pid=1357888 so> Jul 27 10:34:13 openshift-worker-2.example.com kata-runtime[1357888]: time=\"2020-07-27T10:34:13.492166408Z\" level=error msg=\"Container ID (dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103) does not exist\" arch=amd64 command=state container=dc7a95dc3219fa5> Jul 27 10:34:29 openshift-worker-2.example.com kata-runtime[1358196]: time=\"2020-07-27T10:34:29.677420912Z\" level=info msg=\"loaded configuration\" arch=amd64 command=state file=/usr/share/kata-containers/defaults/configuration.toml format=TOML name=kata-runtime pid=135819> Jul 27 10:34:29 openshift-worker-2.example.com kata-runtime[1358196]: time=\"2020-07-27T10:34:29.677612114Z\" level=info msg=\"vsock supported\" arch=amd64 command=state name=kata-runtime pid=1358196 source=katautils Jul 27 10:34:29 openshift-worker-2.example.com kata-runtime[1358196]: time=\"2020-07-27T10:34:29.677660668Z\" level=info msg=\"VSOCK supported, configure to not use proxy\" arch=amd64 command=state name=kata-runtime pid=1358196 source=katautils Jul 27 10:34:29 openshift-worker-2.example.com kata-runtime[1358196]: time=\"2020-07-27T10:34:29.677699242Z\" level=info arch=amd64 arguments=\"\\\"state 6b41cdf410d12e75f99c37c7cadc4c968b1061d388db379aecacf381f46af56a\\\"\" command=state commit= name=kata-runtime pid=1358196 so> Jul 27 10:34:29 openshift-worker-2.example.com kata-runtime[1358196]: time=\"2020-07-27T10:34:29.677801888Z\" level=error msg=\"Container ID (6b41cdf410d12e75f99c37c7cadc4c968b1061d388db379aecacf381f46af56a) does not exist\" arch=amd64 command=state container=6b41cdf410d12e7> Jul 27 10:34:32 openshift-worker-2.example.com kata-runtime[1358313]: time=\"2020-07-27T10:34:32.680058234Z\" level=info msg=\"loaded configuration\" arch=amd64 command=ps file=/usr/share/kata-containers/defaults/configuration.toml format=TOML name=kata-runtime pid=1358313 s> Jul 27 10:34:32 openshift-worker-2.example.com kata-runtime[1358313]: time=\"2020-07-27T10:34:32.680258087Z\" level=info msg=\"vsock supported\" arch=amd64 command=ps name=kata-runtime pid=1358313 source=katautils Jul 27 10:34:32 openshift-worker-2.example.com kata-runtime[1358313]: time=\"2020-07-27T10:34:32.680309596Z\" level=info msg=\"VSOCK supported, configure to not use proxy\" arch=amd64 command=ps name=kata-runtime pid=1358313 source=katautils Jul 27 10:34:32 openshift-worker-2.example.com kata-runtime[1358313]: time=\"2020-07-27T10:34:32.680345567Z\" level=info arch=amd64 arguments=\"\\\"ps\\\"\" command=ps commit= name=kata-runtime pid=1358313 source=runtime version=1.11.1 Jul 27 10:34:32 openshift-worker-2.example.com kata-runtime[1358313]: time=\"2020-07-27T10:34:32.680388502Z\" level=error msg=\"Missing container ID, should at least provide one\" arch=amd64 command=ps name=kata-runtime pid=1358313 source=runtime Jul 27 10:34:35 openshift-worker-2.example.com kata-runtime[1358359]: time=\"2020-07-27T10:34:35.49846456Z\" level=info msg=\"loaded configuration\" arch=amd64 command=list file=/usr/share/kata-containers/defaults/configuration.toml format=TOML name=kata-runtime pid=1358359 > Jul 27 10:34:35 openshift-worker-2.example.com kata-runtime[1358359]: time=\"2020-07-27T10:34:35.498647076Z\" level=info msg=\"vsock supported\" arch=amd64 command=list name=kata-runtime pid=1358359 source=katautils Jul 27 10:34:35 openshift-worker-2.example.com kata-runtime[1358359]: time=\"2020-07-27T10:34:35.498689418Z\" level=info msg=\"VSOCK supported, configure to not use proxy\" arch=amd64 command=list name=kata-runtime pid=1358359 source=katautils Jul 27 10:34:35 openshift-worker-2.example.com kata-runtime[1358359]: time=\"2020-07-27T10:34:35.498725525Z\" level=info arch=amd64 arguments=\"\\\"list\\\"\" command=list commit= name=kata-runtime pid=1358359 source=runtime version=1.11.1 Jul 27 10:34:35 openshift-worker-2.example.com kata-runtime[1358359]: time=\"2020-07-27T10:34:35.498825313Z\" level=info msg=\"fetch sandbox\" arch=amd64 command=list name=kata-runtime pid=1358359 source=virtcontainers Jul 27 10:34:35 openshift-worker-2.example.com kata-runtime[1358359]: time=\"2020-07-27T10:34:35.498906984Z\" level=warning msg=\"failed to get sandbox config from old store: open /var/lib/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/config.json: > Jul 27 10:34:51 openshift-worker-2.example.com kata-runtime[1358694]: time=\"2020-07-27T10:34:51.733126256Z\" level=info msg=\"loaded configuration\" arch=amd64 command=events file=/usr/share/kata-containers/defaults/configuration.toml format=TOML name=kata-runtime pid=13586> Jul 27 10:34:51 openshift-worker-2.example.com kata-runtime[1358694]: time=\"2020-07-27T10:34:51.73331869Z\" level=info msg=\"vsock supported\" arch=amd64 command=events name=kata-runtime pid=1358694 source=katautils Jul 27 10:34:51 openshift-worker-2.example.com kata-runtime[1358694]: time=\"2020-07-27T10:34:51.733376204Z\" level=info msg=\"VSOCK supported, configure to not use proxy\" arch=amd64 command=events name=kata-runtime pid=1358694 source=katautils Jul 27 10:34:51 openshift-worker-2.example.com kata-runtime[1358694]: time=\"2020-07-27T10:34:51.733416598Z\" level=info arch=amd64 arguments=\"\\\"events\\\"\" command=events commit= name=kata-runtime pid=1358694 source=runtime version=1.11.1 Jul 27 10:34:51 openshift-worker-2.example.com kata-runtime[1358694]: time=\"2020-07-27T10:34:51.733458859Z\" level=error msg=\"container id cannot be empty\" arch=amd64 command=events name=kata-runtime pid=1358694 source=runtime Jul 27 10:34:56 openshift-worker-2.example.com kata-runtime[1358808]: time=\"2020-07-27T10:34:56.767961592Z\" level=info msg=\"loaded configuration\" arch=amd64 command=events file=/usr/share/kata-containers/defaults/configuration.toml format=TOML name=kata-runtime pid=13588> Jul 27 10:34:56 openshift-worker-2.example.com kata-runtime[1358808]: time=\"2020-07-27T10:34:56.768145404Z\" level=info msg=\"vsock supported\" arch=amd64 command=events name=kata-runtime pid=1358808 source=katautils Jul 27 10:34:56 openshift-worker-2.example.com kata-runtime[1358808]: time=\"2020-07-27T10:34:56.768196771Z\" level=info msg=\"VSOCK supported, configure to not use proxy\" arch=amd64 command=events name=kata-runtime pid=1358808 source=katautils Jul 27 10:34:56 openshift-worker-2.example.com kata-runtime[1358808]: time=\"2020-07-27T10:34:56.768235257Z\" level=info arch=amd64 arguments=\"\\\"events dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103\\\"\" command=events commit= name=kata-runtime pid=1358808 > Jul 27 10:34:56 openshift-worker-2.example.com kata-runtime[1358808]: time=\"2020-07-27T10:34:56.768401639Z\" level=error msg=\"Container ID (dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103) does not exist\" arch=amd64 command=events container=dc7a95dc3219fa> Jul 27 10:35:16 openshift-worker-2.example.com kata-runtime[1359217]: time=\"2020-07-27T10:35:16.137798572Z\" level=info msg=\"loaded configuration\" arch=amd64 command=list file=/usr/share/kata-containers/defaults/configuration.toml format=TOML name=kata-runtime pid=1359217> Jul 27 10:35:16 openshift-worker-2.example.com kata-runtime[1359217]: time=\"2020-07-27T10:35:16.137979805Z\" level=info msg=\"vsock supported\" arch=amd64 command=list name=kata-runtime pid=1359217 source=katautils Jul 27 10:35:16 openshift-worker-2.example.com kata-runtime[1359217]: time=\"2020-07-27T10:35:16.138022584Z\" level=info msg=\"VSOCK supported, configure to not use proxy\" arch=amd64 command=list name=kata-runtime pid=1359217 source=katautils Jul 27 10:35:16 openshift-worker-2.example.com kata-runtime[1359217]: time=\"2020-07-27T10:35:16.138057713Z\" level=info arch=amd64 arguments=\"\\\"list\\\"\" command=list commit= name=kata-runtime pid=1359217 source=runtime version=1.11.1 Jul 27 10:35:16 openshift-worker-2.example.com kata-runtime[1359217]: time=\"2020-07-27T10:35:16.138166603Z\" level=info msg=\"fetch sandbox\" arch=amd64 command=list name=kata-runtime pid=1359217 source=virtcontainers Jul 27 10:35:16 openshift-worker-2.example.com kata-runtime[1359217]: time=\"2020-07-27T10:35:16.138244704Z\" level=warning msg=\"failed to get sandbox config from old store: open /var/lib/vc/sbs/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/config.json: > Jul 27 10:35:20 openshift-worker-2.example.com kata-runtime[1359329]: time=\"2020-07-27T10:35:20.836297534Z\" level=info msg=\"loaded configuration\" arch=amd64 command=state file=/usr/share/kata-containers/defaults/configuration.toml format=TOML name=kata-runtime pid=135932> Jul 27 10:35:20 openshift-worker-2.example.com kata-runtime[1359329]: time=\"2020-07-27T10:35:20.836493897Z\" level=info msg=\"vsock supported\" arch=amd64 command=state name=kata-runtime pid=1359329 source=katautils [root@openshift-worker-2 ~]# journalctl -t kata -- Logs begin at Thu 2020-07-23 14:27:02 UTC, end at Mon 2020-07-27 10:47:59 UTC. -- Jul 26 16:03:42 openshift-worker-2.example.com kata[27183]: time=\"2020-07-26T16:03:42.108410531Z\" level=error msg=\"post event\" error=\"failed to publish event: exit status 1\" Jul 26 16:03:42 openshift-worker-2.example.com kata[27183]: time=\"2020-07-26T16:03:42.172077874Z\" level=error msg=\"post event\" error=\"failed to publish event: exit status 1\" Jul 26 16:03:46 openshift-worker-2.example.com kata[27183]: time=\"2020-07-26T16:03:46.380966347Z\" level=error msg=\"post event\" error=\"failed to publish event: exit status 1\" Jul 26 16:03:46 openshift-worker-2.example.com kata[27183]: time=\"2020-07-26T16:03:46.444520024Z\" level=error msg=\"post event\" error=\"failed to publish event: exit status 1\" Jul 26 16:03:53 openshift-worker-2.example.com kata[27183]: time=\"2020-07-26T16:03:53.458482832Z\" level=warning msg=\"Could not remove container share dir\" ID=f3bfda981bbafd53ed8f8e1d29a52e9297db0532ba3c564449b43fe79ec31506 error=\"no such file or directory\" sandbox=f3bfda> Jul 26 16:03:53 openshift-worker-2.example.com kata[27183]: time=\"2020-07-26T16:03:53.493291963Z\" level=warning msg=\"failed to cleanup rootfs mount\" error=\"no such file or directory\" Jul 26 16:03:53 openshift-worker-2.example.com kata[27183]: time=\"2020-07-26T16:03:53.525282501Z\" level=error msg=\"post event\" error=\"failed to publish event: exit status 1\" Jul 26 16:03:53 openshift-worker-2.example.com kata[27183]: time=\"2020-07-26T16:03:53.597489614Z\" level=warning msg=\"Could not remove container share dir\" ID=f3bfda981bbafd53ed8f8e1d29a52e9297db0532ba3c564449b43fe79ec31506 error=\"no such file or directory\" sandbox=f3bfda> Jul 26 16:03:53 openshift-worker-2.example.com kata[27183]: time=\"2020-07-26T16:03:53.599932291Z\" level=error msg=\"post event\" error=\"failed to publish event: exit status 1\" Jul 26 16:03:53 openshift-worker-2.example.com kata[27183]: time=\"2020-07-26T16:03:53.631820928Z\" level=error msg=\"Could not read qemu pid file\" ID=f3bfda981bbafd53ed8f8e1d29a52e9297db0532ba3c564449b43fe79ec31506 error=\"open /run/vc/vm/f3bfda981bbafd53ed8f8e1d29a52e9297d> Jul 26 16:03:53 openshift-worker-2.example.com kata[27183]: time=\"2020-07-26T16:03:53.632319378Z\" level=error msg=\"Could not read qemu pid file\" ID=f3bfda981bbafd53ed8f8e1d29a52e9297db0532ba3c564449b43fe79ec31506 error=\"open /run/vc/vm/f3bfda981bbafd53ed8f8e1d29a52e9297d> Jul 26 16:03:53 openshift-worker-2.example.com kata[27183]: time=\"2020-07-26T16:03:53.735993003Z\" level=error msg=\"post event\" error=\"failed to publish event: exit status 1\" Jul 26 16:04:25 openshift-worker-2.example.com kata[28433]: time=\"2020-07-26T16:04:25.25620997Z\" level=error msg=\"post event\" error=\"failed to publish event: exit status 1\" Jul 26 16:04:25 openshift-worker-2.example.com kata[28433]: time=\"2020-07-26T16:04:25.324699487Z\" level=error msg=\"post event\" error=\"failed to publish event: exit status 1\" Jul 26 16:04:25 openshift-worker-2.example.com kata[28433]: time=\"2020-07-26T16:04:25.532722656Z\" level=error msg=\"post event\" error=\"failed to publish event: exit status 1\" Jul 26 16:04:25 openshift-worker-2.example.com kata[28433]: time=\"2020-07-26T16:04:25.59569864Z\" level=error msg=\"post event\" error=\"failed to publish event: exit status 1\" Jul 26 16:04:37 openshift-worker-2.example.com kata[27183]: time=\"2020-07-26T16:04:37.109804951Z\" level=warning msg=\"failed to cleanup rootfs mount\" error=\"no such file or directory\" Jul 27 09:22:48 openshift-worker-2.example.com kata[28433]: time=\"2020-07-27T09:22:48.313239015Z\" level=error msg=\"post event\" error=\"failed to publish event: exit status 1\" Jul 27 09:22:48 openshift-worker-2.example.com kata[28433]: time=\"2020-07-27T09:22:48.461956579Z\" level=error msg=\"post event\" error=\"failed to publish event: exit status 1\" Jul 27 09:23:05 openshift-worker-2.example.com kata[28433]: time=\"2020-07-27T09:23:05.940817814Z\" level=error msg=\"post event\" error=\"failed to publish event: exit status 1\" [root@openshift-worker-2 ~]#","title":"kata-runtime"},{"location":"openshift/kata/#how-does-kata-containers-plug-the-vm-networking","text":"The namespace looks like this: sh-4.4# ip netns exec 317575d4-1143-4aab-b896-5767221326ec ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 3: eth0@if18: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1400 qdisc noqueue state UP group default qlen 1000 link/ether d6:96:23:1b:02:05 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.27.2.4/23 brd 172.27.3.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::bce2:c3ff:feca:293b/64 scope link valid_lft forever preferred_lft forever 4: tap0_kata: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1400 qdisc mq state UNKNOWN group default qlen 1000 link/ether 9a:b6:22:f7:1c:26 brd ff:ff:ff:ff:ff:ff inet6 fe80::98b6:22ff:fef7:1c26/64 scope link valid_lft forever preferred_lft forever And the VM's tap configuration looks like this, using ifindex 4 within the namespace: -netdev tap,id=network-0,vhost=on,vhostfds=4,fds=5 All traffic that arrives on tap0_kata will be mirrored to eth0 and vise versa. In the below case, the VM sends TCP SYNs. These SYNs enter on tap0_kata and are mirrored to eth0: sh-4.4# ip netns exec 317575d4-1143-4aab-b896-5767221326ec tcpdump -nne -i tap0_kata tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on tap0_kata, link-type EN10MB (Ethernet), capture size 262144 bytes 16:49:30.018482 d6:96:23:1b:02:05 > 0a:58:ac:1b:02:01, ethertype IPv4 (0x0800), length 74: 172.27.2.4.44268 > 172.30.0.10.80: Flags [S], seq 2995123630, win 27200, options [mss 1360,sackOK,TS val 3516836732 ecr 0,nop,wscale 7], length 0 16:49:31.038346 d6:96:23:1b:02:05 > 0a:58:ac:1b:02:01, ethertype IPv4 (0x0800), length 74: 172.27.2.4.44270 > 172.30.0.10.80: Flags [S], seq 1381681461, win 27200, options [mss 1360,sackOK,TS val 3516837752 ecr 0,nop,wscale 7], length 0 ^C 2 packets captured 2 packets received by filter 0 packets dropped by kernel sh-4.4# ip netns exec 317575d4-1143-4aab-b896-5767221326ec tcpdump -nne -i eth0 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 16:49:40.197817 d6:96:23:1b:02:05 > 0a:58:ac:1b:02:01, ethertype IPv4 (0x0800), length 74: 172.27.2.4.44288 > 172.30.0.10.80: Flags [S], seq 712892146, win 27200, options [mss 1360,sackOK,TS val 3516846912 ecr 0,nop,wscale 7], length 0 16:49:41.215135 d6:96:23:1b:02:05 > 0a:58:ac:1b:02:01, ethertype IPv4 (0x0800), length 74: 172.27.2.4.44290 > 172.30.0.10.80: Flags [S], seq 3944941969, win 27200, options [mss 1360,sackOK,TS val 3516847929 ecr 0,nop,wscale 7], length 0 ^C 2 packets captured 2 packets received by filter 0 packets dropped by kernel sh-4.4# The kata documentation explains how this works: https://github.com/kata-containers/documentation/blob/master/design/architecture.md#networking Containers will typically live in their own, possibly shared, networking namespace. At some point in a container lifecycle, container engines will set up that namespace to add the container to a network which is isolated from the host network, but which is shared between containers In order to do so, container engines will usually add one end of a virtual ethernet (veth) pair into the container networking namespace. The other end of the veth pair is added to the host networking namespace. This is a very namespace-centric approach as many hypervisors (in particular QEMU) cannot handle veth interfaces. Typically, TAP interfaces are created for VM connectivity. To overcome incompatibility between typical container engines expectations and virtual machines, kata-runtime networking transparently connects veth interfaces with TAP ones using MACVTAP: See the picture in the documentation for further details. eth0 and tap_kata0 are transparently connected via tc rules: # ingress qdisc configured for both interfaces sh-4.4# ip netns exec 317575d4-1143-4aab-b896-5767221326ec tc qdisc ls dev eth0 qdisc noqueue 0: root refcnt 2 qdisc ingress ffff: parent ffff:fff1 ---------------- sh-4.4# ip netns exec 317575d4-1143-4aab-b896-5767221326ec tc qdisc ls dev tap0_kata qdisc mq 0: root qdisc fq_codel 0: parent :1 limit 10240p flows 1024 quantum 1414 target 5.0ms interval 100.0ms memory_limit 32Mb ecn qdisc ingress ffff: parent ffff:fff1 ---------------- # filter configured for both interfaces sh-4.4# ip netns exec 317575d4-1143-4aab-b896-5767221326ec tc filter ls dev eth0 ingress filter protocol all pref 49152 u32 chain 0 filter protocol all pref 49152 u32 chain 0 fh 800: ht divisor 1 filter protocol all pref 49152 u32 chain 0 fh 800::800 order 2048 key ht 800 bkt 0 terminal flowid ??? not_in_hw match 00000000/00000000 at 0 action order 1: mirred (Egress Redirect to device tap0_kata) stolen index 1 ref 1 bind 1 sh-4.4# ip netns exec 317575d4-1143-4aab-b896-5767221326ec tc filter ls dev tap0_kata ingress filter protocol all pref 49152 u32 chain 0 filter protocol all pref 49152 u32 chain 0 fh 800: ht divisor 1 filter protocol all pref 49152 u32 chain 0 fh 800::800 order 2048 key ht 800 bkt 0 terminal flowid ??? not_in_hw match 00000000/00000000 at 0 action order 1: mirred (Egress Redirect to device eth0) stolen index 2 ref 1 bind 1 Further details about this in: * https://gist.github.com/mcastelino/7d85f4164ffdaf48242f9281bb1d0f9b * https://man7.org/linux/man-pages/man8/tc-mirred.8.html","title":"How does kata containers plug the VM networking"},{"location":"openshift/kata/#selecting-the-appropriate-runtime-class-per-container","text":"Kubernetes allows users to choose the appropriate runtimeClass for their pod: https://kubernetes.io/docs/concepts/containers/runtime-class/ [root@openshift-jumpserver-0 ~]# oc explain runtimeclass KIND: RuntimeClass VERSION: node.k8s.io/v1beta1 DESCRIPTION: RuntimeClass defines a class of container runtime supported in the cluster. The RuntimeClass is used to determine which container runtime is used to run all containers in a pod. RuntimeClasses are (currently) manually defined by a user or cluster provisioner, and referenced in the PodSpec. The Kubelet is responsible for resolving the RuntimeClassName reference before running the pod. For more details, see https://git.k8s.io/enhancements/keps/sig-node/runtime-class.md FIELDS: apiVersion <string> APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources handler <string> -required- Handler specifies the underlying runtime and configuration that the CRI implementation will use to handle pods of this class. The possible values are specific to the node & CRI configuration. It is assumed that all handlers are available on every node, and handlers of the same name are equivalent on every node. For example, a handler called \"runc\" might specify that the runc OCI runtime (using native Linux containers) will be used to run the containers in a pod. The Handler must conform to the DNS Label (RFC 1123) requirements, and is immutable. kind <string> Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds metadata <Object> More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata overhead <Object> Overhead represents the resource overhead associated with running a pod for a given RuntimeClass. For more details, see https://git.k8s.io/enhancements/keps/sig-node/20190226-pod-overhead.md This field is alpha-level as of Kubernetes v1.15, and is only honored by servers that enable the PodOverhead feature. scheduling <Object> Scheduling holds the scheduling constraints to ensure that pods running with this RuntimeClass are scheduled to nodes that support it. If scheduling is nil, this RuntimeClass is assumed to be supported by all nodes. The runtime class definition looks like this: [root@openshift-jumpserver-0 ~]# oc get runtimeclass kata-oc -o yaml apiVersion: node.k8s.io/v1beta1 handler: kata-oc kind: RuntimeClass metadata: creationTimestamp: \"2020-07-26T15:48:38Z\" managedFields: - apiVersion: node.k8s.io/v1beta1 fieldsType: FieldsV1 fieldsV1: f:handler: {} f:metadata: f:ownerReferences: .: {} k:{\"uid\":\"e8e76d58-2615-4787-9d4c-7b89dd275131\"}: .: {} f:apiVersion: {} f:blockOwnerDeletion: {} f:controller: {} f:kind: {} f:name: {} f:uid: {} f:scheduling: .: {} f:nodeSelector: .: {} f:kata-containers: {} manager: kata-operator operation: Update time: \"2020-07-26T15:48:38Z\" name: kata-oc ownerReferences: - apiVersion: kataconfiguration.openshift.io/v1alpha1 blockOwnerDeletion: true controller: true kind: KataConfig name: example-kataconfig uid: e8e76d58-2615-4787-9d4c-7b89dd275131 resourceVersion: \"3111152\" selfLink: /apis/node.k8s.io/v1beta1/runtimeclasses/kata-oc uid: bd0fea1a-2d41-43aa-90ba-c9804aabaaa7 scheduling: nodeSelector: kata-containers: \"\" And the scheduling nodeSelector makes sure that kata pods will only be spawned on correctly configured workers: [root@openshift-jumpserver-0 ~]# oc get nodes -l kata-containers= NAME STATUS ROLES AGE VERSION openshift-worker-2.example.com Ready worker 4d19h v1.18.3+b74c5ed On these worker nodes, the kata operator configures the kata-oc handler in crio with: [root@openshift-worker-2 ~]# cat /etc/crio/crio.conf.d/kata-50.conf [crio.runtime] manage_ns_lifecycle = true [crio.runtime.runtimes.kata-oc] runtime_path = \"/usr/bin/containerd-shim-kata-v2\" runtime_type = \"vm\" runtime_root = \"/run/vc\" [crio.runtime.runtimes.runc] runtime_path = \"\" runtime_type = \"oci\" runtime_root = \"/run/runc\" And default runtime: [root@openshift-worker-2 ~]# grep default_runtime /etc/crio/ -R /etc/crio/crio.conf:# default_runtime is the _name_ of the OCI runtime to be used as the default. /etc/crio/crio.conf:default_runtime = \"runc\" [root@openshift-worker-2 ~]# Check the crio implementation for further details, e.g.: https://github.com/cri-o/cri-o/blob/d23a830d170ae177c922fe63611f8d1d3772ef8a/server/sandbox_run_linux.go#L494 // If using kata runtime, the process label should be set to container_kvm_t // Keep in mind that kata does *not* apply any process label to containers within the VM // Note: the requirement here is that the name used for the runtime class has \"kata\" in it // or the runtime_type is set to \"vm\" if runtimeType == libconfig.RuntimeTypeVM || strings.Contains(strings.ToLower(runtimeHandler), \"kata\") { processLabel, err = selinux.SELinuxKVMLabel(processLabel) if err != nil { return nil, err } g.SetProcessSelinuxLabel(processLabel) } For an explanation of the fields, see: https://github.com/cri-o/cri-o/blob/master/docs/crio.conf.5.md#crioruntime-table CRIO.RUNTIME.RUNTIMES TABLE The \"crio.runtime.runtimes\" table defines a list of OCI compatible runtimes. The runtime to use is picked based on the runtime_handler provided by the CRI. If no runtime_handler is provided, the runtime will be picked based on the level of trust of the workload. runtime_path=\"\" Path to the OCI compatible runtime used for this runtime handler. runtime_root=\"\" Root directory used to store runtime data runtime_type=\"oci\" Type of the runtime used for this runtime handler. \"oci\", \"vm\" The runtime class in a pod is then selected with runtimeClassName : [root@openshift-jumpserver-0 ~]# cat katapod.yaml apiVersion: v1 kind: Pod metadata: name: katapod spec: containers: - name: sample-container image: fedora:latest imagePullPolicy: IfNotPresent command: [\"sleep\", \"infinity\"] runtimeClassName: kata-oc","title":"Selecting the appropriate runtime class per container"},{"location":"openshift/kata/#crio-debugging-kata-container-launch","text":"Change log level to debug: [root@openshift-worker-2 ~]# grep log_level /etc/crio/ -R /etc/crio/crio.conf:log_level = \"info\" /etc/crio/crio.conf.d/00-default:log_level = \"debug\" <----------------------- here [root@openshift-worker-2 ~]# systemctl restart crio Then, spawn a new pod, named katapod. The logs will look like this: [root@openshift-worker-2 ~]# journalctl -u crio --since \"7 minutes ago\" | grep kata | cut -b 1-250 Jul 28 10:09:11 openshift-worker-2.example.com crio[2156]: time=\"2020-07-28T10:09:11.335500711Z\" level=warning msg=\"Could not remove container share dir\" ID=dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103 error=\"no such file or direc Jul 28 10:09:11 openshift-worker-2.example.com crio[2156]: time=\"2020-07-28 10:09:11.360578238Z\" level=info msg=\"Stopped container 6b41cdf410d12e75f99c37c7cadc4c968b1061d388db379aecacf381f46af56a: katatest/katapod/sample-container\" id=2a25e783-dc3f-4 Jul 28 10:09:11 openshift-worker-2.example.com crio[2156]: time=\"2020-07-28 10:09:11.361758542Z\" level=info msg=\"Got pod network &{Name:katapod Namespace:katatest ID:dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103 NetNS:/var/run/netn Jul 28 10:09:11 openshift-worker-2.example.com crio[2156]: 2020-07-28T10:09:11Z [verbose] Del: katatest:katapod:a7633f9b-5ce7-4a97-9137-206ce44b58ac:ovn-kubernetes:eth0 {\"cniVersion\":\"0.4.0\",\"dns\":{},\"ipam\":{},\"logFile\":\"/var/log/ovn-kubernetes/ovn-k Jul 28 10:09:11 openshift-worker-2.example.com crio[2156]: time=\"2020-07-28T10:09:11.477479323Z\" level=warning msg=\"Could not remove container share dir\" ID=dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103 error=\"no such file or direc Jul 28 10:09:11 openshift-worker-2.example.com crio[2156]: time=\"2020-07-28 10:09:11.808648343Z\" level=info msg=\"Removed container 6b41cdf410d12e75f99c37c7cadc4c968b1061d388db379aecacf381f46af56a: katatest/katapod/sample-container\" id=573efd66-576b-4 Jul 28 10:10:03 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:03.191724317Z\" level=debug msg=\"Found valid runtime \\\"kata-oc\\\" for runtime_path \\\"/usr/bin/containerd-shim-kata-v2\\\"\" file=\"config/config.go:940\" Jul 28 10:10:03 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:03.225896842Z\" level=warning msg=\"Unable to delete container k8s_katapod_katatest_a7633f9b-5ce7-4a97-9137-206ce44b58ac_0: identifier is not a container\" file=\"server Jul 28 10:10:04 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:04.343526624Z\" level=debug msg=\"Response: &ListImagesResponse{Images:[]*Image{&Image{Id:aa9557bde2f3e1699a119eea4fe53bfef7232628a8c03597816b58a77cd47297,RepoTags:[qu Jul 28 10:10:14 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:14.380904505Z\" level=debug msg=\"Response: &ListImagesResponse{Images:[]*Image{&Image{Id:aa9557bde2f3e1699a119eea4fe53bfef7232628a8c03597816b58a77cd47297,RepoTags:[qu Jul 28 10:10:16 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:16.654826038Z\" level=debug msg=\"Response: &ListImagesResponse{Images:[]*Image{&Image{Id:aa9557bde2f3e1699a119eea4fe53bfef7232628a8c03597816b58a77cd47297,RepoTags:[qu Jul 28 10:10:20 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:20.903822858Z\" level=debug msg=\"Request: &RunPodSandboxRequest{Config:&PodSandboxConfig{Metadata:&PodSandboxMetadata{Name:katapod,Uid:fdc30295-d339-448c-9019-5a7cfc9 Jul 28 10:10:20 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:20.903981027Z\" level=info msg=\"Running pod sandbox: katatest/katapod/POD\" file=\"server/sandbox_run_linux.go:55\" id=06c123b2-8741-42d2-9e49-4a89bd681d1a name=/runtime Jul 28 10:10:20 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:20.917167736Z\" level=info msg=\"Got pod network &{Name:katapod Namespace:katatest ID:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 NetNS:/var/run/n Jul 28 10:10:22 openshift-worker-2.example.com crio[3067623]: 2020-07-28T10:10:22Z [verbose] Add: katatest:katapod:fdc30295-d339-448c-9019-5a7cfc9585c2:(ovn-kubernetes):eth0 {\"cniVersion\":\"0.4.0\",\"interfaces\":[{\"name\":\"9d8a2e790b7836e\",\"mac\":\"ea:0a:b Jul 28 10:10:22 openshift-worker-2.example.com crio[3067623]: I0728 10:10:22.448390 3068254 event.go:221] Event(v1.ObjectReference{Kind:\"Pod\", Namespace:\"katatest\", Name:\"katapod\", UID:\"fdc30295-d339-448c-9019-5a7cfc9585c2\", APIVersion:\"v1\", Resource Jul 28 10:10:22 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:22.462209769Z\" level=info msg=\"Got pod network &{Name:katapod Namespace:katatest ID:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 NetNS:/var/run/n Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.530538309Z\" level=info msg=\"loaded configuration\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 file=/usr/share/kata-containers/defaults/con Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.530688992Z\" level=info msg=\"vsock supported\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 source=katautils Jul 28 10:10:22 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28T10:10:22.530538309Z\" level=info msg=\"loaded configuration\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 file=/usr/share/kata-containers/defaults/con Jul 28 10:10:22 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28T10:10:22.530688992Z\" level=info msg=\"vsock supported\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 source=katautils Jul 28 10:10:22 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28T10:10:22.530725036Z\" level=info msg=\"VSOCK supported, configure to not use proxy\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 source=katautils Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.530725036Z\" level=info msg=\"VSOCK supported, configure to not use proxy\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 source=katautils Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.530785191Z\" level=info msg=\"shm-size detected: 67108864\" source=virtcontainers subsystem=oci Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.533182605Z\" level=info msg=\"adding volume\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 source=virtcontainers subsystem=qemu volume-type=vi Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.534052168Z\" level=info msg=\"Endpoints found after scan\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 endpoints=\"[0xc000592780]\" source=virt Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.534183103Z\" level=info msg=\"Attaching endpoint\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 endpoint-type=virtual hotplug=false source=vir Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.576732075Z\" level=info msg=\"Starting VM\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 sandbox=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92 Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.577702797Z\" level=info msg=\"Adding extra file [0xc0000109f8 0xc000010ad0 0xc000010ab8]\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 source Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.577783973Z\" level=info msg=\"launching /usr/libexec/qemu-kvm with: [-name sandbox-9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 -uuid 59a1fe1e-8 Jul 28 10:10:22 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28T10:10:22.577783973Z\" level=info msg=\"launching /usr/libexec/qemu-kvm with: [-name sandbox-9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 -uuid 59a1fe1e-8 Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.662797076Z\" level=info msg=\"{\\\"QMP\\\": {\\\"version\\\": {\\\"qemu\\\": {\\\"micro\\\": 0, \\\"minor\\\": 2, \\\"major\\\": 4}, \\\"package\\\": \\\"qemu-kvm-4.2.0-19.el8\\\"}, \\\"capabilities Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.663082377Z\" level=info msg=\"QMP details\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 qmp-capabilities=oob qmp-major-version=4 qmp-micro-ve Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.663188824Z\" level=info msg=\"{\\\"execute\\\":\\\"qmp_capabilities\\\"}\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 source=virtcontainers subsyste Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.669410267Z\" level=info msg=\"{\\\"return\\\": {}}\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 source=virtcontainers subsystem=qmp Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.669553509Z\" level=info msg=\"sanner return error: read unix @->/run/vc/vm/9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10/qmp.sock: use of closed Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.669631223Z\" level=info msg=\"VM started\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 sandbox=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.669728645Z\" level=info msg=\"proxy started\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 proxy-pid=3068393 proxy-url=\"vsock://3022826186:102 Jul 28 10:10:22 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28T10:10:22.669728645Z\" level=info msg=\"proxy started\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 proxy-pid=3068393 proxy-url=\"vsock://3022826186:102 Jul 28 10:10:22 openshift-worker-2.example.com kata[3068374]: time=\"2020-07-28T10:10:22.669812928Z\" level=info msg=\"New client\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 proxy=3068393 source=virtcontainers subsystem=kata_age Jul 28 10:10:22 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28T10:10:22.669812928Z\" level=info msg=\"New client\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 proxy=3068393 source=virtcontainers subsystem=kata_age Jul 28 10:10:24 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:24.400551122Z\" level=debug msg=\"Response: &ListImagesResponse{Images:[]*Image{&Image{Id:aa9557bde2f3e1699a119eea4fe53bfef7232628a8c03597816b58a77cd47297,RepoTags:[qu Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28T10:10:25.048437517Z\" level=info msg=\"Using sandbox shm\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 shm-size=67108864 source=virtcontainers subsyst Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28T10:10:25.048700740Z\" level=info msg=\"SELinux label from config will be applied to the hypervisor process, not the VM workload\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb0125 Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:25.343910261Z\" level=info msg=\"Ran pod sandbox 9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 with infra container: katatest/katapod/POD\" file=\"ser Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:25.344647979Z\" level=debug msg=\"Response: &PodSandboxStatusResponse{Status:&PodSandboxStatus{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10,Meta Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:25.349933952Z\" level=debug msg=\"Request: &CreateContainerRequest{PodSandboxId:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10,Config:&ContainerConfi Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:25.350113427Z\" level=info msg=\"Creating container: katatest/katapod/sample-container\" file=\"server/container_create.go:524\" id=ea52ea9c-e9ca-4d9e-8002-da3d3166fe7c n Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:25.362037581Z\" level=debug msg=\"Setting container's log_path = /var/log/pods/katatest_katapod_fdc30295-d339-448c-9019-5a7cfc9585c2, sbox.logdir = sample-container/0. Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28T10:10:25.402397904Z\" level=info msg=\"Using sandbox shm\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 shm-size=67108864 source=virtcontainers subsyst Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28T10:10:25.402627956Z\" level=info msg=\"SELinux label from config will be applied to the hypervisor process, not the VM workload\" ID=9d8a2e790b7836e0e2cebc446996acd9ef0cb0125 Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:25.501604493Z\" level=info msg=\"Created container 8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6cc4: katatest/katapod/sample-container\" file=\"server/co Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:25.507785167Z\" level=info msg=\"Started container 8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6cc4: katatest/katapod/sample-container\" file=\"server/co Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:25.883031245Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:25.884143520Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:25.885476950Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:25.885897712Z\" level=debug msg=\"Response: &PodSandboxStatusResponse{Status:&PodSandboxStatus{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10,Meta Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:25.886321570Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:25 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:25.886665870Z\" level=debug msg=\"Response: &ContainerStatusResponse{Status:&ContainerStatus{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6cc4,Metada Jul 28 10:10:26 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:26.887642507Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:26 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:26.889329001Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:27 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:27.198982924Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:27 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:27.200308230Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:27 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:27.201527681Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:27 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:27.202571813Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:27 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:27.891804708Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:27 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:27.892945930Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:28 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:28.894628148Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:28 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:28.895748056Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:29 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:29.197921214Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:29 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:29.198882077Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:29 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:29.897301640Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:29 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:29.898454054Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:30 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:30.901632289Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:30 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:30.902863561Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:31 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:31.198043649Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:31 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:31.199211238Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:31 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:31.200295569Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:31 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:31.201321049Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:31 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:31.904501224Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:31 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:31.905741478Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:32 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:32.907397641Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:32 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:32.908636589Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:33 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:33.197869270Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:33 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:33.198924362Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:33 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:33.910225026Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:33 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:33.911466751Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:34 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:34.437957994Z\" level=debug msg=\"Response: &ListImagesResponse{Images:[]*Image{&Image{Id:aa9557bde2f3e1699a119eea4fe53bfef7232628a8c03597816b58a77cd47297,RepoTags:[qu Jul 28 10:10:34 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:34.913063858Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:34 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:34.914176789Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:35 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:35.197818930Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:35 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:35.198847621Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:35 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:35.199968036Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:35 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:35.200945896Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:35 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:35.491404744Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:35 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:35.492513493Z\" level=debug msg=\"Response: &ContainerStatusResponse{Status:&ContainerStatus{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6cc4,Metada Jul 28 10:10:35 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:35.915801132Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:35 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:35.918835240Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:36 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:36.920490279Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:36 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:36.921919929Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:37 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:37.197865644Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:37 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:37.198828183Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:37 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:37.199978674Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:37 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:37.201046190Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:37 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:37.923587550Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:37 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:37.924768896Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:38 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:38.926437831Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:38 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:38.927535330Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:39 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:39.197561502Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:39 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:39.198589011Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:39 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:39.929242132Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:39 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:39.930624954Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:40 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:40.932460677Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:40 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:40.933685014Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:41 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:41.198117596Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:41 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:41.199189319Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:41 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:41.200312331Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:41 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:41.201397260Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:41 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:41.935267335Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:41 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:41.936533454Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:42 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:42.938964939Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:42 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:42.940206741Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:43 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:43.198051418Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:43 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:43.199127960Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:43 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:43.941875134Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:43 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:43.943014454Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:44 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:44.477098929Z\" level=debug msg=\"Response: &ListImagesResponse{Images:[]*Image{&Image{Id:aa9557bde2f3e1699a119eea4fe53bfef7232628a8c03597816b58a77cd47297,RepoTags:[qu Jul 28 10:10:44 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:44.944623020Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:44 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:44.945858418Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:45 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:45.198112719Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:45 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:45.199403987Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:45 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:45.201342432Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:45 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:45.202303876Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:45 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:45.497788583Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:45 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:45.498908989Z\" level=debug msg=\"Response: &ContainerStatusResponse{Status:&ContainerStatus{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6cc4,Metada Jul 28 10:10:45 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:45.543711612Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:45 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:45.545123859Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:45 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:45.546270304Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:45 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:45.947309093Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:45 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:45.948492558Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:46 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:46.659075373Z\" level=debug msg=\"Response: &ListImagesResponse{Images:[]*Image{&Image{Id:aa9557bde2f3e1699a119eea4fe53bfef7232628a8c03597816b58a77cd47297,RepoTags:[qu Jul 28 10:10:46 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:46.950714682Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:46 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:46.951824792Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:47 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:47.198041957Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:47 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:47.198993669Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:47 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:47.953643279Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:47 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:47.954838431Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:48 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:48.956694271Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:48 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:48.957802676Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:49 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:49.200294004Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:49 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:49.201241261Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:49 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:49.202321480Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:49 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:49.203692827Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:49 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:49.959650132Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:49 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:49.960738848Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:50 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:50.962629963Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:50 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:50.964061786Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:51 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:51.198210094Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:51 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:51.199165518Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:51 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:51.965826824Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:51 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:51.967090960Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:52 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:52.968922540Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:52 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:52.970185474Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:53 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:53.197881649Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:53 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:53.199482168Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:53 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:53.201311069Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:53 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:53.202378219Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:53 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:53.971897478Z\" level=debug msg=\"Response: &ListPodSandboxResponse{Items:[]*PodSandbox{&PodSandbox{Id:9d8a2e790b7836e0e2cebc446996acd9ef0cb01256b92d348e6fff710ce15f10 Jul 28 10:10:53 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:53.973242764Z\" level=debug msg=\"Response: &ListContainersResponse{Containers:[]*Container{&Container{Id:8c884690004b734b63c1c7a04139f3570cb57846e5c948f4fe28756e017b6 Jul 28 10:10:54 openshift-worker-2.example.com crio[3067623]: time=\"2020-07-28 10:10:54.491767193Z\" level=debug msg=\"Response: &ListImagesResponse{Images:[]*Image{&Image{Id:aa9557bde2f3e1699a119eea4fe53bfef7232628a8c03597816b58a77cd47297,RepoTags:[qu Jul 28 10:10:55 openshift-worker-2.example.com crio[3069288]: time=\"2020-07-28 10:10:55.236031546Z\" level=warning msg=\"Unable to delete container k8s_POD_katapod_katatest_fdc30295-d339-448c-9019-5a7cfc9585c2_0: 1 error occurred:\\n\\t* unlinkat /var/ru Jul 28 10:10:55 openshift-worker-2.example.com crio[3069288]: time=\"2020-07-28 10:10:55.236122983Z\" level=warning msg=\"Unable to delete container k8s_katapod_katatest_fdc30295-d339-448c-9019-5a7cfc9585c2_0: identifier is not a container\" Jul 28 10:10:55 openshift-worker-2.example.com crio[3069288]: time=\"2020-07-28 10:10:55.350843107Z\" level=warning msg=\"Unable to delete container k8s_sample-container_katapod_katatest_fdc30295-d339-448c-9019-5a7cfc9585c2_0: identifier is not a contai Jul 28 10:10:55 openshift-worker-2.example.com crio[3069288]: time=\"2020-07-28 10:10:55.977080875Z\" level=info msg=\"Running pod sandbox: katatest/katapod/POD\" id=00ec3ec8-8a02-4626-a4be-3eee173a95ed name=/runtime.v1alpha2.RuntimeService/RunPodSandbox Jul 28 10:10:55 openshift-worker-2.example.com crio[3069288]: time=\"2020-07-28 10:10:55.989280275Z\" level=info msg=\"Got pod network &{Name:katapod Namespace:katatest ID:a9765d6aa6a1ceb776f4726aab1ce901cbd4d10c2c41c008fe2d4579e36b4c3b NetNS:/var/run/n Jul 28 10:10:58 openshift-worker-2.example.com crio[3069288]: 2020-07-28T10:10:58Z [verbose] Add: katatest:katapod:fdc30295-d339-448c-9019-5a7cfc9585c2:(ovn-kubernetes):eth0 {\"cniVersion\":\"0.4.0\",\"interfaces\":[{\"name\":\"a9765d6aa6a1ceb\",\"mac\":\"72:e1:9 Jul 28 10:10:58 openshift-worker-2.example.com crio[3069288]: I0728 10:10:58.038289 3069588 event.go:221] Event(v1.ObjectReference{Kind:\"Pod\", Namespace:\"katatest\", Name:\"katapod\", UID:\"fdc30295-d339-448c-9019-5a7cfc9585c2\", APIVersion:\"v1\", Resource Jul 28 10:10:58 openshift-worker-2.example.com crio[3069288]: time=\"2020-07-28 10:10:58.051337687Z\" level=info msg=\"Got pod network &{Name:katapod Namespace:katatest ID:a9765d6aa6a1ceb776f4726aab1ce901cbd4d10c2c41c008fe2d4579e36b4c3b NetNS:/var/run/n Jul 28 10:11:00 openshift-worker-2.example.com crio[3069288]: time=\"2020-07-28 10:11:00.933784352Z\" level=info msg=\"Ran pod sandbox a9765d6aa6a1ceb776f4726aab1ce901cbd4d10c2c41c008fe2d4579e36b4c3b with infra container: katatest/katapod/POD\" id=00ec3e Jul 28 10:11:00 openshift-worker-2.example.com crio[3069288]: time=\"2020-07-28 10:11:00.938623997Z\" level=info msg=\"Creating container: katatest/katapod/sample-container\" id=25867b9e-a543-42e7-ba98-fb38ddd938db name=/runtime.v1alpha2.RuntimeService/C Jul 28 10:11:01 openshift-worker-2.example.com crio[3069288]: time=\"2020-07-28 10:11:01.091184087Z\" level=info msg=\"Created container ed470742207b8df91b5b194d07c0f0db2746470accdeeba28d90d37cfc3a8447: katatest/katapod/sample-container\" id=25867b9e-a54 Jul 28 10:11:01 openshift-worker-2.example.com crio[3069288]: time=\"2020-07-28 10:11:01.096407977Z\" level=info msg=\"Started container ed470742207b8df91b5b194d07c0f0db2746470accdeeba28d90d37cfc3a8447: katatest/katapod/sample-container\" id=5ec1b483-d18 Disable debugging after the test as crio logs quite a lot in debug mode.","title":"crio - debugging kata container launch"},{"location":"openshift/kata/#finding-the-kata-containers-socket","text":"WIP [root@openshift-worker-2 /]# ps aux | grep crio | grep -v conmon root 2156 0.5 0.1 4854356 145820 ? Ssl Jul26 13:56 /usr/bin/crio --enable-metrics=true --metrics-port=9537 root 2210 5.5 0.1 5937816 168356 ? Ssl Jul26 139:27 kubelet --config=/etc/kubernetes/kubelet.conf --bootstrap-kubeconfig=/etc/kubernetes/kubeconfig --kubeconfig=/var/lib/kubelet/kubeconfig --container-runtime=remote --container-runtime-endpoint=/var/run/crio/crio.sock --runtime-cgroups=/system.slice/crio.service --node-labels=node-role.kubernetes.io/worker,node.openshift.io/os_id=rhcos --minimum-container-ttl-duration=6m0s --volume-plugin-dir=/etc/kubernetes/kubelet-plugins/volume/exec --cloud-provider= --pod-infra-container-image=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:636ba95d1f2717a876ab2cb1b09ac9b40739db00d26b5cdb9c3a7d6114b75494 --v=4 root 28433 0.0 0.0 878648 31028 ? Sl Jul26 2:14 /usr/bin/containerd-shim-kata-v2 -namespace default -address -publish-binary /usr/bin/crio -id dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103 root 3047886 0.0 0.0 16368 972 pts/0 S+ 09:53 0:00 grep --color=auto crio [root@openshift-worker-2 /]# lsof -nn -p 2156 | grep sock crio 2156 root 10u unix 0xffff92a9d4622400 0t0 52558 /var/run/crio/crio.sock type=STREAM crio 2156 root 22u unix 0xffff92a9d3f10480 0t0 35195 /var/run/crio/crio.sock type=STREAM crio 2156 root 23u unix 0xffff92a9d3f5e300 0t0 35197 /var/run/crio/crio.sock type=STREAM crio 2156 root 36u sock 0,9 0t0 11508128 protocol: TCPv6 crio 2156 root 40u sock 0,9 0t0 13352222 protocol: TCPv6 crio 2156 root 46u unix 0xffff9299d3cb4c80 0t0 19430168 /var/run/crio/crio.sock type=STREAM crio 2156 root 64u unix 0xffff92a9c2873180 0t0 19407526 /var/run/crio/crio.sock type=STREAM [root@openshift-worker-2 /]# ps aux | grep kata root 28433 0.0 0.0 878648 31028 ? Sl Jul26 2:14 /usr/bin/containerd-shim-kata-v2 -namespace default -address -publish-binary /usr/bin/crio -id dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103 root 28445 0.0 0.0 68424 5664 ? S Jul26 0:00 /usr/libexec/virtiofsd --fd=3 -o source=/run/kata-containers/shared/sandboxes/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/shared -o cache=always --syslog -o no_posix_lock -f root 28450 0.1 0.7 2599904 975108 ? Sl Jul26 4:35 /usr/libexec/qemu-kvm -name sandbox-dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103 -uuid 6f03e56e-efa1-4f0f-9b8f-492cf1824c83 -machine q35,accel=kvm,kernel_irqchip -cpu host -qmp unix:/run/vc/vm/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/qmp.sock,server,nowait -m 2048M,slots=10,maxmem=129853M -device pci-bridge,bus=pcie.0,id=pci-bridge-0,chassis_nr=1,shpc=on,addr=2,romfile= -device virtio-serial-pci,disable-modern=false,id=serial0,romfile= -device virtconsole,chardev=charconsole0,id=console0 -chardev socket,id=charconsole0,path=/run/vc/vm/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/console.sock,server,nowait -device virtio-scsi-pci,id=scsi0,disable-modern=false,romfile= -object rng-random,id=rng0,filename=/dev/urandom -device virtio-rng-pci,rng=rng0,romfile= -device vhost-vsock-pci,disable-modern=false,vhostfd=3,id=vsock-555233078,guest-cid=555233078,romfile= -chardev socket,id=char-1082a328d9d79959,path=/run/vc/vm/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/vhost-fs.sock -device vhost-user-fs-pci,chardev=char-1082a328d9d79959,tag=kataShared,romfile= -netdev tap,id=network-0,vhost=on,vhostfds=4,fds=5 -device driver=virtio-net-pci,netdev=network-0,mac=d6:96:23:1b:02:05,disable-modern=false,mq=on,vectors=4,romfile= -global kvm-pit.lost_tick_policy=discard -vga none -no-user-config -nodefaults -nographic -daemonize -object memory-backend-file,id=dimm1,size=2048M,mem-path=/dev/shm,share=on -numa node,memdev=dimm1 -kernel /usr/lib/modules/4.18.0-193.13.2.el8_2.x86_64/vmlinuz -initrd /var/cache/kata-containers/osbuilder-images/4.18.0-193.13.2.el8_2.x86_64/\"rhcos\"-kata-4.18.0-193.13.2.el8_2.x86_64.initrd -append tsc=reliable no_timer_check rcupdate.rcu_expedited=1 i8042.direct=1 i8042.dumbkbd=1 i8042.nopnp=1 i8042.noaux=1 noreplace-smp reboot=k console=hvc0 console=hvc1 iommu=off cryptomgr.notests net.ifnames=0 pci=lastbus=0 quiet panic=1 nr_cpus=40 agent.use_vsock=true scsi_mod.scan=none -pidfile /run/vc/vm/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/pid -smp 1,cores=1,threads=1,sockets=40,maxcpus=40 root 28454 0.0 0.2 4216312 349024 ? Sl Jul26 0:23 /usr/libexec/virtiofsd --fd=3 -o source=/run/kata-containers/shared/sandboxes/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/shared -o cache=always --syslog -o no_posix_lock -f root 3048279 0.0 0.0 16368 972 pts/0 S+ 09:54 0:00 grep --color=auto kata [root@openshift-worker-2 /]# lsof -nn -p 28433 | grep sock container 28433 root 3u sock 0,9 0t0 3589421 protocol: AF_VSOCK container 28433 root 7u unix 0xffff92a9cbf2de80 0t0 3038410 @/containerd-shim/default/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/shim.sock@ type=STREAM container 28433 root 8u unix 0xffff92a9cbf08480 0t0 3131447 @/containerd-shim/default/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/shim.sock@ type=STREAM container 28433 root 17u unix 0xffff9299e58c5a00 0t0 3535834 @/containerd-shim/default/dc7a95dc3219fa58f6b87ac10cb2f5b0a562d8434c2cdc8d4d940d8e763e5103/shim.sock@ type=STREAM [root@openshift-worker-2 /]#","title":"Finding the kata-containers socket"},{"location":"openshift/kata/#connecting-a-debug-console-to-the-kata-vm","text":"https://github.com/kata-containers/documentation/blob/master/Developer-Guide.md#set-up-a-debug-console","title":"Connecting a debug console to the kata VM"},{"location":"openshift/kata/#resources","text":"","title":"Resources"},{"location":"openshift/kata/#kata-operator_1","text":"https://github.com/harche/kata-operator https://www.youtube.com/watch?v=k7rLIU3L94w","title":"Kata operator"},{"location":"openshift/kata/#kata-containers","text":"https://katacontainers.io/ https://github.com/kata-containers/kata-containers/tree/2.0-dev/tools/packaging/kata-deploy https://github.com/kata-containers/kata-containers/blob/2.0-dev/tools/packaging/kata-deploy/scripts/kata-deploy.sh https://github.com/kata-containers/osbuilder https://github.com/kata-containers/runtime https://github.com/kata-containers/documentation/blob/master/design/architecture.md https://github.com/kata-containers/documentation/blob/master/how-to/run-kata-with-k8s.md https://github.com/kata-containers/documentation/blob/master/design/virtualization.md https://katacontainers.io/learn/","title":"Kata containers"},{"location":"openshift/kata/#kubernetes","text":"https://kubernetes.io/docs/concepts/containers/runtime-class/","title":"kubernetes"},{"location":"openshift/kata/#firecracker","text":"https://medium.com/@gokulchandrapr/kata-containers-on-kubernetes-and-kata-firecracker-vmm-support-28abb3a196e7 https://github.com/kata-containers/documentation/wiki/Initial-release-of-Kata-Containers-with-Firecracker-support","title":"Firecracker"},{"location":"openshift/ocp4-infra-nodes-with-machineset-without-worker-label/","text":"OCP 4.5 Infra nodes with MachineSets without worker label How to create Infra nodes with Machinesets but without using the worker label https://docs.openshift.com/container-platform/4.5/machine_management/creating-infrastructure-machinesets.html states how to create infra nodes with MachineSets. However, the \"inconvenience\" is that these infra nodes will show up as \"infra,worker\". How can one not have the \"worker\" label? The worker label is added by kubelet configuration as part of --node-labels=node-role.kubernetes.io/worker,node.openshift.io/os_id=${ID} . So this will require a custom systemd drop-in for the kubelet.service. Step 0: Switch to project openshift-machine-api Switch to the openshift-machine-api project: oc project openshift-machine-api Step 1: Create kubelet configuration drop-in for infra-node To change booting node-labels a MachineConfig needs to be created as the labels are defined when the Kubelet is started with: --node-labels=node-role.kubernetes.io/worker,node.openshift.io/os_id=${ID} \\ In order to create a new kubelet, create 03-infar-kubelet.yaml: cat <<'EOF' > 03-infra-kubelet.yaml apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: labels: machineconfiguration.openshift.io/role: infra name: 03-infra-kubelet spec: config: ignition: config: {} security: tls: {} timeouts: {} version: 2.2.0 systemd: units: - name: kubelet.service dropins: - name: 20-change-label.conf contents: | [Service] ExecStart= ExecStart=/usr/bin/hyperkube \\ kubelet \\ --config=/etc/kubernetes/kubelet.conf \\ --bootstrap-kubeconfig=/etc/kubernetes/kubeconfig \\ --kubeconfig=/var/lib/kubelet/kubeconfig \\ --container-runtime=remote \\ --container-runtime-endpoint=/var/run/crio/crio.sock \\ --runtime-cgroups=/system.slice/crio.service \\ --node-labels=node-role.kubernetes.io/infra,node.openshift.io/os_id=${ID} \\ --minimum-container-ttl-duration=6m0s \\ --volume-plugin-dir=/etc/kubernetes/kubelet-plugins/volume/exec \\ --cloud-provider=aws \\ \\ --pod-infra-container-image=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dc0fe885d41cc4029caa3feacf71343806c81c8123abc91db90dc0e555fa5636 \\ --v=${KUBELET_LOG_LEVEL} EOF $ oc apply -f 03-infra-kubelet.yaml Step 2: Create infra secret Create new user data for infra worker. The following takes the userdata from worker nodes (from the worker-user-data secret) and replaces any occurrence of 'worker' with 'infra': oc get secret -n openshift-machine-api worker-user-data -o yaml | awk '/userData:/ {print $2}' | base64 -d | sed 's/worker/infra/g' | base64 -w0 > userdata.base64 Then, create a new secret: cat << EOF > infra-user-data.yaml apiVersion: v1 data: disableTemplating: dHJ1ZQo= userData: $(cat userdata.base64) kind: Secret metadata: name: infra-user-data namespace: openshift-machine-api type: Opaque EOF oc apply -f infra-user-data.yaml Verify: $ oc get secret -n openshift-machine-api infra-user-data -o yaml | awk '/userData:/ {print $2}' | base64 -d | jq '.ignition.config.append[0].source' \"https://<cluster URL>:22623/config/infra\" Step 3: Create machine configuration for infra Create a machine configuration pool. This machine configuration, for simplicity, will include all machine configurations for worker nodes, as well as for infra nodes. This is why it is important to name the machine configuration for the kubelet 03-infra-kubelet (see above): cat <<'EOF' > infra-mcp.yaml apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfigPool metadata: name: infra spec: machineConfigSelector: matchExpressions: - key: machineconfiguration.openshift.io/role operator: In values: - worker - infra maxUnavailable: 1 nodeSelector: matchLabels: node-role.kubernetes.io/infra: \"\" paused: false EOF Apply: oc apply -f infra-mcp.yaml Step 4: Create a machineset for the new infra node First, create a new machineset: https://docs.openshift.com/container-platform/4.5/machine_management/creating-infrastructure-machinesets.html#machineset-yaml-aws_creating-infrastructure-machinesets Look at an existing machineset for how to fill in some of the blank values: [akaris@linux infra-nodes]$ oc get machinesets -A NAMESPACE NAME DESIRED CURRENT READY AVAILABLE AGE openshift-machine-api akaris2-4kpk4-worker-eu-west-3a 1 1 1 1 112m openshift-machine-api akaris2-4kpk4-worker-eu-west-3b 1 1 1 1 112m openshift-machine-api akaris2-4kpk4-worker-eu-west-3c 1 1 1 1 112m $ oc get machineset -n openshift-machine-api akaris2-4kpk4-worker-eu-west-3a -o yaml > akaris2-4kpk4-infra-eu-west-3a.yaml Clean up the file and modify as instructed in https://docs.openshift.com/container-platform/4.5/machine_management/creating-infrastructure-machinesets.html#machineset-yaml-aws_creating-infrastructure-machinesets The example from the documentation will contain 'worker' in: iamInstanceProfile: id: <infrastructureID>-worker-profile securityGroups: - filters: - name: tag:Name values: - <infrastructureID>-worker-sg userDataSecret: name: worker-user-data Make sure to change the userDataSecret to infra-user-data , contrary to what the documentation says: userDataSecret: name: infra-user-data The MachineSet, after modification - in this case, 3 replicas are requested, right away: $ cat akaris2-4kpk4-infra-eu-west-3a.yaml apiVersion: machine.openshift.io/v1beta1 kind: MachineSet metadata: labels: machine.openshift.io/cluster-api-cluster: akaris2-4kpk4 name: akaris2-4kpk4-infra-eu-west-3a namespace: openshift-machine-api spec: replicas: 3 selector: matchLabels: machine.openshift.io/cluster-api-cluster: akaris2-4kpk4 machine.openshift.io/cluster-api-machineset: akaris2-4kpk4-infra-eu-west-3a template: metadata: labels: machine.openshift.io/cluster-api-cluster: akaris2-4kpk4 machine.openshift.io/cluster-api-machine-role: infra machine.openshift.io/cluster-api-machine-type: infra machine.openshift.io/cluster-api-machineset: akaris2-4kpk4-infra-eu-west-3a spec: metadata: labels: node-role.kubernetes.io/infra: \"\" providerSpec: value: ami: id: ami-(...) apiVersion: awsproviderconfig.openshift.io/v1beta1 blockDevices: - ebs: iops: 0 volumeSize: 120 volumeType: gp2 credentialsSecret: name: aws-cloud-credentials deviceIndex: 0 iamInstanceProfile: id: akaris2-4kpk4-worker-profile instanceType: m5.large kind: AWSMachineProviderConfig metadata: creationTimestamp: null placement: availabilityZone: eu-west-3a region: eu-west-3 publicIp: null securityGroups: - filters: - name: tag:Name values: - akaris2-4kpk4-worker-sg subnet: filters: - name: tag:Name values: - akaris2-4kpk4-private-eu-west-3a tags: - name: kubernetes.io/cluster/akaris2-4kpk4 value: owned userDataSecret: name: infra-user-data Apply the MachineSet: $ oc apply -f akaris2-4kpk4-infra-eu-west-3a.yaml Step 5: Verify Machine creation Monitor machine configuration - right after creation of the MachineSet: [akaris@linux infra-nodes]$ oc get machineconfigpool NAME CONFIG UPDATED UPDATING DEGRADED MACHINECOUNT READYMACHINECOUNT UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT AGE infra rendered-infra-d63625ee2cdc4637cc156a1ebf25e926 True False False 0 0 0 0 4m9s master rendered-master-8d007e81b9ebb00b4042722f2713f1ae True False False 3 3 3 0 116m worker rendered-worker-9d6647aa43752e0eb71a221dd8b7e32e True False False 3 3 3 0 116m [akaris@linux infra-nodes]$ oc get machines NAME PHASE TYPE REGION ZONE AGE akaris2-f74ht-infra-eu-west-3a-fctd5 Provisioned m5.large eu-west-3 eu-west-3a 49s akaris2-f74ht-infra-eu-west-3a-xtpcv Provisioned m5.large eu-west-3 eu-west-3a 49s akaris2-f74ht-infra-eu-west-3a-zmvzh Provisioned m5.large eu-west-3 eu-west-3a 49s akaris2-f74ht-master-0 Running m5.xlarge eu-west-3 eu-west-3a 119m akaris2-f74ht-master-1 Running m5.xlarge eu-west-3 eu-west-3b 119m akaris2-f74ht-master-2 Running m5.xlarge eu-west-3 eu-west-3c 119m akaris2-f74ht-worker-eu-west-3a-4krrd Running m5.large eu-west-3 eu-west-3a 111m akaris2-f74ht-worker-eu-west-3b-59z6n Running m5.large eu-west-3 eu-west-3b 111m akaris2-f74ht-worker-eu-west-3c-chst9 Running m5.large eu-west-3 eu-west-3c 111m [akaris@linux infra-nodes]$ oc get machineset NAME DESIRED CURRENT READY AVAILABLE AGE akaris2-f74ht-infra-eu-west-3a 3 3 58s akaris2-f74ht-worker-eu-west-3a 1 1 1 1 119m akaris2-f74ht-worker-eu-west-3b 1 1 1 1 119m akaris2-f74ht-worker-eu-west-3c 1 1 1 1 119m [akaris@linux infra-nodes]$ oc get nodes NAME STATUS ROLES AGE VERSION ip-10-0-148-174.eu-west-3.compute.internal Ready worker 105m v1.17.1+b83bc57 ip-10-0-159-35.eu-west-3.compute.internal Ready master 117m v1.17.1+b83bc57 ip-10-0-168-64.eu-west-3.compute.internal Ready master 118m v1.17.1+b83bc57 ip-10-0-191-238.eu-west-3.compute.internal Ready worker 105m v1.17.1+b83bc57 ip-10-0-208-80.eu-west-3.compute.internal Ready master 117m v1.17.1+b83bc57 ip-10-0-220-32.eu-west-3.compute.internal Ready worker 105m v1.17.1+b83bc57 Wait: [akaris@linux infra-nodes]$ oc get machineconfigpool NAME CONFIG UPDATED UPDATING DEGRADED MACHINECOUNT READYMACHINECOUNT UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT AGE infra rendered-infra-d63625ee2cdc4637cc156a1ebf25e926 True False False 3 3 3 0 10m master rendered-master-8d007e81b9ebb00b4042722f2713f1ae True False False 3 3 3 0 122m worker rendered-worker-9d6647aa43752e0eb71a221dd8b7e32e True False False 3 3 3 0 122m [akaris@linux infra-nodes]$ oc get machineset NAME DESIRED CURRENT READY AVAILABLE AGE akaris2-f74ht-infra-eu-west-3a 3 3 3 3 7m15s akaris2-f74ht-worker-eu-west-3a 1 1 1 1 125m akaris2-f74ht-worker-eu-west-3b 1 1 1 1 125m akaris2-f74ht-worker-eu-west-3c 1 1 1 1 125m [akaris@linux infra-nodes]$ oc get machines NAME PHASE TYPE REGION ZONE AGE akaris2-f74ht-infra-eu-west-3a-fctd5 Running m5.large eu-west-3 eu-west-3a 7m17s akaris2-f74ht-infra-eu-west-3a-xtpcv Running m5.large eu-west-3 eu-west-3a 7m17s akaris2-f74ht-infra-eu-west-3a-zmvzh Running m5.large eu-west-3 eu-west-3a 7m17s akaris2-f74ht-master-0 Running m5.xlarge eu-west-3 eu-west-3a 125m akaris2-f74ht-master-1 Running m5.xlarge eu-west-3 eu-west-3b 125m akaris2-f74ht-master-2 Running m5.xlarge eu-west-3 eu-west-3c 125m akaris2-f74ht-worker-eu-west-3a-4krrd Running m5.large eu-west-3 eu-west-3a 117m akaris2-f74ht-worker-eu-west-3b-59z6n Running m5.large eu-west-3 eu-west-3b 117m akaris2-f74ht-worker-eu-west-3c-chst9 Running m5.large eu-west-3 eu-west-3c 117m [akaris@linux infra-nodes]$ oc get nodes NAME STATUS ROLES AGE VERSION ip-10-0-128-80.eu-west-3.compute.internal Ready infra 2m39s v1.17.1+b83bc57 ip-10-0-145-35.eu-west-3.compute.internal Ready infra 2m43s v1.17.1+b83bc57 ip-10-0-148-174.eu-west-3.compute.internal Ready worker 111m v1.17.1+b83bc57 ip-10-0-159-154.eu-west-3.compute.internal Ready infra 2m7s v1.17.1+b83bc57 ip-10-0-159-35.eu-west-3.compute.internal Ready master 124m v1.17.1+b83bc57 ip-10-0-168-64.eu-west-3.compute.internal Ready master 124m v1.17.1+b83bc57 ip-10-0-191-238.eu-west-3.compute.internal Ready worker 111m v1.17.1+b83bc57 ip-10-0-208-80.eu-west-3.compute.internal Ready master 124m v1.17.1+b83bc57 ip-10-0-220-32.eu-west-3.compute.internal Ready worker 111m v1.17.1+b83bc57 Step 7: Moving resources to the infra nodes The following steps are from: https://github.com/lbohnsac/OCP4/tree/master/infrastructure-node-setup Also see steps from: https://docs.openshift.com/container-platform/4.5/machine_management/creating-infrastructure-machinesets.html Move the router bits Move the internal routers to the infra nodes oc patch ingresscontrollers.operator.openshift.io default -n openshift-ingress-operator --type=merge \\ --patch '{\"spec\":{\"nodePlacement\":{\"nodeSelector\":{\"matchLabels\":{\"node-role.kubernetes.io/infra\":\"\"}}}}}' Define 3 routers (default are 2!) oc patch ingresscontrollers.operator.openshift.io default -n openshift-ingress-operator --type=merge \\ --patch '{\"spec\":{\"replicas\": 3}}' Move the registry bits Move the internal image registry to the infra nodes oc patch configs.imageregistry.operator.openshift.io cluster --type=merge \\ --patch '{\"spec\":{\"nodeSelector\":{\"node-role.kubernetes.io/infra\": \"\"}}}' Move the registry image pruner (from 4.4) to the infra nodes oc patch imagepruners.imageregistry.operator.openshift.io cluster --type=merge \\ --patch '{\"spec\":{\"nodeSelector\": {\"node-role.kubernetes.io/infra\": \"\"}}}' Move the logging bits Move the Kibana pod to the infras oc patch clusterloggings.logging.openshift.io instance -n openshift-logging --type=merge \\ --patch '{\"spec\":{\"visualization\":{\"kibana\":{\"nodeSelector\":{\"node-role.kubernetes.io/infra\":\"\"}}}}}' Move the curator pod to the infras oc patch clusterloggings.logging.openshift.io instance -n openshift-logging --type=merge \\ --patch '{\"spec\":{\"curation\":{\"curator\":{\"nodeSelector\":{\"node-role.kubernetes.io/infra\":\"\"}}}}}' Move the elasticsearch pods to the infras oc patch clusterloggings.logging.openshift.io instance -n openshift-logging --type=merge \\ --patch '{\"spec\":{\"logStore\":{\"elasticsearch\":{\"nodeSelector\":{\"node-role.kubernetes.io/infra\":\"\"}}}}}' Issues The inconvenience with this is that the ExecStart of the dropin is now \"hardcoded\" and will not be updated / changed by the installer upon upgrade. Step 6: Testing upgrades [akaris@linux infra-nodes]$ oc get clusterversion NAME VERSION AVAILABLE PROGRESSING SINCE STATUS version 4.4.16 True False 123m Cluster version is 4.4.16 [akaris@linux infra-nodes]$ oc adm upgrade Cluster version is 4.4.16 No updates available. You may force an upgrade to a specific release image, but doing so may not be supported and result in downtime or data loss. ############################# # set to 4.5-stable channel ############################# [akaris@linux infra-nodes]$ oc edit clusterversion clusterversion.config.openshift.io/version edited [akaris@linux infra-nodes]$ oc adm upgrade Cluster version is 4.4.16 Updates: VERSION IMAGE 4.5.5 quay.io/openshift-release-dev/ocp-release@sha256:a58573e1c92f5258219022ec104ec254ded0a70370ee8ed2aceea52525639bd4 [akaris@linux infra-nodes]$ oc adm upgrade --to-latest Updating to latest version 4.5.5 The upgrade succeeds: [akaris@linux ipi]$ export KUBECONFIG=/home/akaris/cases/02729138/ipi/install-config/auth/kubeconfig [akaris@linux ipi]$ oc get nodes NAME STATUS ROLES AGE VERSION ip-10-0-128-80.eu-west-3.compute.internal Ready infra 17h v1.18.3+08c38ef ip-10-0-145-35.eu-west-3.compute.internal Ready infra 17h v1.18.3+08c38ef ip-10-0-148-174.eu-west-3.compute.internal Ready worker 19h v1.18.3+08c38ef ip-10-0-159-154.eu-west-3.compute.internal Ready infra 17h v1.18.3+08c38ef ip-10-0-159-35.eu-west-3.compute.internal Ready master 19h v1.18.3+08c38ef ip-10-0-168-64.eu-west-3.compute.internal Ready master 19h v1.18.3+08c38ef ip-10-0-191-238.eu-west-3.compute.internal Ready worker 19h v1.18.3+08c38ef ip-10-0-208-80.eu-west-3.compute.internal Ready master 19h v1.18.3+08c38ef ip-10-0-220-32.eu-west-3.compute.internal Ready worker 19h v1.18.3+08c38ef [akaris@linux ipi]$ oc get clusterversion NAME VERSION AVAILABLE PROGRESSING SINCE STATUS version 4.5.5 True False 16h Cluster version is 4.5.5 [akaris@linux ipi]$ oc get machinesets NAME DESIRED CURRENT READY AVAILABLE AGE akaris2-f74ht-infra-eu-west-3a 3 3 3 3 17h akaris2-f74ht-worker-eu-west-3a 1 1 1 1 19h akaris2-f74ht-worker-eu-west-3b 1 1 1 1 19h akaris2-f74ht-worker-eu-west-3c 1 1 1 1 19h [akaris@linux ipi]$ oc get machines NAME PHASE TYPE REGION ZONE AGE akaris2-f74ht-infra-eu-west-3a-fctd5 Running m5.large eu-west-3 eu-west-3a 17h akaris2-f74ht-infra-eu-west-3a-xtpcv Running m5.large eu-west-3 eu-west-3a 17h akaris2-f74ht-infra-eu-west-3a-zmvzh Running m5.large eu-west-3 eu-west-3a 17h akaris2-f74ht-master-0 Running m5.xlarge eu-west-3 eu-west-3a 19h akaris2-f74ht-master-1 Running m5.xlarge eu-west-3 eu-west-3b 19h akaris2-f74ht-master-2 Running m5.xlarge eu-west-3 eu-west-3c 19h akaris2-f74ht-worker-eu-west-3a-4krrd Running m5.large eu-west-3 eu-west-3a 19h akaris2-f74ht-worker-eu-west-3b-59z6n Running m5.large eu-west-3 eu-west-3b 19h akaris2-f74ht-worker-eu-west-3c-chst9 Running m5.large eu-west-3 eu-west-3c 19h [akaris@linux ipi]$ oc get machineconfigpool NAME CONFIG UPDATED UPDATING DEGRADED MACHINECOUNT READYMACHINECOUNT UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT AGE infra rendered-infra-aa17b9b35abf210691d268dd4c7449d3 True False False 3 3 3 0 17h master rendered-master-2cc3abe4c49c6db04d9456744d0079f9 True False False 3 3 3 0 19h worker rendered-worker-d65740ba72e305c96782a27b8255dd89 True False False 3 3 3 0 19h [akaris@linux ipi]$ oc get machineconfig NAME GENERATEDBYCONTROLLER IGNITIONVERSION AGE 00-master 807abb900cf9976a1baad66eab17c6d76016e7b7 2.2.0 19h 00-worker 807abb900cf9976a1baad66eab17c6d76016e7b7 2.2.0 19h 01-master-container-runtime 807abb900cf9976a1baad66eab17c6d76016e7b7 2.2.0 19h 01-master-kubelet 807abb900cf9976a1baad66eab17c6d76016e7b7 2.2.0 19h 01-worker-container-runtime 807abb900cf9976a1baad66eab17c6d76016e7b7 2.2.0 19h 01-worker-kubelet 807abb900cf9976a1baad66eab17c6d76016e7b7 2.2.0 19h 03-infra-kubelet 2.2.0 17h 99-master-d32a1dcd-1b7b-4999-8919-3bd59c24fdae-registries 807abb900cf9976a1baad66eab17c6d76016e7b7 2.2.0 19h 99-master-ssh 2.2.0 19h 99-worker-f569c8d2-84df-4a5b-846a-9e090b9fb68d-registries 807abb900cf9976a1baad66eab17c6d76016e7b7 2.2.0 19h 99-worker-ssh 2.2.0 19h rendered-infra-aa17b9b35abf210691d268dd4c7449d3 807abb900cf9976a1baad66eab17c6d76016e7b7 2.2.0 16h rendered-infra-d63625ee2cdc4637cc156a1ebf25e926 601c2285f497bf7c73d84737b9977a0e697cb86a 2.2.0 17h rendered-master-2cc3abe4c49c6db04d9456744d0079f9 807abb900cf9976a1baad66eab17c6d76016e7b7 2.2.0 16h rendered-master-8d007e81b9ebb00b4042722f2713f1ae 601c2285f497bf7c73d84737b9977a0e697cb86a 2.2.0 19h rendered-worker-9d6647aa43752e0eb71a221dd8b7e32e 601c2285f497bf7c73d84737b9977a0e697cb86a 2.2.0 19h rendered-worker-d65740ba72e305c96782a27b8255dd89 807abb900cf9976a1baad66eab17c6d76016e7b7 2.2.0 16h [akaris@linux ipi]$ The one problem is the pod version on the infra nodes: [akaris@linux ipi]$ oc get nodes NAME STATUS ROLES AGE VERSION ip-10-0-128-80.eu-west-3.compute.internal Ready infra 17h v1.18.3+08c38ef ip-10-0-145-35.eu-west-3.compute.internal Ready infra 17h v1.18.3+08c38ef ip-10-0-148-174.eu-west-3.compute.internal Ready worker 19h v1.18.3+08c38ef ip-10-0-159-154.eu-west-3.compute.internal Ready infra 17h v1.18.3+08c38ef ip-10-0-159-35.eu-west-3.compute.internal Ready master 19h v1.18.3+08c38ef ip-10-0-168-64.eu-west-3.compute.internal Ready master 19h v1.18.3+08c38ef ip-10-0-191-238.eu-west-3.compute.internal Ready worker 19h v1.18.3+08c38ef ip-10-0-208-80.eu-west-3.compute.internal Ready master 19h v1.18.3+08c38ef ip-10-0-220-32.eu-west-3.compute.internal Ready worker 19h v1.18.3+08c38ef [akaris@linux ipi]$ oc debug node/ip-10-0-128-80.eu-west-3.compute.internal Starting pod/ip-10-0-128-80eu-west-3computeinternal-debug ... To use host binaries, run `chroot /host` Pod IP: 10.0.128.80 If you don't see a command prompt, try pressing enter. sh-4.2# chroot /host sh-4.4# cat /etc/redhat-release Red Hat Enterprise Linux CoreOS release 4.5 sh-4.4# ps aux | grep kubelet root 1422 3.3 2.0 1461696 161600 ? Ssl Aug19 33:46 kubelet --config=/etc/kubernetes/kubelet.conf --bootstrap-kubeconfig=/etc/kubernetes/kubeconfig --kubeconfig=/var/lib/kubelet/kubeconfig --container-runtime=remote --container-runtime-endpoint=/var/run/crio/crio.sock --runtime-cgroups=/system.slice/crio.service --node-labels=node-role.kubernetes.io/infra,node.openshift.io/os_id=rhcos --minimum-container-ttl-duration=6m0s --volume-plugin-dir=/etc/kubernetes/kubelet-plugins/volume/exec --cloud-provider=aws --pod-infra-container-image=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dc0fe885d41cc4029caa3feacf71343806c81c8123abc91db90dc0e555fa5636 --v=4 root 1232029 0.0 0.0 9180 1032 ? S+ 10:02 0:00 grep kubelet sh-4.4# sh-4.4# grep pause /etc/crio/ -R /etc/crio/seccomp.json: \"pause\", /etc/crio/crio.conf.d/00-default:pause_image = \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6e8bcc9c91267283f8b0edf1af0cb0d310c54d4f73905d9bcfb2a103a664fcb0\" /etc/crio/crio.conf.d/00-default:pause_image_auth_file = \"/var/lib/kubelet/config.json\" /etc/crio/crio.conf.d/00-default:pause_command = \"/usr/bin/pod\" sh-4.4# Compare that to the version on the worker nodes: [akaris@linux ipi]$ oc get nodes NAME STATUS ROLES AGE VERSION ip-10-0-128-80.eu-west-3.compute.internal Ready infra 17h v1.18.3+08c38ef ip-10-0-145-35.eu-west-3.compute.internal Ready infra 17h v1.18.3+08c38ef ip-10-0-148-174.eu-west-3.compute.internal Ready worker 19h v1.18.3+08c38ef ip-10-0-159-154.eu-west-3.compute.internal Ready infra 17h v1.18.3+08c38ef ip-10-0-159-35.eu-west-3.compute.internal Ready master 19h v1.18.3+08c38ef ip-10-0-168-64.eu-west-3.compute.internal Ready master 19h v1.18.3+08c38ef ip-10-0-191-238.eu-west-3.compute.internal Ready worker 19h v1.18.3+08c38ef ip-10-0-208-80.eu-west-3.compute.internal Ready master 19h v1.18.3+08c38ef ip-10-0-220-32.eu-west-3.compute.internal Ready worker 19h v1.18.3+08c38ef [akaris@linux ipi]$ oc debug node/ip-10-0-148-174.eu-west-3.compute.internal Starting pod/ip-10-0-148-174eu-west-3computeinternal-debug ... To use host binaries, run `chroot /host` Pod IP: 10.0.148.174 If you don't see a command prompt, try pressing enter. sh-4.2# chroot /host sh-4.4# cat /etc/redhat-release Red Hat Enterprise Linux CoreOS release 4.5 sh-4.4# ps aux | grep kubelet root 1421 5.5 2.5 1454268 196984 ? Ssl Aug19 56:01 kubelet --config=/etc/kubernetes/kubelet.conf --bootstrap-kubeconfig=/etc/kubernetes/kubeconfig --kubeconfig=/var/lib/kubelet/kubeconfig --container-runtime=remote --container-runtime-endpoint=/var/run/crio/crio.sock --runtime-cgroups=/system.slice/crio.service --node-labels=node-role.kubernetes.io/worker,node.openshift.io/os_id=rhcos --minimum-container-ttl-duration=6m0s --volume-plugin-dir=/etc/kubernetes/kubelet-plugins/volume/exec --cloud-provider=aws --pod-infra-container-image=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6e8bcc9c91267283f8b0edf1af0cb0d310c54d4f73905d9bcfb2a103a664fcb0 --v=4 root 2138381 0.0 0.0 9180 1060 ? S+ 10:02 0:00 grep kubelet sh-4.4# grep pause /etc/crio/ -R /etc/crio/seccomp.json: \"pause\", /etc/crio/crio.conf.d/00-default:pause_image = \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6e8bcc9c91267283f8b0edf1af0cb0d310c54d4f73905d9bcfb2a103a664fcb0\" /etc/crio/crio.conf.d/00-default:pause_image_auth_file = \"/var/lib/kubelet/config.json\" /etc/crio/crio.conf.d/00-default:pause_command = \"/usr/bin/pod\" That --pod-infra-container-image= pod version is for the pause container; although as you can see above, the pause_image in crio.conf.d is always updated: https://stackoverflow.com/questions/46630377/what-is-pod-infra-container-image-meant-for The pause container, which image the --pod-infra-container flag selects, is used so that multiple containers can be launched in a pod, while sharing resources. It mostly does nothing, and unless you have a very good reason to replace it with something custom, you shouldn't. It mostly invokes the pause system call (hence its name) but it also performs the important function of having PID 1 and making sure no zombie processes are kept around. An extremely complete article on the subject can be found here, from where I also shamelessly stole the following picture which illustrates where the pause container lives: According to https://github.com/kubernetes/kubernetes/pull/70603 (although that comment is from 2018): The kubelet allows you to set --pod-infra-container-image (also called PodSandboxImage in the kubelet config), which can be a custom location to the \"pause\" image in the case of Docker. Other CRIs are not supported. And according to https://github.com/kubernetes/kubernetes/issues/86081 (from November 2019): Kubelet flag \"pod-infra-container-image\" is invalid when using CRI-O And according to the kubelet help, this only affects docker and doesn't affect us: sh-4.4# kubelet --help | grep infra --pod-infra-container-image string The image whose network/ipc namespaces containers in each pod will use. This docker-specific flag only works when container-runtime is set to docker. (default \"k8s.gcr.io/pause:3.2\") As an additional step just to bring the infra nodes in line with the workers and the rest of the cluster, we could update that by inspecting the current worker kubelet configuration and then modify this post-upgrade: [akaris@linux ipi]$ oc get machineconfig 01-worker-kubelet -o yaml | grep pod-infra-container-image --pod-infra-container-image=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6e8bcc9c91267283f8b0edf1af0cb0d310c54d4f73905d9bcfb2a103a664fcb0 \\ [akaris@linux ipi]$ oc get machineconfig 03-infra-kubelet -o yaml | grep pod-infra-container-image | grep -v apiVersion --pod-infra-container-image=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dc0fe885d41cc4029caa3feacf71343806c81c8123abc91db90dc0e555fa5636 \\ [akaris@linux ipi]$ oc edit machineconfig 03-infra-kubelet machineconfig.machineconfiguration.openshift.io/03-infra-kubelet edited [akaris@linux ipi]$ oc get machineconfig 03-infra-kubelet -o yaml | grep pod-infra-container-image | grep -v apiVersion --pod-infra-container-image=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6e8bcc9c91267283f8b0edf1af0cb0d310c54d4f73905d9bcfb2a103a664fcb0 \\ Which will then trigger a restart of the infra nodes: [akaris@linux ipi]$ oc get machineconfigpool NAME CONFIG UPDATED UPDATING DEGRADED MACHINECOUNT READYMACHINECOUNT UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT AGE infra rendered-infra-aa17b9b35abf210691d268dd4c7449d3 False True False 3 0 0 0 17h master rendered-master-2cc3abe4c49c6db04d9456744d0079f9 True False False 3 3 3 0 19h worker rendered-worker-d65740ba72e305c96782a27b8255dd89 True False False 3 3 3 0 19h [akaris@linux ipi]$ oc get machineconfig | grep infra 03-infra-kubelet 2.2.0 17h rendered-infra-aa17b9b35abf210691d268dd4c7449d3 807abb900cf9976a1baad66eab17c6d76016e7b7 2.2.0 17h rendered-infra-d63625ee2cdc4637cc156a1ebf25e926 601c2285f497bf7c73d84737b9977a0e697cb86a 2.2.0 17h rendered-infra-f3df01ebf7429a29582f3c46132fcf88 807abb900cf9976a1baad66eab17c6d76016e7b7 2.2.0 44s [akaris@linux ipi]$ And then eventually: [akaris@linux ipi]$ oc get nodes NAME STATUS ROLES AGE VERSION ip-10-0-128-80.eu-west-3.compute.internal Ready infra 17h v1.18.3+08c38ef ip-10-0-145-35.eu-west-3.compute.internal Ready infra 17h v1.18.3+08c38ef ip-10-0-148-174.eu-west-3.compute.internal Ready worker 19h v1.18.3+08c38ef ip-10-0-159-154.eu-west-3.compute.internal Ready infra 17h v1.18.3+08c38ef ip-10-0-159-35.eu-west-3.compute.internal Ready master 19h v1.18.3+08c38ef ip-10-0-168-64.eu-west-3.compute.internal Ready master 19h v1.18.3+08c38ef ip-10-0-191-238.eu-west-3.compute.internal Ready worker 19h v1.18.3+08c38ef ip-10-0-208-80.eu-west-3.compute.internal Ready master 19h v1.18.3+08c38ef ip-10-0-220-32.eu-west-3.compute.internal Ready worker 19h v1.18.3+08c38ef [akaris@linux ipi]$ oc get machineconfigpool NAME CONFIG UPDATED UPDATING DEGRADED MACHINECOUNT READYMACHINECOUNT UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT AGE infra rendered-infra-f3df01ebf7429a29582f3c46132fcf88 True False False 3 3 3 0 18h master rendered-master-2cc3abe4c49c6db04d9456744d0079f9 True False False 3 3 3 0 19h worker rendered-worker-d65740ba72e305c96782a27b8255dd89 True False False 3 3 3 0 19h [akaris@linux ipi]$ oc get machines NAME PHASE TYPE REGION ZONE AGE akaris2-f74ht-infra-eu-west-3a-fctd5 Running m5.large eu-west-3 eu-west-3a 18h akaris2-f74ht-infra-eu-west-3a-xtpcv Running m5.large eu-west-3 eu-west-3a 18h akaris2-f74ht-infra-eu-west-3a-zmvzh Running m5.large eu-west-3 eu-west-3a 18h akaris2-f74ht-master-0 Running m5.xlarge eu-west-3 eu-west-3a 19h akaris2-f74ht-master-1 Running m5.xlarge eu-west-3 eu-west-3b 19h akaris2-f74ht-master-2 Running m5.xlarge eu-west-3 eu-west-3c 19h akaris2-f74ht-worker-eu-west-3a-4krrd Running m5.large eu-west-3 eu-west-3a 19h akaris2-f74ht-worker-eu-west-3b-59z6n Running m5.large eu-west-3 eu-west-3b 19h akaris2-f74ht-worker-eu-west-3c-chst9 Running m5.large eu-west-3 eu-west-3c 19h [akaris@linux ipi]$ oc debug node/ip-10-0-128-80.eu-west-3.compute.internal Starting pod/ip-10-0-128-80eu-west-3computeinternal-debug ... To use host binaries, run `chroot /host` Pod IP: 10.0.128.80 If you don't see a command prompt, try pressing enter. sh-4.2# chroot /host sh-4.4# ps aux | grep kubelet root 1411 3.5 1.8 1484684 147368 ? Ssl 10:16 0:23 kubelet --config=/etc/kubernetes/kubelet.conf --bootstrap-kubeconfig=/etc/kubernetes/kubeconfig --kubeconfig=/var/lib/kubelet/kubeconfig --container-runtime=remote --container-runtime-endpoint=/var/run/crio/crio.sock --runtime-cgroups=/system.slice/crio.service --node-labels=node-role.kubernetes.io/infra,node.openshift.io/os_id=rhcos --minimum-container-ttl-duration=6m0s --volume-plugin-dir=/etc/kubernetes/kubelet-plugins/volume/exec --cloud-provider=aws --pod-infra-container-image=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6e8bcc9c91267283f8b0edf1af0cb0d310c54d4f73905d9bcfb2a103a664fcb0 --v=4 root 17471 0.0 0.0 9180 1052 ? S+ 10:27 0:00 grep kubelet","title":"Infra nodes with MachineSets without worker label"},{"location":"openshift/ocp4-infra-nodes-with-machineset-without-worker-label/#ocp-45-infra-nodes-with-machinesets-without-worker-label","text":"","title":"OCP 4.5 Infra nodes with MachineSets without worker label"},{"location":"openshift/ocp4-infra-nodes-with-machineset-without-worker-label/#how-to-create-infra-nodes-with-machinesets-but-without-using-the-worker-label","text":"https://docs.openshift.com/container-platform/4.5/machine_management/creating-infrastructure-machinesets.html states how to create infra nodes with MachineSets. However, the \"inconvenience\" is that these infra nodes will show up as \"infra,worker\". How can one not have the \"worker\" label? The worker label is added by kubelet configuration as part of --node-labels=node-role.kubernetes.io/worker,node.openshift.io/os_id=${ID} . So this will require a custom systemd drop-in for the kubelet.service.","title":"How to create Infra nodes with Machinesets but without using the worker label"},{"location":"openshift/ocp4-infra-nodes-with-machineset-without-worker-label/#step-0-switch-to-project-openshift-machine-api","text":"Switch to the openshift-machine-api project: oc project openshift-machine-api","title":"Step 0: Switch to project openshift-machine-api"},{"location":"openshift/ocp4-infra-nodes-with-machineset-without-worker-label/#step-1-create-kubelet-configuration-drop-in-for-infra-node","text":"To change booting node-labels a MachineConfig needs to be created as the labels are defined when the Kubelet is started with: --node-labels=node-role.kubernetes.io/worker,node.openshift.io/os_id=${ID} \\ In order to create a new kubelet, create 03-infar-kubelet.yaml: cat <<'EOF' > 03-infra-kubelet.yaml apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: labels: machineconfiguration.openshift.io/role: infra name: 03-infra-kubelet spec: config: ignition: config: {} security: tls: {} timeouts: {} version: 2.2.0 systemd: units: - name: kubelet.service dropins: - name: 20-change-label.conf contents: | [Service] ExecStart= ExecStart=/usr/bin/hyperkube \\ kubelet \\ --config=/etc/kubernetes/kubelet.conf \\ --bootstrap-kubeconfig=/etc/kubernetes/kubeconfig \\ --kubeconfig=/var/lib/kubelet/kubeconfig \\ --container-runtime=remote \\ --container-runtime-endpoint=/var/run/crio/crio.sock \\ --runtime-cgroups=/system.slice/crio.service \\ --node-labels=node-role.kubernetes.io/infra,node.openshift.io/os_id=${ID} \\ --minimum-container-ttl-duration=6m0s \\ --volume-plugin-dir=/etc/kubernetes/kubelet-plugins/volume/exec \\ --cloud-provider=aws \\ \\ --pod-infra-container-image=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dc0fe885d41cc4029caa3feacf71343806c81c8123abc91db90dc0e555fa5636 \\ --v=${KUBELET_LOG_LEVEL} EOF $ oc apply -f 03-infra-kubelet.yaml","title":"Step 1: Create kubelet configuration drop-in for infra-node"},{"location":"openshift/ocp4-infra-nodes-with-machineset-without-worker-label/#step-2-create-infra-secret","text":"Create new user data for infra worker. The following takes the userdata from worker nodes (from the worker-user-data secret) and replaces any occurrence of 'worker' with 'infra': oc get secret -n openshift-machine-api worker-user-data -o yaml | awk '/userData:/ {print $2}' | base64 -d | sed 's/worker/infra/g' | base64 -w0 > userdata.base64 Then, create a new secret: cat << EOF > infra-user-data.yaml apiVersion: v1 data: disableTemplating: dHJ1ZQo= userData: $(cat userdata.base64) kind: Secret metadata: name: infra-user-data namespace: openshift-machine-api type: Opaque EOF oc apply -f infra-user-data.yaml Verify: $ oc get secret -n openshift-machine-api infra-user-data -o yaml | awk '/userData:/ {print $2}' | base64 -d | jq '.ignition.config.append[0].source' \"https://<cluster URL>:22623/config/infra\"","title":"Step 2: Create infra secret"},{"location":"openshift/ocp4-infra-nodes-with-machineset-without-worker-label/#step-3-create-machine-configuration-for-infra","text":"Create a machine configuration pool. This machine configuration, for simplicity, will include all machine configurations for worker nodes, as well as for infra nodes. This is why it is important to name the machine configuration for the kubelet 03-infra-kubelet (see above): cat <<'EOF' > infra-mcp.yaml apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfigPool metadata: name: infra spec: machineConfigSelector: matchExpressions: - key: machineconfiguration.openshift.io/role operator: In values: - worker - infra maxUnavailable: 1 nodeSelector: matchLabels: node-role.kubernetes.io/infra: \"\" paused: false EOF Apply: oc apply -f infra-mcp.yaml","title":"Step 3: Create machine configuration for infra"},{"location":"openshift/ocp4-infra-nodes-with-machineset-without-worker-label/#step-4-create-a-machineset-for-the-new-infra-node","text":"First, create a new machineset: https://docs.openshift.com/container-platform/4.5/machine_management/creating-infrastructure-machinesets.html#machineset-yaml-aws_creating-infrastructure-machinesets Look at an existing machineset for how to fill in some of the blank values: [akaris@linux infra-nodes]$ oc get machinesets -A NAMESPACE NAME DESIRED CURRENT READY AVAILABLE AGE openshift-machine-api akaris2-4kpk4-worker-eu-west-3a 1 1 1 1 112m openshift-machine-api akaris2-4kpk4-worker-eu-west-3b 1 1 1 1 112m openshift-machine-api akaris2-4kpk4-worker-eu-west-3c 1 1 1 1 112m $ oc get machineset -n openshift-machine-api akaris2-4kpk4-worker-eu-west-3a -o yaml > akaris2-4kpk4-infra-eu-west-3a.yaml Clean up the file and modify as instructed in https://docs.openshift.com/container-platform/4.5/machine_management/creating-infrastructure-machinesets.html#machineset-yaml-aws_creating-infrastructure-machinesets The example from the documentation will contain 'worker' in: iamInstanceProfile: id: <infrastructureID>-worker-profile securityGroups: - filters: - name: tag:Name values: - <infrastructureID>-worker-sg userDataSecret: name: worker-user-data Make sure to change the userDataSecret to infra-user-data , contrary to what the documentation says: userDataSecret: name: infra-user-data The MachineSet, after modification - in this case, 3 replicas are requested, right away: $ cat akaris2-4kpk4-infra-eu-west-3a.yaml apiVersion: machine.openshift.io/v1beta1 kind: MachineSet metadata: labels: machine.openshift.io/cluster-api-cluster: akaris2-4kpk4 name: akaris2-4kpk4-infra-eu-west-3a namespace: openshift-machine-api spec: replicas: 3 selector: matchLabels: machine.openshift.io/cluster-api-cluster: akaris2-4kpk4 machine.openshift.io/cluster-api-machineset: akaris2-4kpk4-infra-eu-west-3a template: metadata: labels: machine.openshift.io/cluster-api-cluster: akaris2-4kpk4 machine.openshift.io/cluster-api-machine-role: infra machine.openshift.io/cluster-api-machine-type: infra machine.openshift.io/cluster-api-machineset: akaris2-4kpk4-infra-eu-west-3a spec: metadata: labels: node-role.kubernetes.io/infra: \"\" providerSpec: value: ami: id: ami-(...) apiVersion: awsproviderconfig.openshift.io/v1beta1 blockDevices: - ebs: iops: 0 volumeSize: 120 volumeType: gp2 credentialsSecret: name: aws-cloud-credentials deviceIndex: 0 iamInstanceProfile: id: akaris2-4kpk4-worker-profile instanceType: m5.large kind: AWSMachineProviderConfig metadata: creationTimestamp: null placement: availabilityZone: eu-west-3a region: eu-west-3 publicIp: null securityGroups: - filters: - name: tag:Name values: - akaris2-4kpk4-worker-sg subnet: filters: - name: tag:Name values: - akaris2-4kpk4-private-eu-west-3a tags: - name: kubernetes.io/cluster/akaris2-4kpk4 value: owned userDataSecret: name: infra-user-data Apply the MachineSet: $ oc apply -f akaris2-4kpk4-infra-eu-west-3a.yaml","title":"Step 4: Create a machineset for the new infra node"},{"location":"openshift/ocp4-infra-nodes-with-machineset-without-worker-label/#step-5-verify-machine-creation","text":"Monitor machine configuration - right after creation of the MachineSet: [akaris@linux infra-nodes]$ oc get machineconfigpool NAME CONFIG UPDATED UPDATING DEGRADED MACHINECOUNT READYMACHINECOUNT UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT AGE infra rendered-infra-d63625ee2cdc4637cc156a1ebf25e926 True False False 0 0 0 0 4m9s master rendered-master-8d007e81b9ebb00b4042722f2713f1ae True False False 3 3 3 0 116m worker rendered-worker-9d6647aa43752e0eb71a221dd8b7e32e True False False 3 3 3 0 116m [akaris@linux infra-nodes]$ oc get machines NAME PHASE TYPE REGION ZONE AGE akaris2-f74ht-infra-eu-west-3a-fctd5 Provisioned m5.large eu-west-3 eu-west-3a 49s akaris2-f74ht-infra-eu-west-3a-xtpcv Provisioned m5.large eu-west-3 eu-west-3a 49s akaris2-f74ht-infra-eu-west-3a-zmvzh Provisioned m5.large eu-west-3 eu-west-3a 49s akaris2-f74ht-master-0 Running m5.xlarge eu-west-3 eu-west-3a 119m akaris2-f74ht-master-1 Running m5.xlarge eu-west-3 eu-west-3b 119m akaris2-f74ht-master-2 Running m5.xlarge eu-west-3 eu-west-3c 119m akaris2-f74ht-worker-eu-west-3a-4krrd Running m5.large eu-west-3 eu-west-3a 111m akaris2-f74ht-worker-eu-west-3b-59z6n Running m5.large eu-west-3 eu-west-3b 111m akaris2-f74ht-worker-eu-west-3c-chst9 Running m5.large eu-west-3 eu-west-3c 111m [akaris@linux infra-nodes]$ oc get machineset NAME DESIRED CURRENT READY AVAILABLE AGE akaris2-f74ht-infra-eu-west-3a 3 3 58s akaris2-f74ht-worker-eu-west-3a 1 1 1 1 119m akaris2-f74ht-worker-eu-west-3b 1 1 1 1 119m akaris2-f74ht-worker-eu-west-3c 1 1 1 1 119m [akaris@linux infra-nodes]$ oc get nodes NAME STATUS ROLES AGE VERSION ip-10-0-148-174.eu-west-3.compute.internal Ready worker 105m v1.17.1+b83bc57 ip-10-0-159-35.eu-west-3.compute.internal Ready master 117m v1.17.1+b83bc57 ip-10-0-168-64.eu-west-3.compute.internal Ready master 118m v1.17.1+b83bc57 ip-10-0-191-238.eu-west-3.compute.internal Ready worker 105m v1.17.1+b83bc57 ip-10-0-208-80.eu-west-3.compute.internal Ready master 117m v1.17.1+b83bc57 ip-10-0-220-32.eu-west-3.compute.internal Ready worker 105m v1.17.1+b83bc57 Wait: [akaris@linux infra-nodes]$ oc get machineconfigpool NAME CONFIG UPDATED UPDATING DEGRADED MACHINECOUNT READYMACHINECOUNT UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT AGE infra rendered-infra-d63625ee2cdc4637cc156a1ebf25e926 True False False 3 3 3 0 10m master rendered-master-8d007e81b9ebb00b4042722f2713f1ae True False False 3 3 3 0 122m worker rendered-worker-9d6647aa43752e0eb71a221dd8b7e32e True False False 3 3 3 0 122m [akaris@linux infra-nodes]$ oc get machineset NAME DESIRED CURRENT READY AVAILABLE AGE akaris2-f74ht-infra-eu-west-3a 3 3 3 3 7m15s akaris2-f74ht-worker-eu-west-3a 1 1 1 1 125m akaris2-f74ht-worker-eu-west-3b 1 1 1 1 125m akaris2-f74ht-worker-eu-west-3c 1 1 1 1 125m [akaris@linux infra-nodes]$ oc get machines NAME PHASE TYPE REGION ZONE AGE akaris2-f74ht-infra-eu-west-3a-fctd5 Running m5.large eu-west-3 eu-west-3a 7m17s akaris2-f74ht-infra-eu-west-3a-xtpcv Running m5.large eu-west-3 eu-west-3a 7m17s akaris2-f74ht-infra-eu-west-3a-zmvzh Running m5.large eu-west-3 eu-west-3a 7m17s akaris2-f74ht-master-0 Running m5.xlarge eu-west-3 eu-west-3a 125m akaris2-f74ht-master-1 Running m5.xlarge eu-west-3 eu-west-3b 125m akaris2-f74ht-master-2 Running m5.xlarge eu-west-3 eu-west-3c 125m akaris2-f74ht-worker-eu-west-3a-4krrd Running m5.large eu-west-3 eu-west-3a 117m akaris2-f74ht-worker-eu-west-3b-59z6n Running m5.large eu-west-3 eu-west-3b 117m akaris2-f74ht-worker-eu-west-3c-chst9 Running m5.large eu-west-3 eu-west-3c 117m [akaris@linux infra-nodes]$ oc get nodes NAME STATUS ROLES AGE VERSION ip-10-0-128-80.eu-west-3.compute.internal Ready infra 2m39s v1.17.1+b83bc57 ip-10-0-145-35.eu-west-3.compute.internal Ready infra 2m43s v1.17.1+b83bc57 ip-10-0-148-174.eu-west-3.compute.internal Ready worker 111m v1.17.1+b83bc57 ip-10-0-159-154.eu-west-3.compute.internal Ready infra 2m7s v1.17.1+b83bc57 ip-10-0-159-35.eu-west-3.compute.internal Ready master 124m v1.17.1+b83bc57 ip-10-0-168-64.eu-west-3.compute.internal Ready master 124m v1.17.1+b83bc57 ip-10-0-191-238.eu-west-3.compute.internal Ready worker 111m v1.17.1+b83bc57 ip-10-0-208-80.eu-west-3.compute.internal Ready master 124m v1.17.1+b83bc57 ip-10-0-220-32.eu-west-3.compute.internal Ready worker 111m v1.17.1+b83bc57","title":"Step 5: Verify Machine creation"},{"location":"openshift/ocp4-infra-nodes-with-machineset-without-worker-label/#step-7-moving-resources-to-the-infra-nodes","text":"The following steps are from: https://github.com/lbohnsac/OCP4/tree/master/infrastructure-node-setup Also see steps from: https://docs.openshift.com/container-platform/4.5/machine_management/creating-infrastructure-machinesets.html","title":"Step 7: Moving resources to the infra nodes"},{"location":"openshift/ocp4-infra-nodes-with-machineset-without-worker-label/#move-the-router-bits","text":"Move the internal routers to the infra nodes oc patch ingresscontrollers.operator.openshift.io default -n openshift-ingress-operator --type=merge \\ --patch '{\"spec\":{\"nodePlacement\":{\"nodeSelector\":{\"matchLabels\":{\"node-role.kubernetes.io/infra\":\"\"}}}}}' Define 3 routers (default are 2!) oc patch ingresscontrollers.operator.openshift.io default -n openshift-ingress-operator --type=merge \\ --patch '{\"spec\":{\"replicas\": 3}}'","title":"Move the router bits"},{"location":"openshift/ocp4-infra-nodes-with-machineset-without-worker-label/#move-the-registry-bits","text":"Move the internal image registry to the infra nodes oc patch configs.imageregistry.operator.openshift.io cluster --type=merge \\ --patch '{\"spec\":{\"nodeSelector\":{\"node-role.kubernetes.io/infra\": \"\"}}}' Move the registry image pruner (from 4.4) to the infra nodes oc patch imagepruners.imageregistry.operator.openshift.io cluster --type=merge \\ --patch '{\"spec\":{\"nodeSelector\": {\"node-role.kubernetes.io/infra\": \"\"}}}'","title":"Move the registry bits"},{"location":"openshift/ocp4-infra-nodes-with-machineset-without-worker-label/#move-the-logging-bits","text":"Move the Kibana pod to the infras oc patch clusterloggings.logging.openshift.io instance -n openshift-logging --type=merge \\ --patch '{\"spec\":{\"visualization\":{\"kibana\":{\"nodeSelector\":{\"node-role.kubernetes.io/infra\":\"\"}}}}}' Move the curator pod to the infras oc patch clusterloggings.logging.openshift.io instance -n openshift-logging --type=merge \\ --patch '{\"spec\":{\"curation\":{\"curator\":{\"nodeSelector\":{\"node-role.kubernetes.io/infra\":\"\"}}}}}' Move the elasticsearch pods to the infras oc patch clusterloggings.logging.openshift.io instance -n openshift-logging --type=merge \\ --patch '{\"spec\":{\"logStore\":{\"elasticsearch\":{\"nodeSelector\":{\"node-role.kubernetes.io/infra\":\"\"}}}}}'","title":"Move the logging bits"},{"location":"openshift/ocp4-infra-nodes-with-machineset-without-worker-label/#issues","text":"The inconvenience with this is that the ExecStart of the dropin is now \"hardcoded\" and will not be updated / changed by the installer upon upgrade.","title":"Issues"},{"location":"openshift/ocp4-infra-nodes-with-machineset-without-worker-label/#step-6-testing-upgrades","text":"[akaris@linux infra-nodes]$ oc get clusterversion NAME VERSION AVAILABLE PROGRESSING SINCE STATUS version 4.4.16 True False 123m Cluster version is 4.4.16 [akaris@linux infra-nodes]$ oc adm upgrade Cluster version is 4.4.16 No updates available. You may force an upgrade to a specific release image, but doing so may not be supported and result in downtime or data loss. ############################# # set to 4.5-stable channel ############################# [akaris@linux infra-nodes]$ oc edit clusterversion clusterversion.config.openshift.io/version edited [akaris@linux infra-nodes]$ oc adm upgrade Cluster version is 4.4.16 Updates: VERSION IMAGE 4.5.5 quay.io/openshift-release-dev/ocp-release@sha256:a58573e1c92f5258219022ec104ec254ded0a70370ee8ed2aceea52525639bd4 [akaris@linux infra-nodes]$ oc adm upgrade --to-latest Updating to latest version 4.5.5 The upgrade succeeds: [akaris@linux ipi]$ export KUBECONFIG=/home/akaris/cases/02729138/ipi/install-config/auth/kubeconfig [akaris@linux ipi]$ oc get nodes NAME STATUS ROLES AGE VERSION ip-10-0-128-80.eu-west-3.compute.internal Ready infra 17h v1.18.3+08c38ef ip-10-0-145-35.eu-west-3.compute.internal Ready infra 17h v1.18.3+08c38ef ip-10-0-148-174.eu-west-3.compute.internal Ready worker 19h v1.18.3+08c38ef ip-10-0-159-154.eu-west-3.compute.internal Ready infra 17h v1.18.3+08c38ef ip-10-0-159-35.eu-west-3.compute.internal Ready master 19h v1.18.3+08c38ef ip-10-0-168-64.eu-west-3.compute.internal Ready master 19h v1.18.3+08c38ef ip-10-0-191-238.eu-west-3.compute.internal Ready worker 19h v1.18.3+08c38ef ip-10-0-208-80.eu-west-3.compute.internal Ready master 19h v1.18.3+08c38ef ip-10-0-220-32.eu-west-3.compute.internal Ready worker 19h v1.18.3+08c38ef [akaris@linux ipi]$ oc get clusterversion NAME VERSION AVAILABLE PROGRESSING SINCE STATUS version 4.5.5 True False 16h Cluster version is 4.5.5 [akaris@linux ipi]$ oc get machinesets NAME DESIRED CURRENT READY AVAILABLE AGE akaris2-f74ht-infra-eu-west-3a 3 3 3 3 17h akaris2-f74ht-worker-eu-west-3a 1 1 1 1 19h akaris2-f74ht-worker-eu-west-3b 1 1 1 1 19h akaris2-f74ht-worker-eu-west-3c 1 1 1 1 19h [akaris@linux ipi]$ oc get machines NAME PHASE TYPE REGION ZONE AGE akaris2-f74ht-infra-eu-west-3a-fctd5 Running m5.large eu-west-3 eu-west-3a 17h akaris2-f74ht-infra-eu-west-3a-xtpcv Running m5.large eu-west-3 eu-west-3a 17h akaris2-f74ht-infra-eu-west-3a-zmvzh Running m5.large eu-west-3 eu-west-3a 17h akaris2-f74ht-master-0 Running m5.xlarge eu-west-3 eu-west-3a 19h akaris2-f74ht-master-1 Running m5.xlarge eu-west-3 eu-west-3b 19h akaris2-f74ht-master-2 Running m5.xlarge eu-west-3 eu-west-3c 19h akaris2-f74ht-worker-eu-west-3a-4krrd Running m5.large eu-west-3 eu-west-3a 19h akaris2-f74ht-worker-eu-west-3b-59z6n Running m5.large eu-west-3 eu-west-3b 19h akaris2-f74ht-worker-eu-west-3c-chst9 Running m5.large eu-west-3 eu-west-3c 19h [akaris@linux ipi]$ oc get machineconfigpool NAME CONFIG UPDATED UPDATING DEGRADED MACHINECOUNT READYMACHINECOUNT UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT AGE infra rendered-infra-aa17b9b35abf210691d268dd4c7449d3 True False False 3 3 3 0 17h master rendered-master-2cc3abe4c49c6db04d9456744d0079f9 True False False 3 3 3 0 19h worker rendered-worker-d65740ba72e305c96782a27b8255dd89 True False False 3 3 3 0 19h [akaris@linux ipi]$ oc get machineconfig NAME GENERATEDBYCONTROLLER IGNITIONVERSION AGE 00-master 807abb900cf9976a1baad66eab17c6d76016e7b7 2.2.0 19h 00-worker 807abb900cf9976a1baad66eab17c6d76016e7b7 2.2.0 19h 01-master-container-runtime 807abb900cf9976a1baad66eab17c6d76016e7b7 2.2.0 19h 01-master-kubelet 807abb900cf9976a1baad66eab17c6d76016e7b7 2.2.0 19h 01-worker-container-runtime 807abb900cf9976a1baad66eab17c6d76016e7b7 2.2.0 19h 01-worker-kubelet 807abb900cf9976a1baad66eab17c6d76016e7b7 2.2.0 19h 03-infra-kubelet 2.2.0 17h 99-master-d32a1dcd-1b7b-4999-8919-3bd59c24fdae-registries 807abb900cf9976a1baad66eab17c6d76016e7b7 2.2.0 19h 99-master-ssh 2.2.0 19h 99-worker-f569c8d2-84df-4a5b-846a-9e090b9fb68d-registries 807abb900cf9976a1baad66eab17c6d76016e7b7 2.2.0 19h 99-worker-ssh 2.2.0 19h rendered-infra-aa17b9b35abf210691d268dd4c7449d3 807abb900cf9976a1baad66eab17c6d76016e7b7 2.2.0 16h rendered-infra-d63625ee2cdc4637cc156a1ebf25e926 601c2285f497bf7c73d84737b9977a0e697cb86a 2.2.0 17h rendered-master-2cc3abe4c49c6db04d9456744d0079f9 807abb900cf9976a1baad66eab17c6d76016e7b7 2.2.0 16h rendered-master-8d007e81b9ebb00b4042722f2713f1ae 601c2285f497bf7c73d84737b9977a0e697cb86a 2.2.0 19h rendered-worker-9d6647aa43752e0eb71a221dd8b7e32e 601c2285f497bf7c73d84737b9977a0e697cb86a 2.2.0 19h rendered-worker-d65740ba72e305c96782a27b8255dd89 807abb900cf9976a1baad66eab17c6d76016e7b7 2.2.0 16h [akaris@linux ipi]$ The one problem is the pod version on the infra nodes: [akaris@linux ipi]$ oc get nodes NAME STATUS ROLES AGE VERSION ip-10-0-128-80.eu-west-3.compute.internal Ready infra 17h v1.18.3+08c38ef ip-10-0-145-35.eu-west-3.compute.internal Ready infra 17h v1.18.3+08c38ef ip-10-0-148-174.eu-west-3.compute.internal Ready worker 19h v1.18.3+08c38ef ip-10-0-159-154.eu-west-3.compute.internal Ready infra 17h v1.18.3+08c38ef ip-10-0-159-35.eu-west-3.compute.internal Ready master 19h v1.18.3+08c38ef ip-10-0-168-64.eu-west-3.compute.internal Ready master 19h v1.18.3+08c38ef ip-10-0-191-238.eu-west-3.compute.internal Ready worker 19h v1.18.3+08c38ef ip-10-0-208-80.eu-west-3.compute.internal Ready master 19h v1.18.3+08c38ef ip-10-0-220-32.eu-west-3.compute.internal Ready worker 19h v1.18.3+08c38ef [akaris@linux ipi]$ oc debug node/ip-10-0-128-80.eu-west-3.compute.internal Starting pod/ip-10-0-128-80eu-west-3computeinternal-debug ... To use host binaries, run `chroot /host` Pod IP: 10.0.128.80 If you don't see a command prompt, try pressing enter. sh-4.2# chroot /host sh-4.4# cat /etc/redhat-release Red Hat Enterprise Linux CoreOS release 4.5 sh-4.4# ps aux | grep kubelet root 1422 3.3 2.0 1461696 161600 ? Ssl Aug19 33:46 kubelet --config=/etc/kubernetes/kubelet.conf --bootstrap-kubeconfig=/etc/kubernetes/kubeconfig --kubeconfig=/var/lib/kubelet/kubeconfig --container-runtime=remote --container-runtime-endpoint=/var/run/crio/crio.sock --runtime-cgroups=/system.slice/crio.service --node-labels=node-role.kubernetes.io/infra,node.openshift.io/os_id=rhcos --minimum-container-ttl-duration=6m0s --volume-plugin-dir=/etc/kubernetes/kubelet-plugins/volume/exec --cloud-provider=aws --pod-infra-container-image=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dc0fe885d41cc4029caa3feacf71343806c81c8123abc91db90dc0e555fa5636 --v=4 root 1232029 0.0 0.0 9180 1032 ? S+ 10:02 0:00 grep kubelet sh-4.4# sh-4.4# grep pause /etc/crio/ -R /etc/crio/seccomp.json: \"pause\", /etc/crio/crio.conf.d/00-default:pause_image = \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6e8bcc9c91267283f8b0edf1af0cb0d310c54d4f73905d9bcfb2a103a664fcb0\" /etc/crio/crio.conf.d/00-default:pause_image_auth_file = \"/var/lib/kubelet/config.json\" /etc/crio/crio.conf.d/00-default:pause_command = \"/usr/bin/pod\" sh-4.4# Compare that to the version on the worker nodes: [akaris@linux ipi]$ oc get nodes NAME STATUS ROLES AGE VERSION ip-10-0-128-80.eu-west-3.compute.internal Ready infra 17h v1.18.3+08c38ef ip-10-0-145-35.eu-west-3.compute.internal Ready infra 17h v1.18.3+08c38ef ip-10-0-148-174.eu-west-3.compute.internal Ready worker 19h v1.18.3+08c38ef ip-10-0-159-154.eu-west-3.compute.internal Ready infra 17h v1.18.3+08c38ef ip-10-0-159-35.eu-west-3.compute.internal Ready master 19h v1.18.3+08c38ef ip-10-0-168-64.eu-west-3.compute.internal Ready master 19h v1.18.3+08c38ef ip-10-0-191-238.eu-west-3.compute.internal Ready worker 19h v1.18.3+08c38ef ip-10-0-208-80.eu-west-3.compute.internal Ready master 19h v1.18.3+08c38ef ip-10-0-220-32.eu-west-3.compute.internal Ready worker 19h v1.18.3+08c38ef [akaris@linux ipi]$ oc debug node/ip-10-0-148-174.eu-west-3.compute.internal Starting pod/ip-10-0-148-174eu-west-3computeinternal-debug ... To use host binaries, run `chroot /host` Pod IP: 10.0.148.174 If you don't see a command prompt, try pressing enter. sh-4.2# chroot /host sh-4.4# cat /etc/redhat-release Red Hat Enterprise Linux CoreOS release 4.5 sh-4.4# ps aux | grep kubelet root 1421 5.5 2.5 1454268 196984 ? Ssl Aug19 56:01 kubelet --config=/etc/kubernetes/kubelet.conf --bootstrap-kubeconfig=/etc/kubernetes/kubeconfig --kubeconfig=/var/lib/kubelet/kubeconfig --container-runtime=remote --container-runtime-endpoint=/var/run/crio/crio.sock --runtime-cgroups=/system.slice/crio.service --node-labels=node-role.kubernetes.io/worker,node.openshift.io/os_id=rhcos --minimum-container-ttl-duration=6m0s --volume-plugin-dir=/etc/kubernetes/kubelet-plugins/volume/exec --cloud-provider=aws --pod-infra-container-image=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6e8bcc9c91267283f8b0edf1af0cb0d310c54d4f73905d9bcfb2a103a664fcb0 --v=4 root 2138381 0.0 0.0 9180 1060 ? S+ 10:02 0:00 grep kubelet sh-4.4# grep pause /etc/crio/ -R /etc/crio/seccomp.json: \"pause\", /etc/crio/crio.conf.d/00-default:pause_image = \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6e8bcc9c91267283f8b0edf1af0cb0d310c54d4f73905d9bcfb2a103a664fcb0\" /etc/crio/crio.conf.d/00-default:pause_image_auth_file = \"/var/lib/kubelet/config.json\" /etc/crio/crio.conf.d/00-default:pause_command = \"/usr/bin/pod\" That --pod-infra-container-image= pod version is for the pause container; although as you can see above, the pause_image in crio.conf.d is always updated: https://stackoverflow.com/questions/46630377/what-is-pod-infra-container-image-meant-for The pause container, which image the --pod-infra-container flag selects, is used so that multiple containers can be launched in a pod, while sharing resources. It mostly does nothing, and unless you have a very good reason to replace it with something custom, you shouldn't. It mostly invokes the pause system call (hence its name) but it also performs the important function of having PID 1 and making sure no zombie processes are kept around. An extremely complete article on the subject can be found here, from where I also shamelessly stole the following picture which illustrates where the pause container lives: According to https://github.com/kubernetes/kubernetes/pull/70603 (although that comment is from 2018): The kubelet allows you to set --pod-infra-container-image (also called PodSandboxImage in the kubelet config), which can be a custom location to the \"pause\" image in the case of Docker. Other CRIs are not supported. And according to https://github.com/kubernetes/kubernetes/issues/86081 (from November 2019): Kubelet flag \"pod-infra-container-image\" is invalid when using CRI-O And according to the kubelet help, this only affects docker and doesn't affect us: sh-4.4# kubelet --help | grep infra --pod-infra-container-image string The image whose network/ipc namespaces containers in each pod will use. This docker-specific flag only works when container-runtime is set to docker. (default \"k8s.gcr.io/pause:3.2\") As an additional step just to bring the infra nodes in line with the workers and the rest of the cluster, we could update that by inspecting the current worker kubelet configuration and then modify this post-upgrade: [akaris@linux ipi]$ oc get machineconfig 01-worker-kubelet -o yaml | grep pod-infra-container-image --pod-infra-container-image=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6e8bcc9c91267283f8b0edf1af0cb0d310c54d4f73905d9bcfb2a103a664fcb0 \\ [akaris@linux ipi]$ oc get machineconfig 03-infra-kubelet -o yaml | grep pod-infra-container-image | grep -v apiVersion --pod-infra-container-image=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:dc0fe885d41cc4029caa3feacf71343806c81c8123abc91db90dc0e555fa5636 \\ [akaris@linux ipi]$ oc edit machineconfig 03-infra-kubelet machineconfig.machineconfiguration.openshift.io/03-infra-kubelet edited [akaris@linux ipi]$ oc get machineconfig 03-infra-kubelet -o yaml | grep pod-infra-container-image | grep -v apiVersion --pod-infra-container-image=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6e8bcc9c91267283f8b0edf1af0cb0d310c54d4f73905d9bcfb2a103a664fcb0 \\ Which will then trigger a restart of the infra nodes: [akaris@linux ipi]$ oc get machineconfigpool NAME CONFIG UPDATED UPDATING DEGRADED MACHINECOUNT READYMACHINECOUNT UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT AGE infra rendered-infra-aa17b9b35abf210691d268dd4c7449d3 False True False 3 0 0 0 17h master rendered-master-2cc3abe4c49c6db04d9456744d0079f9 True False False 3 3 3 0 19h worker rendered-worker-d65740ba72e305c96782a27b8255dd89 True False False 3 3 3 0 19h [akaris@linux ipi]$ oc get machineconfig | grep infra 03-infra-kubelet 2.2.0 17h rendered-infra-aa17b9b35abf210691d268dd4c7449d3 807abb900cf9976a1baad66eab17c6d76016e7b7 2.2.0 17h rendered-infra-d63625ee2cdc4637cc156a1ebf25e926 601c2285f497bf7c73d84737b9977a0e697cb86a 2.2.0 17h rendered-infra-f3df01ebf7429a29582f3c46132fcf88 807abb900cf9976a1baad66eab17c6d76016e7b7 2.2.0 44s [akaris@linux ipi]$ And then eventually: [akaris@linux ipi]$ oc get nodes NAME STATUS ROLES AGE VERSION ip-10-0-128-80.eu-west-3.compute.internal Ready infra 17h v1.18.3+08c38ef ip-10-0-145-35.eu-west-3.compute.internal Ready infra 17h v1.18.3+08c38ef ip-10-0-148-174.eu-west-3.compute.internal Ready worker 19h v1.18.3+08c38ef ip-10-0-159-154.eu-west-3.compute.internal Ready infra 17h v1.18.3+08c38ef ip-10-0-159-35.eu-west-3.compute.internal Ready master 19h v1.18.3+08c38ef ip-10-0-168-64.eu-west-3.compute.internal Ready master 19h v1.18.3+08c38ef ip-10-0-191-238.eu-west-3.compute.internal Ready worker 19h v1.18.3+08c38ef ip-10-0-208-80.eu-west-3.compute.internal Ready master 19h v1.18.3+08c38ef ip-10-0-220-32.eu-west-3.compute.internal Ready worker 19h v1.18.3+08c38ef [akaris@linux ipi]$ oc get machineconfigpool NAME CONFIG UPDATED UPDATING DEGRADED MACHINECOUNT READYMACHINECOUNT UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT AGE infra rendered-infra-f3df01ebf7429a29582f3c46132fcf88 True False False 3 3 3 0 18h master rendered-master-2cc3abe4c49c6db04d9456744d0079f9 True False False 3 3 3 0 19h worker rendered-worker-d65740ba72e305c96782a27b8255dd89 True False False 3 3 3 0 19h [akaris@linux ipi]$ oc get machines NAME PHASE TYPE REGION ZONE AGE akaris2-f74ht-infra-eu-west-3a-fctd5 Running m5.large eu-west-3 eu-west-3a 18h akaris2-f74ht-infra-eu-west-3a-xtpcv Running m5.large eu-west-3 eu-west-3a 18h akaris2-f74ht-infra-eu-west-3a-zmvzh Running m5.large eu-west-3 eu-west-3a 18h akaris2-f74ht-master-0 Running m5.xlarge eu-west-3 eu-west-3a 19h akaris2-f74ht-master-1 Running m5.xlarge eu-west-3 eu-west-3b 19h akaris2-f74ht-master-2 Running m5.xlarge eu-west-3 eu-west-3c 19h akaris2-f74ht-worker-eu-west-3a-4krrd Running m5.large eu-west-3 eu-west-3a 19h akaris2-f74ht-worker-eu-west-3b-59z6n Running m5.large eu-west-3 eu-west-3b 19h akaris2-f74ht-worker-eu-west-3c-chst9 Running m5.large eu-west-3 eu-west-3c 19h [akaris@linux ipi]$ oc debug node/ip-10-0-128-80.eu-west-3.compute.internal Starting pod/ip-10-0-128-80eu-west-3computeinternal-debug ... To use host binaries, run `chroot /host` Pod IP: 10.0.128.80 If you don't see a command prompt, try pressing enter. sh-4.2# chroot /host sh-4.4# ps aux | grep kubelet root 1411 3.5 1.8 1484684 147368 ? Ssl 10:16 0:23 kubelet --config=/etc/kubernetes/kubelet.conf --bootstrap-kubeconfig=/etc/kubernetes/kubeconfig --kubeconfig=/var/lib/kubelet/kubeconfig --container-runtime=remote --container-runtime-endpoint=/var/run/crio/crio.sock --runtime-cgroups=/system.slice/crio.service --node-labels=node-role.kubernetes.io/infra,node.openshift.io/os_id=rhcos --minimum-container-ttl-duration=6m0s --volume-plugin-dir=/etc/kubernetes/kubelet-plugins/volume/exec --cloud-provider=aws --pod-infra-container-image=quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6e8bcc9c91267283f8b0edf1af0cb0d310c54d4f73905d9bcfb2a103a664fcb0 --v=4 root 17471 0.0 0.0 9180 1052 ? S+ 10:27 0:00 grep kubelet","title":"Step 6: Testing upgrades"},{"location":"openshift/openshift_httpbin_tshark_sidecar/","text":"AlertManager Configuring Alertmanager with webhooks and httpbin container with tshark sidecar as a consumer Summary The following describe a setup on OCP 3.11 with: * a container running httpbin and a sidecar running tshark and filtering for incoming http requests and logging them * configuration of Alertmanager so that it sends alerts via webhook to httpbin * loading cluster with high number of pods * analyzing generated alarms Prerequisites Make sure that ocntainers can run as any uid: # oc adm policy add-scc-to-user anyuid -z default scc \"anyuid\" added to: [\"system:serviceaccount:default:default\"] OpenShift httpbin with tshark sidecar The following allows us to see any incoming requests to httpbin but to filter out httpbin's answers. Prerequisites: # oc adm policy add-scc-to-user anyuid -z default scc \"anyuid\" added to: [\"system:serviceaccount:default:default\"] Create file httpbin.yaml : apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app: httpbin-deploymentconfig name: httpbin-service spec: host: httpbin.apps.akaris2.lab.pnq2.cee.redhat.com port: targetPort: 80 to: kind: Service name: httpbin-service weight: 100 wildcardPolicy: None --- apiVersion: v1 kind: Service metadata: name: httpbin-service labels: app: httpbin-deploymentconfig spec: selector: app: httpbin-pod ports: - protocol: TCP port: 80 targetPort: 80 --- apiVersion: v1 kind: DeploymentConfig metadata: name: httpbin-deploymentconfig labels: app: httpbin-deploymentconfig spec: replicas: 1 selector: app: httpbin-pod template: metadata: labels: app: httpbin-pod spec: containers: - name: tshark image: danielguerra/alpine-tshark command: - \"tshark\" - \"-i\" - \"eth0\" - \"-Y\" - \"http\" - \"-V\" - \"dst\" - \"port\" - \"80\" - name: httpbin image: kennethreitz/httpbin imagePullPolicy: Always command: - \"gunicorn\" - \"-b\" - \"0.0.0.0:80\" - \"httpbin:app\" - \"-k\" - \"gevent\" - \"--capture-output\" - \"--error-logfile\" - \"-\" - \"--access-logfile\" - \"-\" - \"--access-logformat\" - \"'%(h)s %(t)s %(r)s %(s)s Host: %({Host}i)s} Header-i: %({Header}i)s Header-o: %({Header}o)s'\" Apply config: oc apply -f httpbin.yaml Get the pod name and loolk at the pod's logs for container tshark : [root@master-0 ~]# oc get pods -l app=httpbin-pod NAME READY STATUS RESTARTS AGE httpbin-deploymentconfig-8-tgmvn 2/2 Running 0 3m [root@master-0 ~]# oc logs httpbin-deploymentconfig-8-tgmvn -c tshark -f Capturing on 'eth0' Frame 4: 535 bytes on wire (4280 bits), 535 bytes captured (4280 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 Encapsulation type: Ethernet (1) Arrival Time: Mar 11, 2020 12:17:13.290037158 UTC [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1583929033.290037158 seconds [Time delta from previous captured frame: 0.000002253 seconds] [Time delta from previous displayed frame: 0.000000000 seconds] [Time since reference or first frame: 36.739477011 seconds] Frame Number: 4 Frame Length: 535 bytes (4280 bits) Capture Length: 535 bytes (4280 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:tcp:http:urlencoded-form] Ethernet II, Src: 7a:9c:fa:d2:07:d8 (7a:9c:fa:d2:07:d8), Dst: 0a:58:0a:80:00:0c (0a:58:0a:80:00:0c) Destination: 0a:58:0a:80:00:0c (0a:58:0a:80:00:0c) Address: 0a:58:0a:80:00:0c (0a:58:0a:80:00:0c) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: 7a:9c:fa:d2:07:d8 (7a:9c:fa:d2:07:d8) Address: 7a:9c:fa:d2:07:d8 (7a:9c:fa:d2:07:d8) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 10.130.0.1, Dst: 10.128.0.12 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x00 (DSCP: CS0, ECN: Not-ECT) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Total Length: 521 Identification: 0xdfdf (57311) Flags: 0x02 (Don't Fragment) 0... .... = Reserved bit: Not set .1.. .... = Don't fragment: Set ..0. .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: TCP (6) Header checksum: 0x4401 [validation disabled] [Header checksum status: Unverified] Source: 10.130.0.1 Destination: 10.128.0.12 Transmission Control Protocol, Src Port: 38288, Dst Port: 80, Seq: 1, Ack: 1, Len: 469 Source Port: 38288 Destination Port: 80 [Stream index: 1] [TCP Segment Len: 469] Sequence number: 1 (relative sequence number) [Next sequence number: 470 (relative sequence number)] Acknowledgment number: 1 (relative ack number) 1000 .... = Header Length: 32 bytes (8) Flags: 0x018 (PSH, ACK) 000. .... .... = Reserved: Not set ...0 .... .... = Nonce: Not set .... 0... .... = Congestion Window Reduced (CWR): Not set .... .0.. .... = ECN-Echo: Not set .... ..0. .... = Urgent: Not set .... ...1 .... = Acknowledgment: Set .... .... 1... = Push: Set .... .... .0.. = Reset: Not set .... .... ..0. = Syn: Not set .... .... ...0 = Fin: Not set [TCP Flags: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7AP\u00b7\u00b7\u00b7] Window size value: 221 [Calculated window size: 28288] [Window size scaling factor: 128] Checksum: 0xd9c6 [unverified] [Checksum Status: Unverified] Urgent pointer: 0 Options: (12 bytes), No-Operation (NOP), No-Operation (NOP), Timestamps TCP Option - No-Operation (NOP) Kind: No-Operation (1) TCP Option - No-Operation (NOP) Kind: No-Operation (1) TCP Option - Timestamps: TSval 44637623, TSecr 44644920 Kind: Time Stamp Option (8) Length: 10 Timestamp value: 44637623 Timestamp echo reply: 44644920 [SEQ/ACK analysis] [iRTT: 0.001410475 seconds] [Bytes in flight: 470] [Bytes sent since last PSH flag: 469] TCP payload (469 bytes) Hypertext Transfer Protocol POST /post HTTP/1.1\\r\\n [Expert Info (Chat/Sequence): POST /post HTTP/1.1\\r\\n] [POST /post HTTP/1.1\\r\\n] [Severity level: Chat] [Group: Sequence] Request Method: POST Request URI: /post Request Version: HTTP/1.1 Configuring Alertmanager to send webhooks to httpbin pod Prerequisites: * https://docs.openshift.com/container-platform/3.11/install_config/prometheus_cluster_monitoring.html In the following, replace myuser with the user who shall log into alertmanager: $ oc adm policy add-cluster-role-to-user cluster-monitoring-view myuser cluster role \"cluster-monitoring-view\" added: \"myuser\" We can use the above to tell alertmanager to use httpbin as its web hook: $ oc get routes -n openshift-monitoring NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD alertmanager-main alertmanager-main-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com alertmanager-main web reencrypt None grafana grafana-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com grafana https reencrypt None prometheus-k8s prometheus-k8s-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com prometheus-k8s web reencrypt None Now, access: https://alertmanager-main-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com The status page will show the current alertmanager configuration. The following Red Hat knowledge base solution shows how to update the alertmanager config: https://access.redhat.com/solutions/3804781 Create file: ~/group_vars/OSEv3.yml : openshift_cluster_monitoring_operator_alertmanager_config: |+ global: resolve_timeout: 2m route: group_wait: 5s group_interval: 10s repeat_interval: 20s receiver: default routes: - match: alertname: DeadMansSwitch repeat_interval: 30s receiver: deadmansswitch - match: alertname: DeadMansSwitch repeat_interval: 30s receiver: wh - match: alertname: '*' repeat_interval: 2m receiver: wh - match: severity: critical receiver: wh - match: severity: warning receiver: wh - match: alertname: KubeAPILatencyHigh receiver: wh receivers: - name: default - name: deadmansswitch - name: wh webhook_configs: - url: \"http://httpbin.apps.akaris2.lab.pnq2.cee.redhat.com/anything\" And run: ansible-playbook -i hosts openshift-ansible/playbooks/openshift-monitoring/config.yml -e=\"openshift_cluster_monitoring_operator_install=true\" Verify: $ oc get secret -n openshift-monitoring alertmanager-main -o yaml | awk '/alertmanager.yaml:/ {print $NF}' | base64 -d global: resolve_timeout: 2m route: group_wait: 5s group_interval: 10s repeat_interval: 20s receiver: default routes: - match: alertname: DeadMansSwitch repeat_interval: 30s receiver: deadmansswitch - match: alertname: DeadMansSwitch repeat_interval: 30s receiver: wh - match: alertname: '*' repeat_interval: 2m receiver: wh - match: severity: critical receiver: wh - match: severity: warning receiver: wh - match: alertname: KubeAPILatencyHigh receiver: wh receivers: - name: default - name: deadmansswitch - name: wh webhook_configs: - url: \"http://httpbin.apps.akaris2.lab.pnq2.cee.redhat.com/anything\" Restart pods: $ oc delete pods --selector=app=alertmanager -n openshift-monitoring pod \"alertmanager-main-0\" deleted pod \"alertmanager-main-1\" deleted pod \"alertmanager-main-2\" deleted And check in the web interface of alertmanager to make sure that the new configuration shows up. Loading the cluster An easy way to generate an alert in a small lab is to trigger alert KubeletTooManyPods . Go to prometheus and check its configuration: alert: KubeletTooManyPods expr: kubelet_running_pod_count{job=\"kubelet\"} > 250 * 0.9 for: 15m labels: severity: warning annotations: message: Kubelet {{ $labels.instance }} is running {{ $value }} Pods, close to the limit of 250. Then, create the following busybox deployment with a number of pods that exceeds this number, e.g.: busybox.yaml : apiVersion: apps/v1 kind: Deployment metadata: name: busybox-deployment labels: app: busybox-deployment spec: replicas: 500 selector: matchLabels: app: busybox-pod template: metadata: labels: app: busybox-pod spec: containers: - name: busybox image: busybox command: - sleep - infinity imagePullPolicy: IfNotPresent oc apply -f busybox.yaml The cluster will need some time to create those pods and it'll take 15 minutes for the alarm to fire. So take a coffee and come back later. Once the alarm fires in prometheus, go to alertmanager and make sure that it shows there, too. Among others, Alertmanager should show: alertname=\"KubeletTooManyPods\" 16:06:32, 2020-03-11 message: Kubelet 10.74.176.204:10250 is running 250 Pods, close to the limit of 250. severity=\"warning\" service=\"kubelet\"prometheus=\"openshift-monitoring/k8s\"namespace=\"kube-system\"job=\"kubelet\"instance=\"10.74.176.204:10250\"endpoint=\"https-metrics\" Now, it's time to go back to the httpbin pod. Monitoring incoming webhook reuests Get the pod name: # oc get pods | grep httpbin httpbin-deploymentconfig-8-8crvh 2/2 Running 0 1h And check the logs of the tshark container which will show a verbose packet capture of HTTP with a destination port of 80 (so we are not capturing the response): # oc logs httpbin-deploymentconfig-8-8crvh -c tshark | tail -n 400 (...) Frame 1708: 5220 bytes on wire (41760 bits), 5220 bytes captured (41760 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 (...) Ethernet II, Src: ... (...), Dst: ... (...) (...) Internet Protocol Version 4, Src: ..., Dst: ... (...) Transmission Control Protocol, Src Port: 41606, Dst Port: 80, Seq: 1, Ack: 1, Len: 5154 (...) Hypertext Transfer Protocol POST /anything HTTP/1.1\\r\\n [Expert Info (Chat/Sequence): POST /anything HTTP/1.1\\r\\n] [POST /anything HTTP/1.1\\r\\n] [Severity level: Chat] [Group: Sequence] Request Method: POST Request URI: /anything Request Version: HTTP/1.1 User-Agent: Alertmanager/0.15.2\\r\\n Content-Length: 4743\\r\\n [Content length: 4743] Content-Type: application/json\\r\\n (...) JavaScript Object Notation: application/json Object Member Key: receiver String value: wh Key: receiver Member Key: status String value: firing Key: status Member Key: alerts Array Object Member Key: status String value: firing Key: status Member Key: labels Object Member Key: alertname String value: KubeDaemonSetRolloutStuck Key: alertname Member Key: cluster String value: openshift.akaris2.lab.pnq2.cee.redhat.com Key: cluster Member Key: daemonset String value: node-exporter Key: daemonset Member Key: endpoint String value: https-main Key: endpoint Member Key: instance String value: ...:8443 Key: instance Member Key: job String value: kube-state-metrics Key: job Member Key: namespace String value: openshift-monitoring Key: namespace Member Key: pod String value: kube-state-metrics-6f4c658bcc-v57b6 Key: pod Member Key: prometheus String value: openshift-monitoring/k8s Key: prometheus Member Key: service String value: kube-state-metrics Key: service Member Key: severity String value: critical Key: severity Key: labels Member Key: annotations Object Member Key: message String value: Only 66.66666666666666% of desired pods scheduled and ready for daemon set openshift-monitoring/node-exporter Key: message Key: annotations Member Key: startsAt String value: 2020-03-11T16:07:40.59085788Z Key: startsAt Member Key: endsAt String value: 0001-01-01T00:00:00Z Key: endsAt Member Key: generatorURL String value [truncated]: https://prometheus-k8s-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com/graph?g0.expr=kube_daemonset_status_number_ready%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22%28openshift-.%2A%7Ckube-.%2A%7C Key: generatorURL Object Member Key: status String value: firing Key: status Member Key: labels Object Member Key: alertname String value: KubeDaemonSetRolloutStuck Key: alertname Member Key: cluster String value: openshift.akaris2.lab.pnq2.cee.redhat.com Key: cluster Member Key: daemonset String value: ovs Key: daemonset Member Key: endpoint String value: https-main Key: endpoint Member Key: instance String value: ...:8443 Key: instance Member Key: job String value: kube-state-metrics Key: job Member Key: namespace String value: openshift-sdn Key: namespace Member Key: pod String value: kube-state-metrics-6f4c658bcc-v57b6 Key: pod Member Key: prometheus String value: openshift-monitoring/k8s Key: prometheus Member Key: service String value: kube-state-metrics Key: service Member Key: severity String value: critical Key: severity Key: labels Member Key: annotations Object Member Key: message String value: Only 66.66666666666666% of desired pods scheduled and ready for daemon set openshift-sdn/ovs Key: message Key: annotations Member Key: startsAt String value: 2020-03-11T16:07:40.59085788Z Key: startsAt Member Key: endsAt String value: 0001-01-01T00:00:00Z Key: endsAt Member Key: generatorURL String value [truncated]: https://prometheus-k8s-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com/graph?g0.expr=kube_daemonset_status_number_ready%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22%28openshift-.%2A%7Ckube-.%2A%7C Key: generatorURL Object Member Key: status String value: firing Key: status Member Key: labels Object Member Key: alertname String value: KubeDaemonSetRolloutStuck Key: alertname Member Key: cluster String value: openshift.akaris2.lab.pnq2.cee.redhat.com Key: cluster Member Key: daemonset String value: sdn Key: daemonset Member Key: endpoint String value: https-main Key: endpoint Member Key: instance String value: ...:8443 Key: instance Member Key: job String value: kube-state-metrics Key: job Member Key: namespace String value: openshift-sdn Key: namespace Member Key: pod String value: kube-state-metrics-6f4c658bcc-v57b6 Key: pod Member Key: prometheus String value: openshift-monitoring/k8s Key: prometheus Member Key: service String value: kube-state-metrics Key: service Member Key: severity String value: critical Key: severity Key: labels Member Key: annotations Object Member Key: message String value: Only 66.66666666666666% of desired pods scheduled and ready for daemon set openshift-sdn/sdn Key: message Key: annotations Member Key: startsAt String value: 2020-03-11T16:07:40.59085788Z Key: startsAt Member Key: endsAt String value: 0001-01-01T00:00:00Z Key: endsAt Member Key: generatorURL String value [truncated]: https://prometheus-k8s-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com/graph?g0.expr=kube_daemonset_status_number_ready%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22%28openshift-.%2A%7Ckube-.%2A%7C Key: generatorURL Object Member Key: status String value: firing Key: status Member Key: labels Object Member Key: alertname String value: KubeDaemonSetRolloutStuck Key: alertname Member Key: cluster String value: openshift.akaris2.lab.pnq2.cee.redhat.com Key: cluster Member Key: daemonset String value: sync Key: daemonset Member Key: endpoint String value: https-main Key: endpoint Member Key: instance String value: ...:8443 Key: instance Member Key: job String value: kube-state-metrics Key: job Member Key: namespace String value: openshift-node Key: namespace Member Key: pod String value: kube-state-metrics-6f4c658bcc-v57b6 Key: pod Member Key: prometheus String value: openshift-monitoring/k8s Key: prometheus Member Key: service String value: kube-state-metrics Key: service Member Key: severity String value: critical Key: severity Key: labels Member Key: annotations Object Member Key: message String value: Only 66.66666666666666% of desired pods scheduled and ready for daemon set openshift-node/sync Key: message Key: annotations Member Key: startsAt String value: 2020-03-11T16:07:40.59085788Z Key: startsAt Member Key: endsAt String value: 0001-01-01T00:00:00Z Key: endsAt Member Key: generatorURL String value [truncated]: https://prometheus-k8s-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com/graph?g0.expr=kube_daemonset_status_number_ready%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22%28openshift-.%2A%7Ckube-.%2A%7C Key: generatorURL Key: alerts Member Key: groupLabels Object Key: groupLabels Member Key: commonLabels Object Member Key: alertname String value: KubeDaemonSetRolloutStuck Key: alertname Member Key: cluster String value: openshift.akaris2.lab.pnq2.cee.redhat.com Key: cluster Member Key: endpoint String value: https-main Key: endpoint Member Key: instance String value: ...:8443 Key: instance Member Key: job String value: kube-state-metrics Key: job Member Key: pod String value: kube-state-metrics-6f4c658bcc-v57b6 Key: pod Member Key: prometheus String value: openshift-monitoring/k8s Key: prometheus Member Key: service String value: kube-state-metrics Key: service Member Key: severity String value: critical Key: severity Key: commonLabels","title":"Httpbin tshark sidecar container"},{"location":"openshift/openshift_httpbin_tshark_sidecar/#alertmanager","text":"","title":"AlertManager"},{"location":"openshift/openshift_httpbin_tshark_sidecar/#configuring-alertmanager-with-webhooks-and-httpbin-container-with-tshark-sidecar-as-a-consumer","text":"","title":"Configuring Alertmanager with webhooks and httpbin container with tshark sidecar as a consumer"},{"location":"openshift/openshift_httpbin_tshark_sidecar/#summary","text":"The following describe a setup on OCP 3.11 with: * a container running httpbin and a sidecar running tshark and filtering for incoming http requests and logging them * configuration of Alertmanager so that it sends alerts via webhook to httpbin * loading cluster with high number of pods * analyzing generated alarms","title":"Summary"},{"location":"openshift/openshift_httpbin_tshark_sidecar/#prerequisites","text":"Make sure that ocntainers can run as any uid: # oc adm policy add-scc-to-user anyuid -z default scc \"anyuid\" added to: [\"system:serviceaccount:default:default\"]","title":"Prerequisites"},{"location":"openshift/openshift_httpbin_tshark_sidecar/#openshift-httpbin-with-tshark-sidecar","text":"The following allows us to see any incoming requests to httpbin but to filter out httpbin's answers. Prerequisites: # oc adm policy add-scc-to-user anyuid -z default scc \"anyuid\" added to: [\"system:serviceaccount:default:default\"] Create file httpbin.yaml : apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app: httpbin-deploymentconfig name: httpbin-service spec: host: httpbin.apps.akaris2.lab.pnq2.cee.redhat.com port: targetPort: 80 to: kind: Service name: httpbin-service weight: 100 wildcardPolicy: None --- apiVersion: v1 kind: Service metadata: name: httpbin-service labels: app: httpbin-deploymentconfig spec: selector: app: httpbin-pod ports: - protocol: TCP port: 80 targetPort: 80 --- apiVersion: v1 kind: DeploymentConfig metadata: name: httpbin-deploymentconfig labels: app: httpbin-deploymentconfig spec: replicas: 1 selector: app: httpbin-pod template: metadata: labels: app: httpbin-pod spec: containers: - name: tshark image: danielguerra/alpine-tshark command: - \"tshark\" - \"-i\" - \"eth0\" - \"-Y\" - \"http\" - \"-V\" - \"dst\" - \"port\" - \"80\" - name: httpbin image: kennethreitz/httpbin imagePullPolicy: Always command: - \"gunicorn\" - \"-b\" - \"0.0.0.0:80\" - \"httpbin:app\" - \"-k\" - \"gevent\" - \"--capture-output\" - \"--error-logfile\" - \"-\" - \"--access-logfile\" - \"-\" - \"--access-logformat\" - \"'%(h)s %(t)s %(r)s %(s)s Host: %({Host}i)s} Header-i: %({Header}i)s Header-o: %({Header}o)s'\" Apply config: oc apply -f httpbin.yaml Get the pod name and loolk at the pod's logs for container tshark : [root@master-0 ~]# oc get pods -l app=httpbin-pod NAME READY STATUS RESTARTS AGE httpbin-deploymentconfig-8-tgmvn 2/2 Running 0 3m [root@master-0 ~]# oc logs httpbin-deploymentconfig-8-tgmvn -c tshark -f Capturing on 'eth0' Frame 4: 535 bytes on wire (4280 bits), 535 bytes captured (4280 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 Encapsulation type: Ethernet (1) Arrival Time: Mar 11, 2020 12:17:13.290037158 UTC [Time shift for this packet: 0.000000000 seconds] Epoch Time: 1583929033.290037158 seconds [Time delta from previous captured frame: 0.000002253 seconds] [Time delta from previous displayed frame: 0.000000000 seconds] [Time since reference or first frame: 36.739477011 seconds] Frame Number: 4 Frame Length: 535 bytes (4280 bits) Capture Length: 535 bytes (4280 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:tcp:http:urlencoded-form] Ethernet II, Src: 7a:9c:fa:d2:07:d8 (7a:9c:fa:d2:07:d8), Dst: 0a:58:0a:80:00:0c (0a:58:0a:80:00:0c) Destination: 0a:58:0a:80:00:0c (0a:58:0a:80:00:0c) Address: 0a:58:0a:80:00:0c (0a:58:0a:80:00:0c) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Source: 7a:9c:fa:d2:07:d8 (7a:9c:fa:d2:07:d8) Address: 7a:9c:fa:d2:07:d8 (7a:9c:fa:d2:07:d8) .... ..1. .... .... .... .... = LG bit: Locally administered address (this is NOT the factory default) .... ...0 .... .... .... .... = IG bit: Individual address (unicast) Type: IPv4 (0x0800) Internet Protocol Version 4, Src: 10.130.0.1, Dst: 10.128.0.12 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x00 (DSCP: CS0, ECN: Not-ECT) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Total Length: 521 Identification: 0xdfdf (57311) Flags: 0x02 (Don't Fragment) 0... .... = Reserved bit: Not set .1.. .... = Don't fragment: Set ..0. .... = More fragments: Not set Fragment offset: 0 Time to live: 64 Protocol: TCP (6) Header checksum: 0x4401 [validation disabled] [Header checksum status: Unverified] Source: 10.130.0.1 Destination: 10.128.0.12 Transmission Control Protocol, Src Port: 38288, Dst Port: 80, Seq: 1, Ack: 1, Len: 469 Source Port: 38288 Destination Port: 80 [Stream index: 1] [TCP Segment Len: 469] Sequence number: 1 (relative sequence number) [Next sequence number: 470 (relative sequence number)] Acknowledgment number: 1 (relative ack number) 1000 .... = Header Length: 32 bytes (8) Flags: 0x018 (PSH, ACK) 000. .... .... = Reserved: Not set ...0 .... .... = Nonce: Not set .... 0... .... = Congestion Window Reduced (CWR): Not set .... .0.. .... = ECN-Echo: Not set .... ..0. .... = Urgent: Not set .... ...1 .... = Acknowledgment: Set .... .... 1... = Push: Set .... .... .0.. = Reset: Not set .... .... ..0. = Syn: Not set .... .... ...0 = Fin: Not set [TCP Flags: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7AP\u00b7\u00b7\u00b7] Window size value: 221 [Calculated window size: 28288] [Window size scaling factor: 128] Checksum: 0xd9c6 [unverified] [Checksum Status: Unverified] Urgent pointer: 0 Options: (12 bytes), No-Operation (NOP), No-Operation (NOP), Timestamps TCP Option - No-Operation (NOP) Kind: No-Operation (1) TCP Option - No-Operation (NOP) Kind: No-Operation (1) TCP Option - Timestamps: TSval 44637623, TSecr 44644920 Kind: Time Stamp Option (8) Length: 10 Timestamp value: 44637623 Timestamp echo reply: 44644920 [SEQ/ACK analysis] [iRTT: 0.001410475 seconds] [Bytes in flight: 470] [Bytes sent since last PSH flag: 469] TCP payload (469 bytes) Hypertext Transfer Protocol POST /post HTTP/1.1\\r\\n [Expert Info (Chat/Sequence): POST /post HTTP/1.1\\r\\n] [POST /post HTTP/1.1\\r\\n] [Severity level: Chat] [Group: Sequence] Request Method: POST Request URI: /post Request Version: HTTP/1.1","title":"OpenShift httpbin with tshark sidecar"},{"location":"openshift/openshift_httpbin_tshark_sidecar/#configuring-alertmanager-to-send-webhooks-to-httpbin-pod","text":"Prerequisites: * https://docs.openshift.com/container-platform/3.11/install_config/prometheus_cluster_monitoring.html In the following, replace myuser with the user who shall log into alertmanager: $ oc adm policy add-cluster-role-to-user cluster-monitoring-view myuser cluster role \"cluster-monitoring-view\" added: \"myuser\" We can use the above to tell alertmanager to use httpbin as its web hook: $ oc get routes -n openshift-monitoring NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD alertmanager-main alertmanager-main-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com alertmanager-main web reencrypt None grafana grafana-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com grafana https reencrypt None prometheus-k8s prometheus-k8s-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com prometheus-k8s web reencrypt None Now, access: https://alertmanager-main-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com The status page will show the current alertmanager configuration. The following Red Hat knowledge base solution shows how to update the alertmanager config: https://access.redhat.com/solutions/3804781 Create file: ~/group_vars/OSEv3.yml : openshift_cluster_monitoring_operator_alertmanager_config: |+ global: resolve_timeout: 2m route: group_wait: 5s group_interval: 10s repeat_interval: 20s receiver: default routes: - match: alertname: DeadMansSwitch repeat_interval: 30s receiver: deadmansswitch - match: alertname: DeadMansSwitch repeat_interval: 30s receiver: wh - match: alertname: '*' repeat_interval: 2m receiver: wh - match: severity: critical receiver: wh - match: severity: warning receiver: wh - match: alertname: KubeAPILatencyHigh receiver: wh receivers: - name: default - name: deadmansswitch - name: wh webhook_configs: - url: \"http://httpbin.apps.akaris2.lab.pnq2.cee.redhat.com/anything\" And run: ansible-playbook -i hosts openshift-ansible/playbooks/openshift-monitoring/config.yml -e=\"openshift_cluster_monitoring_operator_install=true\" Verify: $ oc get secret -n openshift-monitoring alertmanager-main -o yaml | awk '/alertmanager.yaml:/ {print $NF}' | base64 -d global: resolve_timeout: 2m route: group_wait: 5s group_interval: 10s repeat_interval: 20s receiver: default routes: - match: alertname: DeadMansSwitch repeat_interval: 30s receiver: deadmansswitch - match: alertname: DeadMansSwitch repeat_interval: 30s receiver: wh - match: alertname: '*' repeat_interval: 2m receiver: wh - match: severity: critical receiver: wh - match: severity: warning receiver: wh - match: alertname: KubeAPILatencyHigh receiver: wh receivers: - name: default - name: deadmansswitch - name: wh webhook_configs: - url: \"http://httpbin.apps.akaris2.lab.pnq2.cee.redhat.com/anything\" Restart pods: $ oc delete pods --selector=app=alertmanager -n openshift-monitoring pod \"alertmanager-main-0\" deleted pod \"alertmanager-main-1\" deleted pod \"alertmanager-main-2\" deleted And check in the web interface of alertmanager to make sure that the new configuration shows up.","title":"Configuring Alertmanager to send webhooks to httpbin pod"},{"location":"openshift/openshift_httpbin_tshark_sidecar/#loading-the-cluster","text":"An easy way to generate an alert in a small lab is to trigger alert KubeletTooManyPods . Go to prometheus and check its configuration: alert: KubeletTooManyPods expr: kubelet_running_pod_count{job=\"kubelet\"} > 250 * 0.9 for: 15m labels: severity: warning annotations: message: Kubelet {{ $labels.instance }} is running {{ $value }} Pods, close to the limit of 250. Then, create the following busybox deployment with a number of pods that exceeds this number, e.g.: busybox.yaml : apiVersion: apps/v1 kind: Deployment metadata: name: busybox-deployment labels: app: busybox-deployment spec: replicas: 500 selector: matchLabels: app: busybox-pod template: metadata: labels: app: busybox-pod spec: containers: - name: busybox image: busybox command: - sleep - infinity imagePullPolicy: IfNotPresent oc apply -f busybox.yaml The cluster will need some time to create those pods and it'll take 15 minutes for the alarm to fire. So take a coffee and come back later. Once the alarm fires in prometheus, go to alertmanager and make sure that it shows there, too. Among others, Alertmanager should show: alertname=\"KubeletTooManyPods\" 16:06:32, 2020-03-11 message: Kubelet 10.74.176.204:10250 is running 250 Pods, close to the limit of 250. severity=\"warning\" service=\"kubelet\"prometheus=\"openshift-monitoring/k8s\"namespace=\"kube-system\"job=\"kubelet\"instance=\"10.74.176.204:10250\"endpoint=\"https-metrics\" Now, it's time to go back to the httpbin pod.","title":"Loading the cluster"},{"location":"openshift/openshift_httpbin_tshark_sidecar/#monitoring-incoming-webhook-reuests","text":"Get the pod name: # oc get pods | grep httpbin httpbin-deploymentconfig-8-8crvh 2/2 Running 0 1h And check the logs of the tshark container which will show a verbose packet capture of HTTP with a destination port of 80 (so we are not capturing the response): # oc logs httpbin-deploymentconfig-8-8crvh -c tshark | tail -n 400 (...) Frame 1708: 5220 bytes on wire (41760 bits), 5220 bytes captured (41760 bits) on interface 0 Interface id: 0 (eth0) Interface name: eth0 (...) Ethernet II, Src: ... (...), Dst: ... (...) (...) Internet Protocol Version 4, Src: ..., Dst: ... (...) Transmission Control Protocol, Src Port: 41606, Dst Port: 80, Seq: 1, Ack: 1, Len: 5154 (...) Hypertext Transfer Protocol POST /anything HTTP/1.1\\r\\n [Expert Info (Chat/Sequence): POST /anything HTTP/1.1\\r\\n] [POST /anything HTTP/1.1\\r\\n] [Severity level: Chat] [Group: Sequence] Request Method: POST Request URI: /anything Request Version: HTTP/1.1 User-Agent: Alertmanager/0.15.2\\r\\n Content-Length: 4743\\r\\n [Content length: 4743] Content-Type: application/json\\r\\n (...) JavaScript Object Notation: application/json Object Member Key: receiver String value: wh Key: receiver Member Key: status String value: firing Key: status Member Key: alerts Array Object Member Key: status String value: firing Key: status Member Key: labels Object Member Key: alertname String value: KubeDaemonSetRolloutStuck Key: alertname Member Key: cluster String value: openshift.akaris2.lab.pnq2.cee.redhat.com Key: cluster Member Key: daemonset String value: node-exporter Key: daemonset Member Key: endpoint String value: https-main Key: endpoint Member Key: instance String value: ...:8443 Key: instance Member Key: job String value: kube-state-metrics Key: job Member Key: namespace String value: openshift-monitoring Key: namespace Member Key: pod String value: kube-state-metrics-6f4c658bcc-v57b6 Key: pod Member Key: prometheus String value: openshift-monitoring/k8s Key: prometheus Member Key: service String value: kube-state-metrics Key: service Member Key: severity String value: critical Key: severity Key: labels Member Key: annotations Object Member Key: message String value: Only 66.66666666666666% of desired pods scheduled and ready for daemon set openshift-monitoring/node-exporter Key: message Key: annotations Member Key: startsAt String value: 2020-03-11T16:07:40.59085788Z Key: startsAt Member Key: endsAt String value: 0001-01-01T00:00:00Z Key: endsAt Member Key: generatorURL String value [truncated]: https://prometheus-k8s-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com/graph?g0.expr=kube_daemonset_status_number_ready%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22%28openshift-.%2A%7Ckube-.%2A%7C Key: generatorURL Object Member Key: status String value: firing Key: status Member Key: labels Object Member Key: alertname String value: KubeDaemonSetRolloutStuck Key: alertname Member Key: cluster String value: openshift.akaris2.lab.pnq2.cee.redhat.com Key: cluster Member Key: daemonset String value: ovs Key: daemonset Member Key: endpoint String value: https-main Key: endpoint Member Key: instance String value: ...:8443 Key: instance Member Key: job String value: kube-state-metrics Key: job Member Key: namespace String value: openshift-sdn Key: namespace Member Key: pod String value: kube-state-metrics-6f4c658bcc-v57b6 Key: pod Member Key: prometheus String value: openshift-monitoring/k8s Key: prometheus Member Key: service String value: kube-state-metrics Key: service Member Key: severity String value: critical Key: severity Key: labels Member Key: annotations Object Member Key: message String value: Only 66.66666666666666% of desired pods scheduled and ready for daemon set openshift-sdn/ovs Key: message Key: annotations Member Key: startsAt String value: 2020-03-11T16:07:40.59085788Z Key: startsAt Member Key: endsAt String value: 0001-01-01T00:00:00Z Key: endsAt Member Key: generatorURL String value [truncated]: https://prometheus-k8s-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com/graph?g0.expr=kube_daemonset_status_number_ready%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22%28openshift-.%2A%7Ckube-.%2A%7C Key: generatorURL Object Member Key: status String value: firing Key: status Member Key: labels Object Member Key: alertname String value: KubeDaemonSetRolloutStuck Key: alertname Member Key: cluster String value: openshift.akaris2.lab.pnq2.cee.redhat.com Key: cluster Member Key: daemonset String value: sdn Key: daemonset Member Key: endpoint String value: https-main Key: endpoint Member Key: instance String value: ...:8443 Key: instance Member Key: job String value: kube-state-metrics Key: job Member Key: namespace String value: openshift-sdn Key: namespace Member Key: pod String value: kube-state-metrics-6f4c658bcc-v57b6 Key: pod Member Key: prometheus String value: openshift-monitoring/k8s Key: prometheus Member Key: service String value: kube-state-metrics Key: service Member Key: severity String value: critical Key: severity Key: labels Member Key: annotations Object Member Key: message String value: Only 66.66666666666666% of desired pods scheduled and ready for daemon set openshift-sdn/sdn Key: message Key: annotations Member Key: startsAt String value: 2020-03-11T16:07:40.59085788Z Key: startsAt Member Key: endsAt String value: 0001-01-01T00:00:00Z Key: endsAt Member Key: generatorURL String value [truncated]: https://prometheus-k8s-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com/graph?g0.expr=kube_daemonset_status_number_ready%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22%28openshift-.%2A%7Ckube-.%2A%7C Key: generatorURL Object Member Key: status String value: firing Key: status Member Key: labels Object Member Key: alertname String value: KubeDaemonSetRolloutStuck Key: alertname Member Key: cluster String value: openshift.akaris2.lab.pnq2.cee.redhat.com Key: cluster Member Key: daemonset String value: sync Key: daemonset Member Key: endpoint String value: https-main Key: endpoint Member Key: instance String value: ...:8443 Key: instance Member Key: job String value: kube-state-metrics Key: job Member Key: namespace String value: openshift-node Key: namespace Member Key: pod String value: kube-state-metrics-6f4c658bcc-v57b6 Key: pod Member Key: prometheus String value: openshift-monitoring/k8s Key: prometheus Member Key: service String value: kube-state-metrics Key: service Member Key: severity String value: critical Key: severity Key: labels Member Key: annotations Object Member Key: message String value: Only 66.66666666666666% of desired pods scheduled and ready for daemon set openshift-node/sync Key: message Key: annotations Member Key: startsAt String value: 2020-03-11T16:07:40.59085788Z Key: startsAt Member Key: endsAt String value: 0001-01-01T00:00:00Z Key: endsAt Member Key: generatorURL String value [truncated]: https://prometheus-k8s-openshift-monitoring.apps.akaris2.lab.pnq2.cee.redhat.com/graph?g0.expr=kube_daemonset_status_number_ready%7Bjob%3D%22kube-state-metrics%22%2Cnamespace%3D~%22%28openshift-.%2A%7Ckube-.%2A%7C Key: generatorURL Key: alerts Member Key: groupLabels Object Key: groupLabels Member Key: commonLabels Object Member Key: alertname String value: KubeDaemonSetRolloutStuck Key: alertname Member Key: cluster String value: openshift.akaris2.lab.pnq2.cee.redhat.com Key: cluster Member Key: endpoint String value: https-main Key: endpoint Member Key: instance String value: ...:8443 Key: instance Member Key: job String value: kube-state-metrics Key: job Member Key: pod String value: kube-state-metrics-6f4c658bcc-v57b6 Key: pod Member Key: prometheus String value: openshift-monitoring/k8s Key: prometheus Member Key: service String value: kube-state-metrics Key: service Member Key: severity String value: critical Key: severity Key: commonLabels","title":"Monitoring incoming webhook reuests"},{"location":"openshift/openshift_mirror_registry/","text":"OpenShfit mirror registry setup In the following, we will run a container registry without hostname (only IP address) and self-signed CA and certificate. We need a virtual machine that serves as the container registry and a host that serves as the OpenShift install server. These can both be on the same machine, to simplify things further. All steps below clarify and should give an easy walkthrough of the OpenShift restricted network installation guide. Follow the OpenShift documentation. The documentation isn't very detailed (at least at the moment), so I'm going through the documentation with an example deployment. https://docs.openshift.com/container-platform/4.5/installing/install_config/installing-restricted-networks-preparations.html Setting up a private registry server After creating a virtual machine that will serve as a registry, install podman and httpd-tools: [root@mirror ~]# sudo yum -y install podman httpd-tools Now, generate a self-signed CA: [root@mirror ~]# mkdir CA [root@mirror ~]# cd CA [root@mirror CA]# openssl genrsa -des3 -out rootCA.key 4096 Generating RSA private key, 4096 bit long modulus .++ ....................................................++ e is 65537 (0x10001) Enter pass phrase for rootCA.key: Verifying - Enter pass phrase for rootCA.key: [root@mirror CA]# ll total 4 -rw-r--r--. 1 root root 3311 Feb 3 09:45 rootCA.key [root@mirror CA]# openssl req -x509 -new -nodes -key rootCA.key -sha256 -days 10240 -out rootCA.crt Enter pass phrase for rootCA.key: You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [XX]:DE State or Province Name (full name) []:NRW Locality Name (eg, city) [Default City]:Acme.Inc Organization Name (eg, company) [Default Company Ltd]: Organizational Unit Name (eg, section) []:acme.root Common Name (eg, your name or your server's hostname) []: Email Address []: [root@mirror CA]# ll total 8 -rw-r--r--. 1 root root 1960 Feb 3 09:47 rootCA.crt -rw-r--r--. 1 root root 3311 Feb 3 09:45 rootCA.key Trust this rootCA on the mirror registry node and on the node where you are running the OpenShift installer: sudo cp rootCA.crt /etc/pki/ca-trust/source/anchors/ sudo update-ca-trust extract Create a certificate signing request: mkdir certificates cd certificates cat<<'EOF'>config [ req ] distinguished_name = req_distinguished_name prompt = no req_extensions = v3_req [ req_distinguished_name ] C=\"DE\" ST=\"NRW\" L=\"Dusseldorf\" O=\"Acme Inc.\" CN=\"192.0.2.100\" emailAddress=\"akaris@example.com\" [ v3_req ] #basicConstraints = CA:FALSE keyUsage = nonRepudiation, digitalSignature, keyEncipherment subjectAltName = @alt_names [alt_names] DNS.1 = 192.0.2.100 IP.1 = 192.0.2.100 DNS.2 = 192.0.3.100 IP.2 = 192.0.3.100 EOF openssl genrsa -out domain.key 4096 openssl req -new -key domain.key -nodes -out domain.csr -config config Verify the CSR: [root@mirror certificates]# openssl req -in domain.csr -noout -text | grep -i dns DNS:192.0.2.100, IP Address:192.0.2.100, DNS:192.0.3.100, IP Address:192.0.3.100 Sign your certificate with the rootCA key and force the SAN entries: [root@mirror certificates]# openssl x509 -req -in domain.csr -CA ../rootCA.crt -CAkey ../rootCA.key -CAcreateserial -out domain.crt -days 3650 -sha256 -extensions v3_req -extfile config Signature ok subject=C = DE, ST = NRW, L = Dusseldorf, O = Acme Inc., CN = 192.0.2.100, emailAddress = akaris@example.com Getting CA Private Key Enter pass phrase for ../rootCA.key: See: https://github.com/openssl/openssl/issues/6481 https://security.stackexchange.com/questions/150078/missing-x509-extensions-with-an-openssl-generated-certificate Then, trust the certificate and make sure that it passes verification and that it contains the SAN entries: [root@mirror certificates]# openssl x509 -in domain.crt -noout -text | grep IP DNS:192.0.2.100, IP Address:192.0.2.100, DNS:192.0.3.100, IP Address:192.0.3.100 [root@mirror certificates]# openssl verify -verbose domain.crt domain.crt: OK In case of issues with certificate verification, verify the cert directly against the root CA file to determine the failure domain: [root@mirror certificates]# openssl verify -CAfile ../rootCA.crt domain.crt domain.crt: OK Generate directories for the registry: sudo mkdir -p /opt/registry/{auth,certs,data} Copy the certificate into /opt/registry/certs as described in the documentation: sudo cp domain.key /opt/registry/certs/ sudo cp domain.crt /opt/registry/certs/ Generate a username and password and store in an htpasswd file for authentication with the registry, e.g. root / password : sudo htpasswd -bBc /opt/registry/auth/htpasswd root password Run the containter with: sudo podman run --name mirror-registry \\ -p 5000:5000 -v /opt/registry/data:/var/lib/registry:z \\ -v /opt/registry/auth:/auth:z -e \"REGISTRY_AUTH=htpasswd\" \\ -e \"REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm\" \\ -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \\ -v /opt/registry/certs:/certs:z \\ -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \\ -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \\ -d docker.io/library/registry:2 Open the firewall with iptables or firewall-cmd as describe in the documentation. In my case, I'm not running a firewall so this step is not needed. Then, verify: [root@mirror ~]# openssl s_client -connect 192.0.2.100:5000 CONNECTED(00000003) Can't use SSL_get_servername depth=1 C = DE, ST = NRW, L = Acme.Inc, O = Default Company Ltd, OU = acme.root verify return:1 depth=0 C = DE, ST = NRW, L = Dusseldorf, O = Acme Inc., CN = 192.0.2.100, emailAddress = akaris@example.com verify return:1 --- Certificate chain 0 s:C = DE, ST = NRW, L = Dusseldorf, O = Acme Inc., CN = 192.0.2.100, emailAddress = akaris@example.com i:C = DE, ST = NRW, L = Acme.Inc, O = Default Company Ltd, OU = acme.root --- (...) Verification: OK (...) And make sure that curl does not report an issue: [root@mirror certificates]# curl -u root:password https://192.0.2.100:5000/v2/_catalog {\"repositories\":[]} If using a separate installation server, make sure that the same verification works from there. Mirroring OpenShift container images into private registry and preparing openshift-install Connect to the installation server and follow steps from the installation guide: https://docs.openshift.com/container-platform/4.5/installing/install_config/installing-restricted-networks-preparations.html Merging the pull secrets Download your pull-secret from https://cloud.redhat.com/openshift/install/pull-secret . At time of this writing, a bug makes this procedure a bit more clumsy: https://bugzilla.redhat.com/show_bug.cgi?id=1866588 $ oc registry login --to ./pull-secret.json --registry \"192.0.2.100:5000\" --auth-basic=root:password error: Missing or incomplete configuration info. Please point to an existing, complete config file: 1. Via the command-line flag --kubeconfig 2. Via the KUBECONFIG environment variable 3. In your home directory as ~/.kube/config To view or setup config directly use the 'config' command. Instead, either follow: https://docs.openshift.com/container-platform/4.2/installing/install_config/installing-restricted-networks-preparations.html#installation-adding-registry-pull-secret_installing-restricted-networks-preparations Or, more elegantly, create a pull secret with podman: $ podman login -u root -p password --authfile pull-secret-podman.json https://192.0.2.100:5000 WARNING! Using --password via the cli is insecure. Please consider using --password-stdin Login Succeeded! $ cat pull-secret-podman.json { \"auths\": { \"192.0.2.100:5000\": { \"auth\": \"cm9vdDpwYXNzd29yZA==\" } } Install jq sudo yum install jq -y Then, merge both secrets: jq -c --argjson var \"$(jq .auths pull-secret-podman.json)\" '.auths += $var' pull-secret.json > pull-secret-merged.json Mirroring the container images Get $OCP_RELEASE from https://quay.io/repository/openshift-release-dev/ocp-release?tag=latest&tab=tags and export required env variables, e.g.: [root@mirror ~]# env | egrep 'OCP|LOCAL|PRODUCT|RELEASE' export RELEASE_NAME=ocp-release export LOCAL_SECRET_JSON=pull-secret-merged.json export OCP_RELEASE=4.5.9 export PRODUCT_REPO=openshift-release-dev export LOCAL_REPOSITORY=ocp4/openshift4 export LOCAL_REGISTRY=192.0.2.100:5000 export ARCHITECTURE=x86_64 export REMOVABLE_MEDIA_PATH=removable_media If you want to simulate the removable media steps, create a directory to simulate removable media and export the variable: export REMOVABLE_MEDIA_PATH=removable_media mkdir removable_media Now, continue with the rest of the instructions from https://docs.openshift.com/container-platform/4.5/installing/install_config/installing-restricted-networks-preparations.html#installation-mirror-repository_installing-restricted-networks-preparations For example: $ oc adm -a ${LOCAL_SECRET_JSON} release mirror --from=quay.io/${PRODUCT_REPO}/${RELEASE_NAME}:${OCP_RELEASE}-${ARCHITECTURE} --to=${LOCAL_REGISTRY}/${LOCAL_REPOSITORY} --to-release-image=${LOCAL_REGISTRY}/${LOCAL_REPOSITORY}:${OCP_RELEASE}-${ARCHITECTURE} After mirroring the images, the following instructions will be presented: (...) Success Update image: 192.0.2.100:5000/ocp4/openshift4:4.5.9-x86_64 Mirror prefix: 192.0.2.100:5000/ocp4/openshift4 To use the new mirrored repository to install, add the following section to the install-config.yaml: imageContentSources: - mirrors: - 192.0.2.100:5000/ocp4/openshift4 source: quay.io/openshift-release-dev/ocp-release - mirrors: - 192.0.2.100:5000/ocp4/openshift4 source: quay.io/openshift-release-dev/ocp-v4.0-art-dev To use the new mirrored repository for upgrades, use the following to create an ImageContentSourcePolicy: apiVersion: operator.openshift.io/v1alpha1 kind: ImageContentSourcePolicy metadata: name: example spec: repositoryDigestMirrors: - mirrors: - 192.0.2.100:5000/ocp4/openshift4 source: quay.io/openshift-release-dev/ocp-release - mirrors: - 192.0.2.100:5000/ocp4/openshift4 source: quay.io/openshift-release-dev/ocp-v4.0-art-dev Make sure to save these instructions for later. Verify contents of the local registry: $ curl -u root:password https://192.0.2.100:5000/v2/_catalog | jq % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 37 100 37 0 0 1121 0 --:--:-- --:--:-- --:--:-- 1121 { \"repositories\": [ \"ocp4/openshift4\" ] } $ curl -u root:password https://192.0.2.100:5000/v2/ocp4/openshift4/tags/list | jq % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 3241 100 3241 0 0 102k 0 --:--:-- --:--:-- --:--:-- 102k { \"name\": \"ocp4/openshift4\", \"tags\": [ \"4.5.9-kube-storage-version-migrator\", \"4.5.9-csi-snapshot-controller\", \"4.5.9-cluster-kube-apiserver-operator\", \"4.5.9-grafana\", \"4.5.9-cluster-update-keys\", \"4.5.9-hyperkube\", \"4.5.9-container-networking-plugins\", \"4.5.9-etcd\", \"4.5.9-baremetal-operator\", \"4.5.9-jenkins-agent-nodejs\", \"4.5.9-cluster-monitoring-operator\", \"4.5.9-installer\", \"4.5.9-aws-machine-controllers\", \"4.5.9-kube-etcd-signer-server\", \"4.5.9-kuryr-controller\", \"4.5.9-oauth-server\", \"4.5.9-cluster-authentication-operator\", \"4.5.9-operator-lifecycle-manager\", \"4.5.9-haproxy-router\", \"4.5.9-cluster-node-tuning-operator\", \"4.5.9-openshift-state-metrics\", \"4.5.9-prom-label-proxy\", \"4.5.9-cluster-kube-scheduler-operator\", \"4.5.9-machine-os-content\", \"4.5.9-cli-artifacts\", \"4.5.9-cluster-kube-controller-manager-operator\", \"4.5.9-libvirt-machine-controllers\", \"4.5.9-console-operator\", \"4.5.9-prometheus-operator\", \"4.5.9-openshift-apiserver\", \"4.5.9-x86_64\", \"4.5.9-console\", \"4.5.9-operator-marketplace\", \"4.5.9-cluster-etcd-operator\", \"4.5.9-k8s-prometheus-adapter\", \"4.5.9-ironic-inspector\", \"4.5.9-kube-rbac-proxy\", \"4.5.9-machine-config-operator\", \"4.5.9-ovirt-machine-controllers\", \"4.5.9-cluster-dns-operator\", \"4.5.9-aws-pod-identity-webhook\", \"4.5.9-ironic-machine-os-downloader\", \"4.5.9-cluster-csi-snapshot-controller-operator\", \"4.5.9-cluster-image-registry-operator\", \"4.5.9-kube-proxy\", \"4.5.9-cluster-autoscaler-operator\", \"4.5.9-multus-whereabouts-ipam-cni\", \"4.5.9-cluster-bootstrap\", \"4.5.9-insights-operator\", \"4.5.9-keepalived-ipfailover\", \"4.5.9-machine-api-operator\", \"4.5.9-azure-machine-controllers\", \"4.5.9-cluster-policy-controller\", \"4.5.9-cluster-machine-approver\", \"4.5.9-deployer\", \"4.5.9-installer-artifacts\", \"4.5.9-gcp-machine-controllers\", \"4.5.9-service-ca-operator\", \"4.5.9-prometheus-config-reloader\", \"4.5.9-oauth-proxy\", \"4.5.9-ovn-kubernetes\", \"4.5.9-prometheus\", \"4.5.9-sdn\", \"4.5.9-ironic-hardware-inventory-recorder\", \"4.5.9-baremetal-runtimecfg\", \"4.5.9-baremetal-installer\", \"4.5.9-ironic-ipa-downloader\", \"4.5.9-operator-registry\", \"4.5.9-pod\", \"4.5.9-multus-cni\", \"4.5.9-cloud-credential-operator\", \"4.5.9-kuryr-cni\", \"4.5.9-cluster-kube-storage-version-migrator-operator\", \"4.5.9-docker-builder\", \"4.5.9-telemeter\", \"4.5.9-tools\", \"4.5.9-docker-registry\", \"4.5.9-baremetal-machine-controllers\", \"4.5.9-tests\", \"4.5.9-cluster-svcat-controller-manager-operator\", \"4.5.9-cluster-ingress-operator\", \"4.5.9-kube-client-agent\", \"4.5.9-cli\", \"4.5.9-ironic-static-ip-manager\", \"4.5.9-jenkins\", \"4.5.9-cluster-node-tuned\", \"4.5.9-cluster-storage-operator\", \"4.5.9-prometheus-node-exporter\", \"4.5.9-openstack-machine-controllers\", \"4.5.9-cluster-openshift-apiserver-operator\", \"4.5.9-ironic\", \"4.5.9-cluster-version-operator\", \"4.5.9-local-storage-static-provisioner\", \"4.5.9-mdns-publisher\", \"4.5.9-multus-route-override-cni\", \"4.5.9-thanos\", \"4.5.9-cluster-config-operator\", \"4.5.9-jenkins-agent-maven\", \"4.5.9-openshift-controller-manager\", \"4.5.9-prometheus-alertmanager\", \"4.5.9-cluster-autoscaler\", \"4.5.9-configmap-reloader\", \"4.5.9-multus-admission-controller\", \"4.5.9-kube-state-metrics\", \"4.5.9-coredns\", \"4.5.9-cluster-openshift-controller-manager-operator\", \"4.5.9-cluster-samples-operator\", \"4.5.9-cluster-svcat-apiserver-operator\", \"4.5.9-must-gather\", \"4.5.9-cluster-network-operator\" ] } Now, extract the new openshift-install command: $ oc adm -a ${LOCAL_SECRET_JSON} release extract --command=openshift-install \"${LOCAL_REGISTRY}/${LOCAL_REPOSITORY}:${OCP_RELEASE}-${ARCHITECTURE} The resulting file can be found in the current working directory: $ ls openshift-install openshift-install Installing a disconnected cluster Once the new openshift-install client was created, continue with the actual cluster installation. For example, on AWS, use https://docs.openshift.com/container-platform/4.5/installing/installing_aws/installing-restricted-networks-aws.html#installing-restricted-networks-aws Create and then modify the install-config.yaml file on the installation server. Make sure to add imageContentSources (from the output of the last command) and also add the rootCA to the additionalTrustBundle : cat install-config.yaml (...) imageContentSources: - mirrors: - 10.10.181.198:5000/ocp4/openshift4 source: quay.io/openshift-release-dev/ocp-release - mirrors: - 10.10.181.198:5000/ocp4/openshift4 source: quay.io/openshift-release-dev/ocp-v4.0-art-dev additionalTrustBundle: | -----BEGIN CERTIFICATE----- MIIFxjCCA66gAwIBAgIJAOpJf+VRkKHUMA0GCSqGSIb3DQEBCwUAMIGKMQswCQYD VQQGEwJVUzEXMBUGA1UECAwOTm9ydGggQ2Fyb2xpbmExEDAOBgNVBAcMB1JhbGVp (... contents of CA/rootCA.crt ...) oxmeuWzlWvcOCb4Usxa9m0rO95fa5Af2QoA4qxlvi7JoTypRR/zAskqHmcsaTfJN k4p3g3YK5u5tvbzERBTfl8bfbd/eIwBLNMDwmKk2z42bcP1OAAHHqCfChvc1Zasr TFVS2yQvfvvYSzW6tQE9UphVIiXeQhGhl7+TQ1wgoHRQL3pZRwxX9PrT -----END CERTIFICATE----- (...) Also, make sure to modify the pullSecret section in install-config.yaml to include the credentials for the custom registry: pullSecret: '(... contents of pull-secret-merged.json ...)' Steps for modifying install-config.yaml can be found in https://docs.openshift.com/container-platform/4.5/installing/installing_aws/installing-restricted-networks-aws.html#installation-generate-aws-user-infra-install-config_installing-restricted-networks-aws Follow any further steps from the restricted installation documentation. Finally, run the installation. Use the custom openshift-install binary that was generated earlier: ./openshift-install create cluster --dir=install-config/ --log-level=debug","title":"Installing a cluster with a mirror registry"},{"location":"openshift/openshift_mirror_registry/#openshfit-mirror-registry-setup","text":"In the following, we will run a container registry without hostname (only IP address) and self-signed CA and certificate. We need a virtual machine that serves as the container registry and a host that serves as the OpenShift install server. These can both be on the same machine, to simplify things further. All steps below clarify and should give an easy walkthrough of the OpenShift restricted network installation guide. Follow the OpenShift documentation. The documentation isn't very detailed (at least at the moment), so I'm going through the documentation with an example deployment. https://docs.openshift.com/container-platform/4.5/installing/install_config/installing-restricted-networks-preparations.html","title":"OpenShfit mirror registry setup"},{"location":"openshift/openshift_mirror_registry/#setting-up-a-private-registry-server","text":"After creating a virtual machine that will serve as a registry, install podman and httpd-tools: [root@mirror ~]# sudo yum -y install podman httpd-tools Now, generate a self-signed CA: [root@mirror ~]# mkdir CA [root@mirror ~]# cd CA [root@mirror CA]# openssl genrsa -des3 -out rootCA.key 4096 Generating RSA private key, 4096 bit long modulus .++ ....................................................++ e is 65537 (0x10001) Enter pass phrase for rootCA.key: Verifying - Enter pass phrase for rootCA.key: [root@mirror CA]# ll total 4 -rw-r--r--. 1 root root 3311 Feb 3 09:45 rootCA.key [root@mirror CA]# openssl req -x509 -new -nodes -key rootCA.key -sha256 -days 10240 -out rootCA.crt Enter pass phrase for rootCA.key: You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [XX]:DE State or Province Name (full name) []:NRW Locality Name (eg, city) [Default City]:Acme.Inc Organization Name (eg, company) [Default Company Ltd]: Organizational Unit Name (eg, section) []:acme.root Common Name (eg, your name or your server's hostname) []: Email Address []: [root@mirror CA]# ll total 8 -rw-r--r--. 1 root root 1960 Feb 3 09:47 rootCA.crt -rw-r--r--. 1 root root 3311 Feb 3 09:45 rootCA.key Trust this rootCA on the mirror registry node and on the node where you are running the OpenShift installer: sudo cp rootCA.crt /etc/pki/ca-trust/source/anchors/ sudo update-ca-trust extract Create a certificate signing request: mkdir certificates cd certificates cat<<'EOF'>config [ req ] distinguished_name = req_distinguished_name prompt = no req_extensions = v3_req [ req_distinguished_name ] C=\"DE\" ST=\"NRW\" L=\"Dusseldorf\" O=\"Acme Inc.\" CN=\"192.0.2.100\" emailAddress=\"akaris@example.com\" [ v3_req ] #basicConstraints = CA:FALSE keyUsage = nonRepudiation, digitalSignature, keyEncipherment subjectAltName = @alt_names [alt_names] DNS.1 = 192.0.2.100 IP.1 = 192.0.2.100 DNS.2 = 192.0.3.100 IP.2 = 192.0.3.100 EOF openssl genrsa -out domain.key 4096 openssl req -new -key domain.key -nodes -out domain.csr -config config Verify the CSR: [root@mirror certificates]# openssl req -in domain.csr -noout -text | grep -i dns DNS:192.0.2.100, IP Address:192.0.2.100, DNS:192.0.3.100, IP Address:192.0.3.100 Sign your certificate with the rootCA key and force the SAN entries: [root@mirror certificates]# openssl x509 -req -in domain.csr -CA ../rootCA.crt -CAkey ../rootCA.key -CAcreateserial -out domain.crt -days 3650 -sha256 -extensions v3_req -extfile config Signature ok subject=C = DE, ST = NRW, L = Dusseldorf, O = Acme Inc., CN = 192.0.2.100, emailAddress = akaris@example.com Getting CA Private Key Enter pass phrase for ../rootCA.key: See: https://github.com/openssl/openssl/issues/6481 https://security.stackexchange.com/questions/150078/missing-x509-extensions-with-an-openssl-generated-certificate Then, trust the certificate and make sure that it passes verification and that it contains the SAN entries: [root@mirror certificates]# openssl x509 -in domain.crt -noout -text | grep IP DNS:192.0.2.100, IP Address:192.0.2.100, DNS:192.0.3.100, IP Address:192.0.3.100 [root@mirror certificates]# openssl verify -verbose domain.crt domain.crt: OK In case of issues with certificate verification, verify the cert directly against the root CA file to determine the failure domain: [root@mirror certificates]# openssl verify -CAfile ../rootCA.crt domain.crt domain.crt: OK Generate directories for the registry: sudo mkdir -p /opt/registry/{auth,certs,data} Copy the certificate into /opt/registry/certs as described in the documentation: sudo cp domain.key /opt/registry/certs/ sudo cp domain.crt /opt/registry/certs/ Generate a username and password and store in an htpasswd file for authentication with the registry, e.g. root / password : sudo htpasswd -bBc /opt/registry/auth/htpasswd root password Run the containter with: sudo podman run --name mirror-registry \\ -p 5000:5000 -v /opt/registry/data:/var/lib/registry:z \\ -v /opt/registry/auth:/auth:z -e \"REGISTRY_AUTH=htpasswd\" \\ -e \"REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm\" \\ -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \\ -v /opt/registry/certs:/certs:z \\ -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \\ -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \\ -d docker.io/library/registry:2 Open the firewall with iptables or firewall-cmd as describe in the documentation. In my case, I'm not running a firewall so this step is not needed. Then, verify: [root@mirror ~]# openssl s_client -connect 192.0.2.100:5000 CONNECTED(00000003) Can't use SSL_get_servername depth=1 C = DE, ST = NRW, L = Acme.Inc, O = Default Company Ltd, OU = acme.root verify return:1 depth=0 C = DE, ST = NRW, L = Dusseldorf, O = Acme Inc., CN = 192.0.2.100, emailAddress = akaris@example.com verify return:1 --- Certificate chain 0 s:C = DE, ST = NRW, L = Dusseldorf, O = Acme Inc., CN = 192.0.2.100, emailAddress = akaris@example.com i:C = DE, ST = NRW, L = Acme.Inc, O = Default Company Ltd, OU = acme.root --- (...) Verification: OK (...) And make sure that curl does not report an issue: [root@mirror certificates]# curl -u root:password https://192.0.2.100:5000/v2/_catalog {\"repositories\":[]} If using a separate installation server, make sure that the same verification works from there.","title":"Setting up a private registry server"},{"location":"openshift/openshift_mirror_registry/#mirroring-openshift-container-images-into-private-registry-and-preparing-openshift-install","text":"Connect to the installation server and follow steps from the installation guide: https://docs.openshift.com/container-platform/4.5/installing/install_config/installing-restricted-networks-preparations.html","title":"Mirroring OpenShift container images into private registry and preparing openshift-install"},{"location":"openshift/openshift_mirror_registry/#merging-the-pull-secrets","text":"Download your pull-secret from https://cloud.redhat.com/openshift/install/pull-secret . At time of this writing, a bug makes this procedure a bit more clumsy: https://bugzilla.redhat.com/show_bug.cgi?id=1866588 $ oc registry login --to ./pull-secret.json --registry \"192.0.2.100:5000\" --auth-basic=root:password error: Missing or incomplete configuration info. Please point to an existing, complete config file: 1. Via the command-line flag --kubeconfig 2. Via the KUBECONFIG environment variable 3. In your home directory as ~/.kube/config To view or setup config directly use the 'config' command. Instead, either follow: https://docs.openshift.com/container-platform/4.2/installing/install_config/installing-restricted-networks-preparations.html#installation-adding-registry-pull-secret_installing-restricted-networks-preparations Or, more elegantly, create a pull secret with podman: $ podman login -u root -p password --authfile pull-secret-podman.json https://192.0.2.100:5000 WARNING! Using --password via the cli is insecure. Please consider using --password-stdin Login Succeeded! $ cat pull-secret-podman.json { \"auths\": { \"192.0.2.100:5000\": { \"auth\": \"cm9vdDpwYXNzd29yZA==\" } } Install jq sudo yum install jq -y Then, merge both secrets: jq -c --argjson var \"$(jq .auths pull-secret-podman.json)\" '.auths += $var' pull-secret.json > pull-secret-merged.json","title":"Merging the pull secrets"},{"location":"openshift/openshift_mirror_registry/#mirroring-the-container-images","text":"Get $OCP_RELEASE from https://quay.io/repository/openshift-release-dev/ocp-release?tag=latest&tab=tags and export required env variables, e.g.: [root@mirror ~]# env | egrep 'OCP|LOCAL|PRODUCT|RELEASE' export RELEASE_NAME=ocp-release export LOCAL_SECRET_JSON=pull-secret-merged.json export OCP_RELEASE=4.5.9 export PRODUCT_REPO=openshift-release-dev export LOCAL_REPOSITORY=ocp4/openshift4 export LOCAL_REGISTRY=192.0.2.100:5000 export ARCHITECTURE=x86_64 export REMOVABLE_MEDIA_PATH=removable_media If you want to simulate the removable media steps, create a directory to simulate removable media and export the variable: export REMOVABLE_MEDIA_PATH=removable_media mkdir removable_media Now, continue with the rest of the instructions from https://docs.openshift.com/container-platform/4.5/installing/install_config/installing-restricted-networks-preparations.html#installation-mirror-repository_installing-restricted-networks-preparations For example: $ oc adm -a ${LOCAL_SECRET_JSON} release mirror --from=quay.io/${PRODUCT_REPO}/${RELEASE_NAME}:${OCP_RELEASE}-${ARCHITECTURE} --to=${LOCAL_REGISTRY}/${LOCAL_REPOSITORY} --to-release-image=${LOCAL_REGISTRY}/${LOCAL_REPOSITORY}:${OCP_RELEASE}-${ARCHITECTURE} After mirroring the images, the following instructions will be presented: (...) Success Update image: 192.0.2.100:5000/ocp4/openshift4:4.5.9-x86_64 Mirror prefix: 192.0.2.100:5000/ocp4/openshift4 To use the new mirrored repository to install, add the following section to the install-config.yaml: imageContentSources: - mirrors: - 192.0.2.100:5000/ocp4/openshift4 source: quay.io/openshift-release-dev/ocp-release - mirrors: - 192.0.2.100:5000/ocp4/openshift4 source: quay.io/openshift-release-dev/ocp-v4.0-art-dev To use the new mirrored repository for upgrades, use the following to create an ImageContentSourcePolicy: apiVersion: operator.openshift.io/v1alpha1 kind: ImageContentSourcePolicy metadata: name: example spec: repositoryDigestMirrors: - mirrors: - 192.0.2.100:5000/ocp4/openshift4 source: quay.io/openshift-release-dev/ocp-release - mirrors: - 192.0.2.100:5000/ocp4/openshift4 source: quay.io/openshift-release-dev/ocp-v4.0-art-dev Make sure to save these instructions for later. Verify contents of the local registry: $ curl -u root:password https://192.0.2.100:5000/v2/_catalog | jq % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 37 100 37 0 0 1121 0 --:--:-- --:--:-- --:--:-- 1121 { \"repositories\": [ \"ocp4/openshift4\" ] } $ curl -u root:password https://192.0.2.100:5000/v2/ocp4/openshift4/tags/list | jq % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 3241 100 3241 0 0 102k 0 --:--:-- --:--:-- --:--:-- 102k { \"name\": \"ocp4/openshift4\", \"tags\": [ \"4.5.9-kube-storage-version-migrator\", \"4.5.9-csi-snapshot-controller\", \"4.5.9-cluster-kube-apiserver-operator\", \"4.5.9-grafana\", \"4.5.9-cluster-update-keys\", \"4.5.9-hyperkube\", \"4.5.9-container-networking-plugins\", \"4.5.9-etcd\", \"4.5.9-baremetal-operator\", \"4.5.9-jenkins-agent-nodejs\", \"4.5.9-cluster-monitoring-operator\", \"4.5.9-installer\", \"4.5.9-aws-machine-controllers\", \"4.5.9-kube-etcd-signer-server\", \"4.5.9-kuryr-controller\", \"4.5.9-oauth-server\", \"4.5.9-cluster-authentication-operator\", \"4.5.9-operator-lifecycle-manager\", \"4.5.9-haproxy-router\", \"4.5.9-cluster-node-tuning-operator\", \"4.5.9-openshift-state-metrics\", \"4.5.9-prom-label-proxy\", \"4.5.9-cluster-kube-scheduler-operator\", \"4.5.9-machine-os-content\", \"4.5.9-cli-artifacts\", \"4.5.9-cluster-kube-controller-manager-operator\", \"4.5.9-libvirt-machine-controllers\", \"4.5.9-console-operator\", \"4.5.9-prometheus-operator\", \"4.5.9-openshift-apiserver\", \"4.5.9-x86_64\", \"4.5.9-console\", \"4.5.9-operator-marketplace\", \"4.5.9-cluster-etcd-operator\", \"4.5.9-k8s-prometheus-adapter\", \"4.5.9-ironic-inspector\", \"4.5.9-kube-rbac-proxy\", \"4.5.9-machine-config-operator\", \"4.5.9-ovirt-machine-controllers\", \"4.5.9-cluster-dns-operator\", \"4.5.9-aws-pod-identity-webhook\", \"4.5.9-ironic-machine-os-downloader\", \"4.5.9-cluster-csi-snapshot-controller-operator\", \"4.5.9-cluster-image-registry-operator\", \"4.5.9-kube-proxy\", \"4.5.9-cluster-autoscaler-operator\", \"4.5.9-multus-whereabouts-ipam-cni\", \"4.5.9-cluster-bootstrap\", \"4.5.9-insights-operator\", \"4.5.9-keepalived-ipfailover\", \"4.5.9-machine-api-operator\", \"4.5.9-azure-machine-controllers\", \"4.5.9-cluster-policy-controller\", \"4.5.9-cluster-machine-approver\", \"4.5.9-deployer\", \"4.5.9-installer-artifacts\", \"4.5.9-gcp-machine-controllers\", \"4.5.9-service-ca-operator\", \"4.5.9-prometheus-config-reloader\", \"4.5.9-oauth-proxy\", \"4.5.9-ovn-kubernetes\", \"4.5.9-prometheus\", \"4.5.9-sdn\", \"4.5.9-ironic-hardware-inventory-recorder\", \"4.5.9-baremetal-runtimecfg\", \"4.5.9-baremetal-installer\", \"4.5.9-ironic-ipa-downloader\", \"4.5.9-operator-registry\", \"4.5.9-pod\", \"4.5.9-multus-cni\", \"4.5.9-cloud-credential-operator\", \"4.5.9-kuryr-cni\", \"4.5.9-cluster-kube-storage-version-migrator-operator\", \"4.5.9-docker-builder\", \"4.5.9-telemeter\", \"4.5.9-tools\", \"4.5.9-docker-registry\", \"4.5.9-baremetal-machine-controllers\", \"4.5.9-tests\", \"4.5.9-cluster-svcat-controller-manager-operator\", \"4.5.9-cluster-ingress-operator\", \"4.5.9-kube-client-agent\", \"4.5.9-cli\", \"4.5.9-ironic-static-ip-manager\", \"4.5.9-jenkins\", \"4.5.9-cluster-node-tuned\", \"4.5.9-cluster-storage-operator\", \"4.5.9-prometheus-node-exporter\", \"4.5.9-openstack-machine-controllers\", \"4.5.9-cluster-openshift-apiserver-operator\", \"4.5.9-ironic\", \"4.5.9-cluster-version-operator\", \"4.5.9-local-storage-static-provisioner\", \"4.5.9-mdns-publisher\", \"4.5.9-multus-route-override-cni\", \"4.5.9-thanos\", \"4.5.9-cluster-config-operator\", \"4.5.9-jenkins-agent-maven\", \"4.5.9-openshift-controller-manager\", \"4.5.9-prometheus-alertmanager\", \"4.5.9-cluster-autoscaler\", \"4.5.9-configmap-reloader\", \"4.5.9-multus-admission-controller\", \"4.5.9-kube-state-metrics\", \"4.5.9-coredns\", \"4.5.9-cluster-openshift-controller-manager-operator\", \"4.5.9-cluster-samples-operator\", \"4.5.9-cluster-svcat-apiserver-operator\", \"4.5.9-must-gather\", \"4.5.9-cluster-network-operator\" ] } Now, extract the new openshift-install command: $ oc adm -a ${LOCAL_SECRET_JSON} release extract --command=openshift-install \"${LOCAL_REGISTRY}/${LOCAL_REPOSITORY}:${OCP_RELEASE}-${ARCHITECTURE} The resulting file can be found in the current working directory: $ ls openshift-install openshift-install","title":"Mirroring the container images"},{"location":"openshift/openshift_mirror_registry/#installing-a-disconnected-cluster","text":"Once the new openshift-install client was created, continue with the actual cluster installation. For example, on AWS, use https://docs.openshift.com/container-platform/4.5/installing/installing_aws/installing-restricted-networks-aws.html#installing-restricted-networks-aws Create and then modify the install-config.yaml file on the installation server. Make sure to add imageContentSources (from the output of the last command) and also add the rootCA to the additionalTrustBundle : cat install-config.yaml (...) imageContentSources: - mirrors: - 10.10.181.198:5000/ocp4/openshift4 source: quay.io/openshift-release-dev/ocp-release - mirrors: - 10.10.181.198:5000/ocp4/openshift4 source: quay.io/openshift-release-dev/ocp-v4.0-art-dev additionalTrustBundle: | -----BEGIN CERTIFICATE----- MIIFxjCCA66gAwIBAgIJAOpJf+VRkKHUMA0GCSqGSIb3DQEBCwUAMIGKMQswCQYD VQQGEwJVUzEXMBUGA1UECAwOTm9ydGggQ2Fyb2xpbmExEDAOBgNVBAcMB1JhbGVp (... contents of CA/rootCA.crt ...) oxmeuWzlWvcOCb4Usxa9m0rO95fa5Af2QoA4qxlvi7JoTypRR/zAskqHmcsaTfJN k4p3g3YK5u5tvbzERBTfl8bfbd/eIwBLNMDwmKk2z42bcP1OAAHHqCfChvc1Zasr TFVS2yQvfvvYSzW6tQE9UphVIiXeQhGhl7+TQ1wgoHRQL3pZRwxX9PrT -----END CERTIFICATE----- (...) Also, make sure to modify the pullSecret section in install-config.yaml to include the credentials for the custom registry: pullSecret: '(... contents of pull-secret-merged.json ...)' Steps for modifying install-config.yaml can be found in https://docs.openshift.com/container-platform/4.5/installing/installing_aws/installing-restricted-networks-aws.html#installation-generate-aws-user-infra-install-config_installing-restricted-networks-aws Follow any further steps from the restricted installation documentation. Finally, run the installation. Use the custom openshift-install binary that was generated earlier: ./openshift-install create cluster --dir=install-config/ --log-level=debug","title":"Installing a disconnected cluster"},{"location":"openshift/openshift_troubleshooting_etcd_state/","text":"Troubleshooting etcd state Troubleshooting steps [akaris@linux 00000001]$ oc get pods -A | grep etcd-member openshift-etcd etcd-member-cluste-xxxxx-m-1.c.akaris-00000001.internal 2/2 Running 0 3h49m openshift-etcd etcd-member-cluste-xxxxx-m-2.c.akaris-00000001.internal 2/2 Running 0 3h49m [akaris@linux 00000001]$ oc exec -n openshift-etcd -it etcd-member-cluste-xxxxx-m-1.c.akaris-00000001.internal /bin/bash Defaulting container name to etcd-member. Use 'oc describe pod/etcd-member-cluste-xxxxx-m-1.c.akaris-00000001.internal -n openshift-etcd' to see all of the containers in this pod. [root@cluste-xxxxx-m-1 /]# export ETCDCTL_API=3 ETCDCTL_CACERT=/etc/ssl/etcd/ca.crt ETCDCTL_CERT=$(find /etc/ssl/ -name *peer*crt) ETCDCTL_KEY=$(find /etc/ssl/ -name *peer*key) [root@cluste-xxxxx-m-1 /]# etcdctl member list -w table +------------------+---------+---------------------------------------------------------+-------------------------------------------------+-----------------------+ | ID | STATUS | NAME | PEER ADDRS | CLIENT ADDRS | +------------------+---------+---------------------------------------------------------+-------------------------------------------------+-----------------------+ | 8338e0f35d1ea667 | started | etcd-member-cluste-xxxxx-m-0.c.akaris-00000001.internal | https://etcd-0.cluster.ocptest.example.net:2380 | https://10.0.0.5:2379 | | 87cd63e5849290c1 | started | etcd-member-cluste-xxxxx-m-2.c.akaris-00000001.internal | https://etcd-2.cluster.ocptest.example.net:2380 | https://10.0.0.3:2379 | | f2a4a192cecf2103 | started | etcd-member-cluste-xxxxx-m-1.c.akaris-00000001.internal | https://etcd-1.cluster.ocptest.example.net:2380 | https://10.0.0.4:2379 | +------------------+---------+---------------------------------------------------------+-------------------------------------------------+-----------------------+ [root@cluste-xxxxx-m-1 /]# etcdctl endpoint status --endpoints=$(etcdctl member list | cut -d, -f5 | sed -e 's/ //g' | paste -sd ',') --write-out table {\"level\":\"warn\",\"ts\":\"2020-02-18T12:46:55.693Z\",\"caller\":\"clientv3/retry_interceptor.go:61\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"passthrough:///https://10.0.0.5:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = context deadline exceeded\"} Failed to get the status of endpoint https://10.0.0.5:2379 (context deadline exceeded) +-----------------------+------------------+---------+---------+-----------+-----------+------------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX | +-----------------------+------------------+---------+---------+-----------+-----------+------------+ | https://10.0.0.3:2379 | 87cd63e5849290c1 | 3.3.17 | 69 MB | false | 11 | 88235 | | https://10.0.0.4:2379 | f2a4a192cecf2103 | 3.3.17 | 68 MB | true | 11 | 88235 | +-----------------------+------------------+---------+---------+-----------+-----------+------------+ [root@cluste-xxxxx-m-1 /]# etcdctl endpoint health --endpoints=$(etcdctl member list | cut -d, -f5 | sed -e 's/ //g' | paste -sd ',') --write-out table {\"level\":\"warn\",\"ts\":\"2020-02-18T12:47:08.048Z\",\"caller\":\"clientv3/retry_interceptor.go:61\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"endpoint://client-01c6d062-363d-44cf-b92b-3e5356ed2579/10.0.0.5:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = context deadline exceeded\"} +-----------------------+--------+--------------+---------------------------+ | ENDPOINT | HEALTH | TOOK | ERROR | +-----------------------+--------+--------------+---------------------------+ | https://10.0.0.4:2379 | true | 23.353459ms | | | https://10.0.0.3:2379 | true | 22.503798ms | | | https://10.0.0.5:2379 | false | 5.000398701s | context deadline exceeded | +-----------------------+--------+--------------+---------------------------+ Error: unhealthy cluster [root@cluste-xxxxx-m-1 /]# etcdctl alarm list [root@cluste-xxxxx-m-1 /]# Resources https://rancher.com/docs/rancher/v2.x/en/troubleshooting/kubernetes-components/etcd/#etcd-cluster-and-connectivity-checks","title":"Troubleshooting etcd state"},{"location":"openshift/openshift_troubleshooting_etcd_state/#troubleshooting-etcd-state","text":"","title":"Troubleshooting etcd state"},{"location":"openshift/openshift_troubleshooting_etcd_state/#troubleshooting-steps","text":"[akaris@linux 00000001]$ oc get pods -A | grep etcd-member openshift-etcd etcd-member-cluste-xxxxx-m-1.c.akaris-00000001.internal 2/2 Running 0 3h49m openshift-etcd etcd-member-cluste-xxxxx-m-2.c.akaris-00000001.internal 2/2 Running 0 3h49m [akaris@linux 00000001]$ oc exec -n openshift-etcd -it etcd-member-cluste-xxxxx-m-1.c.akaris-00000001.internal /bin/bash Defaulting container name to etcd-member. Use 'oc describe pod/etcd-member-cluste-xxxxx-m-1.c.akaris-00000001.internal -n openshift-etcd' to see all of the containers in this pod. [root@cluste-xxxxx-m-1 /]# export ETCDCTL_API=3 ETCDCTL_CACERT=/etc/ssl/etcd/ca.crt ETCDCTL_CERT=$(find /etc/ssl/ -name *peer*crt) ETCDCTL_KEY=$(find /etc/ssl/ -name *peer*key) [root@cluste-xxxxx-m-1 /]# etcdctl member list -w table +------------------+---------+---------------------------------------------------------+-------------------------------------------------+-----------------------+ | ID | STATUS | NAME | PEER ADDRS | CLIENT ADDRS | +------------------+---------+---------------------------------------------------------+-------------------------------------------------+-----------------------+ | 8338e0f35d1ea667 | started | etcd-member-cluste-xxxxx-m-0.c.akaris-00000001.internal | https://etcd-0.cluster.ocptest.example.net:2380 | https://10.0.0.5:2379 | | 87cd63e5849290c1 | started | etcd-member-cluste-xxxxx-m-2.c.akaris-00000001.internal | https://etcd-2.cluster.ocptest.example.net:2380 | https://10.0.0.3:2379 | | f2a4a192cecf2103 | started | etcd-member-cluste-xxxxx-m-1.c.akaris-00000001.internal | https://etcd-1.cluster.ocptest.example.net:2380 | https://10.0.0.4:2379 | +------------------+---------+---------------------------------------------------------+-------------------------------------------------+-----------------------+ [root@cluste-xxxxx-m-1 /]# etcdctl endpoint status --endpoints=$(etcdctl member list | cut -d, -f5 | sed -e 's/ //g' | paste -sd ',') --write-out table {\"level\":\"warn\",\"ts\":\"2020-02-18T12:46:55.693Z\",\"caller\":\"clientv3/retry_interceptor.go:61\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"passthrough:///https://10.0.0.5:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = context deadline exceeded\"} Failed to get the status of endpoint https://10.0.0.5:2379 (context deadline exceeded) +-----------------------+------------------+---------+---------+-----------+-----------+------------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX | +-----------------------+------------------+---------+---------+-----------+-----------+------------+ | https://10.0.0.3:2379 | 87cd63e5849290c1 | 3.3.17 | 69 MB | false | 11 | 88235 | | https://10.0.0.4:2379 | f2a4a192cecf2103 | 3.3.17 | 68 MB | true | 11 | 88235 | +-----------------------+------------------+---------+---------+-----------+-----------+------------+ [root@cluste-xxxxx-m-1 /]# etcdctl endpoint health --endpoints=$(etcdctl member list | cut -d, -f5 | sed -e 's/ //g' | paste -sd ',') --write-out table {\"level\":\"warn\",\"ts\":\"2020-02-18T12:47:08.048Z\",\"caller\":\"clientv3/retry_interceptor.go:61\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"endpoint://client-01c6d062-363d-44cf-b92b-3e5356ed2579/10.0.0.5:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = context deadline exceeded\"} +-----------------------+--------+--------------+---------------------------+ | ENDPOINT | HEALTH | TOOK | ERROR | +-----------------------+--------+--------------+---------------------------+ | https://10.0.0.4:2379 | true | 23.353459ms | | | https://10.0.0.3:2379 | true | 22.503798ms | | | https://10.0.0.5:2379 | false | 5.000398701s | context deadline exceeded | +-----------------------+--------+--------------+---------------------------+ Error: unhealthy cluster [root@cluste-xxxxx-m-1 /]# etcdctl alarm list [root@cluste-xxxxx-m-1 /]#","title":"Troubleshooting steps"},{"location":"openshift/openshift_troubleshooting_etcd_state/#resources","text":"https://rancher.com/docs/rancher/v2.x/en/troubleshooting/kubernetes-components/etcd/#etcd-cluster-and-connectivity-checks","title":"Resources"},{"location":"openshift/proxy-ocp-4.5/","text":"Configuring a proxy for OCP 4.5 When applying proxy settings to an existing cluster, make sure not to block important traffic from the internet and do not block too early. Meaning that configuring a proxy in an environment with limited internet access, and where the proxy configuration is not pushed from the start, for obvious reasons will create issues and is a bit of a chicken/egg issue. So first, I need to make sure that the nodes are all up and healthy, that the machineconfigpools are updated, and that the cluster can reach the internet. Then, I can configure a proxy. Then, I can block all traffic to the internet and rely on the proxy. Here are the steps to set up a proxy and steps for validation, as well. Lab setup/clarification In my lab, all traffic goes through my jumpserver, 192.168.123.1. All servers are on 192.168.123.0/24 and domain is example.com [root@openshift-master-1 ~]# ip r | grep default default via 192.168.123.1 dev ens3 proto dhcp metric 100 So, on the jumpserver, I install squid and then block all direct traffic: yum install squid -y systemctl start squid Testing the proxy: [root@openshift-master-0 ~]# curl --proxy http://192.168.123.1:3128 http://www.httpbin.org/get { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Cache-Control\": \"max-age=259200\", \"Host\": \"www.httpbin.org\", \"If-Modified-Since\": \"Tue, 08 Sep 2020 15:33:44 GMT\", \"User-Agent\": \"curl/7.61.1\", \"X-Amzn-Trace-Id\": \"Root=1-5f57b2df-1a16ccc15b6a08f114114fc6\" }, \"origin\": \"192.168.123.200, 66.187.232.131\", \"url\": \"http://www.httpbin.org/get\" } [root@openshift-master-0 ~]# curl --proxy http://192.168.123.1:3128 https://www.httpbin.org/get { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Host\": \"www.httpbin.org\", \"User-Agent\": \"curl/7.61.1\", \"X-Amzn-Trace-Id\": \"Root=1-5f57b2e7-49d5b86c15a39eaba3a017c1\" }, \"origin\": \"66.187.232.131\", \"url\": \"https://www.httpbin.org/get\" } Before blocking all internet traffic, I must set up a proxy. It's chicken egg problem as the machineconfigoperator cannot configure the nodes without internet access (this cluster was installed with internet access, so the proxy needs to be configured first). Blocking traffic for demonstration purposes Just a quick and ugly REJECT all in the FORWARD chain. Later on, we'll use a more coarse block as this one here is too extreme: iptables -I FORWARD --src 192.168.123.0/24 --j REJECT Starting from that moment on, direct traffic to the internet is completely blocked. [root@openshift-jumpserver-0 ~]# oc debug node/openshift-master-0.example.com --image=registry.redhat.io/rhel8/support-tools Starting pod/openshift-master-0examplecom-debug ... To use host binaries, run `chroot /host` Removing debug pod ... error: Back-off pulling image \"registry.redhat.io/rhel8/support-tools\" Now, when I conntect to the server, and try: [root@openshift-master-1 ~]# timeout 10 curl www.httpbin.org/get [root@openshift-master-1 ~]# [root@openshift-master-1 ~]# curl --proxy http://192.168.123.1:3128 http://www.httpbin.org/get { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Cache-Control\": \"max-age=259200\", \"Host\": \"www.httpbin.org\", \"If-Modified-Since\": \"Tue, 08 Sep 2020 15:31:09 GMT\", \"User-Agent\": \"curl/7.61.1\", \"X-Amzn-Trace-Id\": \"Root=1-5f57a45c-5d80e17e71555d043de9db02\" }, \"origin\": \"192.168.123.201, 66.187.232.131\", \"url\": \"http://www.httpbin.org/get\" } [root@openshift-master-1 ~]# curl --proxy http://192.168.123.1:3128 https://www.httpbin.org/get { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Host\": \"www.httpbin.org\", \"User-Agent\": \"curl/7.61.1\", \"X-Amzn-Trace-Id\": \"Root=1-5f57a45f-7ae7bbe010e4b5ae6635162e\" }, \"origin\": \"66.187.232.131\", \"url\": \"https://www.httpbin.org/get\" } Unblocking traffic Reallow outbound connections as otherwise the cluster will fail: iptables -D FORWARD --src 192.168.123.0/24 --j REJECT Verifying nodes and machineconfigpools Make sure that nodes, machineconfigpools, machineconfigs are all o.k.: [root@openshift-jumpserver-0 ~]# oc get nodes NAME STATUS ROLES AGE VERSION openshift-master-0.example.com Ready master 4d1h v1.18.3+2cf11e2 openshift-master-1.example.com Ready master 4d1h v1.18.3+2cf11e2 openshift-master-2.example.com Ready master 4d1h v1.18.3+2cf11e2 openshift-worker-0.example.com Ready worker 4d1h v1.18.3+2cf11e2 openshift-worker-1.example.com Ready worker 4d1h v1.18.3+2cf11e2 openshift-worker-2.example.com Ready elastic 4d1h v1.18.3+2cf11e2 [root@openshift-jumpserver-0 ~]# oc get machineconfigpool NAME CONFIG UPDATED UPDATING DEGRADED MACHINECOUNT READYMACHINECOUNT UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT AGE elastic rendered-elastic-c3d458a557429fb1abeb6d1c09a69dfc True False False 1 1 1 0 26h master rendered-master-edd2a0d47b9511f4a90b6d79798ca16e True False False 3 3 3 0 4d1h worker rendered-worker-e5312f943ba9275872060e9753288916 True False False 2 2 2 0 4d1h [root@openshift-jumpserver-0 ~]# oc get machineconfig NAME GENERATEDBYCONTROLLER IGNITIONVERSION AGE 00-master f6ec58e7b69f4fc1eb2297c2734b0470a581f378 2.2.0 4d1h 00-worker f6ec58e7b69f4fc1eb2297c2734b0470a581f378 2.2.0 4d1h 01-master-container-runtime f6ec58e7b69f4fc1eb2297c2734b0470a581f378 2.2.0 4d1h 01-master-kubelet f6ec58e7b69f4fc1eb2297c2734b0470a581f378 2.2.0 4d1h 01-worker-container-runtime f6ec58e7b69f4fc1eb2297c2734b0470a581f378 2.2.0 4d1h 01-worker-kubelet f6ec58e7b69f4fc1eb2297c2734b0470a581f378 2.2.0 4d1h 50-elastic 2.2.0 26h 99-master-367f0f30-d480-4fa4-86ae-21dc30d2b7ce-registries f6ec58e7b69f4fc1eb2297c2734b0470a581f378 2.2.0 4d1h 99-master-ssh 2.2.0 4d1h 99-worker-bb447ace-f8a6-4456-b49d-64acef50a333-registries f6ec58e7b69f4fc1eb2297c2734b0470a581f378 2.2.0 4d1h 99-worker-ssh 2.2.0 4d1h rendered-elastic-c3d458a557429fb1abeb6d1c09a69dfc f6ec58e7b69f4fc1eb2297c2734b0470a581f378 2.2.0 26h rendered-master-edd2a0d47b9511f4a90b6d79798ca16e f6ec58e7b69f4fc1eb2297c2734b0470a581f378 2.2.0 4d1h rendered-worker-e5312f943ba9275872060e9753288916 f6ec58e7b69f4fc1eb2297c2734b0470a581f378 2.2.0 4d1h And make sure that the used machineconfig does not contain proxy settings: [root@openshift-jumpserver-0 ~]# oc get machineconfig rendered-master-edd2a0d47b9511f4a90b6d79798ca16e -o yaml | grep -i proxy [root@openshift-jumpserver-0 ~]# Configuring proxy in OCP 4.x Now, configure the proxy: [root@openshift-jumpserver-0 ~]# oc edit proxy/cluster proxy.config.openshift.io/cluster edited [root@openshift-jumpserver-0 ~]# oc get proxy/cluster -o yaml apiVersion: config.openshift.io/v1 kind: Proxy metadata: creationTimestamp: \"2020-09-04T15:01:16Z\" generation: 2 managedFields: - apiVersion: config.openshift.io/v1 fieldsType: FieldsV1 fieldsV1: f:spec: .: {} f:trustedCA: .: {} f:name: {} f:status: {} manager: cluster-bootstrap operation: Update time: \"2020-09-04T15:01:17Z\" - apiVersion: config.openshift.io/v1 fieldsType: FieldsV1 fieldsV1: f:spec: f:httpProxy: {} f:httpsProxy: {} f:noProxy: {} f:readinessEndpoints: {} manager: oc operation: Update time: \"2020-09-08T15:37:36Z\" name: cluster resourceVersion: \"1934446\" selfLink: /apis/config.openshift.io/v1/proxies/cluster uid: 9917e11d-6bc7-460a-8d5c-6c2bcf191f6e spec: httpProxy: http://192.168.123.1:3128 httpsProxy: http://192.168.123.1:3128 noProxy: example.com,172.16.0.0/12,10.0.0.0/16,192.168.0.0/16 readinessEndpoints: - http://www.google.com - https://www.google.com trustedCA: name: \"\" status: {} It's of utmost important to configure the noProxy correctly. Otherwise, the openshift-api server might not be able to function any more later down the road, for example with bad noProxy, after all steps completed, one might see: oc logs -n openshift-kube-apiserver kube-apiserver-openshift-master-1.example.com (...) 43: i/o timeout', Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]] I0908 17:44:54.369013 1 controller.go:127] OpenAPI AggregationController: action for item v1.project.openshift.io: Rate Limited Requeue. E0908 17:45:24.376826 1 controller.go:114] loading OpenAPI spec for \"v1.route.openshift.io\" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: Error trying to reach service: 'dial tcp 172.25.0.12:8443: i/o timeout', Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]] This will create machineconfigs with the proxy settings: [root@openshift-jumpserver-0 ~]# oc get machineconfig | grep 1m rendered-elastic-862e04ea44d8822c0343fcd1ec4a1fcb f6ec58e7b69f4fc1eb2297c2734b0470a581f378 2.2.0 1m rendered-master-0ac69c11e1e16fcd06a8ea95e2b605af f6ec58e7b69f4fc1eb2297c2734b0470a581f378 2.2.0 1m rendered-worker-de2ff42fcefa2bef931fa94091019990 f6ec58e7b69f4fc1eb2297c2734b0470a581f378 2.2.0 1m [root@openshift-jumpserver-0 ~]# oc describe machineconfigpool master | grep -i 'All nodes are updating to' Message: All nodes are updating to rendered-master-0ac69c11e1e16fcd06a8ea95e2b605af If you inspect these machineconfigs, you can see that they will push changes to the environment configuration, to crio, etc.: [root@openshift-jumpserver-0 ~]# oc get machineconfig rendered-master-0ac69c11e1e16fcd06a8ea95e2b605af -o yaml | grep -i proxy -C20 (...) Wait a bit as all nodes need to restart with the new configuration: [root@openshift-jumpserver-0 ~]# oc get nodes NAME STATUS ROLES AGE VERSION openshift-master-0.example.com Ready master 4d2h v1.18.3+2cf11e2 openshift-master-1.example.com NotReady master 4d2h v1.18.3+2cf11e2 openshift-master-2.example.com Ready master 4d2h v1.18.3+2cf11e2 openshift-worker-0.example.com Ready worker 4d1h v1.18.3+2cf11e2 openshift-worker-1.example.com Ready,SchedulingDisabled worker 4d1h v1.18.3+2cf11e2 openshift-worker-2.example.com Ready elastic 4d1h v1.18.3+2cf11e2 [root@openshift-jumpserver-0 ~]# oc get machineconfigpool NAME CONFIG UPDATED UPDATING DEGRADED MACHINECOUNT READYMACHINECOUNT UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT AGE elastic rendered-elastic-862e04ea44d8822c0343fcd1ec4a1fcb True False False 1 1 1 0 26h master rendered-master-edd2a0d47b9511f4a90b6d79798ca16e False True False 3 1 1 0 4d1h worker rendered-worker-e5312f943ba9275872060e9753288916 False True False 2 1 1 0 4d1h Verification The environment should stabilize: [root@openshift-jumpserver-0 ~]# oc get nodes NAME STATUS ROLES AGE VERSION openshift-master-0.example.com Ready master 4d2h v1.18.3+2cf11e2 openshift-master-1.example.com Ready master 4d2h v1.18.3+2cf11e2 openshift-master-2.example.com Ready master 4d2h v1.18.3+2cf11e2 openshift-worker-0.example.com Ready worker 4d2h v1.18.3+2cf11e2 openshift-worker-1.example.com Ready worker 4d2h v1.18.3+2cf11e2 openshift-worker-2.example.com Ready elastic 4d2h v1.18.3+2cf11e2 [root@openshift-jumpserver-0 ~]# oc get machineconfigpool NAME CONFIG UPDATED UPDATING DEGRADED MACHINECOUNT READYMACHINECOUNT UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT AGE elastic rendered-elastic-862e04ea44d8822c0343fcd1ec4a1fcb True False False 1 1 1 0 27h master rendered-master-0ac69c11e1e16fcd06a8ea95e2b605af True False False 3 3 3 0 4d2h worker rendered-worker-de2ff42fcefa2bef931fa94091019990 True False False 2 2 2 0 4d2h Connect to the nodes and make sure that the proxy configuration was pushed: [root@openshift-master-0 ~]# grep -i proxy /etc/systemd -R /etc/systemd/system/multi-user.target.wants/machine-config-daemon-firstboot.service:Environment=HTTP_PROXY=http://192.168.123.1:3128 /etc/systemd/system/multi-user.target.wants/machine-config-daemon-firstboot.service:Environment=HTTPS_PROXY=http://192.168.123.1:3128 /etc/systemd/system/multi-user.target.wants/machine-config-daemon-firstboot.service:Environment=NO_PROXY=.cluster.local,.svc,10.0.0.0/16,127.0.0.1,172.24.0.0/14,172.30.0.0/16,192.168.123.0/24,api-int.cluster.example.com,etcd-0.cluster.example.com,etcd-1.cluster.example.com,etcd-2.cluster.example.com,example.com,localhost /etc/systemd/system/machine-config-daemon-host.service.d/10-mco-default-env.conf:Environment=HTTP_PROXY=http://192.168.123.1:3128 /etc/systemd/system/machine-config-daemon-host.service.d/10-mco-default-env.conf:Environment=HTTPS_PROXY=http://192.168.123.1:3128 /etc/systemd/system/machine-config-daemon-host.service.d/10-mco-default-env.conf:Environment=NO_PROXY=.cluster.local,.svc,10.0.0.0/16,127.0.0.1,172.24.0.0/14,172.30.0.0/16,192.168.123.0/24,api-int.cluster.example.com,etcd-0.cluster.example.com,etcd-1.cluster.example.com,etcd-2.cluster.example.com,example.com,localhost /etc/systemd/system/kubelet.service.requires/machine-config-daemon-firstboot.service:Environment=HTTP_PROXY=http://192.168.123.1:3128 /etc/systemd/system/kubelet.service.requires/machine-config-daemon-firstboot.service:Environment=HTTPS_PROXY=http://192.168.123.1:3128 /etc/systemd/system/kubelet.service.requires/machine-config-daemon-firstboot.service:Environment=NO_PROXY=.cluster.local,.svc,10.0.0.0/16,127.0.0.1,172.24.0.0/14,172.30.0.0/16,192.168.123.0/24,api-int.cluster.example.com,etcd-0.cluster.example.com,etcd-1.cluster.example.com,etcd-2.cluster.example.com,example.com,localhost /etc/systemd/system/crio.service.d/10-mco-default-env.conf:Environment=HTTP_PROXY=http://192.168.123.1:3128 /etc/systemd/system/crio.service.d/10-mco-default-env.conf:Environment=HTTPS_PROXY=http://192.168.123.1:3128 /etc/systemd/system/crio.service.d/10-mco-default-env.conf:Environment=NO_PROXY=.cluster.local,.svc,10.0.0.0/16,127.0.0.1,172.24.0.0/14,172.30.0.0/16,192.168.123.0/24,api-int.cluster.example.com,etcd-0.cluster.example.com,etcd-1.cluster.example.com,etcd-2.cluster.example.com,example.com,localhost /etc/systemd/system/kubelet.service.d/10-mco-default-env.conf:Environment=HTTP_PROXY=http://192.168.123.1:3128 /etc/systemd/system/kubelet.service.d/10-mco-default-env.conf:Environment=HTTPS_PROXY=http://192.168.123.1:3128 /etc/systemd/system/kubelet.service.d/10-mco-default-env.conf:Environment=NO_PROXY=.cluster.local,.svc,10.0.0.0/16,127.0.0.1,172.24.0.0/14,172.30.0.0/16,192.168.123.0/24,api-int.cluster.example.com,etcd-0.cluster.example.com,etcd-1.cluster.example.com,etcd-2.cluster.example.com,example.com,localhost /etc/systemd/system/pivot.service.d/10-mco-default-env.conf:Environment=HTTP_PROXY=http://192.168.123.1:3128 /etc/systemd/system/pivot.service.d/10-mco-default-env.conf:Environment=HTTPS_PROXY=http://192.168.123.1:3128 /etc/systemd/system/pivot.service.d/10-mco-default-env.conf:Environment=NO_PROXY=.cluster.local,.svc,10.0.0.0/16,127.0.0.1,172.24.0.0/14,172.30.0.0/16,192.168.123.0/24,api-int.cluster.example.com,etcd-0.cluster.example.com,etcd-1.cluster.example.com,etcd-2.cluster.example.com,example.com,localhost /etc/systemd/system/machine-config-daemon-firstboot.service:Environment=HTTP_PROXY=http://192.168.123.1:3128 /etc/systemd/system/machine-config-daemon-firstboot.service:Environment=HTTPS_PROXY=http://192.168.123.1:3128 /etc/systemd/system/machine-config-daemon-firstboot.service:Environment=NO_PROXY=.cluster.local,.svc,10.0.0.0/16,127.0.0.1,172.24.0.0/14,172.30.0.0/16,192.168.123.0/24,api-int.cluster.example.com,etcd-0.cluster.example.com,etcd-1.cluster.example.com,etcd-2.cluster.example.com,example.com,localhost /etc/systemd/system/crio.service.requires/machine-config-daemon-firstboot.service:Environment=HTTP_PROXY=http://192.168.123.1:3128 /etc/systemd/system/crio.service.requires/machine-config-daemon-firstboot.service:Environment=HTTPS_PROXY=http://192.168.123.1:3128 /etc/systemd/system/crio.service.requires/machine-config-daemon-firstboot.service:Environment=NO_PROXY=.cluster.local,.svc,10.0.0.0/16,127.0.0.1,172.24.0.0/14,172.30.0.0/16,192.168.123.0/24,api-int.cluster.example.com,etcd-0.cluster.example.com,etcd-1.cluster.example.com,etcd-2.cluster.example.com,example.com,localhost Blocking traffic again Now, once the cluster is stable and the proxy settings were pushed, REJECT unwanted traffic again. For example: [root@openshift-jumpserver-0 ~]# iptables -I FORWARD --src 192.168.123.0/24 -p tcp --dport 80 --j REJECT [root@openshift-jumpserver-0 ~]# iptables -I FORWARD --src 192.168.123.0/24 --dst 10.0.0.0/8,192.168.0.0/16,172.16.0.0/12 -p tcp --dport 80 --j ACCEPT [root@openshift-jumpserver-0 ~]# iptables -I FORWARD --src 192.168.123.0/24 -p tcp --dport 443 --j REJECT [root@openshift-jumpserver-0 ~]# iptables -I FORWARD --src 192.168.123.0/24 --dst 10.0.0.0/8,192.168.0.0/16,172.16.0.0/12 -p tcp --dport 443 --j ACCEPT [root@openshift-jumpserver-0 ~]# iptables -L FORWARD -nv Chain FORWARD (policy ACCEPT 4004K packets, 16G bytes) pkts bytes target prot opt in out source destination 0 0 ACCEPT tcp -- * * 192.168.123.0/24 172.16.0.0/12 tcp dpt:443 0 0 ACCEPT tcp -- * * 192.168.123.0/24 192.168.0.0/16 tcp dpt:443 0 0 ACCEPT tcp -- * * 192.168.123.0/24 10.0.0.0/8 tcp dpt:443 0 0 REJECT tcp -- * * 192.168.123.0/24 0.0.0.0/0 tcp dpt:443 reject-with icmp-port-unreachable 0 0 ACCEPT tcp -- * * 192.168.123.0/24 172.16.0.0/12 tcp dpt:80 0 0 ACCEPT tcp -- * * 192.168.123.0/24 192.168.0.0/16 tcp dpt:80 0 0 ACCEPT tcp -- * * 192.168.123.0/24 10.0.0.0/8 tcp dpt:80 0 0 REJECT tcp -- * * 192.168.123.0/24 0.0.0.0/0 tcp dpt:80 reject-with icmp-port-unreachable 13M 18G CNI-FORWARD all -- * * 0.0.0.0/0 0.0.0.0/0 /* CNI firewall plugin rules */ 4889K 420M ACCEPT all -- eth0 * 192.168.123.0/24 0.0.0.0/0 Verify from a host: [root@openshift-master-0 ~]# curl https://www.httpbin.org/get curl: (7) Failed to connect to www.httpbin.org port 443: Connection refused [root@openshift-master-0 ~]# [root@openshift-master-0 ~]# curl --proxy http://192.168.123.1:3128 https://www.httpbin.org/get { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Host\": \"www.httpbin.org\", \"User-Agent\": \"curl/7.61.1\", \"X-Amzn-Trace-Id\": \"Root=1-5f57d689-5ebe776493157066d0909100\" }, \"origin\": \"66.187.232.131\", \"url\": \"https://www.httpbin.org/get\" } Make sure that you can run oc debug/node : [root@openshift-jumpserver-0 ~]# oc debug node/openshift-master-0.example.com Starting pod/openshift-master-0examplecom-debug ... To use host binaries, run `chroot /host` Pod IP: 192.168.123.200 If you don't see a command prompt, try pressing enter. sh-4.2# sh-4.2# exit exit Removing debug pod ... [root@openshift-jumpserver-0 ~]# oc debug node/openshift-master-1.example.com Starting pod/openshift-master-1examplecom-debug ... To use host binaries, run `chroot /host` Pod IP: 192.168.123.201 If you don't see a command prompt, try pressing enter. sh-4.2# exit exit Removing debug pod ... [root@openshift-jumpserver-0 ~]# oc debug node/openshift-master-2.example.com Starting pod/openshift-master-2examplecom-debug ... To use host binaries, run `chroot /host` Pod IP: 192.168.123.202 If you don't see a command prompt, try pressing enter. sh-4.2# exit exit Removing debug pod ... Make sure that you can run oc adm must-gather : [root@openshift-jumpserver-0 ~]# oc adm must-gather [must-gather ] OUT Using must-gather plugin-in image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4dc41732afbccb4d882234749ac6031d8ec15715230df8058ec0ad7ccee31a94 [must-gather ] OUT namespace/openshift-must-gather-tnj99 created [must-gather ] OUT clusterrolebinding.rbac.authorization.k8s.io/must-gather-bll5p created [must-gather ] OUT pod for plug-in image quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4dc41732afbccb4d882234749ac6031d8ec15715230df8058ec0ad7ccee31a94 created [must-gather-fj4qr] POD Wrote inspect data to must-gather. [must-gather-fj4qr] POD Gathering data for ns/openshift-cluster-version... [must-gather-fj4qr] POD Wrote inspect data to must-gather. [must-gather-fj4qr] POD Gathering data for ns/openshift-config... [must-gather-fj4qr] POD Gathering data for ns/openshift-config-managed... [must-gather-fj4qr] POD Gathering data for ns/openshift-authentication... [must-gather-fj4qr] POD Gathering data for ns/openshift-authentication-operator... [must-gather-fj4qr] POD Gathering data for ns/openshift-ingress... [must-gather-fj4qr] POD Gathering data for ns/openshift-cloud-credential-operator... [must-gather-fj4qr] POD Gathering data for ns/openshift-machine-api... [must-gather-fj4qr] POD Gathering data for ns/openshift-config-operator... [must-gather-fj4qr] POD Gathering data for ns/openshift-console-operator... [must-gather-fj4qr] POD Gathering data for ns/openshift-console... [must-gather-fj4qr] POD Gathering data for ns/openshift-cluster-storage-operator... [must-gather-fj4qr] POD Gathering data for ns/openshift-dns-operator... [must-gather-fj4qr] POD Gathering data for ns/openshift-dns... [must-gather-fj4qr] POD Gathering data for ns/openshift-etcd-operator... [must-gather-fj4qr] POD Gathering data for ns/openshift-etcd... [must-gather-fj4qr] POD Gathering data for ns/openshift-image-registry... [must-gather-fj4qr] POD Gathering data for ns/openshift-ingress-operator... [must-gather-fj4qr] POD Gathering data for ns/openshift-insights... (...) Make sure that you can create a deployment: [root@openshift-jumpserver-0 ~]# cat fedora.yaml apiVersion: apps/v1 kind: Deployment metadata: name: fedora-deployment-user labels: app: fedora-deployment-user spec: replicas: 3 selector: matchLabels: app: fedora-deployment-user template: metadata: labels: app: fedora-deployment-user spec: containers: - name: fedora image: fedora command: - sleep - infinity imagePullPolicy: Always While applying the deployment, run tshark to verify: [root@openshift-jumpserver-0 ~]# oc get pods NAME READY STATUS RESTARTS AGE fedora-deployment-user-5df6fb4b4b-4nznr 1/1 Running 0 42s fedora-deployment-user-5df6fb4b4b-dnpbn 1/1 Running 0 42s fedora-deployment-user-5df6fb4b4b-zb64c 1/1 Running 0 42s","title":"Proxy OCP 4.5"},{"location":"openshift/proxy-ocp-4.5/#configuring-a-proxy-for-ocp-45","text":"When applying proxy settings to an existing cluster, make sure not to block important traffic from the internet and do not block too early. Meaning that configuring a proxy in an environment with limited internet access, and where the proxy configuration is not pushed from the start, for obvious reasons will create issues and is a bit of a chicken/egg issue. So first, I need to make sure that the nodes are all up and healthy, that the machineconfigpools are updated, and that the cluster can reach the internet. Then, I can configure a proxy. Then, I can block all traffic to the internet and rely on the proxy. Here are the steps to set up a proxy and steps for validation, as well.","title":"Configuring a proxy for OCP 4.5"},{"location":"openshift/proxy-ocp-4.5/#lab-setupclarification","text":"In my lab, all traffic goes through my jumpserver, 192.168.123.1. All servers are on 192.168.123.0/24 and domain is example.com [root@openshift-master-1 ~]# ip r | grep default default via 192.168.123.1 dev ens3 proto dhcp metric 100 So, on the jumpserver, I install squid and then block all direct traffic: yum install squid -y systemctl start squid Testing the proxy: [root@openshift-master-0 ~]# curl --proxy http://192.168.123.1:3128 http://www.httpbin.org/get { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Cache-Control\": \"max-age=259200\", \"Host\": \"www.httpbin.org\", \"If-Modified-Since\": \"Tue, 08 Sep 2020 15:33:44 GMT\", \"User-Agent\": \"curl/7.61.1\", \"X-Amzn-Trace-Id\": \"Root=1-5f57b2df-1a16ccc15b6a08f114114fc6\" }, \"origin\": \"192.168.123.200, 66.187.232.131\", \"url\": \"http://www.httpbin.org/get\" } [root@openshift-master-0 ~]# curl --proxy http://192.168.123.1:3128 https://www.httpbin.org/get { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Host\": \"www.httpbin.org\", \"User-Agent\": \"curl/7.61.1\", \"X-Amzn-Trace-Id\": \"Root=1-5f57b2e7-49d5b86c15a39eaba3a017c1\" }, \"origin\": \"66.187.232.131\", \"url\": \"https://www.httpbin.org/get\" } Before blocking all internet traffic, I must set up a proxy. It's chicken egg problem as the machineconfigoperator cannot configure the nodes without internet access (this cluster was installed with internet access, so the proxy needs to be configured first).","title":"Lab setup/clarification"},{"location":"openshift/proxy-ocp-4.5/#blocking-traffic-for-demonstration-purposes","text":"Just a quick and ugly REJECT all in the FORWARD chain. Later on, we'll use a more coarse block as this one here is too extreme: iptables -I FORWARD --src 192.168.123.0/24 --j REJECT Starting from that moment on, direct traffic to the internet is completely blocked. [root@openshift-jumpserver-0 ~]# oc debug node/openshift-master-0.example.com --image=registry.redhat.io/rhel8/support-tools Starting pod/openshift-master-0examplecom-debug ... To use host binaries, run `chroot /host` Removing debug pod ... error: Back-off pulling image \"registry.redhat.io/rhel8/support-tools\" Now, when I conntect to the server, and try: [root@openshift-master-1 ~]# timeout 10 curl www.httpbin.org/get [root@openshift-master-1 ~]# [root@openshift-master-1 ~]# curl --proxy http://192.168.123.1:3128 http://www.httpbin.org/get { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Cache-Control\": \"max-age=259200\", \"Host\": \"www.httpbin.org\", \"If-Modified-Since\": \"Tue, 08 Sep 2020 15:31:09 GMT\", \"User-Agent\": \"curl/7.61.1\", \"X-Amzn-Trace-Id\": \"Root=1-5f57a45c-5d80e17e71555d043de9db02\" }, \"origin\": \"192.168.123.201, 66.187.232.131\", \"url\": \"http://www.httpbin.org/get\" } [root@openshift-master-1 ~]# curl --proxy http://192.168.123.1:3128 https://www.httpbin.org/get { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Host\": \"www.httpbin.org\", \"User-Agent\": \"curl/7.61.1\", \"X-Amzn-Trace-Id\": \"Root=1-5f57a45f-7ae7bbe010e4b5ae6635162e\" }, \"origin\": \"66.187.232.131\", \"url\": \"https://www.httpbin.org/get\" }","title":"Blocking traffic for demonstration purposes"},{"location":"openshift/proxy-ocp-4.5/#unblocking-traffic","text":"Reallow outbound connections as otherwise the cluster will fail: iptables -D FORWARD --src 192.168.123.0/24 --j REJECT","title":"Unblocking traffic"},{"location":"openshift/proxy-ocp-4.5/#verifying-nodes-and-machineconfigpools","text":"Make sure that nodes, machineconfigpools, machineconfigs are all o.k.: [root@openshift-jumpserver-0 ~]# oc get nodes NAME STATUS ROLES AGE VERSION openshift-master-0.example.com Ready master 4d1h v1.18.3+2cf11e2 openshift-master-1.example.com Ready master 4d1h v1.18.3+2cf11e2 openshift-master-2.example.com Ready master 4d1h v1.18.3+2cf11e2 openshift-worker-0.example.com Ready worker 4d1h v1.18.3+2cf11e2 openshift-worker-1.example.com Ready worker 4d1h v1.18.3+2cf11e2 openshift-worker-2.example.com Ready elastic 4d1h v1.18.3+2cf11e2 [root@openshift-jumpserver-0 ~]# oc get machineconfigpool NAME CONFIG UPDATED UPDATING DEGRADED MACHINECOUNT READYMACHINECOUNT UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT AGE elastic rendered-elastic-c3d458a557429fb1abeb6d1c09a69dfc True False False 1 1 1 0 26h master rendered-master-edd2a0d47b9511f4a90b6d79798ca16e True False False 3 3 3 0 4d1h worker rendered-worker-e5312f943ba9275872060e9753288916 True False False 2 2 2 0 4d1h [root@openshift-jumpserver-0 ~]# oc get machineconfig NAME GENERATEDBYCONTROLLER IGNITIONVERSION AGE 00-master f6ec58e7b69f4fc1eb2297c2734b0470a581f378 2.2.0 4d1h 00-worker f6ec58e7b69f4fc1eb2297c2734b0470a581f378 2.2.0 4d1h 01-master-container-runtime f6ec58e7b69f4fc1eb2297c2734b0470a581f378 2.2.0 4d1h 01-master-kubelet f6ec58e7b69f4fc1eb2297c2734b0470a581f378 2.2.0 4d1h 01-worker-container-runtime f6ec58e7b69f4fc1eb2297c2734b0470a581f378 2.2.0 4d1h 01-worker-kubelet f6ec58e7b69f4fc1eb2297c2734b0470a581f378 2.2.0 4d1h 50-elastic 2.2.0 26h 99-master-367f0f30-d480-4fa4-86ae-21dc30d2b7ce-registries f6ec58e7b69f4fc1eb2297c2734b0470a581f378 2.2.0 4d1h 99-master-ssh 2.2.0 4d1h 99-worker-bb447ace-f8a6-4456-b49d-64acef50a333-registries f6ec58e7b69f4fc1eb2297c2734b0470a581f378 2.2.0 4d1h 99-worker-ssh 2.2.0 4d1h rendered-elastic-c3d458a557429fb1abeb6d1c09a69dfc f6ec58e7b69f4fc1eb2297c2734b0470a581f378 2.2.0 26h rendered-master-edd2a0d47b9511f4a90b6d79798ca16e f6ec58e7b69f4fc1eb2297c2734b0470a581f378 2.2.0 4d1h rendered-worker-e5312f943ba9275872060e9753288916 f6ec58e7b69f4fc1eb2297c2734b0470a581f378 2.2.0 4d1h And make sure that the used machineconfig does not contain proxy settings: [root@openshift-jumpserver-0 ~]# oc get machineconfig rendered-master-edd2a0d47b9511f4a90b6d79798ca16e -o yaml | grep -i proxy [root@openshift-jumpserver-0 ~]#","title":"Verifying nodes and machineconfigpools"},{"location":"openshift/proxy-ocp-4.5/#configuring-proxy-in-ocp-4x","text":"Now, configure the proxy: [root@openshift-jumpserver-0 ~]# oc edit proxy/cluster proxy.config.openshift.io/cluster edited [root@openshift-jumpserver-0 ~]# oc get proxy/cluster -o yaml apiVersion: config.openshift.io/v1 kind: Proxy metadata: creationTimestamp: \"2020-09-04T15:01:16Z\" generation: 2 managedFields: - apiVersion: config.openshift.io/v1 fieldsType: FieldsV1 fieldsV1: f:spec: .: {} f:trustedCA: .: {} f:name: {} f:status: {} manager: cluster-bootstrap operation: Update time: \"2020-09-04T15:01:17Z\" - apiVersion: config.openshift.io/v1 fieldsType: FieldsV1 fieldsV1: f:spec: f:httpProxy: {} f:httpsProxy: {} f:noProxy: {} f:readinessEndpoints: {} manager: oc operation: Update time: \"2020-09-08T15:37:36Z\" name: cluster resourceVersion: \"1934446\" selfLink: /apis/config.openshift.io/v1/proxies/cluster uid: 9917e11d-6bc7-460a-8d5c-6c2bcf191f6e spec: httpProxy: http://192.168.123.1:3128 httpsProxy: http://192.168.123.1:3128 noProxy: example.com,172.16.0.0/12,10.0.0.0/16,192.168.0.0/16 readinessEndpoints: - http://www.google.com - https://www.google.com trustedCA: name: \"\" status: {} It's of utmost important to configure the noProxy correctly. Otherwise, the openshift-api server might not be able to function any more later down the road, for example with bad noProxy, after all steps completed, one might see: oc logs -n openshift-kube-apiserver kube-apiserver-openshift-master-1.example.com (...) 43: i/o timeout', Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]] I0908 17:44:54.369013 1 controller.go:127] OpenAPI AggregationController: action for item v1.project.openshift.io: Rate Limited Requeue. E0908 17:45:24.376826 1 controller.go:114] loading OpenAPI spec for \"v1.route.openshift.io\" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: Error trying to reach service: 'dial tcp 172.25.0.12:8443: i/o timeout', Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]] This will create machineconfigs with the proxy settings: [root@openshift-jumpserver-0 ~]# oc get machineconfig | grep 1m rendered-elastic-862e04ea44d8822c0343fcd1ec4a1fcb f6ec58e7b69f4fc1eb2297c2734b0470a581f378 2.2.0 1m rendered-master-0ac69c11e1e16fcd06a8ea95e2b605af f6ec58e7b69f4fc1eb2297c2734b0470a581f378 2.2.0 1m rendered-worker-de2ff42fcefa2bef931fa94091019990 f6ec58e7b69f4fc1eb2297c2734b0470a581f378 2.2.0 1m [root@openshift-jumpserver-0 ~]# oc describe machineconfigpool master | grep -i 'All nodes are updating to' Message: All nodes are updating to rendered-master-0ac69c11e1e16fcd06a8ea95e2b605af If you inspect these machineconfigs, you can see that they will push changes to the environment configuration, to crio, etc.: [root@openshift-jumpserver-0 ~]# oc get machineconfig rendered-master-0ac69c11e1e16fcd06a8ea95e2b605af -o yaml | grep -i proxy -C20 (...) Wait a bit as all nodes need to restart with the new configuration: [root@openshift-jumpserver-0 ~]# oc get nodes NAME STATUS ROLES AGE VERSION openshift-master-0.example.com Ready master 4d2h v1.18.3+2cf11e2 openshift-master-1.example.com NotReady master 4d2h v1.18.3+2cf11e2 openshift-master-2.example.com Ready master 4d2h v1.18.3+2cf11e2 openshift-worker-0.example.com Ready worker 4d1h v1.18.3+2cf11e2 openshift-worker-1.example.com Ready,SchedulingDisabled worker 4d1h v1.18.3+2cf11e2 openshift-worker-2.example.com Ready elastic 4d1h v1.18.3+2cf11e2 [root@openshift-jumpserver-0 ~]# oc get machineconfigpool NAME CONFIG UPDATED UPDATING DEGRADED MACHINECOUNT READYMACHINECOUNT UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT AGE elastic rendered-elastic-862e04ea44d8822c0343fcd1ec4a1fcb True False False 1 1 1 0 26h master rendered-master-edd2a0d47b9511f4a90b6d79798ca16e False True False 3 1 1 0 4d1h worker rendered-worker-e5312f943ba9275872060e9753288916 False True False 2 1 1 0 4d1h","title":"Configuring proxy in OCP 4.x"},{"location":"openshift/proxy-ocp-4.5/#verification","text":"The environment should stabilize: [root@openshift-jumpserver-0 ~]# oc get nodes NAME STATUS ROLES AGE VERSION openshift-master-0.example.com Ready master 4d2h v1.18.3+2cf11e2 openshift-master-1.example.com Ready master 4d2h v1.18.3+2cf11e2 openshift-master-2.example.com Ready master 4d2h v1.18.3+2cf11e2 openshift-worker-0.example.com Ready worker 4d2h v1.18.3+2cf11e2 openshift-worker-1.example.com Ready worker 4d2h v1.18.3+2cf11e2 openshift-worker-2.example.com Ready elastic 4d2h v1.18.3+2cf11e2 [root@openshift-jumpserver-0 ~]# oc get machineconfigpool NAME CONFIG UPDATED UPDATING DEGRADED MACHINECOUNT READYMACHINECOUNT UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT AGE elastic rendered-elastic-862e04ea44d8822c0343fcd1ec4a1fcb True False False 1 1 1 0 27h master rendered-master-0ac69c11e1e16fcd06a8ea95e2b605af True False False 3 3 3 0 4d2h worker rendered-worker-de2ff42fcefa2bef931fa94091019990 True False False 2 2 2 0 4d2h Connect to the nodes and make sure that the proxy configuration was pushed: [root@openshift-master-0 ~]# grep -i proxy /etc/systemd -R /etc/systemd/system/multi-user.target.wants/machine-config-daemon-firstboot.service:Environment=HTTP_PROXY=http://192.168.123.1:3128 /etc/systemd/system/multi-user.target.wants/machine-config-daemon-firstboot.service:Environment=HTTPS_PROXY=http://192.168.123.1:3128 /etc/systemd/system/multi-user.target.wants/machine-config-daemon-firstboot.service:Environment=NO_PROXY=.cluster.local,.svc,10.0.0.0/16,127.0.0.1,172.24.0.0/14,172.30.0.0/16,192.168.123.0/24,api-int.cluster.example.com,etcd-0.cluster.example.com,etcd-1.cluster.example.com,etcd-2.cluster.example.com,example.com,localhost /etc/systemd/system/machine-config-daemon-host.service.d/10-mco-default-env.conf:Environment=HTTP_PROXY=http://192.168.123.1:3128 /etc/systemd/system/machine-config-daemon-host.service.d/10-mco-default-env.conf:Environment=HTTPS_PROXY=http://192.168.123.1:3128 /etc/systemd/system/machine-config-daemon-host.service.d/10-mco-default-env.conf:Environment=NO_PROXY=.cluster.local,.svc,10.0.0.0/16,127.0.0.1,172.24.0.0/14,172.30.0.0/16,192.168.123.0/24,api-int.cluster.example.com,etcd-0.cluster.example.com,etcd-1.cluster.example.com,etcd-2.cluster.example.com,example.com,localhost /etc/systemd/system/kubelet.service.requires/machine-config-daemon-firstboot.service:Environment=HTTP_PROXY=http://192.168.123.1:3128 /etc/systemd/system/kubelet.service.requires/machine-config-daemon-firstboot.service:Environment=HTTPS_PROXY=http://192.168.123.1:3128 /etc/systemd/system/kubelet.service.requires/machine-config-daemon-firstboot.service:Environment=NO_PROXY=.cluster.local,.svc,10.0.0.0/16,127.0.0.1,172.24.0.0/14,172.30.0.0/16,192.168.123.0/24,api-int.cluster.example.com,etcd-0.cluster.example.com,etcd-1.cluster.example.com,etcd-2.cluster.example.com,example.com,localhost /etc/systemd/system/crio.service.d/10-mco-default-env.conf:Environment=HTTP_PROXY=http://192.168.123.1:3128 /etc/systemd/system/crio.service.d/10-mco-default-env.conf:Environment=HTTPS_PROXY=http://192.168.123.1:3128 /etc/systemd/system/crio.service.d/10-mco-default-env.conf:Environment=NO_PROXY=.cluster.local,.svc,10.0.0.0/16,127.0.0.1,172.24.0.0/14,172.30.0.0/16,192.168.123.0/24,api-int.cluster.example.com,etcd-0.cluster.example.com,etcd-1.cluster.example.com,etcd-2.cluster.example.com,example.com,localhost /etc/systemd/system/kubelet.service.d/10-mco-default-env.conf:Environment=HTTP_PROXY=http://192.168.123.1:3128 /etc/systemd/system/kubelet.service.d/10-mco-default-env.conf:Environment=HTTPS_PROXY=http://192.168.123.1:3128 /etc/systemd/system/kubelet.service.d/10-mco-default-env.conf:Environment=NO_PROXY=.cluster.local,.svc,10.0.0.0/16,127.0.0.1,172.24.0.0/14,172.30.0.0/16,192.168.123.0/24,api-int.cluster.example.com,etcd-0.cluster.example.com,etcd-1.cluster.example.com,etcd-2.cluster.example.com,example.com,localhost /etc/systemd/system/pivot.service.d/10-mco-default-env.conf:Environment=HTTP_PROXY=http://192.168.123.1:3128 /etc/systemd/system/pivot.service.d/10-mco-default-env.conf:Environment=HTTPS_PROXY=http://192.168.123.1:3128 /etc/systemd/system/pivot.service.d/10-mco-default-env.conf:Environment=NO_PROXY=.cluster.local,.svc,10.0.0.0/16,127.0.0.1,172.24.0.0/14,172.30.0.0/16,192.168.123.0/24,api-int.cluster.example.com,etcd-0.cluster.example.com,etcd-1.cluster.example.com,etcd-2.cluster.example.com,example.com,localhost /etc/systemd/system/machine-config-daemon-firstboot.service:Environment=HTTP_PROXY=http://192.168.123.1:3128 /etc/systemd/system/machine-config-daemon-firstboot.service:Environment=HTTPS_PROXY=http://192.168.123.1:3128 /etc/systemd/system/machine-config-daemon-firstboot.service:Environment=NO_PROXY=.cluster.local,.svc,10.0.0.0/16,127.0.0.1,172.24.0.0/14,172.30.0.0/16,192.168.123.0/24,api-int.cluster.example.com,etcd-0.cluster.example.com,etcd-1.cluster.example.com,etcd-2.cluster.example.com,example.com,localhost /etc/systemd/system/crio.service.requires/machine-config-daemon-firstboot.service:Environment=HTTP_PROXY=http://192.168.123.1:3128 /etc/systemd/system/crio.service.requires/machine-config-daemon-firstboot.service:Environment=HTTPS_PROXY=http://192.168.123.1:3128 /etc/systemd/system/crio.service.requires/machine-config-daemon-firstboot.service:Environment=NO_PROXY=.cluster.local,.svc,10.0.0.0/16,127.0.0.1,172.24.0.0/14,172.30.0.0/16,192.168.123.0/24,api-int.cluster.example.com,etcd-0.cluster.example.com,etcd-1.cluster.example.com,etcd-2.cluster.example.com,example.com,localhost","title":"Verification"},{"location":"openshift/proxy-ocp-4.5/#blocking-traffic-again","text":"Now, once the cluster is stable and the proxy settings were pushed, REJECT unwanted traffic again. For example: [root@openshift-jumpserver-0 ~]# iptables -I FORWARD --src 192.168.123.0/24 -p tcp --dport 80 --j REJECT [root@openshift-jumpserver-0 ~]# iptables -I FORWARD --src 192.168.123.0/24 --dst 10.0.0.0/8,192.168.0.0/16,172.16.0.0/12 -p tcp --dport 80 --j ACCEPT [root@openshift-jumpserver-0 ~]# iptables -I FORWARD --src 192.168.123.0/24 -p tcp --dport 443 --j REJECT [root@openshift-jumpserver-0 ~]# iptables -I FORWARD --src 192.168.123.0/24 --dst 10.0.0.0/8,192.168.0.0/16,172.16.0.0/12 -p tcp --dport 443 --j ACCEPT [root@openshift-jumpserver-0 ~]# iptables -L FORWARD -nv Chain FORWARD (policy ACCEPT 4004K packets, 16G bytes) pkts bytes target prot opt in out source destination 0 0 ACCEPT tcp -- * * 192.168.123.0/24 172.16.0.0/12 tcp dpt:443 0 0 ACCEPT tcp -- * * 192.168.123.0/24 192.168.0.0/16 tcp dpt:443 0 0 ACCEPT tcp -- * * 192.168.123.0/24 10.0.0.0/8 tcp dpt:443 0 0 REJECT tcp -- * * 192.168.123.0/24 0.0.0.0/0 tcp dpt:443 reject-with icmp-port-unreachable 0 0 ACCEPT tcp -- * * 192.168.123.0/24 172.16.0.0/12 tcp dpt:80 0 0 ACCEPT tcp -- * * 192.168.123.0/24 192.168.0.0/16 tcp dpt:80 0 0 ACCEPT tcp -- * * 192.168.123.0/24 10.0.0.0/8 tcp dpt:80 0 0 REJECT tcp -- * * 192.168.123.0/24 0.0.0.0/0 tcp dpt:80 reject-with icmp-port-unreachable 13M 18G CNI-FORWARD all -- * * 0.0.0.0/0 0.0.0.0/0 /* CNI firewall plugin rules */ 4889K 420M ACCEPT all -- eth0 * 192.168.123.0/24 0.0.0.0/0 Verify from a host: [root@openshift-master-0 ~]# curl https://www.httpbin.org/get curl: (7) Failed to connect to www.httpbin.org port 443: Connection refused [root@openshift-master-0 ~]# [root@openshift-master-0 ~]# curl --proxy http://192.168.123.1:3128 https://www.httpbin.org/get { \"args\": {}, \"headers\": { \"Accept\": \"*/*\", \"Host\": \"www.httpbin.org\", \"User-Agent\": \"curl/7.61.1\", \"X-Amzn-Trace-Id\": \"Root=1-5f57d689-5ebe776493157066d0909100\" }, \"origin\": \"66.187.232.131\", \"url\": \"https://www.httpbin.org/get\" } Make sure that you can run oc debug/node : [root@openshift-jumpserver-0 ~]# oc debug node/openshift-master-0.example.com Starting pod/openshift-master-0examplecom-debug ... To use host binaries, run `chroot /host` Pod IP: 192.168.123.200 If you don't see a command prompt, try pressing enter. sh-4.2# sh-4.2# exit exit Removing debug pod ... [root@openshift-jumpserver-0 ~]# oc debug node/openshift-master-1.example.com Starting pod/openshift-master-1examplecom-debug ... To use host binaries, run `chroot /host` Pod IP: 192.168.123.201 If you don't see a command prompt, try pressing enter. sh-4.2# exit exit Removing debug pod ... [root@openshift-jumpserver-0 ~]# oc debug node/openshift-master-2.example.com Starting pod/openshift-master-2examplecom-debug ... To use host binaries, run `chroot /host` Pod IP: 192.168.123.202 If you don't see a command prompt, try pressing enter. sh-4.2# exit exit Removing debug pod ... Make sure that you can run oc adm must-gather : [root@openshift-jumpserver-0 ~]# oc adm must-gather [must-gather ] OUT Using must-gather plugin-in image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4dc41732afbccb4d882234749ac6031d8ec15715230df8058ec0ad7ccee31a94 [must-gather ] OUT namespace/openshift-must-gather-tnj99 created [must-gather ] OUT clusterrolebinding.rbac.authorization.k8s.io/must-gather-bll5p created [must-gather ] OUT pod for plug-in image quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4dc41732afbccb4d882234749ac6031d8ec15715230df8058ec0ad7ccee31a94 created [must-gather-fj4qr] POD Wrote inspect data to must-gather. [must-gather-fj4qr] POD Gathering data for ns/openshift-cluster-version... [must-gather-fj4qr] POD Wrote inspect data to must-gather. [must-gather-fj4qr] POD Gathering data for ns/openshift-config... [must-gather-fj4qr] POD Gathering data for ns/openshift-config-managed... [must-gather-fj4qr] POD Gathering data for ns/openshift-authentication... [must-gather-fj4qr] POD Gathering data for ns/openshift-authentication-operator... [must-gather-fj4qr] POD Gathering data for ns/openshift-ingress... [must-gather-fj4qr] POD Gathering data for ns/openshift-cloud-credential-operator... [must-gather-fj4qr] POD Gathering data for ns/openshift-machine-api... [must-gather-fj4qr] POD Gathering data for ns/openshift-config-operator... [must-gather-fj4qr] POD Gathering data for ns/openshift-console-operator... [must-gather-fj4qr] POD Gathering data for ns/openshift-console... [must-gather-fj4qr] POD Gathering data for ns/openshift-cluster-storage-operator... [must-gather-fj4qr] POD Gathering data for ns/openshift-dns-operator... [must-gather-fj4qr] POD Gathering data for ns/openshift-dns... [must-gather-fj4qr] POD Gathering data for ns/openshift-etcd-operator... [must-gather-fj4qr] POD Gathering data for ns/openshift-etcd... [must-gather-fj4qr] POD Gathering data for ns/openshift-image-registry... [must-gather-fj4qr] POD Gathering data for ns/openshift-ingress-operator... [must-gather-fj4qr] POD Gathering data for ns/openshift-insights... (...) Make sure that you can create a deployment: [root@openshift-jumpserver-0 ~]# cat fedora.yaml apiVersion: apps/v1 kind: Deployment metadata: name: fedora-deployment-user labels: app: fedora-deployment-user spec: replicas: 3 selector: matchLabels: app: fedora-deployment-user template: metadata: labels: app: fedora-deployment-user spec: containers: - name: fedora image: fedora command: - sleep - infinity imagePullPolicy: Always While applying the deployment, run tshark to verify: [root@openshift-jumpserver-0 ~]# oc get pods NAME READY STATUS RESTARTS AGE fedora-deployment-user-5df6fb4b4b-4nznr 1/1 Running 0 42s fedora-deployment-user-5df6fb4b4b-dnpbn 1/1 Running 0 42s fedora-deployment-user-5df6fb4b4b-zb64c 1/1 Running 0 42s","title":"Blocking traffic again"},{"location":"openshift/scc/","text":"How Security Context Constraints (SCCs) work in OpenShift How SCCs work in OpenShift by example. The following tests were run in Red Hat OpenShift Platform 3.11. Depending on the version in use, the cluster's behavior may be slightly different. Impact of the default restricted SCC The following example spawns a statefulset with a pod as user 65000. This user falls not into the supplemental-groups of the project and can hence not be used by the restricted SCC. The deployment will fail: $ oc describe project test | grep scc openshift.io/sa.scc.mcs=s0:c10,c0 openshift.io/sa.scc.supplemental-groups=1000090000/10000 openshift.io/sa.scc.uid-range=1000090000/10000 Storage class is a local provisioner with 4 100MB local mounts, set up with https://github.com/andreaskaris/kubernetes/tree/master/localvolume : $ oc get storageclass NAME PROVISIONER AGE local-loopbacks kubernetes.io/no-provisioner 3d $ oc get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE local-pv-18dc9ec0 92Mi RWO Delete Available local-loopbacks 36s local-pv-89b778f1 92Mi RWO Delete Available local-loopbacks 19m local-pv-9e67e7d6 92Mi RWO Delete Available local-loopbacks 28m local-pv-b6a7cd37 92Mi RWO Delete Available local-loopbacks 3d statefulset.yaml : kind: Service apiVersion: v1 metadata: name: \"test\" spec: clusterIP: None # the list of ports that are exposed by this service ports: - name: http port: 80 # will route traffic to pods having labels matching this selector selector: name: \"test\" --- apiVersion: apps/v1 kind: StatefulSet metadata: name: test spec: selector: matchLabels: app: test serviceName: test replicas: 1 template: metadata: labels: app: test spec: securityContext: runAsUser: 65000 containers: - image: gcr.io/google_containers/busybox command: - \"/bin/sh\" - \"-c\" - \"while true; do date; sleep 1; done\" name: busybox volumeMounts: - name: vol mountPath: /mnt volumeClaimTemplates: - metadata: name: vol spec: accessModes: [\"ReadWriteOnce\"] storageClassName: local-loopbacks resources: requests: storage: 90Mi $ oc apply -f statefulset.yaml OpenShift's admission controller will complain that the range is not correct: $ oc get pods No resources found. $ oc describe statefulset test | tail StorageClass: local-loopbacks Labels: <none> Annotations: <none> Capacity: 90Mi Access Modes: [ReadWriteOnce] Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 40s statefulset-controller create Claim vol-test-0 Pod test-0 in StatefulSet test success Warning FailedCreate 19s (x14 over 40s) statefulset-controller create Pod test-0 in StatefulSet test failed error: pods \"test-0\" is forbidden: unable to validate against any security context constraint: [spec.containers[0].securityContext.securityContext.runAsUser: Invalid value: 65000: must be in the ranges: [1000090000, 1000099999]] Fixing failed validation of security context constraints There are 3 solutions to this. Either: create a new SCC (or modify the restricted policy which is not recommended) or modify the runAsUser field to run the pod as a user inside range 1000090000, 1000099999 or change the namespace's openshift.io/sa.scc.uid-range . Resetting the lab First, delete the statefulSet, PVC, etc. Continue once all is deleted: $ oc get statefulset No resources found. $ oc get pods No resources found. $ oc get pvc No resources found. Changing the namespace's openshift.io/sa.scc.uid-range Let's change the namespace's uid-range : $ oc edit namespace test namespace/test edited $ oc describe project test | grep scc openshift.io/sa.scc.mcs=s0:c10,c0 openshift.io/sa.scc.supplemental-groups=1000090000/10000 openshift.io/sa.scc.uid-range=65000/1000 $ oc apply -f statefulset.yaml service/test created statefulset.apps/test created $ oc get statefulset NAME DESIRED CURRENT AGE test 1 1 24s $ oc get pods NAME READY STATUS RESTARTS AGE test-0 1/1 Running 0 26s $ oc get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE vol-test-0 Bound local-pv-9e67e7d6 92Mi RWO local-loopbacks 30s The pod was assigned to the restricted SCC: $ oc get pod test-0 -o yaml | grep scc openshift.io/scc: restricted This is because the restricted SCC is the default. If other SCCs were assigned to the default serviceaccount of the project, then SCC priorization would come into play. For more about SCC priorization, see: https://docs.openshift.com/container-platform/3.11/architecture/additional_concepts/authorization.html#scc-prioritization On the worker that the pod runs at, verify the container's and processes' UID: [root@node-0 ~]# docker ps | grep test 47b2d7df970c gcr.io/google_containers/busybox@sha256:d8d3bc2c183ed2f9f10e7258f84971202325ee6011ba137112e01e30f206de67 \"/bin/sh -c 'while...\" About a minute ago Up About a minute k8s_busybox_test-0_test_6dfbf91e-d5a1-11ea-a312-fa163e8752f1_0 8b3ef77e9213 registry.redhat.io/openshift3/ose-pod:v3.11.248 \"/usr/bin/pod\" About a minute ago Up About a minute k8s_POD_test-0_test_6dfbf91e-d5a1-11ea-a312-fa163e8752f1_0 [root@node-0 ~]# docker inspect 47b2d7df970c | grep -i user \"UsernsMode\": \"\", \"User\": \"65000\", [root@node-0 ~]# ps aux | grep 'date; sleep 1' 65000 5172 0.0 0.0 3164 352 ? Ss 11:53 0:00 /bin/sh -c while true; do date; sleep 1; done root 6616 0.0 0.0 112808 972 pts/0 R+ 11:55 0:00 grep --color=auto date; sleep 1 Also note that the FsGroupID was set automatically for the filesystem: [root@node-0 ~]# ls /mnt/local-storage/loopbacks/ -al total 8 drwxr-xr-x. 6 root root 42 Jul 30 13:30 . drwxr-xr-x. 3 root root 23 Jul 30 13:28 .. drwxrwsr-x. 2 root root 1024 Aug 3 10:06 a drwxrwsr-x. 2 root root 1024 Aug 3 09:55 b drwxrwsr-x. 2 root 1000090000 1024 Aug 3 09:55 c drwxr-xr-x. 3 root root 1024 Jul 30 13:30 d And it was assigned as GID to the user inside the pod: $ oc rsh test-0 / $ id uid=65000 gid=0(root) groups=1000090000 Resetting the lab Delete the statefulSet, PVC, etc again. Continue once all is deleted: $ oc get statefulset No resources found. $ oc get pods No resources found. $ oc get pvc No resources found. Creating a custom SCC Let's now modify pod.spec.securityContext and set fsGroup: kind: Service apiVersion: v1 metadata: name: \"test\" spec: clusterIP: None # the list of ports that are exposed by this service ports: - name: http port: 80 # will route traffic to pods having labels matching this selector selector: name: \"test\" --- apiVersion: apps/v1 kind: StatefulSet metadata: name: test spec: selector: matchLabels: app: test serviceName: test replicas: 1 template: metadata: labels: app: test spec: securityContext: runAsUser: 65000 fsGroup: 65000 containers: - image: gcr.io/google_containers/busybox command: - \"/bin/sh\" - \"-c\" - \"while true; do date; sleep 1; done\" name: busybox volumeMounts: - name: vol mountPath: /mnt volumeClaimTemplates: - metadata: name: vol spec: accessModes: [\"ReadWriteOnce\"] storageClassName: local-loopbacks resources: requests: storage: 90Mi This of course must fail: oc apply -f statefulset.yaml service/test created statefulset.apps/test created $ oc describe statefulset test | tail StorageClass: local-loopbacks Labels: <none> Annotations: <none> Capacity: 90Mi Access Modes: [ReadWriteOnce] Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 7s statefulset-controller create Claim vol-test-0 Pod test-0 in StatefulSet test success Warning FailedCreate 2s (x11 over 7s) statefulset-controller create Pod test-0 in StatefulSet test failed error: pods \"test-0\" is forbidden: unable to validate against any security context constraint: [fsGroup: Invalid value: []int64{65000}: 65000 is not an allowed group] Let's create a custom SCC to change this. The following group was created by inspecting the restricted SCC with oc get scc restricted -o yaml and then by changing the name, the fsGroup ranges and by removing some of the metadata. restricted-fsgroup.yaml : allowHostDirVolumePlugin: false allowHostIPC: false allowHostNetwork: false allowHostPID: false allowHostPorts: false allowPrivilegeEscalation: true allowPrivilegedContainer: false allowedCapabilities: null apiVersion: security.openshift.io/v1 defaultAddCapabilities: null fsGroup: type: MustRunAs ranges: - min: 65000 max: 66000 kind: SecurityContextConstraints metadata: name: restricted-fsgroup # priority: 10 # it's possible to prioritize this SCC - we are not doing this here readOnlyRootFilesystem: false requiredDropCapabilities: - KILL - MKNOD - SETUID - SETGID runAsUser: type: MustRunAsRange seLinuxContext: type: MustRunAs supplementalGroups: type: RunAsAny users: [] volumes: - configMap - downwardAPI - emptyDir - persistentVolumeClaim - projected - secret $ oc apply -f restricted-fsgroup.yaml securitycontextconstraints.security.openshift.io/restricted-fsgroup created $ oc get scc NAME PRIV CAPS SELINUX RUNASUSER FSGROUP SUPGROUP PRIORITY READONLYROOTFS VOLUMES anyuid false [] MustRunAs RunAsAny RunAsAny RunAsAny 10 false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] hostaccess false [] MustRunAs MustRunAsRange MustRunAs RunAsAny <none> false [configMap downwardAPI emptyDir hostPath persistentVolumeClaim projected secret] hostmount-anyuid false [] MustRunAs RunAsAny RunAsAny RunAsAny <none> false [configMap downwardAPI emptyDir hostPath nfs persistentVolumeClaim projected secret] hostnetwork false [] MustRunAs MustRunAsRange MustRunAs MustRunAs <none> false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] kube-state-metrics false [] RunAsAny RunAsAny RunAsAny RunAsAny <none> false [*] node-exporter false [] RunAsAny RunAsAny RunAsAny RunAsAny <none> false [*] nonroot false [] MustRunAs MustRunAsNonRoot RunAsAny RunAsAny <none> false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] privileged true [*] RunAsAny RunAsAny RunAsAny RunAsAny <none> false [*] restricted false [] MustRunAs MustRunAsRange MustRunAs RunAsAny <none> false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] restricted-fsgroup false [] MustRunAs MustRunAsRange MustRunAs RunAsAny <none> false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] Add this restricted-fsgroup to the default service account's SCCs: $ oc adm policy add-scc-to-user restricted-fsgroup -z default scc \"restricted-fsgroup\" added to: [\"system:serviceaccount:test:default\"] $ oc get scc restricted-fsgroup -o yaml | grep users -A1 (...) users: - system:serviceaccount:test:default (...) Delete the statefulSet, PVC, etc again. Continue once all is deleted: $ oc get statefulset No resources found. $ oc get pods No resources found. $ oc get pvc No resources found. The statefulSet can now spawn the pod and the pod is assigned to SCC restricted-fsgroup : $ oc apply -f statefulset.yaml $ oc get pods NAME READY STATUS RESTARTS AGE test-0 1/1 Running 0 17s $ oc describe statefulset test | tail StorageClass: local-loopbacks Labels: <none> Annotations: <none> Capacity: 90Mi Access Modes: [ReadWriteOnce] Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 30s statefulset-controller create Claim vol-test-0 Pod test-0 in StatefulSet test success Normal SuccessfulCreate 30s statefulset-controller create Pod test-0 in StatefulSet test successful $ oc get pod test-0 -o yaml | grep scc openshift.io/scc: restricted-fsgroup The pod now runs with group ID 65000: $ oc rsh test-0 / $ id uid=65000 gid=0(root) groups=65000 And that matches the group ID of the mount's directory: [root@node-0 ~]# ls /mnt/local-storage/loopbacks/ -al total 8 drwxr-xr-x. 6 root root 42 Jul 30 13:30 . drwxr-xr-x. 3 root root 23 Jul 30 13:28 .. drwxrwsr-x. 2 root root 1024 Aug 3 10:06 a drwxrwsr-x. 2 root 65000 1024 Aug 3 09:55 b drwxrwsr-x. 2 root root 1024 Aug 3 09:55 c drwxr-xr-x. 3 root root 1024 Jul 30 13:30 d SCC matching and priority Keep in mind that both SCCs are active at the same time - and will be selected depending on which SCC is matched by the requested pod. Matching based on allowed security context Let's change fsGroup: 65000 to fsGroup: 1000090000 : kind: Service apiVersion: v1 metadata: name: \"test\" spec: clusterIP: None # the list of ports that are exposed by this service ports: - name: http port: 80 # will route traffic to pods having labels matching this selector selector: name: \"test\" --- apiVersion: apps/v1 kind: StatefulSet metadata: name: test spec: selector: matchLabels: app: test serviceName: test replicas: 1 template: metadata: labels: app: test spec: securityContext: runAsUser: 65000 fsGroup: 1000090000 containers: - image: gcr.io/google_containers/busybox command: - \"/bin/sh\" - \"-c\" - \"while true; do date; sleep 1; done\" name: busybox volumeMounts: - name: vol mountPath: /mnt volumeClaimTemplates: - metadata: name: vol spec: accessModes: [\"ReadWriteOnce\"] storageClassName: local-loopbacks resources: requests: storage: 90Mi Reset the lab again, then deploy the modified deployment. Now, the pod's SCC is restricted because the requested GID only matches the restricted but not the restricted-fsgroup SCC: $ oc apply -f statefulset.yaml service/test created $ oc get pod test-0 -o yaml | grep scc openshift.io/scc: restricted Name as tiebreaker Now, what happens if we remove the request for a specific fsGroup from the pod? kind: Service apiVersion: v1 metadata: name: \"test\" spec: clusterIP: None # the list of ports that are exposed by this service ports: - name: http port: 80 # will route traffic to pods having labels matching this selector selector: name: \"test\" --- apiVersion: apps/v1 kind: StatefulSet metadata: name: test spec: selector: matchLabels: app: test serviceName: test replicas: 1 template: metadata: labels: app: test spec: securityContext: runAsUser: 65000 containers: - image: gcr.io/google_containers/busybox command: - \"/bin/sh\" - \"-c\" - \"while true; do date; sleep 1; done\" name: busybox volumeMounts: - name: vol mountPath: /mnt volumeClaimTemplates: - metadata: name: vol spec: accessModes: [\"ReadWriteOnce\"] storageClassName: local-loopbacks resources: requests: storage: 90Mi Reset the lab again, then apply the statefulset. $ oc apply -f statefulset.yaml service/test created statefulset.apps/test created $ oc get pod test-0 -o yaml | grep scc openshift.io/scc: restricted $ oc rsh test-0 / $ id uid=65000 gid=0(root) groups=1000090000 [root@node-0 ~]# ls /mnt/local-storage/loopbacks/ -al total 8 drwxr-xr-x. 6 root root 42 Jul 30 13:30 . drwxr-xr-x. 3 root root 23 Jul 30 13:28 .. drwxrwsr-x. 2 root 1000090000 1024 Aug 3 10:06 a drwxrwsr-x. 2 root root 1024 Aug 3 09:55 b drwxrwsr-x. 2 root root 1024 Aug 3 09:55 c drwxr-xr-x. 3 root root 1024 Jul 30 13:30 d Both SCCs have the same priority, are equally restrictive. So the tiebreaker is the name: https://docs.openshift.com/container-platform/3.11/architecture/additional_concepts/authorization.html#scc-prioritization Highest priority first, nil is considered a 0 priority If priorities are equal, the SCCs will be sorted from most restrictive to least restrictive If both priorities and restrictions are equal the SCCs will be sorted by name Let's suppose this had a different name: $ oc delete -f restricted-fsgroup.yaml securitycontextconstraints.security.openshift.io \"restricted-fsgroup\" deleted $ oc apply -f fsgroup.yaml securitycontextconstraints.security.openshift.io/fsgroup created $ oc adm policy add-scc-to-user fsgroup -z default scc \"fsgroup\" added to: [\"system:serviceaccount:test:default\"] $ oc get scc NAME PRIV CAPS SELINUX RUNASUSER FSGROUP SUPGROUP PRIORITY READONLYROOTFS VOLUMES anyuid false [] MustRunAs RunAsAny RunAsAny RunAsAny 10 false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] fsgroup false [] MustRunAs MustRunAsRange MustRunAs RunAsAny <none> false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] hostaccess false [] MustRunAs MustRunAsRange MustRunAs RunAsAny <none> false [configMap downwardAPI emptyDir hostPath persistentVolumeClaim projected secret] hostmount-anyuid false [] MustRunAs RunAsAny RunAsAny RunAsAny <none> false [configMap downwardAPI emptyDir hostPath nfs persistentVolumeClaim projected secret] hostnetwork false [] MustRunAs MustRunAsRange MustRunAs MustRunAs <none> false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] kube-state-metrics false [] RunAsAny RunAsAny RunAsAny RunAsAny <none> false [*] node-exporter false [] RunAsAny RunAsAny RunAsAny RunAsAny <none> false [*] nonroot false [] MustRunAs MustRunAsNonRoot RunAsAny RunAsAny <none> false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] privileged true [*] RunAsAny RunAsAny RunAsAny RunAsAny <none> false [*] restricted false [] MustRunAs MustRunAsRange MustRunAs RunAsAny <none> false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] Then we'd use the fsgroup SCC: $ oc apply -f statefulset.yaml service/test created statefulset.apps/test created $ oc get pod test-0 -o yaml | grep scc openshift.io/scc: fsgroup $ oc rsh test-0 id / $ id uid=65000 gid=0(root) groups=65000 [root@node-0 ~]# ls /mnt/local-storage/loopbacks/ -al total 8 drwxr-xr-x. 6 root root 42 Jul 30 13:30 . drwxr-xr-x. 3 root root 23 Jul 30 13:28 .. drwxrwsr-x. 2 root 65000 1024 Aug 3 10:06 a drwxrwsr-x. 2 root root 1024 Aug 3 09:55 b drwxrwsr-x. 2 root root 1024 Aug 3 09:55 c drwxr-xr-x. 3 root root 1024 Jul 30 13:30 d Using the priority field as a tiebreaker And is there a way to prioritize our policy without changing the name? Yes, with the priority field. Change the SCC's name again to restricted-fsgroup . So based on the name, it would lose against restricted . But also set the priority to 1 which is higher than restricted s priority of 0 : $ cat restricted-fsgroup.yaml allowHostDirVolumePlugin: false allowHostIPC: false allowHostNetwork: false allowHostPID: false allowHostPorts: false allowPrivilegeEscalation: true allowPrivilegedContainer: false allowedCapabilities: null apiVersion: security.openshift.io/v1 defaultAddCapabilities: null fsGroup: type: MustRunAs ranges: - min: 65000 max: 66000 kind: SecurityContextConstraints metadata: name: restricted-fsgroup readOnlyRootFilesystem: false requiredDropCapabilities: - KILL - MKNOD - SETUID - SETGID runAsUser: type: MustRunAsRange seLinuxContext: type: MustRunAs supplementalGroups: type: RunAsAny users: [] volumes: - configMap - downwardAPI - emptyDir - persistentVolumeClaim - projected - secret priority: 1 After applying this, we see: $ oc get scc NAME PRIV CAPS SELINUX RUNASUSER FSGROUP SUPGROUP PRIORITY READONLYROOTFS VOLUMES anyuid false [] MustRunAs RunAsAny RunAsAny RunAsAny 10 false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] hostaccess false [] MustRunAs MustRunAsRange MustRunAs RunAsAny <none> false [configMap downwardAPI emptyDir hostPath persistentVolumeClaim projected secret] hostmount-anyuid false [] MustRunAs RunAsAny RunAsAny RunAsAny <none> false [configMap downwardAPI emptyDir hostPath nfs persistentVolumeClaim projected secret] hostnetwork false [] MustRunAs MustRunAsRange MustRunAs MustRunAs <none> false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] kube-state-metrics false [] RunAsAny RunAsAny RunAsAny RunAsAny <none> false [*] node-exporter false [] RunAsAny RunAsAny RunAsAny RunAsAny <none> false [*] nonroot false [] MustRunAs MustRunAsNonRoot RunAsAny RunAsAny <none> false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] privileged true [*] RunAsAny RunAsAny RunAsAny RunAsAny <none> false [*] restricted false [] MustRunAs MustRunAsRange MustRunAs RunAsAny <none> false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] restricted-fsgroup false [] MustRunAs MustRunAsRange MustRunAs RunAsAny 1 false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] $ oc get pod test-0 -o yaml | grep scc openshift.io/scc: restricted-fsgroup Demonstrating that priority only comes into play if SCCs overlap But should we decide that we not like a fsGroup within 65000-65999, we can request the namespace's default and still use restricted , even though we changed the priority: kind: Service apiVersion: v1 metadata: name: \"test\" spec: clusterIP: None # the list of ports that are exposed by this service ports: - name: http port: 80 # will route traffic to pods having labels matching this selector selector: name: \"test\" --- apiVersion: apps/v1 kind: StatefulSet metadata: name: test spec: selector: matchLabels: app: test serviceName: test replicas: 1 template: metadata: labels: app: test spec: securityContext: runAsUser: 65000 fsGroup: 1000090000 containers: - image: gcr.io/google_containers/busybox command: - \"/bin/sh\" - \"-c\" - \"while true; do date; sleep 1; done\" name: busybox volumeMounts: - name: vol mountPath: /mnt volumeClaimTemplates: - metadata: name: vol spec: accessModes: [\"ReadWriteOnce\"] storageClassName: local-loopbacks resources: requests: storage: 90Mi See this: $ oc apply -f statefulset.yaml service/test created statefulset.apps/test created $ oc get pod test-0 -o yaml | grep scc openshift.io/scc: restricted Summary Lessons learned: we can apply multiple SCCs to a ServiceAccount. The default SCC is always restricted . The matching goes by priority first, then most restrictive policy, then by name. And if a policy does not match a specific request, another one might \"jump in\" and \"help out\". Resources https://www.openshift.com/blog/managing-sccs-in-openshift https://docs.openshift.com/container-platform/3.11/admin_guide/manage_scc.html https://docs.openshift.com/container-platform/3.11/install_config/persistent_storage/pod_security_context.html","title":"Security Context Constraints (SCC)"},{"location":"openshift/scc/#how-security-context-constraints-sccs-work-in-openshift","text":"How SCCs work in OpenShift by example. The following tests were run in Red Hat OpenShift Platform 3.11. Depending on the version in use, the cluster's behavior may be slightly different.","title":"How Security Context Constraints (SCCs) work in OpenShift"},{"location":"openshift/scc/#impact-of-the-default-restricted-scc","text":"The following example spawns a statefulset with a pod as user 65000. This user falls not into the supplemental-groups of the project and can hence not be used by the restricted SCC. The deployment will fail: $ oc describe project test | grep scc openshift.io/sa.scc.mcs=s0:c10,c0 openshift.io/sa.scc.supplemental-groups=1000090000/10000 openshift.io/sa.scc.uid-range=1000090000/10000 Storage class is a local provisioner with 4 100MB local mounts, set up with https://github.com/andreaskaris/kubernetes/tree/master/localvolume : $ oc get storageclass NAME PROVISIONER AGE local-loopbacks kubernetes.io/no-provisioner 3d $ oc get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE local-pv-18dc9ec0 92Mi RWO Delete Available local-loopbacks 36s local-pv-89b778f1 92Mi RWO Delete Available local-loopbacks 19m local-pv-9e67e7d6 92Mi RWO Delete Available local-loopbacks 28m local-pv-b6a7cd37 92Mi RWO Delete Available local-loopbacks 3d statefulset.yaml : kind: Service apiVersion: v1 metadata: name: \"test\" spec: clusterIP: None # the list of ports that are exposed by this service ports: - name: http port: 80 # will route traffic to pods having labels matching this selector selector: name: \"test\" --- apiVersion: apps/v1 kind: StatefulSet metadata: name: test spec: selector: matchLabels: app: test serviceName: test replicas: 1 template: metadata: labels: app: test spec: securityContext: runAsUser: 65000 containers: - image: gcr.io/google_containers/busybox command: - \"/bin/sh\" - \"-c\" - \"while true; do date; sleep 1; done\" name: busybox volumeMounts: - name: vol mountPath: /mnt volumeClaimTemplates: - metadata: name: vol spec: accessModes: [\"ReadWriteOnce\"] storageClassName: local-loopbacks resources: requests: storage: 90Mi $ oc apply -f statefulset.yaml OpenShift's admission controller will complain that the range is not correct: $ oc get pods No resources found. $ oc describe statefulset test | tail StorageClass: local-loopbacks Labels: <none> Annotations: <none> Capacity: 90Mi Access Modes: [ReadWriteOnce] Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 40s statefulset-controller create Claim vol-test-0 Pod test-0 in StatefulSet test success Warning FailedCreate 19s (x14 over 40s) statefulset-controller create Pod test-0 in StatefulSet test failed error: pods \"test-0\" is forbidden: unable to validate against any security context constraint: [spec.containers[0].securityContext.securityContext.runAsUser: Invalid value: 65000: must be in the ranges: [1000090000, 1000099999]]","title":"Impact of the default restricted SCC"},{"location":"openshift/scc/#fixing-failed-validation-of-security-context-constraints","text":"There are 3 solutions to this. Either: create a new SCC (or modify the restricted policy which is not recommended) or modify the runAsUser field to run the pod as a user inside range 1000090000, 1000099999 or change the namespace's openshift.io/sa.scc.uid-range .","title":"Fixing failed validation of security context constraints"},{"location":"openshift/scc/#resetting-the-lab","text":"First, delete the statefulSet, PVC, etc. Continue once all is deleted: $ oc get statefulset No resources found. $ oc get pods No resources found. $ oc get pvc No resources found.","title":"Resetting the lab"},{"location":"openshift/scc/#changing-the-namespaces-openshiftiosasccuid-range","text":"Let's change the namespace's uid-range : $ oc edit namespace test namespace/test edited $ oc describe project test | grep scc openshift.io/sa.scc.mcs=s0:c10,c0 openshift.io/sa.scc.supplemental-groups=1000090000/10000 openshift.io/sa.scc.uid-range=65000/1000 $ oc apply -f statefulset.yaml service/test created statefulset.apps/test created $ oc get statefulset NAME DESIRED CURRENT AGE test 1 1 24s $ oc get pods NAME READY STATUS RESTARTS AGE test-0 1/1 Running 0 26s $ oc get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE vol-test-0 Bound local-pv-9e67e7d6 92Mi RWO local-loopbacks 30s The pod was assigned to the restricted SCC: $ oc get pod test-0 -o yaml | grep scc openshift.io/scc: restricted This is because the restricted SCC is the default. If other SCCs were assigned to the default serviceaccount of the project, then SCC priorization would come into play. For more about SCC priorization, see: https://docs.openshift.com/container-platform/3.11/architecture/additional_concepts/authorization.html#scc-prioritization On the worker that the pod runs at, verify the container's and processes' UID: [root@node-0 ~]# docker ps | grep test 47b2d7df970c gcr.io/google_containers/busybox@sha256:d8d3bc2c183ed2f9f10e7258f84971202325ee6011ba137112e01e30f206de67 \"/bin/sh -c 'while...\" About a minute ago Up About a minute k8s_busybox_test-0_test_6dfbf91e-d5a1-11ea-a312-fa163e8752f1_0 8b3ef77e9213 registry.redhat.io/openshift3/ose-pod:v3.11.248 \"/usr/bin/pod\" About a minute ago Up About a minute k8s_POD_test-0_test_6dfbf91e-d5a1-11ea-a312-fa163e8752f1_0 [root@node-0 ~]# docker inspect 47b2d7df970c | grep -i user \"UsernsMode\": \"\", \"User\": \"65000\", [root@node-0 ~]# ps aux | grep 'date; sleep 1' 65000 5172 0.0 0.0 3164 352 ? Ss 11:53 0:00 /bin/sh -c while true; do date; sleep 1; done root 6616 0.0 0.0 112808 972 pts/0 R+ 11:55 0:00 grep --color=auto date; sleep 1 Also note that the FsGroupID was set automatically for the filesystem: [root@node-0 ~]# ls /mnt/local-storage/loopbacks/ -al total 8 drwxr-xr-x. 6 root root 42 Jul 30 13:30 . drwxr-xr-x. 3 root root 23 Jul 30 13:28 .. drwxrwsr-x. 2 root root 1024 Aug 3 10:06 a drwxrwsr-x. 2 root root 1024 Aug 3 09:55 b drwxrwsr-x. 2 root 1000090000 1024 Aug 3 09:55 c drwxr-xr-x. 3 root root 1024 Jul 30 13:30 d And it was assigned as GID to the user inside the pod: $ oc rsh test-0 / $ id uid=65000 gid=0(root) groups=1000090000","title":"Changing the namespace's openshift.io/sa.scc.uid-range"},{"location":"openshift/scc/#resetting-the-lab_1","text":"Delete the statefulSet, PVC, etc again. Continue once all is deleted: $ oc get statefulset No resources found. $ oc get pods No resources found. $ oc get pvc No resources found.","title":"Resetting the lab"},{"location":"openshift/scc/#creating-a-custom-scc","text":"Let's now modify pod.spec.securityContext and set fsGroup: kind: Service apiVersion: v1 metadata: name: \"test\" spec: clusterIP: None # the list of ports that are exposed by this service ports: - name: http port: 80 # will route traffic to pods having labels matching this selector selector: name: \"test\" --- apiVersion: apps/v1 kind: StatefulSet metadata: name: test spec: selector: matchLabels: app: test serviceName: test replicas: 1 template: metadata: labels: app: test spec: securityContext: runAsUser: 65000 fsGroup: 65000 containers: - image: gcr.io/google_containers/busybox command: - \"/bin/sh\" - \"-c\" - \"while true; do date; sleep 1; done\" name: busybox volumeMounts: - name: vol mountPath: /mnt volumeClaimTemplates: - metadata: name: vol spec: accessModes: [\"ReadWriteOnce\"] storageClassName: local-loopbacks resources: requests: storage: 90Mi This of course must fail: oc apply -f statefulset.yaml service/test created statefulset.apps/test created $ oc describe statefulset test | tail StorageClass: local-loopbacks Labels: <none> Annotations: <none> Capacity: 90Mi Access Modes: [ReadWriteOnce] Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 7s statefulset-controller create Claim vol-test-0 Pod test-0 in StatefulSet test success Warning FailedCreate 2s (x11 over 7s) statefulset-controller create Pod test-0 in StatefulSet test failed error: pods \"test-0\" is forbidden: unable to validate against any security context constraint: [fsGroup: Invalid value: []int64{65000}: 65000 is not an allowed group] Let's create a custom SCC to change this. The following group was created by inspecting the restricted SCC with oc get scc restricted -o yaml and then by changing the name, the fsGroup ranges and by removing some of the metadata. restricted-fsgroup.yaml : allowHostDirVolumePlugin: false allowHostIPC: false allowHostNetwork: false allowHostPID: false allowHostPorts: false allowPrivilegeEscalation: true allowPrivilegedContainer: false allowedCapabilities: null apiVersion: security.openshift.io/v1 defaultAddCapabilities: null fsGroup: type: MustRunAs ranges: - min: 65000 max: 66000 kind: SecurityContextConstraints metadata: name: restricted-fsgroup # priority: 10 # it's possible to prioritize this SCC - we are not doing this here readOnlyRootFilesystem: false requiredDropCapabilities: - KILL - MKNOD - SETUID - SETGID runAsUser: type: MustRunAsRange seLinuxContext: type: MustRunAs supplementalGroups: type: RunAsAny users: [] volumes: - configMap - downwardAPI - emptyDir - persistentVolumeClaim - projected - secret $ oc apply -f restricted-fsgroup.yaml securitycontextconstraints.security.openshift.io/restricted-fsgroup created $ oc get scc NAME PRIV CAPS SELINUX RUNASUSER FSGROUP SUPGROUP PRIORITY READONLYROOTFS VOLUMES anyuid false [] MustRunAs RunAsAny RunAsAny RunAsAny 10 false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] hostaccess false [] MustRunAs MustRunAsRange MustRunAs RunAsAny <none> false [configMap downwardAPI emptyDir hostPath persistentVolumeClaim projected secret] hostmount-anyuid false [] MustRunAs RunAsAny RunAsAny RunAsAny <none> false [configMap downwardAPI emptyDir hostPath nfs persistentVolumeClaim projected secret] hostnetwork false [] MustRunAs MustRunAsRange MustRunAs MustRunAs <none> false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] kube-state-metrics false [] RunAsAny RunAsAny RunAsAny RunAsAny <none> false [*] node-exporter false [] RunAsAny RunAsAny RunAsAny RunAsAny <none> false [*] nonroot false [] MustRunAs MustRunAsNonRoot RunAsAny RunAsAny <none> false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] privileged true [*] RunAsAny RunAsAny RunAsAny RunAsAny <none> false [*] restricted false [] MustRunAs MustRunAsRange MustRunAs RunAsAny <none> false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] restricted-fsgroup false [] MustRunAs MustRunAsRange MustRunAs RunAsAny <none> false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] Add this restricted-fsgroup to the default service account's SCCs: $ oc adm policy add-scc-to-user restricted-fsgroup -z default scc \"restricted-fsgroup\" added to: [\"system:serviceaccount:test:default\"] $ oc get scc restricted-fsgroup -o yaml | grep users -A1 (...) users: - system:serviceaccount:test:default (...) Delete the statefulSet, PVC, etc again. Continue once all is deleted: $ oc get statefulset No resources found. $ oc get pods No resources found. $ oc get pvc No resources found. The statefulSet can now spawn the pod and the pod is assigned to SCC restricted-fsgroup : $ oc apply -f statefulset.yaml $ oc get pods NAME READY STATUS RESTARTS AGE test-0 1/1 Running 0 17s $ oc describe statefulset test | tail StorageClass: local-loopbacks Labels: <none> Annotations: <none> Capacity: 90Mi Access Modes: [ReadWriteOnce] Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 30s statefulset-controller create Claim vol-test-0 Pod test-0 in StatefulSet test success Normal SuccessfulCreate 30s statefulset-controller create Pod test-0 in StatefulSet test successful $ oc get pod test-0 -o yaml | grep scc openshift.io/scc: restricted-fsgroup The pod now runs with group ID 65000: $ oc rsh test-0 / $ id uid=65000 gid=0(root) groups=65000 And that matches the group ID of the mount's directory: [root@node-0 ~]# ls /mnt/local-storage/loopbacks/ -al total 8 drwxr-xr-x. 6 root root 42 Jul 30 13:30 . drwxr-xr-x. 3 root root 23 Jul 30 13:28 .. drwxrwsr-x. 2 root root 1024 Aug 3 10:06 a drwxrwsr-x. 2 root 65000 1024 Aug 3 09:55 b drwxrwsr-x. 2 root root 1024 Aug 3 09:55 c drwxr-xr-x. 3 root root 1024 Jul 30 13:30 d","title":"Creating a custom SCC"},{"location":"openshift/scc/#scc-matching-and-priority","text":"Keep in mind that both SCCs are active at the same time - and will be selected depending on which SCC is matched by the requested pod.","title":"SCC matching and priority"},{"location":"openshift/scc/#matching-based-on-allowed-security-context","text":"Let's change fsGroup: 65000 to fsGroup: 1000090000 : kind: Service apiVersion: v1 metadata: name: \"test\" spec: clusterIP: None # the list of ports that are exposed by this service ports: - name: http port: 80 # will route traffic to pods having labels matching this selector selector: name: \"test\" --- apiVersion: apps/v1 kind: StatefulSet metadata: name: test spec: selector: matchLabels: app: test serviceName: test replicas: 1 template: metadata: labels: app: test spec: securityContext: runAsUser: 65000 fsGroup: 1000090000 containers: - image: gcr.io/google_containers/busybox command: - \"/bin/sh\" - \"-c\" - \"while true; do date; sleep 1; done\" name: busybox volumeMounts: - name: vol mountPath: /mnt volumeClaimTemplates: - metadata: name: vol spec: accessModes: [\"ReadWriteOnce\"] storageClassName: local-loopbacks resources: requests: storage: 90Mi Reset the lab again, then deploy the modified deployment. Now, the pod's SCC is restricted because the requested GID only matches the restricted but not the restricted-fsgroup SCC: $ oc apply -f statefulset.yaml service/test created $ oc get pod test-0 -o yaml | grep scc openshift.io/scc: restricted","title":"Matching based on allowed security context"},{"location":"openshift/scc/#name-as-tiebreaker","text":"Now, what happens if we remove the request for a specific fsGroup from the pod? kind: Service apiVersion: v1 metadata: name: \"test\" spec: clusterIP: None # the list of ports that are exposed by this service ports: - name: http port: 80 # will route traffic to pods having labels matching this selector selector: name: \"test\" --- apiVersion: apps/v1 kind: StatefulSet metadata: name: test spec: selector: matchLabels: app: test serviceName: test replicas: 1 template: metadata: labels: app: test spec: securityContext: runAsUser: 65000 containers: - image: gcr.io/google_containers/busybox command: - \"/bin/sh\" - \"-c\" - \"while true; do date; sleep 1; done\" name: busybox volumeMounts: - name: vol mountPath: /mnt volumeClaimTemplates: - metadata: name: vol spec: accessModes: [\"ReadWriteOnce\"] storageClassName: local-loopbacks resources: requests: storage: 90Mi Reset the lab again, then apply the statefulset. $ oc apply -f statefulset.yaml service/test created statefulset.apps/test created $ oc get pod test-0 -o yaml | grep scc openshift.io/scc: restricted $ oc rsh test-0 / $ id uid=65000 gid=0(root) groups=1000090000 [root@node-0 ~]# ls /mnt/local-storage/loopbacks/ -al total 8 drwxr-xr-x. 6 root root 42 Jul 30 13:30 . drwxr-xr-x. 3 root root 23 Jul 30 13:28 .. drwxrwsr-x. 2 root 1000090000 1024 Aug 3 10:06 a drwxrwsr-x. 2 root root 1024 Aug 3 09:55 b drwxrwsr-x. 2 root root 1024 Aug 3 09:55 c drwxr-xr-x. 3 root root 1024 Jul 30 13:30 d Both SCCs have the same priority, are equally restrictive. So the tiebreaker is the name: https://docs.openshift.com/container-platform/3.11/architecture/additional_concepts/authorization.html#scc-prioritization Highest priority first, nil is considered a 0 priority If priorities are equal, the SCCs will be sorted from most restrictive to least restrictive If both priorities and restrictions are equal the SCCs will be sorted by name Let's suppose this had a different name: $ oc delete -f restricted-fsgroup.yaml securitycontextconstraints.security.openshift.io \"restricted-fsgroup\" deleted $ oc apply -f fsgroup.yaml securitycontextconstraints.security.openshift.io/fsgroup created $ oc adm policy add-scc-to-user fsgroup -z default scc \"fsgroup\" added to: [\"system:serviceaccount:test:default\"] $ oc get scc NAME PRIV CAPS SELINUX RUNASUSER FSGROUP SUPGROUP PRIORITY READONLYROOTFS VOLUMES anyuid false [] MustRunAs RunAsAny RunAsAny RunAsAny 10 false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] fsgroup false [] MustRunAs MustRunAsRange MustRunAs RunAsAny <none> false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] hostaccess false [] MustRunAs MustRunAsRange MustRunAs RunAsAny <none> false [configMap downwardAPI emptyDir hostPath persistentVolumeClaim projected secret] hostmount-anyuid false [] MustRunAs RunAsAny RunAsAny RunAsAny <none> false [configMap downwardAPI emptyDir hostPath nfs persistentVolumeClaim projected secret] hostnetwork false [] MustRunAs MustRunAsRange MustRunAs MustRunAs <none> false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] kube-state-metrics false [] RunAsAny RunAsAny RunAsAny RunAsAny <none> false [*] node-exporter false [] RunAsAny RunAsAny RunAsAny RunAsAny <none> false [*] nonroot false [] MustRunAs MustRunAsNonRoot RunAsAny RunAsAny <none> false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] privileged true [*] RunAsAny RunAsAny RunAsAny RunAsAny <none> false [*] restricted false [] MustRunAs MustRunAsRange MustRunAs RunAsAny <none> false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] Then we'd use the fsgroup SCC: $ oc apply -f statefulset.yaml service/test created statefulset.apps/test created $ oc get pod test-0 -o yaml | grep scc openshift.io/scc: fsgroup $ oc rsh test-0 id / $ id uid=65000 gid=0(root) groups=65000 [root@node-0 ~]# ls /mnt/local-storage/loopbacks/ -al total 8 drwxr-xr-x. 6 root root 42 Jul 30 13:30 . drwxr-xr-x. 3 root root 23 Jul 30 13:28 .. drwxrwsr-x. 2 root 65000 1024 Aug 3 10:06 a drwxrwsr-x. 2 root root 1024 Aug 3 09:55 b drwxrwsr-x. 2 root root 1024 Aug 3 09:55 c drwxr-xr-x. 3 root root 1024 Jul 30 13:30 d","title":"Name as tiebreaker"},{"location":"openshift/scc/#using-the-priority-field-as-a-tiebreaker","text":"And is there a way to prioritize our policy without changing the name? Yes, with the priority field. Change the SCC's name again to restricted-fsgroup . So based on the name, it would lose against restricted . But also set the priority to 1 which is higher than restricted s priority of 0 : $ cat restricted-fsgroup.yaml allowHostDirVolumePlugin: false allowHostIPC: false allowHostNetwork: false allowHostPID: false allowHostPorts: false allowPrivilegeEscalation: true allowPrivilegedContainer: false allowedCapabilities: null apiVersion: security.openshift.io/v1 defaultAddCapabilities: null fsGroup: type: MustRunAs ranges: - min: 65000 max: 66000 kind: SecurityContextConstraints metadata: name: restricted-fsgroup readOnlyRootFilesystem: false requiredDropCapabilities: - KILL - MKNOD - SETUID - SETGID runAsUser: type: MustRunAsRange seLinuxContext: type: MustRunAs supplementalGroups: type: RunAsAny users: [] volumes: - configMap - downwardAPI - emptyDir - persistentVolumeClaim - projected - secret priority: 1 After applying this, we see: $ oc get scc NAME PRIV CAPS SELINUX RUNASUSER FSGROUP SUPGROUP PRIORITY READONLYROOTFS VOLUMES anyuid false [] MustRunAs RunAsAny RunAsAny RunAsAny 10 false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] hostaccess false [] MustRunAs MustRunAsRange MustRunAs RunAsAny <none> false [configMap downwardAPI emptyDir hostPath persistentVolumeClaim projected secret] hostmount-anyuid false [] MustRunAs RunAsAny RunAsAny RunAsAny <none> false [configMap downwardAPI emptyDir hostPath nfs persistentVolumeClaim projected secret] hostnetwork false [] MustRunAs MustRunAsRange MustRunAs MustRunAs <none> false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] kube-state-metrics false [] RunAsAny RunAsAny RunAsAny RunAsAny <none> false [*] node-exporter false [] RunAsAny RunAsAny RunAsAny RunAsAny <none> false [*] nonroot false [] MustRunAs MustRunAsNonRoot RunAsAny RunAsAny <none> false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] privileged true [*] RunAsAny RunAsAny RunAsAny RunAsAny <none> false [*] restricted false [] MustRunAs MustRunAsRange MustRunAs RunAsAny <none> false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] restricted-fsgroup false [] MustRunAs MustRunAsRange MustRunAs RunAsAny 1 false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] $ oc get pod test-0 -o yaml | grep scc openshift.io/scc: restricted-fsgroup","title":"Using the priority field as a tiebreaker"},{"location":"openshift/scc/#demonstrating-that-priority-only-comes-into-play-if-sccs-overlap","text":"But should we decide that we not like a fsGroup within 65000-65999, we can request the namespace's default and still use restricted , even though we changed the priority: kind: Service apiVersion: v1 metadata: name: \"test\" spec: clusterIP: None # the list of ports that are exposed by this service ports: - name: http port: 80 # will route traffic to pods having labels matching this selector selector: name: \"test\" --- apiVersion: apps/v1 kind: StatefulSet metadata: name: test spec: selector: matchLabels: app: test serviceName: test replicas: 1 template: metadata: labels: app: test spec: securityContext: runAsUser: 65000 fsGroup: 1000090000 containers: - image: gcr.io/google_containers/busybox command: - \"/bin/sh\" - \"-c\" - \"while true; do date; sleep 1; done\" name: busybox volumeMounts: - name: vol mountPath: /mnt volumeClaimTemplates: - metadata: name: vol spec: accessModes: [\"ReadWriteOnce\"] storageClassName: local-loopbacks resources: requests: storage: 90Mi See this: $ oc apply -f statefulset.yaml service/test created statefulset.apps/test created $ oc get pod test-0 -o yaml | grep scc openshift.io/scc: restricted","title":"Demonstrating that priority only comes into play if SCCs overlap"},{"location":"openshift/scc/#summary","text":"Lessons learned: we can apply multiple SCCs to a ServiceAccount. The default SCC is always restricted . The matching goes by priority first, then most restrictive policy, then by name. And if a policy does not match a specific request, another one might \"jump in\" and \"help out\".","title":"Summary"},{"location":"openshift/scc/#resources","text":"https://www.openshift.com/blog/managing-sccs-in-openshift https://docs.openshift.com/container-platform/3.11/admin_guide/manage_scc.html https://docs.openshift.com/container-platform/3.11/install_config/persistent_storage/pod_security_context.html","title":"Resources"},{"location":"openshift/troubleshooting_openshift_on_openstack_worker_creation/","text":"Troubleshooting OpenShift on OpenStack worker creation See https://bugzilla.redhat.com/show_bug.cgi?id=1785705 for the bugzilla. The following are steps which I executed in my lab to troubleshoot issues with worker creation for OpenShift on OpenStack. (overcloud) [stack@undercloud-0 clouds]$ oc describe machine -n openshift-machine-api osc-c5r5c-worker-bgt9b Name: osc-c5r5c-worker-bgt9b Namespace: openshift-machine-api Labels: machine.openshift.io/cluster-api-cluster=osc-c5r5c machine.openshift.io/cluster-api-machine-role=worker machine.openshift.io/cluster-api-machine-type=worker machine.openshift.io/cluster-api-machineset=osc-c5r5c-worker Annotations: <none> API Version: machine.openshift.io/v1beta1 Kind: Machine Metadata: Creation Timestamp: 2019-12-20T10:23:14Z Finalizers: machine.machine.openshift.io Generate Name: osc-c5r5c-worker- Generation: 1 Owner References: API Version: machine.openshift.io/v1beta1 Block Owner Deletion: true Controller: true Kind: MachineSet Name: osc-c5r5c-worker UID: 9077b6df-2312-11ea-9b6c-fa163e431263 Resource Version: 3455 Self Link: /apis/machine.openshift.io/v1beta1/namespaces/openshift-machine-api/machines/osc-c5r5c-worker-bgt9b UID: bb0af782-2312-11ea-9b6c-fa163e431263 Spec: Metadata: Creation Timestamp: <nil> Provider Spec: Value: API Version: openstackproviderconfig.openshift.io/v1alpha1 Cloud Name: openstack Clouds Secret: Name: openstack-cloud-credentials Namespace: openshift-machine-api Flavor: m1.openshift Image: rhcos Kind: OpenstackProviderSpec Metadata: Creation Timestamp: <nil> Networks: Filter: Subnets: Filter: Name: osc-c5r5c-nodes Tags: openshiftClusterID=osc-c5r5c Security Groups: Filter: Name: osc-c5r5c-worker Server Metadata: Name: osc-c5r5c-worker Openshift Cluster ID: osc-c5r5c Tags: openshiftClusterID=osc-c5r5c Trunk: true User Data Secret: Name: worker-user-data Events: <none> (overcloud) [stack@undercloud-0 clouds]$ oc get machine -n openshift-machine-api NAME STATE TYPE REGION ZONE AGE osc-c5r5c-master-0 4h52m osc-c5r5c-master-1 4h52m osc-c5r5c-master-2 4h52m osc-c5r5c-worker-bgt9b 4h51m osc-c5r5c-worker-qphk7 4h51m osc-c5r5c-worker-vs85h 4h51m (overcloud) [stack@undercloud-0 clouds]$ oc get machineset -n openshift-machine-api NAME DESIRED CURRENT READY AVAILABLE AGE osc-c5r5c-worker 3 3 4h54m (overcloud) [stack@undercloud-0 clouds]$ (overcloud) [stack@undercloud-0 clouds]$ kubectl get machineset -n openshift-machine-api osc-c5r5c-worker -o yaml apiVersion: machine.openshift.io/v1beta1 kind: MachineSet metadata: creationTimestamp: \"2019-12-20T10:22:02Z\" generation: 1 labels: machine.openshift.io/cluster-api-cluster: osc-c5r5c machine.openshift.io/cluster-api-machine-role: worker machine.openshift.io/cluster-api-machine-type: worker name: osc-c5r5c-worker namespace: openshift-machine-api resourceVersion: \"3448\" selfLink: /apis/machine.openshift.io/v1beta1/namespaces/openshift-machine-api/machinesets/osc-c5r5c-worker uid: 9077b6df-2312-11ea-9b6c-fa163e431263 spec: replicas: 3 selector: matchLabels: machine.openshift.io/cluster-api-cluster: osc-c5r5c machine.openshift.io/cluster-api-machineset: osc-c5r5c-worker template: metadata: creationTimestamp: null labels: machine.openshift.io/cluster-api-cluster: osc-c5r5c machine.openshift.io/cluster-api-machine-role: worker machine.openshift.io/cluster-api-machine-type: worker machine.openshift.io/cluster-api-machineset: osc-c5r5c-worker spec: metadata: creationTimestamp: null providerSpec: value: apiVersion: openstackproviderconfig.openshift.io/v1alpha1 cloudName: openstack cloudsSecret: name: openstack-cloud-credentials namespace: openshift-machine-api flavor: m1.openshift image: rhcos kind: OpenstackProviderSpec metadata: creationTimestamp: null networks: - filter: {} subnets: - filter: name: osc-c5r5c-nodes tags: openshiftClusterID=osc-c5r5c securityGroups: - filter: {} name: osc-c5r5c-worker serverMetadata: Name: osc-c5r5c-worker openshiftClusterID: osc-c5r5c tags: - openshiftClusterID=osc-c5r5c trunk: true userDataSecret: name: worker-user-data status: fullyLabeledReplicas: 3 observedGeneration: 1 replicas: 3 /var/log/pods/openshift-machine-api_machine-api-controllers-f64b7f7b8-tm7qc_b1f05af2-2312-11ea-9b6c-fa163e431263/machine-controller/0.log (...) 2019-12-20T10:23:14.635223358+00:00 stderr F E1220 10:23:14.635191 1 controller.go:239] Failed to check if machine \"osc-c5r5c-worker-bgt9b\" exists: Error checking if instance exists (machine/actuator.go 346): 2019-12-20T10:23:14.635223358+00:00 stderr F Error getting a new instance service from the machine (machine/actuator.go 467): Create providerClient err: You must provide exactly one of DomainID or DomainName in a Scope with ProjectName 2019-12-20T10:23:15.636135623+00:00 stderr F I1220 10:23:15.635874 1 controller.go:133] Reconciling Machine \"osc-c5r5c-master-0\" 2019-12-20T10:23:15.636215058+00:00 stderr F I1220 10:23:15.636192 1 controller.go:304] Machine \"osc-c5r5c-master-0\" in namespace \"openshift-machine-api\" doesn't specify \"cluster.k8s.io/cluster-name\" label, assuming nil cluster 2019-12-20T10:23:15.641348382+00:00 stderr F E1220 10:23:15.641317 1 controller.go:239] Failed to check if machine \"osc-c5r5c-master-0\" exists: Error checking if instance exists (machine/actuator.go 346): 2019-12-20T10:23:15.641348382+00:00 stderr F Error getting a new instance service from the machine (machine/actuator.go 467): Create providerClient err: You must provide exactly one of DomainID or DomainName in a Scope with ProjectName 2019-12-20T10:23:16.641614350+00:00 stderr F I1220 10:23:16.641562 1 controller.go:133] Reconciling Machine \"osc-c5r5c-master-1\" 2019-12-20T10:23:16.641614350+00:00 stderr F I1220 10:23:16.641591 1 controller.go:304] Machine \"osc-c5r5c-master-1\" in namespace \"openshift-machine-api\" doesn't specify \"cluster.k8s.io/cluster-name\" label, assuming nil cluster 2019-12-20T10:23:16.647323379+00:00 stderr F E1220 10:23:16.647295 1 controller.go:239] Failed to check if machine \"osc-c5r5c-master-1\" exists: Error checking if instance exists (machine/actuator.go 346): 2019-12-20T10:23:16.647323379+00:00 stderr F Error getting a new instance service from the machine (machine/actuator.go 467): Create providerClient err: You must provide exactly one of DomainID or DomainName in a Scope with ProjectName 2019-12-20T10:23:17.647630531+00:00 stderr F I1220 10:23:17.647587 1 controller.go:133] Reconciling Machine \"osc-c5r5c-master-2\" 2019-12-20T10:23:17.647695173+00:00 stderr F I1220 10:23:17.647677 1 controller.go:304] Machine \"osc-c5r5c-master-2\" in namespace \"openshift-machine-api\" doesn't specify \"cluster.k8s.io/cluster-name\" label, assuming nil cluster 2019-12-20T10:23:17.652613228+00:00 stderr F E1220 10:23:17.652551 1 controller.go:239] Failed to check if machine \"osc-c5r5c-master-2\" exists: Error checking if instance exists (machine/actuator.go 346): 2019-12-20T10:23:17.652613228+00:00 stderr F Error getting a new instance service from the machine (machine/actuator.go 467): Create providerClient err: You must provide exactly one of DomainID or DomainName in a Scope with ProjectName 2019-12-20T10:23:18.652945792+00:00 stderr F I1220 10:23:18.652884 1 controller.go:133] Reconciling Machine \"osc-c5r5c-worker-qphk7\" Looking at the secret: (overcloud) [stack@undercloud-0 clouds]$ kubectl get secrets -n openshift-machine-api openstack-cloud-credentials -o yaml apiVersion: v1 data: clouds.yaml: Y2xvdWRzOgogIG9wZW5zdGFjazoKICAgIGF1dGg6CiAgICAgIGFwcGxpY2F0aW9uX2NyZWRlbnRpYWxfaWQ6ICIiCiAgICAgIGFwcGxpY2F0aW9uX2NyZWRlbnRpYWxfbmFtZTogIiIKICAgICAgYXBwbGljYXRpb25fY3JlZGVudGlhbF9zZWNyZXQ6ICIiCiAgICAgIGF1dGhfdXJsOiBodHRwOi8vMTcyLjE2LjAuMTMwOjUwMDAvL3YzCiAgICAgIGRlZmF1bHRfZG9tYWluOiAiIgogICAgICBkb21haW5faWQ6ICIiCiAgICAgIGRvbWFpbl9uYW1lOiAiIgogICAgICBwYXNzd29yZDogelA0YmUydWtocENrajR6cVJmVWs4WGpRYgogICAgICBwcm9qZWN0X2RvbWFpbl9pZDogIiIKICAgICAgcHJvamVjdF9kb21haW5fbmFtZTogIiIKICAgICAgcHJvamVjdF9pZDogIiIKICAgICAgcHJvamVjdF9uYW1lOiBhZG1pbgogICAgICB0b2tlbjogIiIKICAgICAgdXNlcl9kb21haW5faWQ6ICIiCiAgICAgIHVzZXJfZG9tYWluX25hbWU6IERlZmF1bHQKICAgICAgdXNlcl9pZDogIiIKICAgICAgdXNlcm5hbWU6IGFkbWluCiAgICBhdXRoX3R5cGU6ICIiCiAgICBjYWNlcnQ6ICIiCiAgICBjZXJ0OiAiIgogICAgY2xvdWQ6ICIiCiAgICBpZGVudGl0eV9hcGlfdmVyc2lvbjogIjMiCiAgICBrZXk6ICIiCiAgICBwcm9maWxlOiAiIgogICAgcmVnaW9uX25hbWU6IHJlZ2lvbk9uZQogICAgcmVnaW9uczogbnVsbAogICAgdmVyaWZ5OiB0cnVlCiAgICB2b2x1bWVfYXBpX3ZlcnNpb246ICIiCg== kind: Secret metadata: annotations: cloudcredential.openshift.io/credentials-request: openshift-cloud-credential-operator/openshift-machine-api-openstack creationTimestamp: \"2019-12-20T10:23:00Z\" name: openstack-cloud-credentials namespace: openshift-machine-api resourceVersion: \"2525\" selfLink: /api/v1/namespaces/openshift-machine-api/secrets/openstack-cloud-credentials uid: b2bded52-2312-11ea-9b6c-fa163e431263 type: Opaque (overcloud) [stack@undercloud-0 clouds]$ base64 -d <(echo 'Y2xvdWRzOgogIG9wZW5zdGFjazoKICAgIGF1dGg6CiAgICAgIGFwcGxpY2F0aW9uX2NyZWRlbnRpYWxfaWQ6ICIiCiAgICAgIGFwcGxpY2F0aW9uX2NyZWRlbnRpYWxfbmFtZTogIiIKICAgICAgYXBwbGljYXRpb25fY3JlZGVudGlhbF9zZWNyZXQ6ICIiCiAgICAgIGF1dGhfdXJsOiBodHRwOi8vMTcyLjE2LjAuMTMwOjUwMDAvL3YzCiAgICAgIGRlZmF1bHRfZG9tYWluOiAiIgogICAgICBkb21haW5faWQ6ICIiCiAgICAgIGRvbWFpbl9uYW1lOiAiIgogICAgICBwYXNzd29yZDogelA0YmUydWtocENrajR6cVJmVWs4WGpRYgogICAgICBwcm9qZWN0X2RvbWFpbl9pZDogIiIKICAgICAgcHJvamVjdF9kb21haW5fbmFtZTogIiIKICAgICAgcHJvamVjdF9pZDogIiIKICAgICAgcHJvamVjdF9uYW1lOiBhZG1pbgogICAgICB0b2tlbjogIiIKICAgICAgdXNlcl9kb21haW5faWQ6ICIiCiAgICAgIHVzZXJfZG9tYWluX25hbWU6IERlZmF1bHQKICAgICAgdXNlcl9pZDogIiIKICAgICAgdXNlcm5hbWU6IGFkbWluCiAgICBhdXRoX3R5cGU6ICIiCiAgICBjYWNlcnQ6ICIiCiAgICBjZXJ0OiAiIgogICAgY2xvdWQ6ICIiCiAgICBpZGVudGl0eV9hcGlfdmVyc2lvbjogIjMiCiAgICBrZXk6ICIiCiAgICBwcm9maWxlOiAiIgogICAgcmVnaW9uX25hbWU6IHJlZ2lvbk9uZQogICAgcmVnaW9uczogbnVsbAogICAgdmVyaWZ5OiB0cnVlCiAgICB2b2x1bWVfYXBpX3ZlcnNpb246ICIiCg==') clouds: openstack: auth: application_credential_id: \"\" application_credential_name: \"\" application_credential_secret: \"\" auth_url: http://172.16.0.130:5000//v3 default_domain: \"\" domain_id: \"\" domain_name: \"\" password: zP4be2ukhpCkj4zqRfUk8XjQb project_domain_id: \"\" project_domain_name: \"\" project_id: \"\" project_name: admin token: \"\" user_domain_id: \"\" user_domain_name: Default user_id: \"\" username: admin auth_type: \"\" cacert: \"\" cert: \"\" cloud: \"\" identity_api_version: \"3\" key: \"\" profile: \"\" region_name: regionOne regions: null verify: true volume_api_version: \"\" (overcloud) [stack@undercloud-0 clouds]$ cat clouds.yaml clouds: overcloud: auth: auth_url: http://172.16.0.130:5000//v3 username: \"admin\" password: zP4be2ukhpCkj4zqRfUk8XjQb project_name: \"admin\" user_domain_name: \"Default\" region_name: \"regionOne\" interface: \"public\" identity_api_version: 3 The problem is in the generated secret: Error getting a new instance service from the machine (machine/actuator.go 467): Create providerClient err: You must provide exactly one of DomainID or DomainName in a Scope with ProjectName https://github.com/terraform-providers/terraform-provider-openstack/issues/267 Whereas when I check here: https://egallen.com/openshift-42-on-openstack-13-gpu/ This blog article uses project_id, too. clouds: openstack: auth: auth_url: http://192.168.168.54:5000/v3 username: \"admin\" password: XXXXXXXXXXXXXX project_id: XXXXXXXXX project_name: \"admin\" user_domain_name: \"Default\" region_name: \"regionOne\" interface: \"public\" identity_api_version: 3 However, my clouds.yaml file is actually completely correct: [stack@undercloud-0 clouds]$ openstack --os-cloud overcloud token issue +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | expires | 2019-12-20T16:24:40+0000 | | id | gAAAAABd_Oe4sLSSjwQiCPFhVK9PUFBehqVXbj-r96GdFvRieT51YZQUdm5lc5ic5VKYRFPg4jhPat4ZIdyow1QL-vZnxSK8MUAqUMQnc6xjs80JD-ibCNIg1Gac14Idp1CGIutsaUMS-Ms33LDgEw32S2qomv7LRUCLVcEBwrqwYLHXYE2ohyk | | project_id | 1bb14f515f0945a4891fe3fa2372a795 | | user_id | 0d3f5ab158c64c11b57d58c76d9675f0 | +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ I then tried with: clouds: openstack: auth: auth_url: http://172.16.0.130:5000//v3 username: \"admin\" password: zP4be2ukhpCkj4zqRfUk8XjQb project_id: 1bb14f515f0945a4891fe3fa2372a795 project_name: \"admin\" user_domain_name: \"Default\" region_name: \"regionOne\" interface: \"public\" identity_api_version: 3 Note that I made 2 changes: project_id which I got from openstack project list | grep admin ; and I renamed the cloud credential to name openstack . This actually worked: (overcloud) [stack@undercloud-0 ~]$ openstack server list +--------------------------------------+------------------------+--------+-------------------------------------------------------------------------+-------+--------------+ | ID | Name | Status | Networks | Image | Flavor | +--------------------------------------+------------------------+--------+-------------------------------------------------------------------------+-------+--------------+ | 6726f1d5-977f-48da-9554-acf710d459fe | osc-6gzh2-worker-shw2w | ACTIVE | osc-6gzh2-openshift=172.31.0.28 | rhcos | m1.openshift | | 37af5d51-bc08-4002-bb5b-380061d9efbf | osc-6gzh2-master-0 | ACTIVE | osc-6gzh2-openshift=172.31.0.24 | rhcos | m1.openshift | | 4b52d095-e174-47ea-a699-85286f899f67 | osc-6gzh2-master-1 | ACTIVE | osc-6gzh2-openshift=172.31.0.16 | rhcos | m1.openshift | | 73bb2012-d9ff-46e4-8e62-c6d723ca5711 | osc-6gzh2-master-2 | ACTIVE | osc-6gzh2-openshift=172.31.0.15 | rhcos | m1.openshift | | 53e2da7f-1398-45d3-94e3-0008d8e209d9 | rhel-test1 | ACTIVE | private1=2000:192:168:0:f816:3eff:fe13:275, 192.168.0.113, 172.16.0.110 | rhel | m1.small | +--------------------------------------+------------------------+--------+-------------------------------------------------------------------------+-------+--------------+ The cluster deployed: (overcloud) [stack@undercloud-0 clouds]$ ./openshift-install create cluster --dir=install-config/ --log-level=info INFO Consuming \"Install Config\" from target directory INFO Creating infrastructure resources... INFO Waiting up to 30m0s for the Kubernetes API at https://api.osc.redhat.local:6443... INFO API v1.14.6+17b1cc6 up INFO Waiting up to 30m0s for bootstrapping to complete... INFO Destroying the bootstrap resources... INFO Waiting up to 30m0s for the cluster at https://api.osc.redhat.local:6443 to initialize... INFO Waiting up to 10m0s for the openshift-console route to be created... INFO Install complete! INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/home/stack/clouds/install-config/auth/kubeconfig' INFO Access the OpenShift web-console here: https://console-openshift-console.apps.osc.redhat.local INFO Login to the console with user: kubeadmin, password: vFKnm-d43eY-aMMBB-52xcU Now, there was an issue with 2 of the machines: (overcloud) [stack@undercloud-0 clouds]$ oc get machineset -A NAMESPACE NAME DESIRED CURRENT READY AVAILABLE AGE openshift-machine-api osc-6gzh2-worker 3 3 1 1 97m (overcloud) [stack@undercloud-0 clouds]$ oc get machine -A NAMESPACE NAME STATE TYPE REGION ZONE AGE openshift-machine-api osc-6gzh2-master-0 ACTIVE m1.openshift regionOne nova 97m openshift-machine-api osc-6gzh2-master-1 ACTIVE m1.openshift regionOne nova 97m openshift-machine-api osc-6gzh2-master-2 ACTIVE m1.openshift regionOne nova 97m openshift-machine-api osc-6gzh2-worker-5d627 ERROR 96m openshift-machine-api osc-6gzh2-worker-j6j78 ERROR 96m openshift-machine-api osc-6gzh2-worker-shw2w ACTIVE m1.openshift regionOne nova 96m (overcloud) [stack@undercloud-0 clouds]$ oc get machineconfig -A NAME GENERATEDBYCONTROLLER IGNITIONVERSION CREATED 00-master d780d197a9c5848ba786982c0c4aaa7487297046 2.2.0 96m 00-worker d780d197a9c5848ba786982c0c4aaa7487297046 2.2.0 96m 01-master-container-runtime d780d197a9c5848ba786982c0c4aaa7487297046 2.2.0 96m 01-master-kubelet d780d197a9c5848ba786982c0c4aaa7487297046 2.2.0 96m 01-worker-container-runtime d780d197a9c5848ba786982c0c4aaa7487297046 2.2.0 96m 01-worker-kubelet d780d197a9c5848ba786982c0c4aaa7487297046 2.2.0 96m 99-master-51c90b92-2340-11ea-baab-fa163e272495-registries d780d197a9c5848ba786982c0c4aaa7487297046 2.2.0 96m 99-master-ssh 2.2.0 96m 99-worker-51ca001b-2340-11ea-baab-fa163e272495-registries d780d197a9c5848ba786982c0c4aaa7487297046 2.2.0 96m 99-worker-ssh 2.2.0 96m rendered-master-8e766b361a127ddfebde5802f36b90ce d780d197a9c5848ba786982c0c4aaa7487297046 2.2.0 96m rendered-worker-74267811bebab086ede643b9c9b2ba66 d780d197a9c5848ba786982c0c4aaa7487297046 2.2.0 96m And it fails with: (overcloud) [stack@undercloud-0 clouds]$ oc describe machine -n openshift-machine-api osc-6gzh2-worker-5d627 | tail Openshift Cluster ID: osc-6gzh2 Tags: openshiftClusterID=osc-6gzh2 Trunk: true User Data Secret: Name: worker-user-data Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedCreate 2m30s (x23 over 95m) openstack_controller CreateError (overcloud) [stack@undercloud-0 clouds]$ vercloud) [stack@undercloud-0 clouds]$ oc get events -n openshift-machine-api LAST SEEN TYPE REASON OBJECT MESSAGE 95m Normal Scheduled pod/cluster-autoscaler-operator-65dfcc75bb-gv8mg Successfully assigned openshift-machine-api/cluster-autoscaler-operator-65dfcc75bb-gv8mg to osc-6gzh2-master-1 95m Warning FailedMount pod/cluster-autoscaler-operator-65dfcc75bb-gv8mg MountVolume.SetUp failed for volume \"cert\" : couldn't propagate object cache: timed out waiting for the condition 95m Warning FailedMount pod/cluster-autoscaler-operator-65dfcc75bb-gv8mg MountVolume.SetUp failed for volume \"cluster-autoscaler-operator-token-gtb48\" : couldn't propagate object cache: timed out waiting for the condition 95m Warning FailedMount pod/cluster-autoscaler-operator-65dfcc75bb-gv8mg MountVolume.SetUp failed for volume \"ca-cert\" : couldn't propagate object cache: timed out waiting for the condition 94m Normal Pulling pod/cluster-autoscaler-operator-65dfcc75bb-gv8mg Pulling image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e6ea0a063874ff9169ef1e0e58c6399a42e163c49321018fa34c838faec99cb4\" 94m Normal Pulled pod/cluster-autoscaler-operator-65dfcc75bb-gv8mg Successfully pulled image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e6ea0a063874ff9169ef1e0e58c6399a42e163c49321018fa34c838faec99cb4\" 94m Normal Created pod/cluster-autoscaler-operator-65dfcc75bb-gv8mg Created container cluster-autoscaler-operator 94m Normal Started pod/cluster-autoscaler-operator-65dfcc75bb-gv8mg Started container cluster-autoscaler-operator 95m Normal SuccessfulCreate replicaset/cluster-autoscaler-operator-65dfcc75bb Created pod: cluster-autoscaler-operator-65dfcc75bb-gv8mg 94m Normal LeaderElection configmap/cluster-autoscaler-operator-leader cluster-autoscaler-operator-65dfcc75bb-gv8mg_40da4183-2341-11ea-8630-0a58ac150025 became leader 95m Normal ScalingReplicaSet deployment/cluster-autoscaler-operator Scaled up replica set cluster-autoscaler-operator-65dfcc75bb to 1 101m Normal Scheduled pod/machine-api-controllers-f64b7f7b8-xb7qn Successfully assigned openshift-machine-api/machine-api-controllers-f64b7f7b8-xb7qn to osc-6gzh2-master-2 101m Normal Pulling pod/machine-api-controllers-f64b7f7b8-xb7qn Pulling image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fad6250c0e717515d4caf35d6e6e006f7154e2a7bd9d4fec73f0540e155e3119\" 101m Normal Pulled pod/machine-api-controllers-f64b7f7b8-xb7qn Successfully pulled image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fad6250c0e717515d4caf35d6e6e006f7154e2a7bd9d4fec73f0540e155e3119\" 101m Normal Created pod/machine-api-controllers-f64b7f7b8-xb7qn Created container controller-manager 101m Normal Started pod/machine-api-controllers-f64b7f7b8-xb7qn Started container controller-manager 101m Normal Pulled pod/machine-api-controllers-f64b7f7b8-xb7qn Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fad6250c0e717515d4caf35d6e6e006f7154e2a7bd9d4fec73f0540e155e3119\" already present on machine 101m Normal Created pod/machine-api-controllers-f64b7f7b8-xb7qn Created container machine-controller 101m Normal Started pod/machine-api-controllers-f64b7f7b8-xb7qn Started container machine-controller 101m Normal Pulled pod/machine-api-controllers-f64b7f7b8-xb7qn Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8f1a9c01710f09ad1d1c105cbc4f4ff88d7c0f5916a628f9414d3a7905fbced8\" already present on machine 101m Normal Created pod/machine-api-controllers-f64b7f7b8-xb7qn Created container nodelink-controller 101m Normal Started pod/machine-api-controllers-f64b7f7b8-xb7qn Started container nodelink-controller 101m Normal SuccessfulCreate replicaset/machine-api-controllers-f64b7f7b8 Created pod: machine-api-controllers-f64b7f7b8-xb7qn 101m Normal ScalingReplicaSet deployment/machine-api-controllers Scaled up replica set machine-api-controllers-f64b7f7b8 to 1 101m Warning FailedScheduling pod/machine-api-operator-655d94c8fd-6vrqg 0/3 nodes are available: 3 node(s) had taints that the pod didn't tolerate. 101m Normal Scheduled pod/machine-api-operator-655d94c8fd-6vrqg Successfully assigned openshift-machine-api/machine-api-operator-655d94c8fd-6vrqg to osc-6gzh2-master-2 101m Normal Pulling pod/machine-api-operator-655d94c8fd-6vrqg Pulling image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8f1a9c01710f09ad1d1c105cbc4f4ff88d7c0f5916a628f9414d3a7905fbced8\" 101m Normal Pulled pod/machine-api-operator-655d94c8fd-6vrqg Successfully pulled image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8f1a9c01710f09ad1d1c105cbc4f4ff88d7c0f5916a628f9414d3a7905fbced8\" 101m Normal Created pod/machine-api-operator-655d94c8fd-6vrqg Created container machine-api-operator 101m Normal Started pod/machine-api-operator-655d94c8fd-6vrqg Started container machine-api-operator 102m Normal SuccessfulCreate replicaset/machine-api-operator-655d94c8fd Created pod: machine-api-operator-655d94c8fd-6vrqg 102m Normal ScalingReplicaSet deployment/machine-api-operator Scaled up replica set machine-api-operator-655d94c8fd to 1 6m55s Warning FailedCreate machine/osc-6gzh2-worker-5d627 CreateError 7m29s Warning FailedCreate machine/osc-6gzh2-worker-j6j78 CreateError 98m Warning FailedCreate machine/osc-6gzh2-worker-shw2w CreateError 97m Normal Created machine/osc-6gzh2-worker-shw2w Created Machine osc-6gzh2-worker-shw2w I connected to master 2 via master 1: [stack@undercloud-0 ~]$ eval $(ssh-agent) [stack@undercloud-0 ~]$ ssh-add ~/.ssh/id_rsa [stack@undercloud-0 ~]$ ssh core@api.osc.redhat.local -A [stack@undercloud-0 ~]$ ssh core@api.osc.redhat.local -A Warning: Permanently added 'api.osc.redhat.local,172.16.0.108' (ECDSA) to the list of known hosts. Red Hat Enterprise Linux CoreOS 42.81.20191203.0 WARNING: Direct SSH access to machines is not recommended. --- Last login: Fri Dec 20 17:27:11 2019 from 172.16.0.65 [core@osc-6gzh2-master-1 ~]$ ssh core@osc-6gzh2-master-0 Red Hat Enterprise Linux CoreOS 42.81.20191203.0 WARNING: Direct SSH access to machines is not recommended. --- Last login: Fri Dec 20 17:27:13 2019 from 172.31.0.16 sud[core@osc-6gzh2-master-0 ~]$ sudo -i [root@osc-6gzh2-master-0 ~]# And then got: [root@osc-6gzh2-master-2 ~]# grep osc-6gzh2-worker-5d627 /var/log -R /var/log/pods/openshift-machine-api_machine-api-controllers-f64b7f7b8-xb7qn_4d35a879-2340-11ea-baab-fa163e272495/machine-controller/0.log:2019-12-20T15:49:41.304368389+00:00 stderr F I1220 15:49:41.304339 1 controller.go:133] Reconciling Machine \"osc-6gzh2-worker-5d627\" /var/log/pods/openshift-machine-api_machine-api-controllers-f64b7f7b8-xb7qn_4d35a879-2340-11ea-baab-fa163e272495/machine-controller/0.log:2019-12-20T15:49:41.304368389+00:00 stderr F I1220 15:49:41.304356 1 controller.go:304] Machine \"osc-6gzh2-worker-5d627\" in namespace \"openshift-machine-api\" doesn't specify \"cluster.k8s.io/cluster-name\" label, assuming nil cluster /var/log/pods/openshift-machine-api_machine-api-controllers-f64b7f7b8-xb7qn_4d35a879-2340-11ea-baab-fa163e272495/machine-controller/0.log:2019-12-20T15:50:37.383723384+00:00 stderr F I1220 15:50:37.383397 1 controller.go:133] Reconciling Machine \"osc-6gzh2-worker-5d627\" /var/log/pods/openshift-machine-api_machine-api-controllers-f64b7f7b8-xb7qn_4d35a879-2340-11ea-baab-fa163e272495/machine-controller/0.log:2019-12-20T15:50:37.383835414+00:00 stderr F I1220 15:50:37.383814 1 controller.go:304] Machine \"osc-6gzh2-worker-5d627\" in namespace \"openshift-machine-api\" doesn't specify \"cluster.k8s.io/cluster-name\" label, assuming nil cluster /var/log/pods/openshift-machine-api_machine-api-controllers-f64b7f7b8-xb7qn_4d35a879-2340-11ea-baab-fa163e272495/machine-controller/0.log:2019-12-20T15:50:38.552995792+00:00 stderr F I1220 15:50:38.552954 1 controller.go:253] Reconciling machine object osc-6gzh2-worker-5d627 triggers idempotent create. /var/log/pods/openshift-machine-api_machine-api-controllers-f64b7f7b8-xb7qn_4d35a879-2340-11ea-baab-fa163e272495/machine-controller/0.log:2019-12-20T15:50:52.700244622+00:00 stderr F E1220 15:50:52.698880 1 actuator.go:470] Machine error osc-6gzh2-worker-5d627: error creating Openstack instance: Create new server err: Request forbidden: [POST http://172.16.0.130:8774/v2.1/servers], error message: {\"forbidden\": {\"message\": \"Quota exceeded for ram: Requested 32768, but already used 132096 of 153600 ram\", \"code\": 403}} /var/log/pods/openshift-machine-api_machine-api-controllers-f64b7f7b8-xb7qn_4d35a879-2340-11ea-baab-fa163e272495/machine-controller/0.log:2019-12-20T15:50:52.700244622+00:00 stderr F W1220 15:50:52.698926 1 controller.go:255] Failed to create machine \"osc-6gzh2-worker-5d627\": error creating Openstack instance: Create new server err: Request forbidden: [POST http://172.16.0.130:8774/v2.1/servers], error message: {\"forbidden\": {\"message\": \"Quota exceeded for ram: Requested 32768, but already used 132096 of 153600 ram\", \"code\": 403}} /var/log/pods/openshift-machine-api_machine-api-controllers-f64b7f7b8-xb7qn_4d35a879-2340-11ea-baab-fa163e272495/machine-controller/0.log:2019-12-20T15:51:38.126016932+00:00 stderr F I1220 15:51:38.125931 1 controller.go:133] Reconciling Machine \"osc-6gzh2-worker-5d627\" /var/log/pods/openshift-machine-api_machine-api-controllers-f64b7f7b8-xb7qn_4d35a879-2340-11ea-baab-fa163e272495/machine-controller/0.log:2019-12-20T15:51:38.126016932+00:00 stderr F I1220 15:51:38.125971 1 controller.go:304] Machine \"osc-6gzh2-worker-5d627\" in namespace \"openshift-machine-api\" doesn't specify \"cluster.k8s.io/cluster-name\" label, assuming nil cluster /var/log/pods/openshift-machine-api_machine-api-controllers-f64b7f7b8-xb7qn_4d35a879-2340-11ea-baab-fa163e272495/machine-controller/0.log:2019-12-20T15:51:39.076618133+00:00 stderr F I1220 15:51:39.076540 1 controller.go:253] Reconciling machine object osc-6gzh2-worker-5d627 triggers idempotent create. /var/log/pods/openshift-machine-api_machine-api-controllers-f64b7f7b8-xb7qn_4d35a879-2340-11ea-baab-fa163e272495/machine-controller/0.log:2019-12-20T15:51:48.625264976+00:00 stderr F E1220 15:51:48.625215 1 actuator.go:470] Machine error osc-6gzh2-worker-5d627: error creating Openstack instance: Create new server err: Request forbidden: [POST http://172.16.0.130:8774/v2.1/servers], error message: {\"forbidden\": {\"message\": \"Quota exceeded for ram: Requested 32768, but already used 132096 of 153600 ram\", \"code\": 403}} /var/log/pods/openshift-machine-api_machine-api-controllers-f64b7f7b8-xb7qn_4d35a879-2340-11ea-baab-fa163e272495/machine-controller/0.log:2019-12-20T15:51:48.625264976+00:00 stderr F W1220 15:51:48.625239 1 controller.go:255] Failed to create machine \"osc-6gzh2-worker-5d627\": error creating Openstack instance: Create new server err: Request forbidden: [POST http://172.16.0.130:8774/v2.1/servers], error message: {\"forbidden\": {\"message\": \"Quota exceeded for ram: Requested 32768, but already used 132096 of 153600 ram\", \"code\": 403}}","title":"Troubleshooting OpenShift on OpenStack worker creation"},{"location":"openshift/troubleshooting_openshift_on_openstack_worker_creation/#troubleshooting-openshift-on-openstack-worker-creation","text":"See https://bugzilla.redhat.com/show_bug.cgi?id=1785705 for the bugzilla. The following are steps which I executed in my lab to troubleshoot issues with worker creation for OpenShift on OpenStack. (overcloud) [stack@undercloud-0 clouds]$ oc describe machine -n openshift-machine-api osc-c5r5c-worker-bgt9b Name: osc-c5r5c-worker-bgt9b Namespace: openshift-machine-api Labels: machine.openshift.io/cluster-api-cluster=osc-c5r5c machine.openshift.io/cluster-api-machine-role=worker machine.openshift.io/cluster-api-machine-type=worker machine.openshift.io/cluster-api-machineset=osc-c5r5c-worker Annotations: <none> API Version: machine.openshift.io/v1beta1 Kind: Machine Metadata: Creation Timestamp: 2019-12-20T10:23:14Z Finalizers: machine.machine.openshift.io Generate Name: osc-c5r5c-worker- Generation: 1 Owner References: API Version: machine.openshift.io/v1beta1 Block Owner Deletion: true Controller: true Kind: MachineSet Name: osc-c5r5c-worker UID: 9077b6df-2312-11ea-9b6c-fa163e431263 Resource Version: 3455 Self Link: /apis/machine.openshift.io/v1beta1/namespaces/openshift-machine-api/machines/osc-c5r5c-worker-bgt9b UID: bb0af782-2312-11ea-9b6c-fa163e431263 Spec: Metadata: Creation Timestamp: <nil> Provider Spec: Value: API Version: openstackproviderconfig.openshift.io/v1alpha1 Cloud Name: openstack Clouds Secret: Name: openstack-cloud-credentials Namespace: openshift-machine-api Flavor: m1.openshift Image: rhcos Kind: OpenstackProviderSpec Metadata: Creation Timestamp: <nil> Networks: Filter: Subnets: Filter: Name: osc-c5r5c-nodes Tags: openshiftClusterID=osc-c5r5c Security Groups: Filter: Name: osc-c5r5c-worker Server Metadata: Name: osc-c5r5c-worker Openshift Cluster ID: osc-c5r5c Tags: openshiftClusterID=osc-c5r5c Trunk: true User Data Secret: Name: worker-user-data Events: <none> (overcloud) [stack@undercloud-0 clouds]$ oc get machine -n openshift-machine-api NAME STATE TYPE REGION ZONE AGE osc-c5r5c-master-0 4h52m osc-c5r5c-master-1 4h52m osc-c5r5c-master-2 4h52m osc-c5r5c-worker-bgt9b 4h51m osc-c5r5c-worker-qphk7 4h51m osc-c5r5c-worker-vs85h 4h51m (overcloud) [stack@undercloud-0 clouds]$ oc get machineset -n openshift-machine-api NAME DESIRED CURRENT READY AVAILABLE AGE osc-c5r5c-worker 3 3 4h54m (overcloud) [stack@undercloud-0 clouds]$ (overcloud) [stack@undercloud-0 clouds]$ kubectl get machineset -n openshift-machine-api osc-c5r5c-worker -o yaml apiVersion: machine.openshift.io/v1beta1 kind: MachineSet metadata: creationTimestamp: \"2019-12-20T10:22:02Z\" generation: 1 labels: machine.openshift.io/cluster-api-cluster: osc-c5r5c machine.openshift.io/cluster-api-machine-role: worker machine.openshift.io/cluster-api-machine-type: worker name: osc-c5r5c-worker namespace: openshift-machine-api resourceVersion: \"3448\" selfLink: /apis/machine.openshift.io/v1beta1/namespaces/openshift-machine-api/machinesets/osc-c5r5c-worker uid: 9077b6df-2312-11ea-9b6c-fa163e431263 spec: replicas: 3 selector: matchLabels: machine.openshift.io/cluster-api-cluster: osc-c5r5c machine.openshift.io/cluster-api-machineset: osc-c5r5c-worker template: metadata: creationTimestamp: null labels: machine.openshift.io/cluster-api-cluster: osc-c5r5c machine.openshift.io/cluster-api-machine-role: worker machine.openshift.io/cluster-api-machine-type: worker machine.openshift.io/cluster-api-machineset: osc-c5r5c-worker spec: metadata: creationTimestamp: null providerSpec: value: apiVersion: openstackproviderconfig.openshift.io/v1alpha1 cloudName: openstack cloudsSecret: name: openstack-cloud-credentials namespace: openshift-machine-api flavor: m1.openshift image: rhcos kind: OpenstackProviderSpec metadata: creationTimestamp: null networks: - filter: {} subnets: - filter: name: osc-c5r5c-nodes tags: openshiftClusterID=osc-c5r5c securityGroups: - filter: {} name: osc-c5r5c-worker serverMetadata: Name: osc-c5r5c-worker openshiftClusterID: osc-c5r5c tags: - openshiftClusterID=osc-c5r5c trunk: true userDataSecret: name: worker-user-data status: fullyLabeledReplicas: 3 observedGeneration: 1 replicas: 3 /var/log/pods/openshift-machine-api_machine-api-controllers-f64b7f7b8-tm7qc_b1f05af2-2312-11ea-9b6c-fa163e431263/machine-controller/0.log (...) 2019-12-20T10:23:14.635223358+00:00 stderr F E1220 10:23:14.635191 1 controller.go:239] Failed to check if machine \"osc-c5r5c-worker-bgt9b\" exists: Error checking if instance exists (machine/actuator.go 346): 2019-12-20T10:23:14.635223358+00:00 stderr F Error getting a new instance service from the machine (machine/actuator.go 467): Create providerClient err: You must provide exactly one of DomainID or DomainName in a Scope with ProjectName 2019-12-20T10:23:15.636135623+00:00 stderr F I1220 10:23:15.635874 1 controller.go:133] Reconciling Machine \"osc-c5r5c-master-0\" 2019-12-20T10:23:15.636215058+00:00 stderr F I1220 10:23:15.636192 1 controller.go:304] Machine \"osc-c5r5c-master-0\" in namespace \"openshift-machine-api\" doesn't specify \"cluster.k8s.io/cluster-name\" label, assuming nil cluster 2019-12-20T10:23:15.641348382+00:00 stderr F E1220 10:23:15.641317 1 controller.go:239] Failed to check if machine \"osc-c5r5c-master-0\" exists: Error checking if instance exists (machine/actuator.go 346): 2019-12-20T10:23:15.641348382+00:00 stderr F Error getting a new instance service from the machine (machine/actuator.go 467): Create providerClient err: You must provide exactly one of DomainID or DomainName in a Scope with ProjectName 2019-12-20T10:23:16.641614350+00:00 stderr F I1220 10:23:16.641562 1 controller.go:133] Reconciling Machine \"osc-c5r5c-master-1\" 2019-12-20T10:23:16.641614350+00:00 stderr F I1220 10:23:16.641591 1 controller.go:304] Machine \"osc-c5r5c-master-1\" in namespace \"openshift-machine-api\" doesn't specify \"cluster.k8s.io/cluster-name\" label, assuming nil cluster 2019-12-20T10:23:16.647323379+00:00 stderr F E1220 10:23:16.647295 1 controller.go:239] Failed to check if machine \"osc-c5r5c-master-1\" exists: Error checking if instance exists (machine/actuator.go 346): 2019-12-20T10:23:16.647323379+00:00 stderr F Error getting a new instance service from the machine (machine/actuator.go 467): Create providerClient err: You must provide exactly one of DomainID or DomainName in a Scope with ProjectName 2019-12-20T10:23:17.647630531+00:00 stderr F I1220 10:23:17.647587 1 controller.go:133] Reconciling Machine \"osc-c5r5c-master-2\" 2019-12-20T10:23:17.647695173+00:00 stderr F I1220 10:23:17.647677 1 controller.go:304] Machine \"osc-c5r5c-master-2\" in namespace \"openshift-machine-api\" doesn't specify \"cluster.k8s.io/cluster-name\" label, assuming nil cluster 2019-12-20T10:23:17.652613228+00:00 stderr F E1220 10:23:17.652551 1 controller.go:239] Failed to check if machine \"osc-c5r5c-master-2\" exists: Error checking if instance exists (machine/actuator.go 346): 2019-12-20T10:23:17.652613228+00:00 stderr F Error getting a new instance service from the machine (machine/actuator.go 467): Create providerClient err: You must provide exactly one of DomainID or DomainName in a Scope with ProjectName 2019-12-20T10:23:18.652945792+00:00 stderr F I1220 10:23:18.652884 1 controller.go:133] Reconciling Machine \"osc-c5r5c-worker-qphk7\" Looking at the secret: (overcloud) [stack@undercloud-0 clouds]$ kubectl get secrets -n openshift-machine-api openstack-cloud-credentials -o yaml apiVersion: v1 data: clouds.yaml: Y2xvdWRzOgogIG9wZW5zdGFjazoKICAgIGF1dGg6CiAgICAgIGFwcGxpY2F0aW9uX2NyZWRlbnRpYWxfaWQ6ICIiCiAgICAgIGFwcGxpY2F0aW9uX2NyZWRlbnRpYWxfbmFtZTogIiIKICAgICAgYXBwbGljYXRpb25fY3JlZGVudGlhbF9zZWNyZXQ6ICIiCiAgICAgIGF1dGhfdXJsOiBodHRwOi8vMTcyLjE2LjAuMTMwOjUwMDAvL3YzCiAgICAgIGRlZmF1bHRfZG9tYWluOiAiIgogICAgICBkb21haW5faWQ6ICIiCiAgICAgIGRvbWFpbl9uYW1lOiAiIgogICAgICBwYXNzd29yZDogelA0YmUydWtocENrajR6cVJmVWs4WGpRYgogICAgICBwcm9qZWN0X2RvbWFpbl9pZDogIiIKICAgICAgcHJvamVjdF9kb21haW5fbmFtZTogIiIKICAgICAgcHJvamVjdF9pZDogIiIKICAgICAgcHJvamVjdF9uYW1lOiBhZG1pbgogICAgICB0b2tlbjogIiIKICAgICAgdXNlcl9kb21haW5faWQ6ICIiCiAgICAgIHVzZXJfZG9tYWluX25hbWU6IERlZmF1bHQKICAgICAgdXNlcl9pZDogIiIKICAgICAgdXNlcm5hbWU6IGFkbWluCiAgICBhdXRoX3R5cGU6ICIiCiAgICBjYWNlcnQ6ICIiCiAgICBjZXJ0OiAiIgogICAgY2xvdWQ6ICIiCiAgICBpZGVudGl0eV9hcGlfdmVyc2lvbjogIjMiCiAgICBrZXk6ICIiCiAgICBwcm9maWxlOiAiIgogICAgcmVnaW9uX25hbWU6IHJlZ2lvbk9uZQogICAgcmVnaW9uczogbnVsbAogICAgdmVyaWZ5OiB0cnVlCiAgICB2b2x1bWVfYXBpX3ZlcnNpb246ICIiCg== kind: Secret metadata: annotations: cloudcredential.openshift.io/credentials-request: openshift-cloud-credential-operator/openshift-machine-api-openstack creationTimestamp: \"2019-12-20T10:23:00Z\" name: openstack-cloud-credentials namespace: openshift-machine-api resourceVersion: \"2525\" selfLink: /api/v1/namespaces/openshift-machine-api/secrets/openstack-cloud-credentials uid: b2bded52-2312-11ea-9b6c-fa163e431263 type: Opaque (overcloud) [stack@undercloud-0 clouds]$ base64 -d <(echo 'Y2xvdWRzOgogIG9wZW5zdGFjazoKICAgIGF1dGg6CiAgICAgIGFwcGxpY2F0aW9uX2NyZWRlbnRpYWxfaWQ6ICIiCiAgICAgIGFwcGxpY2F0aW9uX2NyZWRlbnRpYWxfbmFtZTogIiIKICAgICAgYXBwbGljYXRpb25fY3JlZGVudGlhbF9zZWNyZXQ6ICIiCiAgICAgIGF1dGhfdXJsOiBodHRwOi8vMTcyLjE2LjAuMTMwOjUwMDAvL3YzCiAgICAgIGRlZmF1bHRfZG9tYWluOiAiIgogICAgICBkb21haW5faWQ6ICIiCiAgICAgIGRvbWFpbl9uYW1lOiAiIgogICAgICBwYXNzd29yZDogelA0YmUydWtocENrajR6cVJmVWs4WGpRYgogICAgICBwcm9qZWN0X2RvbWFpbl9pZDogIiIKICAgICAgcHJvamVjdF9kb21haW5fbmFtZTogIiIKICAgICAgcHJvamVjdF9pZDogIiIKICAgICAgcHJvamVjdF9uYW1lOiBhZG1pbgogICAgICB0b2tlbjogIiIKICAgICAgdXNlcl9kb21haW5faWQ6ICIiCiAgICAgIHVzZXJfZG9tYWluX25hbWU6IERlZmF1bHQKICAgICAgdXNlcl9pZDogIiIKICAgICAgdXNlcm5hbWU6IGFkbWluCiAgICBhdXRoX3R5cGU6ICIiCiAgICBjYWNlcnQ6ICIiCiAgICBjZXJ0OiAiIgogICAgY2xvdWQ6ICIiCiAgICBpZGVudGl0eV9hcGlfdmVyc2lvbjogIjMiCiAgICBrZXk6ICIiCiAgICBwcm9maWxlOiAiIgogICAgcmVnaW9uX25hbWU6IHJlZ2lvbk9uZQogICAgcmVnaW9uczogbnVsbAogICAgdmVyaWZ5OiB0cnVlCiAgICB2b2x1bWVfYXBpX3ZlcnNpb246ICIiCg==') clouds: openstack: auth: application_credential_id: \"\" application_credential_name: \"\" application_credential_secret: \"\" auth_url: http://172.16.0.130:5000//v3 default_domain: \"\" domain_id: \"\" domain_name: \"\" password: zP4be2ukhpCkj4zqRfUk8XjQb project_domain_id: \"\" project_domain_name: \"\" project_id: \"\" project_name: admin token: \"\" user_domain_id: \"\" user_domain_name: Default user_id: \"\" username: admin auth_type: \"\" cacert: \"\" cert: \"\" cloud: \"\" identity_api_version: \"3\" key: \"\" profile: \"\" region_name: regionOne regions: null verify: true volume_api_version: \"\" (overcloud) [stack@undercloud-0 clouds]$ cat clouds.yaml clouds: overcloud: auth: auth_url: http://172.16.0.130:5000//v3 username: \"admin\" password: zP4be2ukhpCkj4zqRfUk8XjQb project_name: \"admin\" user_domain_name: \"Default\" region_name: \"regionOne\" interface: \"public\" identity_api_version: 3 The problem is in the generated secret: Error getting a new instance service from the machine (machine/actuator.go 467): Create providerClient err: You must provide exactly one of DomainID or DomainName in a Scope with ProjectName https://github.com/terraform-providers/terraform-provider-openstack/issues/267 Whereas when I check here: https://egallen.com/openshift-42-on-openstack-13-gpu/ This blog article uses project_id, too. clouds: openstack: auth: auth_url: http://192.168.168.54:5000/v3 username: \"admin\" password: XXXXXXXXXXXXXX project_id: XXXXXXXXX project_name: \"admin\" user_domain_name: \"Default\" region_name: \"regionOne\" interface: \"public\" identity_api_version: 3 However, my clouds.yaml file is actually completely correct: [stack@undercloud-0 clouds]$ openstack --os-cloud overcloud token issue +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | expires | 2019-12-20T16:24:40+0000 | | id | gAAAAABd_Oe4sLSSjwQiCPFhVK9PUFBehqVXbj-r96GdFvRieT51YZQUdm5lc5ic5VKYRFPg4jhPat4ZIdyow1QL-vZnxSK8MUAqUMQnc6xjs80JD-ibCNIg1Gac14Idp1CGIutsaUMS-Ms33LDgEw32S2qomv7LRUCLVcEBwrqwYLHXYE2ohyk | | project_id | 1bb14f515f0945a4891fe3fa2372a795 | | user_id | 0d3f5ab158c64c11b57d58c76d9675f0 | +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ I then tried with: clouds: openstack: auth: auth_url: http://172.16.0.130:5000//v3 username: \"admin\" password: zP4be2ukhpCkj4zqRfUk8XjQb project_id: 1bb14f515f0945a4891fe3fa2372a795 project_name: \"admin\" user_domain_name: \"Default\" region_name: \"regionOne\" interface: \"public\" identity_api_version: 3 Note that I made 2 changes: project_id which I got from openstack project list | grep admin ; and I renamed the cloud credential to name openstack . This actually worked: (overcloud) [stack@undercloud-0 ~]$ openstack server list +--------------------------------------+------------------------+--------+-------------------------------------------------------------------------+-------+--------------+ | ID | Name | Status | Networks | Image | Flavor | +--------------------------------------+------------------------+--------+-------------------------------------------------------------------------+-------+--------------+ | 6726f1d5-977f-48da-9554-acf710d459fe | osc-6gzh2-worker-shw2w | ACTIVE | osc-6gzh2-openshift=172.31.0.28 | rhcos | m1.openshift | | 37af5d51-bc08-4002-bb5b-380061d9efbf | osc-6gzh2-master-0 | ACTIVE | osc-6gzh2-openshift=172.31.0.24 | rhcos | m1.openshift | | 4b52d095-e174-47ea-a699-85286f899f67 | osc-6gzh2-master-1 | ACTIVE | osc-6gzh2-openshift=172.31.0.16 | rhcos | m1.openshift | | 73bb2012-d9ff-46e4-8e62-c6d723ca5711 | osc-6gzh2-master-2 | ACTIVE | osc-6gzh2-openshift=172.31.0.15 | rhcos | m1.openshift | | 53e2da7f-1398-45d3-94e3-0008d8e209d9 | rhel-test1 | ACTIVE | private1=2000:192:168:0:f816:3eff:fe13:275, 192.168.0.113, 172.16.0.110 | rhel | m1.small | +--------------------------------------+------------------------+--------+-------------------------------------------------------------------------+-------+--------------+ The cluster deployed: (overcloud) [stack@undercloud-0 clouds]$ ./openshift-install create cluster --dir=install-config/ --log-level=info INFO Consuming \"Install Config\" from target directory INFO Creating infrastructure resources... INFO Waiting up to 30m0s for the Kubernetes API at https://api.osc.redhat.local:6443... INFO API v1.14.6+17b1cc6 up INFO Waiting up to 30m0s for bootstrapping to complete... INFO Destroying the bootstrap resources... INFO Waiting up to 30m0s for the cluster at https://api.osc.redhat.local:6443 to initialize... INFO Waiting up to 10m0s for the openshift-console route to be created... INFO Install complete! INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/home/stack/clouds/install-config/auth/kubeconfig' INFO Access the OpenShift web-console here: https://console-openshift-console.apps.osc.redhat.local INFO Login to the console with user: kubeadmin, password: vFKnm-d43eY-aMMBB-52xcU Now, there was an issue with 2 of the machines: (overcloud) [stack@undercloud-0 clouds]$ oc get machineset -A NAMESPACE NAME DESIRED CURRENT READY AVAILABLE AGE openshift-machine-api osc-6gzh2-worker 3 3 1 1 97m (overcloud) [stack@undercloud-0 clouds]$ oc get machine -A NAMESPACE NAME STATE TYPE REGION ZONE AGE openshift-machine-api osc-6gzh2-master-0 ACTIVE m1.openshift regionOne nova 97m openshift-machine-api osc-6gzh2-master-1 ACTIVE m1.openshift regionOne nova 97m openshift-machine-api osc-6gzh2-master-2 ACTIVE m1.openshift regionOne nova 97m openshift-machine-api osc-6gzh2-worker-5d627 ERROR 96m openshift-machine-api osc-6gzh2-worker-j6j78 ERROR 96m openshift-machine-api osc-6gzh2-worker-shw2w ACTIVE m1.openshift regionOne nova 96m (overcloud) [stack@undercloud-0 clouds]$ oc get machineconfig -A NAME GENERATEDBYCONTROLLER IGNITIONVERSION CREATED 00-master d780d197a9c5848ba786982c0c4aaa7487297046 2.2.0 96m 00-worker d780d197a9c5848ba786982c0c4aaa7487297046 2.2.0 96m 01-master-container-runtime d780d197a9c5848ba786982c0c4aaa7487297046 2.2.0 96m 01-master-kubelet d780d197a9c5848ba786982c0c4aaa7487297046 2.2.0 96m 01-worker-container-runtime d780d197a9c5848ba786982c0c4aaa7487297046 2.2.0 96m 01-worker-kubelet d780d197a9c5848ba786982c0c4aaa7487297046 2.2.0 96m 99-master-51c90b92-2340-11ea-baab-fa163e272495-registries d780d197a9c5848ba786982c0c4aaa7487297046 2.2.0 96m 99-master-ssh 2.2.0 96m 99-worker-51ca001b-2340-11ea-baab-fa163e272495-registries d780d197a9c5848ba786982c0c4aaa7487297046 2.2.0 96m 99-worker-ssh 2.2.0 96m rendered-master-8e766b361a127ddfebde5802f36b90ce d780d197a9c5848ba786982c0c4aaa7487297046 2.2.0 96m rendered-worker-74267811bebab086ede643b9c9b2ba66 d780d197a9c5848ba786982c0c4aaa7487297046 2.2.0 96m And it fails with: (overcloud) [stack@undercloud-0 clouds]$ oc describe machine -n openshift-machine-api osc-6gzh2-worker-5d627 | tail Openshift Cluster ID: osc-6gzh2 Tags: openshiftClusterID=osc-6gzh2 Trunk: true User Data Secret: Name: worker-user-data Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedCreate 2m30s (x23 over 95m) openstack_controller CreateError (overcloud) [stack@undercloud-0 clouds]$ vercloud) [stack@undercloud-0 clouds]$ oc get events -n openshift-machine-api LAST SEEN TYPE REASON OBJECT MESSAGE 95m Normal Scheduled pod/cluster-autoscaler-operator-65dfcc75bb-gv8mg Successfully assigned openshift-machine-api/cluster-autoscaler-operator-65dfcc75bb-gv8mg to osc-6gzh2-master-1 95m Warning FailedMount pod/cluster-autoscaler-operator-65dfcc75bb-gv8mg MountVolume.SetUp failed for volume \"cert\" : couldn't propagate object cache: timed out waiting for the condition 95m Warning FailedMount pod/cluster-autoscaler-operator-65dfcc75bb-gv8mg MountVolume.SetUp failed for volume \"cluster-autoscaler-operator-token-gtb48\" : couldn't propagate object cache: timed out waiting for the condition 95m Warning FailedMount pod/cluster-autoscaler-operator-65dfcc75bb-gv8mg MountVolume.SetUp failed for volume \"ca-cert\" : couldn't propagate object cache: timed out waiting for the condition 94m Normal Pulling pod/cluster-autoscaler-operator-65dfcc75bb-gv8mg Pulling image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e6ea0a063874ff9169ef1e0e58c6399a42e163c49321018fa34c838faec99cb4\" 94m Normal Pulled pod/cluster-autoscaler-operator-65dfcc75bb-gv8mg Successfully pulled image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e6ea0a063874ff9169ef1e0e58c6399a42e163c49321018fa34c838faec99cb4\" 94m Normal Created pod/cluster-autoscaler-operator-65dfcc75bb-gv8mg Created container cluster-autoscaler-operator 94m Normal Started pod/cluster-autoscaler-operator-65dfcc75bb-gv8mg Started container cluster-autoscaler-operator 95m Normal SuccessfulCreate replicaset/cluster-autoscaler-operator-65dfcc75bb Created pod: cluster-autoscaler-operator-65dfcc75bb-gv8mg 94m Normal LeaderElection configmap/cluster-autoscaler-operator-leader cluster-autoscaler-operator-65dfcc75bb-gv8mg_40da4183-2341-11ea-8630-0a58ac150025 became leader 95m Normal ScalingReplicaSet deployment/cluster-autoscaler-operator Scaled up replica set cluster-autoscaler-operator-65dfcc75bb to 1 101m Normal Scheduled pod/machine-api-controllers-f64b7f7b8-xb7qn Successfully assigned openshift-machine-api/machine-api-controllers-f64b7f7b8-xb7qn to osc-6gzh2-master-2 101m Normal Pulling pod/machine-api-controllers-f64b7f7b8-xb7qn Pulling image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fad6250c0e717515d4caf35d6e6e006f7154e2a7bd9d4fec73f0540e155e3119\" 101m Normal Pulled pod/machine-api-controllers-f64b7f7b8-xb7qn Successfully pulled image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fad6250c0e717515d4caf35d6e6e006f7154e2a7bd9d4fec73f0540e155e3119\" 101m Normal Created pod/machine-api-controllers-f64b7f7b8-xb7qn Created container controller-manager 101m Normal Started pod/machine-api-controllers-f64b7f7b8-xb7qn Started container controller-manager 101m Normal Pulled pod/machine-api-controllers-f64b7f7b8-xb7qn Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:fad6250c0e717515d4caf35d6e6e006f7154e2a7bd9d4fec73f0540e155e3119\" already present on machine 101m Normal Created pod/machine-api-controllers-f64b7f7b8-xb7qn Created container machine-controller 101m Normal Started pod/machine-api-controllers-f64b7f7b8-xb7qn Started container machine-controller 101m Normal Pulled pod/machine-api-controllers-f64b7f7b8-xb7qn Container image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8f1a9c01710f09ad1d1c105cbc4f4ff88d7c0f5916a628f9414d3a7905fbced8\" already present on machine 101m Normal Created pod/machine-api-controllers-f64b7f7b8-xb7qn Created container nodelink-controller 101m Normal Started pod/machine-api-controllers-f64b7f7b8-xb7qn Started container nodelink-controller 101m Normal SuccessfulCreate replicaset/machine-api-controllers-f64b7f7b8 Created pod: machine-api-controllers-f64b7f7b8-xb7qn 101m Normal ScalingReplicaSet deployment/machine-api-controllers Scaled up replica set machine-api-controllers-f64b7f7b8 to 1 101m Warning FailedScheduling pod/machine-api-operator-655d94c8fd-6vrqg 0/3 nodes are available: 3 node(s) had taints that the pod didn't tolerate. 101m Normal Scheduled pod/machine-api-operator-655d94c8fd-6vrqg Successfully assigned openshift-machine-api/machine-api-operator-655d94c8fd-6vrqg to osc-6gzh2-master-2 101m Normal Pulling pod/machine-api-operator-655d94c8fd-6vrqg Pulling image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8f1a9c01710f09ad1d1c105cbc4f4ff88d7c0f5916a628f9414d3a7905fbced8\" 101m Normal Pulled pod/machine-api-operator-655d94c8fd-6vrqg Successfully pulled image \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:8f1a9c01710f09ad1d1c105cbc4f4ff88d7c0f5916a628f9414d3a7905fbced8\" 101m Normal Created pod/machine-api-operator-655d94c8fd-6vrqg Created container machine-api-operator 101m Normal Started pod/machine-api-operator-655d94c8fd-6vrqg Started container machine-api-operator 102m Normal SuccessfulCreate replicaset/machine-api-operator-655d94c8fd Created pod: machine-api-operator-655d94c8fd-6vrqg 102m Normal ScalingReplicaSet deployment/machine-api-operator Scaled up replica set machine-api-operator-655d94c8fd to 1 6m55s Warning FailedCreate machine/osc-6gzh2-worker-5d627 CreateError 7m29s Warning FailedCreate machine/osc-6gzh2-worker-j6j78 CreateError 98m Warning FailedCreate machine/osc-6gzh2-worker-shw2w CreateError 97m Normal Created machine/osc-6gzh2-worker-shw2w Created Machine osc-6gzh2-worker-shw2w I connected to master 2 via master 1: [stack@undercloud-0 ~]$ eval $(ssh-agent) [stack@undercloud-0 ~]$ ssh-add ~/.ssh/id_rsa [stack@undercloud-0 ~]$ ssh core@api.osc.redhat.local -A [stack@undercloud-0 ~]$ ssh core@api.osc.redhat.local -A Warning: Permanently added 'api.osc.redhat.local,172.16.0.108' (ECDSA) to the list of known hosts. Red Hat Enterprise Linux CoreOS 42.81.20191203.0 WARNING: Direct SSH access to machines is not recommended. --- Last login: Fri Dec 20 17:27:11 2019 from 172.16.0.65 [core@osc-6gzh2-master-1 ~]$ ssh core@osc-6gzh2-master-0 Red Hat Enterprise Linux CoreOS 42.81.20191203.0 WARNING: Direct SSH access to machines is not recommended. --- Last login: Fri Dec 20 17:27:13 2019 from 172.31.0.16 sud[core@osc-6gzh2-master-0 ~]$ sudo -i [root@osc-6gzh2-master-0 ~]# And then got: [root@osc-6gzh2-master-2 ~]# grep osc-6gzh2-worker-5d627 /var/log -R /var/log/pods/openshift-machine-api_machine-api-controllers-f64b7f7b8-xb7qn_4d35a879-2340-11ea-baab-fa163e272495/machine-controller/0.log:2019-12-20T15:49:41.304368389+00:00 stderr F I1220 15:49:41.304339 1 controller.go:133] Reconciling Machine \"osc-6gzh2-worker-5d627\" /var/log/pods/openshift-machine-api_machine-api-controllers-f64b7f7b8-xb7qn_4d35a879-2340-11ea-baab-fa163e272495/machine-controller/0.log:2019-12-20T15:49:41.304368389+00:00 stderr F I1220 15:49:41.304356 1 controller.go:304] Machine \"osc-6gzh2-worker-5d627\" in namespace \"openshift-machine-api\" doesn't specify \"cluster.k8s.io/cluster-name\" label, assuming nil cluster /var/log/pods/openshift-machine-api_machine-api-controllers-f64b7f7b8-xb7qn_4d35a879-2340-11ea-baab-fa163e272495/machine-controller/0.log:2019-12-20T15:50:37.383723384+00:00 stderr F I1220 15:50:37.383397 1 controller.go:133] Reconciling Machine \"osc-6gzh2-worker-5d627\" /var/log/pods/openshift-machine-api_machine-api-controllers-f64b7f7b8-xb7qn_4d35a879-2340-11ea-baab-fa163e272495/machine-controller/0.log:2019-12-20T15:50:37.383835414+00:00 stderr F I1220 15:50:37.383814 1 controller.go:304] Machine \"osc-6gzh2-worker-5d627\" in namespace \"openshift-machine-api\" doesn't specify \"cluster.k8s.io/cluster-name\" label, assuming nil cluster /var/log/pods/openshift-machine-api_machine-api-controllers-f64b7f7b8-xb7qn_4d35a879-2340-11ea-baab-fa163e272495/machine-controller/0.log:2019-12-20T15:50:38.552995792+00:00 stderr F I1220 15:50:38.552954 1 controller.go:253] Reconciling machine object osc-6gzh2-worker-5d627 triggers idempotent create. /var/log/pods/openshift-machine-api_machine-api-controllers-f64b7f7b8-xb7qn_4d35a879-2340-11ea-baab-fa163e272495/machine-controller/0.log:2019-12-20T15:50:52.700244622+00:00 stderr F E1220 15:50:52.698880 1 actuator.go:470] Machine error osc-6gzh2-worker-5d627: error creating Openstack instance: Create new server err: Request forbidden: [POST http://172.16.0.130:8774/v2.1/servers], error message: {\"forbidden\": {\"message\": \"Quota exceeded for ram: Requested 32768, but already used 132096 of 153600 ram\", \"code\": 403}} /var/log/pods/openshift-machine-api_machine-api-controllers-f64b7f7b8-xb7qn_4d35a879-2340-11ea-baab-fa163e272495/machine-controller/0.log:2019-12-20T15:50:52.700244622+00:00 stderr F W1220 15:50:52.698926 1 controller.go:255] Failed to create machine \"osc-6gzh2-worker-5d627\": error creating Openstack instance: Create new server err: Request forbidden: [POST http://172.16.0.130:8774/v2.1/servers], error message: {\"forbidden\": {\"message\": \"Quota exceeded for ram: Requested 32768, but already used 132096 of 153600 ram\", \"code\": 403}} /var/log/pods/openshift-machine-api_machine-api-controllers-f64b7f7b8-xb7qn_4d35a879-2340-11ea-baab-fa163e272495/machine-controller/0.log:2019-12-20T15:51:38.126016932+00:00 stderr F I1220 15:51:38.125931 1 controller.go:133] Reconciling Machine \"osc-6gzh2-worker-5d627\" /var/log/pods/openshift-machine-api_machine-api-controllers-f64b7f7b8-xb7qn_4d35a879-2340-11ea-baab-fa163e272495/machine-controller/0.log:2019-12-20T15:51:38.126016932+00:00 stderr F I1220 15:51:38.125971 1 controller.go:304] Machine \"osc-6gzh2-worker-5d627\" in namespace \"openshift-machine-api\" doesn't specify \"cluster.k8s.io/cluster-name\" label, assuming nil cluster /var/log/pods/openshift-machine-api_machine-api-controllers-f64b7f7b8-xb7qn_4d35a879-2340-11ea-baab-fa163e272495/machine-controller/0.log:2019-12-20T15:51:39.076618133+00:00 stderr F I1220 15:51:39.076540 1 controller.go:253] Reconciling machine object osc-6gzh2-worker-5d627 triggers idempotent create. /var/log/pods/openshift-machine-api_machine-api-controllers-f64b7f7b8-xb7qn_4d35a879-2340-11ea-baab-fa163e272495/machine-controller/0.log:2019-12-20T15:51:48.625264976+00:00 stderr F E1220 15:51:48.625215 1 actuator.go:470] Machine error osc-6gzh2-worker-5d627: error creating Openstack instance: Create new server err: Request forbidden: [POST http://172.16.0.130:8774/v2.1/servers], error message: {\"forbidden\": {\"message\": \"Quota exceeded for ram: Requested 32768, but already used 132096 of 153600 ram\", \"code\": 403}} /var/log/pods/openshift-machine-api_machine-api-controllers-f64b7f7b8-xb7qn_4d35a879-2340-11ea-baab-fa163e272495/machine-controller/0.log:2019-12-20T15:51:48.625264976+00:00 stderr F W1220 15:51:48.625239 1 controller.go:255] Failed to create machine \"osc-6gzh2-worker-5d627\": error creating Openstack instance: Create new server err: Request forbidden: [POST http://172.16.0.130:8774/v2.1/servers], error message: {\"forbidden\": {\"message\": \"Quota exceeded for ram: Requested 32768, but already used 132096 of 153600 ram\", \"code\": 403}}","title":"Troubleshooting OpenShift on OpenStack worker creation"},{"location":"openshift/useful-ocp-curl/","text":"Querying the OCP 4.x upgrades info API Function definitions The following bash functions are useful when needing information about possible upgrade paths and OCP versions: function ocp-upgrade-paths() { version=$1 for channel in stable fast candidate ; do echo \"=== $channel-$version ===\" curl -sH 'Accept: application/json' \"https://api.openshift.com/api/upgrades_info/v1/graph?channel=$channel-$version\" | jq -r '[.nodes[].version] | sort | unique[]' done } function ocp-version-info() { channel=$1 version=$2 major=$(echo $version | awk -F '.' '{print $1 \".\" $2}') minor=$(echo $version | awk -F '.' '{print $NF}') echo \"Checking for $channel and major $major and minor $minor\" url=\"https://api.openshift.com/api/upgrades_info/v1/graph?channel=$channel-$major&x86_64'\" echo $url curl -sH \"Accept:application/json\" $url | jq \".nodes[] | select(.version == \\\"$version\\\")\" } Usage Upgrade paths: $ ocp-upgrade-paths 4.4 === stable-4.4 === 4.3.12 4.3.13 4.3.18 4.3.19 4.3.21 4.3.22 4.3.23 4.3.25 4.3.26 4.3.27 4.3.28 4.3.29 4.3.31 4.3.33 4.3.35 4.4.10 4.4.11 4.4.12 4.4.13 4.4.14 4.4.15 4.4.16 4.4.17 4.4.18 4.4.19 4.4.20 4.4.3 4.4.4 4.4.5 4.4.6 4.4.8 4.4.9 === fast-4.4 === 4.3.12 4.3.13 4.3.18 4.3.19 4.3.21 4.3.22 4.3.23 4.3.25 4.3.26 4.3.27 4.3.28 4.3.29 4.3.31 4.3.33 4.3.35 4.4.10 4.4.11 4.4.12 4.4.13 4.4.14 4.4.15 4.4.16 4.4.17 4.4.18 4.4.19 4.4.20 4.4.3 4.4.4 4.4.5 4.4.6 4.4.8 4.4.9 === candidate-4.4 === 4.3.10 4.3.11 4.3.12 4.3.13 4.3.14 4.3.15 4.3.17 4.3.18 4.3.19 4.3.21 4.3.22 4.3.23 4.3.24 4.3.25 4.3.26 4.3.27 4.3.28 4.3.29 4.3.31 4.3.33 4.3.35 4.3.5 4.3.8 4.3.9 4.4.0 4.4.0-rc.0 4.4.0-rc.1 4.4.0-rc.10 4.4.0-rc.11 4.4.0-rc.12 4.4.0-rc.13 4.4.0-rc.2 4.4.0-rc.4 4.4.0-rc.6 4.4.0-rc.7 4.4.0-rc.8 4.4.0-rc.9 4.4.10 4.4.11 4.4.12 4.4.13 4.4.14 4.4.15 4.4.16 4.4.17 4.4.18 4.4.19 4.4.2 4.4.20 4.4.21 4.4.3 4.4.4 4.4.5 4.4.6 4.4.7 4.4.8 4.4.9 Version info: $ ocp-version-info stable 4.5.7 Checking for stable and major 4.5 and minor 7 https://api.openshift.com/api/upgrades_info/v1/graph?channel=stable-4.5&x86_64' { \"version\": \"4.5.7\", \"payload\": \"quay.io/openshift-release-dev/ocp-release@sha256:776b7e8158edf64c82f18f5ec4d6ef378ac3de81ba0dc2700b885ceb62e71279\", \"metadata\": { \"description\": \"\", \"io.openshift.upgrades.graph.previous.remove_regex\": \"4.4.12\", \"io.openshift.upgrades.graph.release.channels\": \"candidate-4.5,fast-4.5,stable-4.5,candidate-4.6\", \"io.openshift.upgrades.graph.release.manifestref\": \"sha256:776b7e8158edf64c82f18f5ec4d6ef378ac3de81ba0dc2700b885ceb62e71279\", \"url\": \"https://access.redhat.com/errata/RHBA-2020:3436\" } } Upgrading to a specific out of graph image [akaris@linux ~]$ ocp-upgrade-paths 4.5 === stable-4.5 === 4.5.1 4.5.2 4.5.3 4.5.4 === fast-4.5 === 4.4.10 4.4.11 4.4.12 4.4.13 4.4.14 4.4.15 4.4.16 4.5.1 4.5.2 4.5.3 4.5.4 === candidate-4.5 === 4.4.10 4.4.11 4.4.12 4.4.13 4.4.14 4.4.15 4.4.16 4.4.6 4.4.8 4.4.9 4.5.0 4.5.0-rc.1 4.5.0-rc.2 4.5.0-rc.4 4.5.0-rc.5 4.5.0-rc.6 4.5.0-rc.7 4.5.1 4.5.1-rc.0 4.5.2 4.5.3 4.5.4 4.5.5 [akaris@linux ~]$ ocp-version-info stable 4.5.7 Checking for stable and major 4.5 and minor 7 https://api.openshift.com/api/upgrades_info/v1/graph?channel=stable-4.5&x86_64' { \"version\": \"4.5.7\", \"payload\": \"quay.io/openshift-release-dev/ocp-release@sha256:776b7e8158edf64c82f18f5ec4d6ef378ac3de81ba0dc2700b885ceb62e71279\", \"metadata\": { \"description\": \"\", \"io.openshift.upgrades.graph.previous.remove_regex\": \"4.4.12\", \"io.openshift.upgrades.graph.release.channels\": \"candidate-4.5,fast-4.5,stable-4.5,candidate-4.6\", \"io.openshift.upgrades.graph.release.manifestref\": \"sha256:776b7e8158edf64c82f18f5ec4d6ef378ac3de81ba0dc2700b885ceb62e71279\", \"url\": \"https://access.redhat.com/errata/RHBA-2020:3436\" } } [akaris@linux ~]$ ocp-version-info stable 4.5.8 Checking for stable and major 4.5 and minor 8 https://api.openshift.com/api/upgrades_info/v1/graph?channel=stable-4.5&x86_64' [akaris@linux ~]$ ocp-version-info candidate 4.5.8 Checking for candidate and major 4.5 and minor 8 https://api.openshift.com/api/upgrades_info/v1/graph?channel=candidate-4.5&x86_64' { \"version\": \"4.5.8\", \"payload\": \"quay.io/openshift-release-dev/ocp-release@sha256:ae61753ad8c8a26ed67fa233eea578194600d6c72622edab2516879cfbf019fd\", \"metadata\": { \"description\": \"\", \"io.openshift.upgrades.graph.release.channels\": \"candidate-4.5,candidate-4.6\", \"io.openshift.upgrades.graph.release.manifestref\": \"sha256:ae61753ad8c8a26ed67fa233eea578194600d6c72622edab2516879cfbf019fd\", \"url\": \"https://access.redhat.com/errata/RHBA-2020:3510\" } } How to uprgade to an image that's not on the graph (not supported). Look at payload from ocp-version-info and use that image: oc adm upgrade --allow-explicit-upgrade --to-image quay.io/openshift-release-dev/ocp-release@sha256:776b7e8158edf64c82f18f5ec4d6ef378ac3de81ba0dc2700b885ceb62e71279","title":"Querying the OCP 4.x upgrades info API"},{"location":"openshift/useful-ocp-curl/#querying-the-ocp-4x-upgrades-info-api","text":"","title":"Querying the OCP 4.x upgrades info API"},{"location":"openshift/useful-ocp-curl/#function-definitions","text":"The following bash functions are useful when needing information about possible upgrade paths and OCP versions: function ocp-upgrade-paths() { version=$1 for channel in stable fast candidate ; do echo \"=== $channel-$version ===\" curl -sH 'Accept: application/json' \"https://api.openshift.com/api/upgrades_info/v1/graph?channel=$channel-$version\" | jq -r '[.nodes[].version] | sort | unique[]' done } function ocp-version-info() { channel=$1 version=$2 major=$(echo $version | awk -F '.' '{print $1 \".\" $2}') minor=$(echo $version | awk -F '.' '{print $NF}') echo \"Checking for $channel and major $major and minor $minor\" url=\"https://api.openshift.com/api/upgrades_info/v1/graph?channel=$channel-$major&x86_64'\" echo $url curl -sH \"Accept:application/json\" $url | jq \".nodes[] | select(.version == \\\"$version\\\")\" }","title":"Function definitions"},{"location":"openshift/useful-ocp-curl/#usage","text":"Upgrade paths: $ ocp-upgrade-paths 4.4 === stable-4.4 === 4.3.12 4.3.13 4.3.18 4.3.19 4.3.21 4.3.22 4.3.23 4.3.25 4.3.26 4.3.27 4.3.28 4.3.29 4.3.31 4.3.33 4.3.35 4.4.10 4.4.11 4.4.12 4.4.13 4.4.14 4.4.15 4.4.16 4.4.17 4.4.18 4.4.19 4.4.20 4.4.3 4.4.4 4.4.5 4.4.6 4.4.8 4.4.9 === fast-4.4 === 4.3.12 4.3.13 4.3.18 4.3.19 4.3.21 4.3.22 4.3.23 4.3.25 4.3.26 4.3.27 4.3.28 4.3.29 4.3.31 4.3.33 4.3.35 4.4.10 4.4.11 4.4.12 4.4.13 4.4.14 4.4.15 4.4.16 4.4.17 4.4.18 4.4.19 4.4.20 4.4.3 4.4.4 4.4.5 4.4.6 4.4.8 4.4.9 === candidate-4.4 === 4.3.10 4.3.11 4.3.12 4.3.13 4.3.14 4.3.15 4.3.17 4.3.18 4.3.19 4.3.21 4.3.22 4.3.23 4.3.24 4.3.25 4.3.26 4.3.27 4.3.28 4.3.29 4.3.31 4.3.33 4.3.35 4.3.5 4.3.8 4.3.9 4.4.0 4.4.0-rc.0 4.4.0-rc.1 4.4.0-rc.10 4.4.0-rc.11 4.4.0-rc.12 4.4.0-rc.13 4.4.0-rc.2 4.4.0-rc.4 4.4.0-rc.6 4.4.0-rc.7 4.4.0-rc.8 4.4.0-rc.9 4.4.10 4.4.11 4.4.12 4.4.13 4.4.14 4.4.15 4.4.16 4.4.17 4.4.18 4.4.19 4.4.2 4.4.20 4.4.21 4.4.3 4.4.4 4.4.5 4.4.6 4.4.7 4.4.8 4.4.9 Version info: $ ocp-version-info stable 4.5.7 Checking for stable and major 4.5 and minor 7 https://api.openshift.com/api/upgrades_info/v1/graph?channel=stable-4.5&x86_64' { \"version\": \"4.5.7\", \"payload\": \"quay.io/openshift-release-dev/ocp-release@sha256:776b7e8158edf64c82f18f5ec4d6ef378ac3de81ba0dc2700b885ceb62e71279\", \"metadata\": { \"description\": \"\", \"io.openshift.upgrades.graph.previous.remove_regex\": \"4.4.12\", \"io.openshift.upgrades.graph.release.channels\": \"candidate-4.5,fast-4.5,stable-4.5,candidate-4.6\", \"io.openshift.upgrades.graph.release.manifestref\": \"sha256:776b7e8158edf64c82f18f5ec4d6ef378ac3de81ba0dc2700b885ceb62e71279\", \"url\": \"https://access.redhat.com/errata/RHBA-2020:3436\" } }","title":"Usage"},{"location":"openshift/useful-ocp-curl/#upgrading-to-a-specific-out-of-graph-image","text":"[akaris@linux ~]$ ocp-upgrade-paths 4.5 === stable-4.5 === 4.5.1 4.5.2 4.5.3 4.5.4 === fast-4.5 === 4.4.10 4.4.11 4.4.12 4.4.13 4.4.14 4.4.15 4.4.16 4.5.1 4.5.2 4.5.3 4.5.4 === candidate-4.5 === 4.4.10 4.4.11 4.4.12 4.4.13 4.4.14 4.4.15 4.4.16 4.4.6 4.4.8 4.4.9 4.5.0 4.5.0-rc.1 4.5.0-rc.2 4.5.0-rc.4 4.5.0-rc.5 4.5.0-rc.6 4.5.0-rc.7 4.5.1 4.5.1-rc.0 4.5.2 4.5.3 4.5.4 4.5.5 [akaris@linux ~]$ ocp-version-info stable 4.5.7 Checking for stable and major 4.5 and minor 7 https://api.openshift.com/api/upgrades_info/v1/graph?channel=stable-4.5&x86_64' { \"version\": \"4.5.7\", \"payload\": \"quay.io/openshift-release-dev/ocp-release@sha256:776b7e8158edf64c82f18f5ec4d6ef378ac3de81ba0dc2700b885ceb62e71279\", \"metadata\": { \"description\": \"\", \"io.openshift.upgrades.graph.previous.remove_regex\": \"4.4.12\", \"io.openshift.upgrades.graph.release.channels\": \"candidate-4.5,fast-4.5,stable-4.5,candidate-4.6\", \"io.openshift.upgrades.graph.release.manifestref\": \"sha256:776b7e8158edf64c82f18f5ec4d6ef378ac3de81ba0dc2700b885ceb62e71279\", \"url\": \"https://access.redhat.com/errata/RHBA-2020:3436\" } } [akaris@linux ~]$ ocp-version-info stable 4.5.8 Checking for stable and major 4.5 and minor 8 https://api.openshift.com/api/upgrades_info/v1/graph?channel=stable-4.5&x86_64' [akaris@linux ~]$ ocp-version-info candidate 4.5.8 Checking for candidate and major 4.5 and minor 8 https://api.openshift.com/api/upgrades_info/v1/graph?channel=candidate-4.5&x86_64' { \"version\": \"4.5.8\", \"payload\": \"quay.io/openshift-release-dev/ocp-release@sha256:ae61753ad8c8a26ed67fa233eea578194600d6c72622edab2516879cfbf019fd\", \"metadata\": { \"description\": \"\", \"io.openshift.upgrades.graph.release.channels\": \"candidate-4.5,candidate-4.6\", \"io.openshift.upgrades.graph.release.manifestref\": \"sha256:ae61753ad8c8a26ed67fa233eea578194600d6c72622edab2516879cfbf019fd\", \"url\": \"https://access.redhat.com/errata/RHBA-2020:3510\" } } How to uprgade to an image that's not on the graph (not supported). Look at payload from ocp-version-info and use that image: oc adm upgrade --allow-explicit-upgrade --to-image quay.io/openshift-release-dev/ocp-release@sha256:776b7e8158edf64c82f18f5ec4d6ef378ac3de81ba0dc2700b885ceb62e71279","title":"Upgrading to a specific out of graph image"},{"location":"openstack/reattach_to_running_deployment/","text":"Reattaching to a running overcloud deployment How to re-attach to a running openstack overcloud deploy deployment in tripleo / Red Hat OpenStack One may inadvertently cancel ... openstack overcloud deploy (...) ... with CTRL-C. In order to reattach to the live event list, one can use this handy command. openstack stack event list overcloud --follow","title":"Reattach to running deployment"},{"location":"openstack/reattach_to_running_deployment/#reattaching-to-a-running-overcloud-deployment","text":"How to re-attach to a running openstack overcloud deploy deployment in tripleo / Red Hat OpenStack One may inadvertently cancel ... openstack overcloud deploy (...) ... with CTRL-C. In order to reattach to the live event list, one can use this handy command. openstack stack event list overcloud --follow","title":"Reattaching to a running overcloud deployment"},{"location":"openstack/using_clouds_yaml/","text":"clouds.yaml - how to generate and use it How to generate and use clouds.yaml Downloading from horizon The easiest way to get the file is opening horizon and going to Project -> Project -> API Access. Then, click on \"Download OpenStack RC File\" and select to download the \"clouds.yaml\" file. Then, copy this file into the current Director as clouds.yaml . This file will need one minor modification, as the password needs to be added to: clouds: <cloud identifier>: auth: (...) password: VrcxVu7RmZAzpzKUaHmcMv22q Test it with: openstack --os-cloud <cloud identifier> token issue Manually creating the file Use the following script: #!/bin/bash source /home/stack/overcloudrc PROJECT_ID=$(openstack project list | grep $OS_PROJECT_NAME | awk '{print $2}') cat << EOF > clouds.yaml clouds: openstack: auth: auth_url: $OS_AUTH_URL username: \"$OS_USERNAME\" password: \"$OS_PASSWORD\" project_name: \"$OS_PROJECT_NAME\" project_id: \"$PROJECT_ID\" user_domain_name: \"$OS_USER_DOMAIN_NAME\" region_name: \"$OS_REGION_NAME\" interface: \"public\" identity_api_version: $OS_IDENTITY_API_VERSION EOF Further details Look at overcloudrc : [stack@undercloud-0 ~]$ cat overcloudrc # Clear any old environment that may conflict. for key in $( set | awk '{FS=\"=\"} /^OS_/ {print $1}' ); do unset $key ; done export OS_NO_CACHE=True export COMPUTE_API_VERSION=1.1 export OS_USERNAME=admin export no_proxy=,172.16.0.199,192.168.24.12 export OS_USER_DOMAIN_NAME=Default export OS_VOLUME_API_VERSION=3 export OS_CLOUDNAME=overcloud export OS_AUTH_URL=http://172.16.0.199:5000//v3 export NOVA_VERSION=1.1 export OS_IMAGE_API_VERSION=2 export OS_PASSWORD=VrcxVu7RmZAzpzKUaHmcMv22q export OS_PROJECT_DOMAIN_NAME=Default export OS_IDENTITY_API_VERSION=3 export OS_PROJECT_NAME=admin export OS_AUTH_TYPE=password export PYTHONWARNINGS=\"ignore:Certificate has no, ignore:A true SSLContext object is not available\" # Add OS_CLOUDNAME to PS1 if [ -z \"${CLOUDPROMPT_ENABLED:-}\" ]; then export PS1=${PS1:-\"\"} export PS1=\\${OS_CLOUDNAME:+\"(\\$OS_CLOUDNAME)\"}\\ $PS1 export CLOUDPROMPT_ENABLED=1 fi Get the project id with: openstack project list | grep $OS_PROJECT_NAME | awk '{print $2}' Generate clouds.yaml with the above credentials. Adjust this to use the actual configuration and credentials as obtained from overcloudrc: cat <<'EOF' > clouds.yaml clouds: openstack: auth: auth_url: http://172.16.0.199:5000//v3 username: \"admin\" password: VrcxVu7RmZAzpzKUaHmcMv22q project_name: \"admin\" project_id: \"a416f556938f454f849da42faa317cd3\" user_domain_name: \"Default\" region_name: \"regionOne\" interface: \"public\" identity_api_version: 3 EOF Using clouds.yaml With the openstack CLI, use clouds.yaml by providing --os-cloud <cloud identifier from YAML file> : [stack@undercloud-0 ~]$ openstack --os-cloud openstack token issue +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | expires | 2019-10-26T11:18:10+0000 | | id | gAAAAABdtB1i8wnsh-pkN6fGMFD5vX7bcvxWms__01c9FFeDp3U-iG6NB31NJc9QhVxVB7WB9_D5J9gwGX91TIdMEiqmhTuI66Wz8eGkw-jxoiAR81y1UYskPrORlAj4Vl0u2L7bifalN7VnPoWu6ISgDCIhd1vF6BZwtU1NrLpfT0KBqMX83_Q | | project_id | a40944973ca8466cb30faeb669646359 | | user_id | f41f1e1433744957985eef31d5d64309 | +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ [stack@undercloud-0 ~]$ openstack --os-cloud openstack network list +--------------------------------------+----------+---------+ | ID | Name | Subnets | +--------------------------------------+----------+---------+ | a01c429e-9095-4838-8ff9-c14ed0683025 | private1 | | +--------------------------------------+----------+---------+","title":"Using clouds.yaml"},{"location":"openstack/using_clouds_yaml/#cloudsyaml-how-to-generate-and-use-it","text":"","title":"clouds.yaml - how to generate and use it"},{"location":"openstack/using_clouds_yaml/#how-to-generate-and-use-cloudsyaml","text":"","title":"How to generate and use clouds.yaml"},{"location":"openstack/using_clouds_yaml/#downloading-from-horizon","text":"The easiest way to get the file is opening horizon and going to Project -> Project -> API Access. Then, click on \"Download OpenStack RC File\" and select to download the \"clouds.yaml\" file. Then, copy this file into the current Director as clouds.yaml . This file will need one minor modification, as the password needs to be added to: clouds: <cloud identifier>: auth: (...) password: VrcxVu7RmZAzpzKUaHmcMv22q Test it with: openstack --os-cloud <cloud identifier> token issue","title":"Downloading from horizon"},{"location":"openstack/using_clouds_yaml/#manually-creating-the-file","text":"Use the following script: #!/bin/bash source /home/stack/overcloudrc PROJECT_ID=$(openstack project list | grep $OS_PROJECT_NAME | awk '{print $2}') cat << EOF > clouds.yaml clouds: openstack: auth: auth_url: $OS_AUTH_URL username: \"$OS_USERNAME\" password: \"$OS_PASSWORD\" project_name: \"$OS_PROJECT_NAME\" project_id: \"$PROJECT_ID\" user_domain_name: \"$OS_USER_DOMAIN_NAME\" region_name: \"$OS_REGION_NAME\" interface: \"public\" identity_api_version: $OS_IDENTITY_API_VERSION EOF","title":"Manually creating the file"},{"location":"openstack/using_clouds_yaml/#further-details","text":"Look at overcloudrc : [stack@undercloud-0 ~]$ cat overcloudrc # Clear any old environment that may conflict. for key in $( set | awk '{FS=\"=\"} /^OS_/ {print $1}' ); do unset $key ; done export OS_NO_CACHE=True export COMPUTE_API_VERSION=1.1 export OS_USERNAME=admin export no_proxy=,172.16.0.199,192.168.24.12 export OS_USER_DOMAIN_NAME=Default export OS_VOLUME_API_VERSION=3 export OS_CLOUDNAME=overcloud export OS_AUTH_URL=http://172.16.0.199:5000//v3 export NOVA_VERSION=1.1 export OS_IMAGE_API_VERSION=2 export OS_PASSWORD=VrcxVu7RmZAzpzKUaHmcMv22q export OS_PROJECT_DOMAIN_NAME=Default export OS_IDENTITY_API_VERSION=3 export OS_PROJECT_NAME=admin export OS_AUTH_TYPE=password export PYTHONWARNINGS=\"ignore:Certificate has no, ignore:A true SSLContext object is not available\" # Add OS_CLOUDNAME to PS1 if [ -z \"${CLOUDPROMPT_ENABLED:-}\" ]; then export PS1=${PS1:-\"\"} export PS1=\\${OS_CLOUDNAME:+\"(\\$OS_CLOUDNAME)\"}\\ $PS1 export CLOUDPROMPT_ENABLED=1 fi Get the project id with: openstack project list | grep $OS_PROJECT_NAME | awk '{print $2}' Generate clouds.yaml with the above credentials. Adjust this to use the actual configuration and credentials as obtained from overcloudrc: cat <<'EOF' > clouds.yaml clouds: openstack: auth: auth_url: http://172.16.0.199:5000//v3 username: \"admin\" password: VrcxVu7RmZAzpzKUaHmcMv22q project_name: \"admin\" project_id: \"a416f556938f454f849da42faa317cd3\" user_domain_name: \"Default\" region_name: \"regionOne\" interface: \"public\" identity_api_version: 3 EOF","title":"Further details"},{"location":"openstack/using_clouds_yaml/#using-cloudsyaml","text":"With the openstack CLI, use clouds.yaml by providing --os-cloud <cloud identifier from YAML file> : [stack@undercloud-0 ~]$ openstack --os-cloud openstack token issue +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | expires | 2019-10-26T11:18:10+0000 | | id | gAAAAABdtB1i8wnsh-pkN6fGMFD5vX7bcvxWms__01c9FFeDp3U-iG6NB31NJc9QhVxVB7WB9_D5J9gwGX91TIdMEiqmhTuI66Wz8eGkw-jxoiAR81y1UYskPrORlAj4Vl0u2L7bifalN7VnPoWu6ISgDCIhd1vF6BZwtU1NrLpfT0KBqMX83_Q | | project_id | a40944973ca8466cb30faeb669646359 | | user_id | f41f1e1433744957985eef31d5d64309 | +------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ [stack@undercloud-0 ~]$ openstack --os-cloud openstack network list +--------------------------------------+----------+---------+ | ID | Name | Subnets | +--------------------------------------+----------+---------+ | a01c429e-9095-4838-8ff9-c14ed0683025 | private1 | | +--------------------------------------+----------+---------+","title":"Using clouds.yaml"}]}